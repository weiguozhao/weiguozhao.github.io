<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/github-logo.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/github-logo.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="目录  SVD 奇异值分解 [无监督] PCA 主成份分析 [无监督] LDA 线性判别分析 [有监督] PCA与LDA的异同   1. SVD 奇异值分解(Singular Value Decomposition)不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。 1.1 特征值与特征向量特征值和特征向量的定义如下：\[Ax = \lambda">
<meta name="keywords" content="技术,算法">
<meta property="og:type" content="article">
<meta property="og:title" content="SVD&amp;PCA&amp;LDA降维">
<meta property="og:url" content="https://weiguozhao.github.io/2018-05-18-dimreduction.html">
<meta property="og:site_name" content="Weiguo&#39;s Station">
<meta property="og:description" content="目录  SVD 奇异值分解 [无监督] PCA 主成份分析 [无监督] LDA 线性判别分析 [有监督] PCA与LDA的异同   1. SVD 奇异值分解(Singular Value Decomposition)不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。 1.1 特征值与特征向量特征值和特征向量的定义如下：\[Ax = \lambda">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-18-dimreduction/1-1.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-18-dimreduction/1-2.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-18-dimreduction/3-1.jpg">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-18-dimreduction/4-1.jpg">
<meta property="og:updated_time" content="2019-06-23T14:18:55.395Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SVD&amp;PCA&amp;LDA降维">
<meta name="twitter:description" content="目录  SVD 奇异值分解 [无监督] PCA 主成份分析 [无监督] LDA 线性判别分析 [有监督] PCA与LDA的异同   1. SVD 奇异值分解(Singular Value Decomposition)不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。 1.1 特征值与特征向量特征值和特征向量的定义如下：\[Ax = \lambda">
<meta name="twitter:image" content="https://weiguozhao.github.io/posts_res/2018-05-18-dimreduction/1-1.png">






  <link rel="canonical" href="https://weiguozhao.github.io/2018-05-18-dimreduction.html">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>SVD&PCA&LDA降维 | Weiguo's Station</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weiguo's Station</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    
  
  
  
  

  

  <a href="https://github.com/weiguozhao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #222; color: #fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://weiguozhao.github.io/2018-05-18-dimreduction.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="WeiguoZHAO">
      <meta itemprop="description" content="Welcome to my blog~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weiguo's Station">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">SVD&PCA&LDA降维

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-05-18 12:10:00" itemprop="dateCreated datePublished" datetime="2018-05-18T12:10:00+08:00">2018-05-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 22:18:55" itemprop="dateModified" datetime="2019-06-23T22:18:55+08:00">2019-06-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/降维/" itemprop="url" rel="index"><span itemprop="name">降维</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>目录</p>
<ul>
<li>SVD 奇异值分解 [无监督]</li>
<li>PCA 主成份分析 [无监督]</li>
<li>LDA 线性判别分析 [有监督]</li>
<li>PCA与LDA的异同</li>
</ul>
<hr>
<h3 id="1-SVD-奇异值分解-Singular-Value-Decomposition"><a href="#1-SVD-奇异值分解-Singular-Value-Decomposition" class="headerlink" title="1. SVD 奇异值分解(Singular Value Decomposition)"></a>1. SVD 奇异值分解(Singular Value Decomposition)</h3><p>不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。</p>
<h4 id="1-1-特征值与特征向量"><a href="#1-1-特征值与特征向量" class="headerlink" title="1.1 特征值与特征向量"></a>1.1 特征值与特征向量</h4><p>特征值和特征向量的定义如下：<br>\[<br>Ax = \lambda x<br>\]</p>
<p>其中<code>A</code>是一个<code>n x n</code>的矩阵，\(x\)是一个<code>n维向量</code>，则我们\(\lambda\)是矩阵<code>A</code>的一个特征值，而<code>x</code>是矩阵<code>A</code>的特征值\( \lambda \)所对应的特征向量。</p>
<p>根据特征值和特征向量，可以将矩阵<code>A</code>特征分解。如果我们求出了矩阵<code>A</code>的<code>n</code>个特征值\( \lambda_1 \leq \lambda_2 \leq … \leq \lambda_n \)以及这<code>n</code>个特征值所对应的特征向量\( \lbrace w_1, w_2, …, w_n \rbrace \)，那么矩阵<code>A</code>就可以用下式的特征分解表示：<br>\[<br>A = W \Sigma W^{-1}<br>\]</p>
<p>其中<code>W</code>是这<code>n</code>个特征向量所组成的<code>n x n</code>维矩阵，而\( \Sigma \)为这<code>n</code>个特征值为主对角线的<code>n x n</code>维矩阵。</p>
<p>一般我们会把<code>W</code>的这<code>n</code>个特征向量标准化，即满足 \( || w_i ||_2=1 \), 或者说 \( w^T_i w_i=1 \)，此时<code>W</code>的<code>n</code>个特征向量为标准正交基，满足\( W^T W=I \)，即\( W^T=W^{−1}\), 也就是说<code>W</code>为酉矩阵。</p>
<p>这样我们的特征分解表达式可以写成<br>\[<br>A=WΣW^T<br>\]</p>
<p>注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。</p>
<h4 id="1-2-SVD的定义"><a href="#1-2-SVD的定义" class="headerlink" title="1.2 SVD的定义"></a>1.2 SVD的定义</h4><p>SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\(A\)是一个<code>m × n</code>的矩阵，那么我们定义矩阵\(A\)的SVD为：<br>\[<br>A=UΣV^T<br>\]</p>
<p>其中\(U\)是一个<code>m × m</code>的矩阵，\(\Sigma\)是一个<code>m × n</code>的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\(V\)是一个<code>n × n</code>的矩阵。\(U\)和\(V\)都是酉矩阵，即满足\( U^TU=I,V^TV=I\)。下图可以很形象的看出上面SVD的定义：</p>
<p><img src="/posts_res/2018-05-18-dimreduction/1-1.png" alt="svd"></p>
<p>如何求出SVD分解后的\( U, \Sigma, V \) 这三个矩阵呢？</p>
<p>如果将\(A^T\)和\(A\)做矩阵乘法，那么会得到<code>n × n</code>的一个方阵\(A^TA\)。既然\(A^TA\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：<br>\[<br>(A^TA)v_i=\lambda_i v_i
\]</p>
<p>这样就可以得到矩阵\(A^TA\)的<code>n</code>个特征值和对应的<code>n</code>个特征向量\(v\)了。将\(A^TA\)的所有特征向量张成一个<code>n × n</code>的矩阵\(V\)，就是SVD公式里面的\(V\)矩阵了。一般将\(V\)中的每个特征向量叫做\(A\)的右奇异向量。</p>
<p>如果将\(A\)和\(A^T\)做矩阵乘法，那么会得到<code>m × m</code>的一个方阵\(AA^T\)。既然\(AA^T\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：<br>\[<br>(AA^T)u_i=\lambda_iu_i
\]</p>
<p>这样就可以得到矩阵\(AA^T\)的<code>m</code>个特征值和对应的<code>m</code>个特征向量\(u\)了。将\(AA^T\)的所有特征向量张成一个<code>m × m</code>的矩阵\(U\)，就是SVD公式里面的\(U\)矩阵了。一般我们将\(U\)中的每个特征向量叫做\(A\)的左奇异向量。</p>
<p>\(U\)和\(V\)都求出来了，现在就剩下奇异值矩阵\( \Sigma \)没有求出了。由于\( \Sigma \)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\( \sigma \)就可以了。</p>
<p>注意到:<br>\[<br>A=UΣV^T \Rightarrow AV=UΣV^TV \Rightarrow AV=UΣ \Rightarrow Av_i=\sigma_iu_i \Rightarrow \sigma_i= \frac{Av_i}{u_i}<br>\]</p>
<p>这样可以求出每个奇异值，进而求出奇异值矩阵\( \Sigma \)。</p>
<p>上面还有一个问题没有讲，就是说\(A^TA\)的特征向量组成的就是我们SVD中的\(V\)矩阵，而\(AA^T\)的特征向量组成的就是我们SVD中的\(U\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\(V\)矩阵的证明为例。<br>\[<br>A=UΣV^T \Rightarrow A^T=VΣ^TU^T \Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T<br>\]</p>
<p>上式证明使用了:\( U^TU=I,Σ^TΣ=Σ^2\)，可以看出\( A^TA \)的特征向量组成的的确就是我们SVD中的\(V\)矩阵。类似的方法可以得到\(AA^T\)的特征向量组成的就是我们SVD中的\(U\)矩阵。</p>
<p>进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：<br>\[<br>\sigma_i=\sqrt{\lambda_i}<br>\]</p>
<p>这样也就是说，可以不用\( \sigma_i= Av_i / u_i \)来计算奇异值，也可以通过求出\( A^TA \)的特征值取平方根来求奇异值。</p>
<h4 id="1-3-SVD的性质"><a href="#1-3-SVD的性质" class="headerlink" title="1.3 SVD的性质"></a>1.3 SVD的性质</h4><p>对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：<br>\[<br>A_{m×n}=U_{m×m} \Sigma_{m×n} V^T_{n×n} \sim U_{m×k} \Sigma_{k×k} V^T_{k×n}<br>\]</p>
<p>其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\( U_{m×k}, \Sigma_{k×k}, V^T_{k×n} \)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。</p>
<p><img src="/posts_res/2018-05-18-dimreduction/1-2.png" alt="property"></p>
<p>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。</p>
<blockquote>
</blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">奇异值分解(SVD)原理与在降维中的应用</a></p>
<hr>
<h3 id="2-PCA-主成份分析"><a href="#2-PCA-主成份分析" class="headerlink" title="2. PCA 主成份分析"></a>2. PCA 主成份分析</h3><p><strong>事先声明</strong></p>
<p>当 <code>X = 样本数量 * 每个样本的维度</code> 时， 协方差矩阵为$ X^T X $；<br>当 <code>X = 每个样本的维度 * 样本数量</code> 时， 协方差矩阵为$ X X^T $；<br><br></p>
<h4 id="2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近"><a href="#2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近" class="headerlink" title="2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)"></a>2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)</h4><p>假设 m 个 n 维数据\( (x^{(1)}, x^{(2)},…,x^{(m)}) \)都已经进行了中心化，即 \( \sum_{i=1}^m x^{(i)}=0 \)。经过投影变换后得到的新坐标系为\( \lbrace w_1,w_2,…,w_n \rbrace \)，其中\(w\)是标准正交基，即\( || w ||^2 = 1, w^T_iw_j=0 \)。</p>
<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\( \lbrace w_1, w_2, …, w_{n′} \rbrace \)，样本点\( x^{(i)} \)在 n’ 维坐标系中的投影为：\( z^{(i)} = ( z^{(i)}_1,z^{(i)}_2, …, z^{(i)}_{n′}) \)。其中，\( z^{(i)}_j = w^T_j x^{(i)} \)是\( x^{(i)} \)在低维坐标系里第 j 维的坐标。</p>
<p>如果用\( z^{(i)} \)来恢复原始数据\( x^{(i)} \)，则得到的恢复数据 \( \bar{x}^{(i)} = \sum_{j=1}^{n′} z^{(i)}_j w_j = Wz^{(i)} \)，其中，\(W\)为标准正交基组成的矩阵。</p>
<p>现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式：<br>\[<br>\sum_{i=1}^m || \bar{x}^{(i)} − x^{(i)} ||^2_2
\]</p>
<p>将这个式子进行整理，可以得到:<br>\[<br>\begin{equation}<br>\begin{aligned}<br>\sum_{i=1}^m || \bar{x}^{(i)} − x^{(i)} ||^2_2 
&amp; = \sum_{i=1}^m || Wz^{(i)} − x^{(i)} ||^2_2 \<br>&amp; = \sum_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \sum_{i=1}^m (Wz^{(i)})^T x^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \<br>&amp; = \sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \sum_{i=1}^m z^{(i)T}W^T x^{(i)} + \sum_{i=1}^m x^{(i)T} x^{(i)} \<br>&amp; = \sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \sum_{i=1}^m z^{(i)T}z^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \<br>&amp; = − \sum_{i=1}^m z^{(i)T}z^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \<br>&amp; = − tr( W^T ( \sum_{i=1}^m x^{(i)} x^{(i)T} ) W) + \sum_{i=1}^m x^{(i)T} x^{(i)} \<br>&amp; = − tr( W^T X X^T W) + \sum_{i=1}^m x^{(i)T} x^{(i)}<br>\end{aligned}<br>\end{equation}<br>\]</p>
<ul>
<li>第1个等式用到了\( \bar{x}^{(i)} = W z^{(i)} \)</li>
<li>第2个等式用到了平方和展开</li>
<li>第3个等式用到了矩阵转置公式\( (AB)^T= B^T A^T \) 和 \( W^T W =I \)</li>
<li>第4个等式用到了\( z^{(i)} = W^T x^{(i)} \)</li>
<li>第5个等式合并同类项</li>
<li>第6个等式用到了\( z^{(i)} = W^T x^{(i)} \)和矩阵的迹</li>
<li>第7个等式将代数和表达为矩阵形式</li>
</ul>
<p>注意到\( \sum_{i=1}^m x^{(i)} x^{(i)T} \)是数据集的协方差矩阵，\(W\)的每一个向量\(w_j\)是标准正交基。而\( \sum_{i=1}^m x^{(i)T} x^{(i)} \) 是一个常量。最小化上式等价于：<br>\[<br>\arg\min_W - tr( W^T X X^T W ) \quad \quad s.t. W^TW = I<br>\]</p>
<p>这个最小化不难，直接观察也可以发现最小值对应的\(W\)由协方差矩阵\(XX^T\)最大的 n’ 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到<br>\[<br>J(W) = −tr( W^T X X^T W) + \lambda ( W^T W − I )<br>\]</p>
<p>对\( W \)求导有\( − X X^T W + \lambda W = 0 \)，整理下即为：<br>\[<br>XX^T W = \lambda W<br>\]</p>
<p>这样可以更清楚的看出，\(W\)为\(X X^T \)的 n’ 个特征向量组成的矩阵，而\( \lambda \) 为 \( XX^T \) 的特征值。当我们将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\( W\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\( z^{(i)} = W^T x^{(i)} \)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。
　</p>
<h4 id="2-2-PCA的推导-基于最大投影方差"><a href="#2-2-PCA的推导-基于最大投影方差" class="headerlink" title="2.2 PCA的推导:基于最大投影方差"></a>2.2 PCA的推导:基于最大投影方差</h4><p>假设 m 个 n 维数据\( (x^{(1)}, x^{(2)}, …, x^{(m)} ) \)都已经进行了中心化，即\( \sum_{i=1}^m x^{(i)} = 0 \)。经过投影变换后得到的新坐标系为\( \lbrace w_1, w_2, …, w_n \rbrace \)，其中\( w\) 是标准正交基，即\( || w ||^2 = 1, w^T_i w_j = 0\)。</p>
<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\( \lbrace w_1, w_2, …, w_{n′} \rbrace \)，样本点\( x^{(i)} \) 在 n’ 维坐标系中的投影为：\( z^{(i)} = ( z^{(i)}_1, z^{(i)}_2, …, z^{(i)}_{n′}) \) 。其中，\( z^{(i)}_j = w^T_j x^{(i)} \)是 \( x^{(i)} \) 在低维坐标系里第 j 维的坐标。</p>
<p>对于任意一个样本\( x^{(i)} \)，在新的坐标系中的投影为\( W^T x^{(i)} \)，在新坐标系中的投影方差为\( W^T x^{(i)} x^{(i)T}W \)，要使所有的样本的投影方差和最大，也就是最大化\( \sum_{i=1}^m W^T x^{(i)} x^{(i)T} W \)，即：<br>\[<br>\arg\max_W tr(W^T X X^T W) \quad \quad s.t. W^TW=I<br>\]</p>
<p>观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。</p>
<p>利用拉格朗日函数可以得到<br>\[<br>J(W) = tr( W^T X X^T W) + \lambda (W^T W − I)<br>\]</p>
<p>对\( W \) 求导有 \( XX^T W + \lambda W = 0 \)，整理下即为：<br>\[<br>XX^TW = (−\lambda)W<br>\]</p>
<p>和上面一样可以看出，\(W\)为\(XX^T\)的 n’ 个特征向量组成的矩阵，而 \( −\lambda \) 为\( XX^T\) 的特征值。将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\( W \)即为需要的矩阵。对于原始数据集，只需要用\( z^{(i)} = W^T x^{(i)} \)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。</p>
<h4 id="2-3-PCA算法流程"><a href="#2-3-PCA算法流程" class="headerlink" title="2.3 PCA算法流程"></a>2.3 PCA算法流程</h4><p>输入: n 维样本集 \( D = ( x^{(1)}, x^{(2)}, …, x^{(m)} ) \)，要降维到的维数 n’。</p>
<p>输出: 降维后的样本集 \( D′ \)</p>
<ol>
<li>对所有的样本进行中心化： \( x^{(i)} = x^{(i)} − \frac{1}{m} \sum_{j=1}^m x^{(j)}  \)</li>
<li>计算样本的协方差矩阵\( XX^T \)</li>
<li>对矩阵\( XX^T \)进行特征值分解</li>
<li>取出最大的 n’ 个特征值对应的特征向量\( (w_1, w_2, …, w_{n′}) \)，将所有的特征向量标准化后，组成特征向量矩阵\(W\)</li>
<li>对样本集中的每一个样本\( x^{(i)}\)，转化为新的样本\( z^{(i)} = W^T x^{(i)} \)</li>
<li>得到输出样本集 \( D′=(z^{(1)}, z^{(2)}, …, z^{(m)} ) \)</li>
</ol>
<p>有时候，我们不指定降维后的 n’ 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \( \lambda_1 \geq \lambda_2 \geq … \geq \lambda_n \)，则 n’ 可以通过下式得到:<br>\[<br>\sum_{i=1}^{n′} \lambda_i / \sum_{i=1}^n \lambda_i \geq t<br>\]</p>
<h4 id="2-4-PCA实例"><a href="#2-4-PCA实例" class="headerlink" title="2.4 PCA实例"></a>2.4 PCA实例</h4><p>假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(2.5, 2.4), (0.5, 0.7), (2.2, 2.9), (1.9, 2.2), (3.1, 3.0), </span><br><span class="line">(2.3, 2.7), (2  , 1.6), (1  , 1.1), (1.5, 1.6), (1.1, 0.9)</span><br></pre></td></tr></table></figure>

<p>　
首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(0.69, 0.49), (-1.31, -1.21), ( 0.39,  0.99), ( 0.09,  0.29), ( 1.29,  1.09), </span><br><span class="line">(0.49, 0.79), ( 0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)</span><br></pre></td></tr></table></figure>

<p>现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为：<br>\[<br>XX^T =<br>\left(<br>\begin{matrix}<br>cov(x1,x1) &amp; cov(x2,x1) \\ cov(x1,x2) &amp; cov(x2,x2)<br>\end{matrix}<br>\right)<br>\]</p>
<p>对于上述数据，求出协方差矩阵为：<br>\[<br>XX^T =<br>\left(<br>\begin{matrix}<br>0.616555556 &amp; 0.615444444 \\ 0.615444444 &amp; 0.716555556<br>\end{matrix}<br>\right)<br>\]</p>
<p>求出特征值为 \(（0.0490834, 1.28402771）\)，对应的特征向量分别为：\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \)，<br>由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \( (0.6778734, -0.73517866)^T \)。<br>则 \( W=(0.6778734, -0.73517866)^T \)。</p>
<p>对所有的数据集进行投影\( z^{(i)} = W^T x^{(i)} \)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 </span><br><span class="line">-0.24863317  0.35670133  0.04641726  0.01776463  0.26124033</span><br></pre></td></tr></table></figure>

<blockquote>
<p>下面是例子的Python代码</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">originalData = np.array([</span><br><span class="line">	[ <span class="number">2.5</span>,  <span class="number">0.5</span>,  <span class="number">2.2</span>,  <span class="number">1.9</span>,  <span class="number">3.1</span>,  <span class="number">2.3</span>,  <span class="number">2.</span> ,  <span class="number">1.</span> ,  <span class="number">1.5</span>,  <span class="number">1.1</span>,],</span><br><span class="line">	[ <span class="number">2.4</span>,  <span class="number">0.7</span>,  <span class="number">2.9</span>,  <span class="number">2.2</span>,  <span class="number">3.</span> ,  <span class="number">2.7</span>,  <span class="number">1.6</span>,  <span class="number">1.1</span>,  <span class="number">1.6</span>,  <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line">mean = np.mean(originalData, axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"均值："</span>)</span><br><span class="line">print(mean) <span class="comment"># [ 1.81  1.91]</span></span><br><span class="line"></span><br><span class="line">preData = (originalData.T - mean).T</span><br><span class="line">print(<span class="string">"中心化后数据："</span>)</span><br><span class="line">print(preData)</span><br><span class="line"><span class="comment"># [[ 0.69 -1.31  0.39  0.09  1.29  0.49  0.19 -0.81 -0.31 -0.71]</span></span><br><span class="line"><span class="comment">#  [ 0.49 -1.21  0.99  0.29  1.09  0.79 -0.31 -0.81 -0.31 -1.01]]</span></span><br><span class="line"></span><br><span class="line">cov = np.cov(preData)</span><br><span class="line">print(<span class="string">"协方差矩阵："</span>)</span><br><span class="line">print(cov)</span><br><span class="line"><span class="comment"># [[ 0.61655556  0.61544444]</span></span><br><span class="line"><span class="comment">#  [ 0.61544444  0.71655556]]</span></span><br><span class="line"></span><br><span class="line">feature_value, feature_vector = np.linalg.eig(cov)</span><br><span class="line">print(<span class="string">"特征值："</span>)</span><br><span class="line">print(feature_value) <span class="comment"># [ 0.0490834   1.28402771]</span></span><br><span class="line">print(<span class="string">"特征向量:"</span>)</span><br><span class="line">print(feature_vector)</span><br><span class="line"><span class="comment"># [[-0.73517866 -0.6778734 ]</span></span><br><span class="line"><span class="comment">#  [ 0.6778734  -0.73517866]]</span></span><br><span class="line"></span><br><span class="line">maxIndex = <span class="number">1</span></span><br><span class="line">W = feature_vector[maxIndex, :]</span><br><span class="line">redData = W.T.dot(preData)</span><br><span class="line">print(<span class="string">"降维后数据："</span>)</span><br><span class="line">print(redData)</span><br><span class="line"><span class="comment"># [ 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 -0.24863317</span></span><br><span class="line"><span class="comment">#   0.35670133  0.04641726  0.01776463  0.26124033]</span></span><br></pre></td></tr></table></figure>

<h4 id="2-5-PCA总结"><a href="#2-5-PCA总结" class="headerlink" title="2.5 PCA总结"></a>2.5 PCA总结</h4><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p>
<p>PCA算法的主要优点有：</p>
<ul>
<li>仅仅需要以方差衡量信息量，不受数据集以外的因素影响</li>
<li>各主成分之间正交，可消除原始数据成分间的相互影响的因素</li>
<li>计算方法简单，主要运算是特征值分解，易于实现</li>
</ul>
<p>PCA算法的主要缺点有：</p>
<ul>
<li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强</li>
<li>方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响</li>
</ul>
<h4 id="2-6-PCA实现方面"><a href="#2-6-PCA实现方面" class="headerlink" title="2.6 PCA实现方面"></a>2.6 PCA实现方面</h4><p>PCA降维需要找到样本协方差矩阵\( X^TX \)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。<br>可以看出，在这个过程中需要先求出协方差矩阵\( X^TX\)，当样本数多样本特征数也多的时候，这个计算量是很大的。</p>
<p>注意到SVD也可以得到协方差矩阵\( X^TX \)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\(X^TX\)，也能求出右奇异矩阵\( V \)。<br>也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。<br>实际上，<a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank" rel="noopener">scikit-learn</a>的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。</p>
<p><em>另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？</em></p>
<p>假设样本是 m×n 的矩阵 \(X\)，如果通过SVD找到了矩阵\( XX^T \)最大的 d 个特征向量张成的 m×d 维矩阵\(U\)，则如果进行如下处理：<br>\[<br>X′_{d×n} = U^T_{d×m} X_{m×n}<br>\]</p>
<p>可以得到一个 d×n 的矩阵\( X’ \)，这个矩阵和原来的 m×n 维样本矩阵\( X \)相比，行数从 m 减到了 k ，可见对行数进行了压缩。<br>也就是说，<strong>左奇异矩阵可以用于行数的压缩</strong>；相对的，<strong>右奇异矩阵可以用于列数即特征维度的压缩</strong>，也就是PCA降维。</p>
<blockquote>
</blockquote>
<p><a href="http://www.cnblogs.com/pinard/p/6239403.html" target="_blank" rel="noopener">主成分分析（PCA）原理总结</a></p>
<hr>
<h3 id="3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant"><a href="#3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant" class="headerlink" title="3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)"></a>3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)</h3><p>LDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。</p>
<p>LDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。<br>我们的目标就是找到一条直线，尽可能满足上面的要求。</p>
<p>来看一个例子：两类会这样被降维</p>
<p><img src="/posts_res/2018-05-18-dimreduction/3-1.jpg" alt="lda"></p>
<p>设数据集\( D=\lbrace (x_i, y_i) \rbrace^m_{i=1} \)，投影向量为\(w\)，则点\(x_i\)经过投影后为\( y=w^Tx_i\)，投影前的样本中心点为\( u \)，投影后的中心点为\( \bar{u} = w^T u\)。</p>
<p>希望投影后不同类别的样本尽量离得较远：使用度量值<br>\[<br>|| \hat{u}_0 - \hat{u}_1 ||^2_2
\]</p>
<p>同时希望投影后相同类别的样本之间尽量离得较近：使用度量值<br>\[<br>\sum_{y \in Y_i} (y - \hat{u}_i)^2<br>\]</p>
<p>这个值其实就是投影后样本的<strong>方差</strong>乘以此类样本集合中样本的数量。</p>
<p>所以总的优化目标函数为：<br>\[<br>J(W) = \frac{|| \hat{u}_0 - \hat{u}_1 ||^2_2}{\sum_{y\in Y_i} ( y - \hat{u}_i )^2} = \frac{|| \hat{u}_0 - \hat{u}_1 ||^2_2}{\sum_0 ( y - \hat{u}_0 )^2 + \sum_1 (y - \hat{u}_1)^2}<br>\]</p>
<p>目标\(J(W)\)当然是越大越好；</p>
<p>定义类内散度矩阵为: \( S_w = \sum_0 + \sum_1 = \sum_{X_0} (x-u_0)(x-u_0)^T + \sum_{X_1} (x-u_1)(x-u_1)^T \)</p>
<p>定义类间散度矩阵：\( S_b = (u_0 - u_1)(u_0 - u_1)^T \)</p>
<p>分子：\( || \hat{u}_0 - \hat{u}_1 ||^2_2 = w^T (u_0 - u_1)(u_0 - u_1)^T w = w^TS_bw \)</p>
<p>分母：\( \sum_0 (y-\hat{u}_0)^2 + \sum_1 (y-\hat{u}_1)^2 = w^T S_w w \)</p>
<p>所以\( J(w)= w^TS_bw / w^T S_w w \)，因为向量\( w \)的长度成比例改变不影响\( J(W) \)的取值，所以我们令\( w^TS_ww=1 \)，那么原优化目标就变为<br>\[<br>\min_{w} J(W) = - w^T S_b w, \quad \quad s.t. \quad w^TS_ww = 1<br>\]</p>
<p>这里直接使用拉格朗日乘子法就可以了，解得<br>\[<br>S_b w = \lambda w S_w
\]</p>
<p>因为\( S_bw = (u_0-u_1)(u_0-u_1)^Tw = (u_0 - u_1) \lambda_t \)，<br>所以\( (u_0 - u_1)\lambda_t = \lambda w S_w \)，可以得到<br>\[<br>w = S_w^{-1} (u_0 - u_1)<br>\]</p>
<p><strong>LDA局限性：</strong></p>
<ol>
<li>当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出</li>
<li>LDA不适合对非高斯分布的样本进行降维</li>
<li>LDA在样本分类信息依赖方差而不是均值时，效果不好</li>
<li>LDA可能过度拟合数据</li>
</ol>
<h3 id="4-PCA与LDA的异同"><a href="#4-PCA与LDA的异同" class="headerlink" title="4. PCA与LDA的异同"></a>4. PCA与LDA的异同</h3><ul>
<li><p><strong>出发思想不同。</strong>PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。</p>
</li>
<li><p><strong>学习模式不同。</strong>PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。</p>
</li>
<li><p><strong>降维后可用维度数量不同。</strong>LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。</p>
</li>
<li><p><strong>投影的坐标系不一定相同。</strong>PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。</p>
</li>
</ul>
<p><img src="/posts_res/2018-05-18-dimreduction/4-1.jpg" alt="pca&amp;lda"></p>
<p>上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。</p>
<blockquote>
</blockquote>
<p><a href="https://blog.csdn.net/dongyanwen6036/article/details/78311071" target="_blank" rel="noopener">LDA与PCA都是常用的降维方法，二者的区别</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/技术/" rel="tag"># 技术</a>
          
            <a href="/tags/算法/" rel="tag"># 算法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018-05-10-LongestPalindromicSubstring.html" rel="next" title="最长回文子串">
                <i class="fa fa-chevron-left"></i> 最长回文子串
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018-05-20-gradientdescent.html" rel="prev" title="梯度下降">
                梯度下降 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MTI1Mi8xNzgwMA"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="WeiguoZHAO">
            
              <p class="site-author-name" itemprop="name">WeiguoZHAO</p>
              <p class="site-description motion-element" itemprop="description">Welcome to my blog~</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">56</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/weiguozhao" title="GitHub &rarr; https://github.com/weiguozhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://weibo.com/ViGorZHAO" title="Weibo &rarr; https://weibo.com/ViGorZHAO" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-SVD-奇异值分解-Singular-Value-Decomposition"><span class="nav-number">1.</span> <span class="nav-text">1. SVD 奇异值分解(Singular Value Decomposition)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-特征值与特征向量"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 特征值与特征向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-SVD的定义"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 SVD的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-SVD的性质"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 SVD的性质</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-PCA-主成份分析"><span class="nav-number">2.</span> <span class="nav-text">2. PCA 主成份分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-PCA的推导-基于最大投影方差"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 PCA的推导:基于最大投影方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-PCA算法流程"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 PCA算法流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-PCA实例"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 PCA实例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-PCA总结"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 PCA总结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-PCA实现方面"><span class="nav-number">2.6.</span> <span class="nav-text">2.6 PCA实现方面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant"><span class="nav-number">3.</span> <span class="nav-text">3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-PCA与LDA的异同"><span class="nav-number">4.</span> <span class="nav-text">4. PCA与LDA的异同</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WeiguoZHAO</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.1"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.1"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.1"></script>


  
  


  
    <script>
  window.livereOptions = {
    refer: '2018-05-18-dimreduction.html'
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow-x: scroll;
  overflow-y: hidden;
}
</style>

    
  


  
  

  
  

  


  

  

  

  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      
        background-color: #eee;
        background-image: linear-gradient(#fcfcfc, #eee);
        border: 1px solid #d5d5d5;
        border-radius: 3px;
      
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      
        right: 4px;
        top: 8px;
      
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1;
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function(i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap');
      $(e).after($wrap);
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
        var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
          return $(e).text();
        }).toArray().join('\n');
        var ta = document.createElement('textarea');
        var yPosition = window.pageYOffset || document.documentElement.scrollTop;
        ta.style.top = yPosition + 'px'; // Prevent page scroll
        ta.style.position = 'absolute';
        ta.style.opacity = '0';
        ta.readOnly = true;
        ta.value = code;
        document.body.appendChild(ta);
        ta.select();
        ta.setSelectionRange(0, code.length);
        ta.readOnly = false;
        var result = document.execCommand('copy');
        
          if (result) $(this).text('复制成功');
          else $(this).text('复制失败');
        
        ta.blur(); // For iOS
        $(this).blur();
      })).on('mouseleave', function(e) {
        var $b = $(this).find('.copy-btn');
        setTimeout(function() {
          $b.text('复制');
        }, 300);
      }).append(e);
    })
  </script>


</body>
</html>
