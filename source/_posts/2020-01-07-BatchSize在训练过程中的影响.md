---
title: BatchSize在训练过程中的影响
tags:
  - Tricks
mathjax: true
comments: false
copyright: false
date: 2020-01-07 20:36:33
categories: 深度学习
---


先看两个问题：
1. 深度学习中batch size的大小对训练过程的影响是什么样的？
2. 有些时候不可避免地要用超大batch，比如人脸识别，可能每个batch要有几万甚至几十万张人脸图像，训练过程中超大batch有什么优缺点，如何尽可能地避免超大batch带来的负面影响？

-------------------

**不考虑Batch Normalization**


### 1. 面试版回答

- Solution1: 

  - 不考虑bn的情况下，`batch size`的大小决定了深度学习训练过程中的完成每个`epoch`所需的时间和每次迭代`(iteration)`之间梯度的平滑程度。`batch size`只能说影响完成每个`epoch`所需要的时间，决定也算不上吧。根本原因还是CPU，GPU算力吧。瓶颈如果在CPU，例如随机数据增强，`batch size`越大有时候计算的越慢。

  - 对于一个大小为`N`的训练集，如果每个epoch中mini-batch的采样方法采用最常规的N个样本每个都采样一次，设mini-batch大小为`b`，那么每个epoch所需的迭代次数(正向+反向)为 `N/b`, 因此完成每个epoch所需的时间大致也随着迭代次数的增加而增加。

  - 由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以`b`的大小决定了相邻迭代之间的梯度平滑程度，`b`太小，相邻mini-batch间的差异相对过大，那么相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；`b`越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来。

  - **总结下来：**
    - batch size过小，花费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。


- Solution2(个人猜想，未证实)

  - 如果硬件资源允许，想要追求训练速度使用超大batch，可以采用一次正向+多次反向的方法，避免模型陷入局部最小值。即使用超大epoch做正向传播，在反向传播的时候，分批次做多次反向转播，比如将一个`batch size`为64的batch，一次正向传播得到结果，instance级别求loss（先不平均），得到64个loss结果；反向传播的过程中，分四次进行反向传播，每次取16个instance的loss求平均，然后进行反向传播，这样可以做到在节约一定的训练时间，利用起硬件资源的优势的情况下，避免模型训练陷入局部最小值。


### 2. 通俗版回答

把第一个问题简化为一个小时候经常玩的游戏：
  - 深度学习训练过程：贴鼻子
  - 训练样本：负责指挥的小朋友们(观察角度各不一样)
  - 模型：负责贴的小朋友
  - 模型衡量指标：最终贴的位置和真实位置之间的距离大小

由于每个小朋友站的位置各不一样，所以他们对鼻子位置的观察也各不一样(训练样本的差异性)。这时候假设小明是负责贴鼻子的小朋友，小朋友A、B、C、D、E是负责指挥的同学(A, B站在图的右边，C，D，E站在左边)，这时候小明如果采用：

1. 每次随机询问一个同学，那么很容易出现，先询问到了A，A说向左2cm，再问C，C说向右5cm，然后B，B说向左4cm，D说向右3cm，这样每次指挥的差异都比较大，结果调过来调过去，没什么进步。
2. 每次随机询问两个同学，每次取询问的意见的平均，比如先问到了(A, C)，A说向左2cm，C说向右5cm，那就取个均值，向右1.5cm。然后再问（B, D），这样的话减少了极端情况（前后两次迭代差异巨大）这种情况的发生，能更好更快的完成游戏。
3. 每次全问一遍，然后取均值，这样每次移动的方向都是所有人决定的均值，这样的话，最后就是哪边的小朋友多最终结果就被很快的拉向哪边了。(梯度方向不变，限于极小值)


### 3. 科学版回答

**使用MINST进行实验。详细实验代码、流程、结果等见参考**

> 由于现在绝大多数的框架在进行mini-batch的反向传播的时候，默认都是将batch中每个instance的loss平均化之后在进行反向传播，所以相对大一点的batch size能够防止loss震荡的情况发生。从这两张图中可以看出batch size越小，相邻iter之间的loss震荡就越厉害，相应的，反传回去的梯度的变化也就越大，也就越不利于收敛。同时很有意思的一个现象，batch size为1的时候，loss到后期会发生爆炸，这主要是lr=0.02设置太大，所以某个异常值的出现会严重扰动到训练过程。这也是为什么对于较小的batchsize，要设置小lr的原因之一，避免异常值对结果造成的扰巨大扰动。而对于较大的batchsize，要设置大一点的lr的原因则是大batch每次迭代的梯度方向相对固定，大lr可以加速其收敛过程。

----------------

> [深度学习 | Batch Size大小对训练过程的影响](https://zhuanlan.zhihu.com/p/86529347)

