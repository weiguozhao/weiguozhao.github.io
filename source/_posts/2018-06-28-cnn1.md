---
layout: post
title: 卷积神经网络CNN初识
date: 2018-06-28 12:10 +0800
categories: 深度学习
tags:
- 模型算法
mathjax: true
copyright: true
---

目录

* 1.引入
* 2.卷积神经网络的结构
* 3.卷积神经网络的正式表达
* 4.理解卷积


译自：[Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)，
[Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)

----------

### 1.引入

在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。

简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 
但是因为其具有的重复性，我们需要学习的参数相对来说非常少。

![1-1](/posts_res/2018-06-28-cnn1/1-1.png)

这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。
这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数，
并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。


-------

### 2.卷积神经网络的结构

假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。
假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。

![2-1](/posts_res/2018-06-28-cnn1/2-1.png)

引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出）

![2-2](/posts_res/2018-06-28-cnn1/2-2.png)

更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？

我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性，
所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。

因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$

![2-3](/posts_res/2018-06-28-cnn1/2-3.png)

在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些
（所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数）

在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。

![2-4](/posts_res/2018-06-28-cnn1/2-4.png)

卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。

如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。

![2-5](/posts_res/2018-06-28-cnn1/2-5.png)

卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。
通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。
比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。

最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。

最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。
最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。
由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。

![2-6](/posts_res/2018-06-28-cnn1/2-6.png)

至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。

<br>

在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。
例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。

![2-7](/posts_res/2018-06-28-cnn1/2-7.png)

上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。

![2-8](/posts_res/2018-06-28-cnn1/2-8.png)

同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。
这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。

![2-9](/posts_res/2018-06-28-cnn1/2-9.png)

<br>

有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。

上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。
在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。

例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。

![2-10](/posts_res/2018-06-28-cnn1/2-10.png)

在论文[Network in Network](http://arxiv.org/abs/1312.4400)中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点，
由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。

![2-11](/posts_res/2018-06-28-cnn1/2-11.png)

这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。


--------

### 3.卷积神经网络的正式表达

考虑一维的卷积层，其中输入为$\lbrace x_n \rbrace$，输出为$ \lbrace y_n \rbrace $

![3-1](/posts_res/2018-06-28-cnn1/3-1.png)

很容易可以用输入表示输出：

$$ y_n = A( x_n, x_{n+1}, \cdots ) $$

如上图：

$$
y_0 = A(x_0, x_1) \\
y_1 = A(x_1, x_2)
$$

相似地，如果我们考虑二维的卷积层，其中输入为$\lbrace x_{n,m} \rbrace$，输出为$ \lbrace y_{n,m} \rbrace $

![3-2](/posts_res/2018-06-28-cnn1/3-2.png)

同样地，我们可以用输入表示输出：

$$ 
y_{n,m} = A 
\begin{pmatrix}
x_{n,m}, & x_{n+1, m}, & \cdots , \\
x_{n,m+1}, & x_{n+1, m+1}, & \cdots , \\
 & \cdots &
\end{pmatrix}
$$

例如：

$$
y_{0,0}=A
\begin{pmatrix}
x_{0,0}, & x_{1,0}, \\
x_{1,0}, & x_{1,1}, \\
\end{pmatrix}
y_{1,0}=A
\begin{pmatrix}
x_{1,0}, & x_{2,0}, \\
x_{1,1}, & x_{2,1}, \\
\end{pmatrix}
$$

把这些和矩阵方程联合起来，得$A(x)$。

$$ A(x) = \sigma (W x + b) $$

实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式--关于卷积的数学运算，这通常更有帮助。

卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。
它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。
其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。
最后，卷积将为我们提供一个非常不同的角度来解释卷积层。


----------

### 4.理解卷积

上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？

**球体下落的启示**

试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。
球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。
那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？

我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。
球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同，
那么$g$和$f$是两个不同的概率分布。

![4-1](/posts_res/2018-06-28-cnn1/4-1.png)

假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \cdot f(b)$。
从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\cdot g(1)$。

![4-2](/posts_res/2018-06-28-cnn1/4-2.png)

然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。

![4-3](/posts_res/2018-06-28-cnn1/4-3.png)

上述两种可能各种的概率表示依次是$f(1) \cdot g(2)$和$ f(0) \cdot g(3)$。

为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。

$$\cdots f(0) \cdot g(3) + f(1) \cdot g(2) + f(2) \cdot g(1) \cdots $$

我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为：

$$ \sum_{a+b=c} f(a) \cdot g(b) $$

事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下:

$$ (f ∗ g) (c) = \sum_{a+b=c} f(a) \cdot g(b) $$

如果用$b=c-a$替换，则有

$$(f ∗ g) (c) = \sum_a f(a) \cdot g(c−a) $$

这正是标准的卷积定义。

更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$

![4-4](/posts_res/2018-06-28-cnn1/4-4.png)

为了完成卷积，我们考虑所有的中转位置。

![4-5](/posts_res/2018-06-28-cnn1/4-5.png)

<br>

**卷积的可视化**

有一个非常好的例子可以帮助我们更好地理解卷积的过程。

首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。

![4-6](/posts_res/2018-06-28-cnn1/4-6.png)

设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？

![4-7](/posts_res/2018-06-28-cnn1/4-7.png)

第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $

我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。

![4-8](/posts_res/2018-06-28-cnn1/4-8.png)

对所有的$a$求和，我们就得到了卷积的结果。
这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。
如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。

具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值)

![4-9](/posts_res/2018-06-28-cnn1/4-9.png)

随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。

![4-10](/posts_res/2018-06-28-cnn1/4-10.png)

当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。
下图，我们用两个箱型图变现了卷积的过程：

![4-11](/posts_res/2018-06-28-cnn1/4-11.gif)

从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。
卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。
采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。

<br>

**高维卷积**

卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。

![4-12](/posts_res/2018-06-28-cnn1/4-12.png)

卷积过程还是和之前的一样

$$ (f ∗ g) ( c ) = \sum_{a+b=c} f(a) \cdot g(b) $$

只是这里的$a，b，c$是向量，更明确地表示是

$$ (f ∗ g)(c_1, c_2) = \sum_{a_1 + b_1 = c_1 \\ a_2 + b_2 = c_2} f ( a_1, a_2) \cdot g(b_1, b_2 ) $$

或者使用标准定义：

$$ (f ∗ g ) ( c_1, c_2) = \sum{a_1,a_2} f(a_1, a_2) \cdot g(c_1−a_1, c_2−a_2) $$

和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。

二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。

![4-13](/posts_res/2018-06-28-cnn1/4-13.png)

核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。

比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9

![4-14](/posts_res/2018-06-28-cnn1/4-14.png)

我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。
也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。
如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。

![4-15](/posts_res/2018-06-28-cnn1/4-15.png)

gimp documentation 中有很多[其他的例子](http://docs.gimp.org/en/plug-in-convmatrix.html)。

<br>

**卷积神经网络**

那么，卷积操作和卷积神经网络是如何联系起来的呢？

首先我们考虑一维卷积神经网络，其中输入为$ \lbrace x_n \rbrace$，输出为$ \lbrace y_n \rbrace $，

![4-16](/posts_res/2018-06-28-cnn1/4-16.png)

由图可得，输入层与输出层之间的函数关系为：

$$ y_n = A ( x_n, x_{n+1}, \cdots) $$

通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成：

$$ \sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \cdots + b) $$

在这个公式中，$x_0, x_1, \cdots $代表输入；$w_0, w_1, \cdots $代表神经节点与输入之间连接的权重。
这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。
正是这些权重，决定了卷积神经网络可以为我们处理的任务。

具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$：

$$ y = \sigma ( W x + b ) $$

由此我们可以得到：

$$
y_0 = \sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \cdots) \\
y_1 = \sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \cdots)
$$

参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。

![4-17](/posts_res/2018-06-28-cnn1/4-17.png)

上图描述的这种神经网络可以表达成如下等式：

$$
y_0 = \sigma ( W_{0} x_0 + W_{1} x_1 - b) \\
y_1 = \sigma ( W_{0} x_1 + W_{1} x_2 - b)
$$

然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）：

$$
W = \left[
\begin{array}{ccccc} 
W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\
W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\
W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\
W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\
...     &   ...   &   ...   &  ...    & ...\\
\end{array}
\right]
$$

上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。

$$
W = \left[
\begin{array}{ccccc} 
w_0 & w_1 &  0  &  0  & ...\\
 0  & w_0 & w_1 &  0  & ...\\
 0  &  0  & w_0 & w_1 & ...\\
 0  &  0  &  0  & w_0 & ...\\
... & ... & ... & ... & ...\\
\end{array}
\right]
$$

与上述矩阵的点乘，其实和卷积一个向量$ \left[ \cdots 0, w_1, w_0, 0 \cdots \right] $是一样的。
当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。

*那么二维的卷积层又是什么样呢？*

![4-18](/posts_res/2018-06-28-cnn1/4-18.png)

上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。

回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。

<br>

**结语**

在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？

- 非常强大的语言来描述网络的连接。
    - 到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。
- 卷积运算具有显著的实现优势。
    - 许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。
- 实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。
