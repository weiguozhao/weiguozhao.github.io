---
title: Spark基础
mathjax: true
copyright: true
date: 2019-03-07 20:39:32
categories: 大数据
tags:
- 语言基础
---

## 多说几句

写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。

Spark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录，
大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。

现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑，
网络上的主要学习来源是[子雨大数据之Spark入门教程](http://dblab.xmu.edu.cn/blog/spark/)，这个教程真的只是入门。

以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述

## RDD编程

- 基本RDD“转换”运算
    - **map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集**
    - **filter(func)：筛选出满足函数func的元素，并返回一个新的数据集**
    - **flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果**
    - distinct（去重运算）
    - randomSplit（根据指定的比例随机分为N各RDD）
    - **reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合**
    - **groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集**
    - union（两个RDD取并集）
    - intersection（两个RDD取交集）
    - subtract（两个RDD取差集）
    - cartesian（两个RDD进行笛卡尔积运算）
- 基本RDD“动作”运算
    - **count() 返回数据集中的元素个数**
    - **collect() 以数组的形式返回数据集中的所有元素**
    - **first() 返回数据集中的第一个元素**
    - **take(n) 以数组的形式返回数据集中的前n个元素**
    - **reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素**
    - **foreach(func) 将数据集中的每个元素传递到函数func中运行**
    - takeOrdered（排序后取前N条数据）
- Key-Value形式 RDD“转换”运算
    - filter（过滤符合条件的数据）
    - mapValues（对value值进行转换）
    - sortByKey（根据key值进行排序）
    - reduceByKey（合并相同key值的数据）
    - join（内连接两个KDD）
    - leftOuterJoin（左外连接两个KDD）
    - rightOuterJoin（右外连接两个RDD）
    - subtractByKey（相当于key值得差集运算）
- Key-Value形式 RDD“动作”运算
    - first（取第一条数据）
    - take（取前几条数据）
    - countByKey（根据key值分组统计）
    - lookup（根据key值查找value值）
- RDD持久化
    - persist用于对RDD进行持久化
    - unpersist取消RDD的持久化，注意持久化的存储等级


## 读text文本数据(需要自己解析)

spark 1.6.1

```scala
val sparkConf = new SparkConf()
sparkConf.setAppName("...")
sparkConf.set("key", "value")
sparkContext = new SparkContext(sparkConf)
sparkContext.textFile("text_path").map(...)
```

读取文本数据后，使用map进行自定义的解析

spark 2.1.0

```scala
val warehouseLocation = "spark-warehouse"
val sparkConf = new SparkConf()
sparkConf.setAppName("...")
sparkConf.set("key", "value")
val sparkSession = SparkSession
      .builder()
      .appName("Spark Hive Example")
      .config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
sparkSession.read.textFile("text_path").map(...)
```

## 读Hive表数据

spark 1.6.1

```scala
val sparkConf = new SparkConf()
sparkConf.setAppName("...")
sparkConf.set("key", "value")
sparkContext = new SparkContext(sparkConf)
hiveContext = new HiveContext(sparkContext)
hiveContext.sql("select * from table where condition").map(...)
```


spark 2.1.0

```scala
val warehouseLocation = "spark-warehouse"
val sparkConf = new SparkConf()
sparkConf.setAppName("...")
sparkConf.set("key", "value")
val sparkSession = SparkSession
      .builder()
      .appName("Spark Hive Example")
      .config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
sparkSession.sql("select * from table where condition").map(...)
```

## 提交到集群中运行

pyspark

```bash
#!/usr/bin/env bash
spark_submit=${spark_bin}/spark-submit
spark_master=yarn-client

time \
$spark_submit \
	--queue whichQueue \
	--master $spark_master \
	--driver-memory 10g \
	--executor-memory 40g \
	--num-executors 24 \
	--executor-cores 8 \
	--jars SomeThirdPartyJarFile.jar \
	--conf spark.yarn.executor.memoryOverhead=600 \
	your_script.py params
```

scala 2.11 

spark 2.1.0

```bash
#!/usr/bin/env bash

spark_submit=${spark_bin}/spark-submit
jar_file=packageAccordingMavenOrSbt.jar

time \
$spark_submit \
    --class "package.className" \
    --master yarn \
    --deploy-mode client \
    --num-executors 8 \
    --executor-cores 3 \
    --driver-memory 8g \
    --executor-memory 24g \
    --queue whichQueue \
    --conf spark.classpath.userClassPathFirst=true \
    ${jar_file} \
    --param ${param}
```

scala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析