---
title: 机器学习中的损失函数
tags:
  - 损失函数
mathjax: true
comments: false
copyright: false
date: 2020-01-04 17:07:03
categories: 机器学习
---


### 1. 平方损失误差

平方误差损失(也称为L2 Loss)是实际值和预测值之差的平方

$$
L = (y - f(x))^2
$$

MSE损失函数通过平方误差来惩罚模型犯的大错误。把一个比较大的数平方会使它变得更大。这个属性使MSE成本函数对异常值的健壮性降低。因此，如果数据容易出现许多的异常值，则不应使用这个它。


### 2. 绝对误差损失

绝对误差是预测值和实际值之间的距离，与符号无关。绝对误差也称为L1 Loss

$$
L = | y - f(x) |
$$

与MSE相比，MAE成本对异常值更加健壮。但是，在数学方程中处理绝对值或模数运算符并不容易。我们可以认为这是MAE的缺点。


### 3. Huber损失

Huber损失结合了MSE和MAE的最佳特性。对于较小的误差，它是二次的，否则是线性的(对于其梯度也是如此)。Huber损失需要确定 $\delta$ 参数：

$$
L(\delta) = 
\begin{cases}
\frac{1}{2} (y - f(x)) ^2 &\quad if | y - f(x) | \le \delta \\
\delta | y - f(x) | - \frac{1}{2} \delta^2 &\quad otherwise
\end{cases}
$$

Huber损失对于异常值比MSE更强。它用于稳健回归(robust regression)，M估计法(M-estimator)和可加模型(additive model)。Huber损失的变体也可以用于分类。


### 4. 交叉熵损失损失函数

#### 4.1 二分类交叉熵

$$
L = -y \times log(p) - (1-y) \times log(1 -p)
$$

其中 $p = sigmoid(f(x)) = 1 / (1 + e^{-f(x)})$。

#### 4.2 多分类交叉熵

$$
L(X_i, Y_i) = - \sum_{j=1}^c y_{i,j} \times log(p_{i,j})
$$

其中 $Y\_i$ 是one-hot形式的label，$y\_{i,j}$非$0$即$1$，$p\_{i,j}$是sigmoid后的概率值，介于 [$0, 1$]。


### 5. Hinge损失函数

Hinge损失主要用于带有类标签-1和1的支持向量机(SVM)。因此，请确保将数据集中"恶性"类的标签从0更改为-1。
Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。

$$
L = max(0, \quad 1 - y \times f(x))
$$

Hinge损失简化了SVM的数学运算，同时最大化了损失(与对数损失(Log-Loss)相比)。当我们想要做实时决策而不是高度关注准确性时，就可以使用它。


### 6. KL散度

KL散度通常不用来做损失函数，其转化后的形式(交叉熵)经常应用于分类问题做损失函数。
KL散度概率分布与另一个概率分布区别的度量。KL散度为零表示分布相同。

假定有两个概率分布 $P$ 和 $Q$，则 $P$ 和 $Q$ 之间的KL散度定义为：

$$
D_{KL}(P \parallel Q) =
\begin{cases}
-\sum_x P(x) \cdot log \frac{Q(x)}{P(x)} = \sum_x P(x) \cdot log \frac{P(x)}{Q(x)}, &\quad 对离散分布 \\
-\int P(x) \cdot log \frac{Q(x)}{P(x)} \cdot dx = \int P(x) \cdot log \frac{P(x)}{Q(x)} \cdot dx, &\quad 对连续分布
\end{cases}
$$

Note: 发散函数不对称。
与多分类分类相比，KL散度更常用于逼近复杂函数。我们在使用变分自动编码器(VAE)等深度生成模型时经常使用KL散度。

> 
> [机器学习中的 7 大损失函数实战总结（附Python演练）](https://zhuanlan.zhihu.com/p/80370381)
