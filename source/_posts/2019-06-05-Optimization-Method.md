---
title: Optimization Method
tags:
- ä¼˜åŒ–ç®—æ³•
mathjax: true
copyright: true
date: 2019-06-05 18:05:24
categories: æœºå™¨å­¦ä¹ 
---

*æœ¬æ–‡æ˜¯ä»å„ä¸ªè®ºæ–‡ã€åšå®¢ã€ä¸“æ ç­‰å­¦ä¹ æ•´ç†æ‰€å¾—,å¦‚æœ‰ä»»ä½•é”™è¯¯ç–æ¼ç­‰é—®é¢˜,æ¬¢è¿è¯„è®ºæˆ–é‚®ç®±æå‡º,å¤§å®¶ä¸€èµ·å­¦ä¹ è¿›æ­¥ï¼*

## 0 ä¼˜åŒ–ç®—æ³•æ¡†æ¶

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{0-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  m_t = \phi (g_1, g_2, \cdots, g_t)
  \tag{0-2}
  $$
  $$
  V_t = \psi (g_1, g_2, \cdots, g_t)
  \tag{0-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t
  \tag{0-4}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  w_{t+1} = w_t - \eta_t
  \tag{0-5}
  $$

**æ ¸å¿ƒåŒºåˆ«æ˜¯ç¬¬3æ­¥æ‰§è¡Œçš„ä¸‹é™æ–¹å‘,åœ¨è¿™ä¸ªå¼å­ä¸­,å‰åŠéƒ¨åˆ†æ˜¯å®é™…çš„å­¦ä¹ ç‡ï¼ˆä¹Ÿå³ä¸‹é™æ­¥é•¿ï¼‰,ååŠéƒ¨åˆ†æ˜¯å®é™…çš„ä¸‹é™æ–¹å‘ã€‚ä¸åŒä¼˜åŒ–ç®—æ³•ä¹Ÿå°±æ˜¯ä¸æ–­åœ°åœ¨è¿™ä¸¤éƒ¨åˆ†ä¸Šåšæ–‡ç« ã€‚ä¸‹æ–‡ä¼šå°†é‡è¦çš„åœ°æ–¹æ ‡çº¢æ˜¾ç¤º**


## 1. Gradient Descent Variants

### 1.1 Batch Gradient Descent

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{1.1-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red} {m_t = g_t}
  \tag{1.1-2}
  $$
  $$
  \color{red} {V_t = 1}
  \tag{1.1-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
  \tag{1.1-4}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\
  &= w_t - \alpha \cdot g_t
  \end{align*}
  \tag{1.1-5}
  $$
  


### 1.2 Stochastic Gradient Descent

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  \color{red}  { g_t = \nabla f(w_t; x^{(i)}, y^{(i)}) } 
  \tag{1.2-1}
  $$

  å…¶ä¸­$(x^{(i)}, y^{(i)})$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬ï¼›

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red} {m_t = g_t}
  \tag{1.2-2}
  $$
  $$
  \color{red} {V_t = 1}
  \tag{1.2-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
  \tag{1.2-4}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t
  \end{align*}
  \tag{1.2-5}
  $$


### 1.3 Mini-Bach Gradient Descent

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  \color{red}  { g_t = \nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } 
  \tag{1.3-1}
  $$

  å…¶ä¸­$(x^{(i:i+n)}, y^{(i:i+n)})$è¡¨ç¤ºç¬¬$i$ä¸ªæ ·æœ¬åˆ°ç¬¬$i+n$ä¸ªæ ·æœ¬,$n$è¡¨ç¤ºmini-batchçš„å¤§å°ï¼›

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red} {m_t = g_t}
  \tag{1.3-2}
  $$
  $$
  \color{red} {V_t = 1}
  \tag{1.3-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
  \tag{1.3-4}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t
  \end{align*}
  \tag{1.3-5}
  $$


**ä¸Šè¿°ç®—æ³•å­˜åœ¨çš„é—®é¢˜ï¼š**
- å¾ˆéš¾è°ƒæ•´å‡ºä¸€ä¸ªåˆé€‚çš„learning_rate
- learning_rateçš„å˜åŒ–è§„åˆ™å¾€å¾€æ˜¯é¢„å®šä¹‰çš„,å¾ˆéš¾é€‚åº”ä¸åŒçš„æ•°æ®
- æ‰€æœ‰çš„ç‰¹å¾å…±äº«ç›¸åŒçš„learning_rate
- å±€éƒ¨æœ€æœ‰è§£çš„é—®é¢˜


---------------------------------------------------------------


## 2. Gradient Descent Optimization Algorithm

### 2.1 Gradient Descent with Momentum

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{2.1-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡,è®¡ç®—å½“å‰æ—¶åˆ»ä¸‹é™æ¢¯åº¦(*å°†æ¡†æ¶çš„ç¬¬2æ­¥å’Œç¬¬3æ­¥åˆå¹¶*)
  $$
  \color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t }
  \tag{2.1-2&3}
  $$

  $$
  \color{red} { \eta_t = m_t }
  \tag{2.1-4}
  $$

3. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t )
  \end{align*}
  \tag{2.1-5}
  $$

ä¸€é˜¶åŠ¨é‡æ˜¯ç§»åŠ¨å¹³å‡å€¼,è¿™é‡Œ $\gamma $ çš„ç»éªŒå€¼ä¸º`0.9`ã€‚
ä»¥å†å²åŠ¨é‡çš„å’Œä¸ºä¸»,é€‚å½“åŠ ä¸Šä¸€ç‚¹å½“å‰çš„åç§»,å³è€ƒè™‘æƒ¯æ€§çš„å› ç´ ã€‚


### 2.2 Nesterov Accelerated Gradient

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°+ä¸‹æ¬¡å˜åŒ–çš„æ¢¯åº¦
  $$
  \color{red} { g_t = \nabla f(w_t - \gamma m_{t-1}) }
  \tag{2.2-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡,è®¡ç®—å½“å‰æ—¶åˆ»ä¸‹é™æ¢¯åº¦(*å°†æ¡†æ¶çš„ç¬¬2æ­¥å’Œç¬¬3æ­¥åˆå¹¶*)
  $$
  \color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t }
  \tag{2.2-2&3}
  $$

  $$
  \color{red} { \eta_t = m_t }
  \tag{2.2-4}
  $$

3. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t )
  \end{align*}
  \tag{2.2-5}
  $$

è¿™é‡Œ $\gamma $ çš„ç»éªŒå€¼ä¸º`0.9`,åœ¨å¯¹å‚æ•°æ±‚å¯¼æ—¶,ä¸å†å’Œä¹‹å‰æ–¹æ³•ä¸€ç›´,è€Œæ˜¯å¯¹ä¸‹æ¬¡å‚æ•°æ±‚å¯¼,å³å¤šå‘å‰çœ‹ä¸€æ­¥å¯ä»¥ç”¨æ¥æŒ‡å¯¼å½“å‰æ€ä¹ˆèµ°ã€‚


### 2.3 AdaGrad

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{2.3-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  m_t = g_t
  \tag{2.3-2}
  $$
  $$
  \color{red} {V_t = \sum_{\tau = 1}^t g_{\tau}^2 }
  \tag{2.3-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \begin{align*}
  \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t
  \end{align*}
  \tag{2.3-4}
  $$

  è¿™é‡Œ$\epsilon$æ˜¯ä¸ºäº†é¿å…åˆ†æ¯ä¸º$0$,é€šå¸¸è®¾ç½®ä¸º$1e-8$ã€‚

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t
  \end{align*}
  \tag{2.3-5}
  $$

ä¸åŒçš„å‚æ•°å…·æœ‰ä¸åŒçš„æ¢¯åº¦,ä»è€Œè¾¾åˆ°äº†ä¸åŒå‚æ•°å…·æœ‰ä¸åŒå­¦ä¹ ç‡çš„ç›®çš„,è¿™æ ·æ¢¯åº¦æ›´æ–°å¿«çš„å‚æ•°,å­¦ä¹ ç‡(æ­¥é•¿)ä¼šæ¸æ¸å˜å°ã€‚
åŒæ ·æœ€ç»ˆä¼šé¢ä¸´å­¦ä¹ ç‡è¿‡å°,æ¨¡å‹æ— æ³•ç»§ç»­å­¦ä¹ çš„é—®é¢˜ã€‚


### 2.4 AdaDelta

é¦–å…ˆå®šä¹‰åŠ¨æ€å¹³å‡å€¼ $\color{red}{ E[ g^2 ]\_t = \gamma E[ g^2 ]\_{t-1} + (1 - \gamma) g_t^2 }$,è¯¥å€¼ä»…å–å†³äºå½“å‰æ¢¯åº¦å€¼ä¸ä¸Šä¸€æ—¶åˆ»çš„åŠ¨æ€å¹³å‡å€¼,å…¶ä¸­$\gamma$é€šå¸¸è®¾ç½®æˆ$0.9$ã€‚

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{2.4-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  m_t = g_t
  \tag{2.4-2}
  $$
  $$
  \color{red} {V_t = E[ g^2 ]_t }
  \tag{2.4-3}
  $$

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \begin{align*}
  \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ E[ g^2 ]_t + \epsilon }} \cdot m_t
  \end{align*}
  \tag{2.4-4.1}
  $$

  è¿™é‡Œ$\epsilon$æ˜¯ä¸ºäº†é¿å…åˆ†æ¯ä¸º$0$ã€‚å°†åˆ†æ¯$ \sqrt{ E[ g^2 ]\_t + \epsilon} $ è®°ä¸º $\color{red}{ RMS[g]\_t} $,å®šä¹‰
  $$
  E[ \Delta g^2 ] _t = \gamma E[ \Delta g^2 ] _{t-1} + (1 - \gamma) \Delta g _t^2
  $$

  åˆ™ï¼š
  $$
  RMS[\Delta g] _t = \sqrt{ E[ \Delta g^2 ] _t + \epsilon }
  $$

  ç”¨$RMS[\Delta g]_{t-1}$ä»£æ›¿å­¦ä¹ ç‡$\alpha$,åˆ™å¼$(2.4-4.1)$å¯ä»¥è½¬åŒ–ä¸º:
  $$
  \begin{align*}
  \eta_t &= \frac{\alpha}{RMS[g] _t} \cdot g_t \\ &= \frac{ RMS[\Delta g] _{t-1} }{ RMS[g] _t } \cdot g _t
  \end{align*}
  \tag{2.4-4.2}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{RMS[\Delta g] _{t-1}}{RMS[g] _t} \cdot g_t
  \end{align*}
  \tag{2.4-5}
  $$

$\color{red}{ä¸ºä»€ä¹ˆ}$ç”¨$RMS[\Delta g]_{t-1}$ä»£æ›¿å­¦ä¹ ç‡$\alpha$??

### 2.5 RMSprop

RMSpropæ˜¯AdaDeltaç®—æ³•çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚

$$
E[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t
$$

$$
w_{t+1} = w_t - \frac{\alpha}{ \sqrt{ E[g^2] _t + \epsilon } } g_t
$$

Hintonå»ºè®®$\gamma$è®¾ç½®æˆ$0.9$,å­¦ä¹ ç‡è®¾ç½®æˆ$0.001$ã€‚


### 2.6 Adam

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{2.6-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red}{ \hat{m_t} = \beta_1 \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\
  \color{red}{ m_t = \frac{\hat{m_t}}{1 - \beta_1^t} }
  \tag{2.6-2}
  $$
  $$
  \color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\
  \color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } }
  \tag{2.6-3}
  $$

  å…¶ä¸­çš„$\beta_1$æ§åˆ¶ä¸€é˜¶åŠ¨é‡,$\beta_2$æ§åˆ¶äºŒé˜¶åŠ¨é‡ï¼›

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \begin{align*}
  \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t
  \end{align*}
  \tag{2.6-4}
  $$

  å…¶ä¸­å¢åŠ çš„$\epsilon$ä¸ºäº†é˜²æ­¢åˆ†æ¯ç­‰äº$0$ï¼›

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t
  \end{align*}
  \tag{2.6-5}
  $$

  ä½œè€…å»ºè®®é»˜è®¤å€¼$\beta_1 = 0.9$,$\beta_2 = 0.999$,$\epsilon = 1e-8$ã€‚


### 2.7 AdaMax

Adamaxæ˜¯Adamçš„ä¸€ç§å˜ä½“,æ­¤æ–¹æ³•å¯¹å­¦ä¹ ç‡çš„ä¸Šé™æä¾›äº†ä¸€ä¸ªæ›´ç®€å•çš„èŒƒå›´ã€‚

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  g_t = \nabla f(w_t)
  \tag{2.7-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red}{ m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t }
  \tag{2.7-2}
  $$
  $$
  \color{red}{ 
  \begin{align*}
  V_t &= \beta_2^{\infty} V_{t-1} + (1 - \beta_2^{\infty}) \| g_t \| ^{\infty} \\
  &= max( \beta_2 \cdot V_{t-1}, \| g_t \| )
  \end{align*}
  }
  \tag{2.7-3}
  $$

  å…¶ä¸­çš„$\beta_1$æ§åˆ¶ä¸€é˜¶åŠ¨é‡,$\beta_2$æ§åˆ¶äºŒé˜¶åŠ¨é‡ï¼›

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \color{red}{
  \begin{align*}
  \eta_t &= \frac{\alpha}{V_t} \cdot m_t \\
  &= \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \}
  \end{align*}
  }
  \tag{2.7-4}
  $$

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t \\ 
  &= w_t - \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \}
  \end{align*}
  \tag{2.7-5}
  $$

  è®ºæ–‡è¯´åˆé€‚çš„é»˜è®¤å€¼ä¸º$\alpha = 0.002, \beta_1 = 0.9, \beta_2 = 0.999$ã€‚


### 2.8 Nadam

1. è®¡ç®—ç›®æ ‡å‡½æ•°å…³äºå½“å‰å‚æ•°çš„æ¢¯åº¦
  $$
  \color{red}{ g_t = \nabla f(w_t - \frac{\alpha}{\sqrt{V_t}} \cdot m_{t-1}) }
  \tag{2.8-1}
  $$

2. æ ¹æ®å†å²æ¢¯åº¦è®¡ç®—ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡
  $$
  \color{red}{ \hat{m_t} = \gamma \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\
  \color{red}{ m_t = \frac{\hat{m_t}}{ 1 - \beta_1^t } }
  \tag{2.8-2}
  $$
  $$
  \color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\
  \color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } }
  \tag{2.8-3}
  $$

  å…¶ä¸­çš„$\beta_1$æ§åˆ¶ä¸€é˜¶åŠ¨é‡,$\beta_2$æ§åˆ¶äºŒé˜¶åŠ¨é‡ï¼›

3. è®¡ç®—å½“å‰æ—¶åˆ»çš„ä¸‹é™æ¢¯åº¦
  $$
  \begin{align*}
  \eta_t &= \frac{\alpha}{\sqrt{V_t} + \epsilon } \cdot m_t
  \end{align*}
  \tag{2.8-4}
  $$

  å…¶ä¸­å¢åŠ çš„$\epsilon$ä¸ºäº†é˜²æ­¢åˆ†æ¯ç­‰äº$0$ï¼›

4. æ ¹æ®ä¸‹é™æ¢¯åº¦è¿›è¡Œæ›´æ–°
  $$
  \begin{align*}
  w_{t+1} &= w_t - \eta_t
  \end{align*}
  \tag{2.8-5}
  $$


-----------------------

## 3. Online Optimization Algorithm

ä¸Šé¢æè¿°çš„ä¸»è¦æ˜¯æ‰¹é‡è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ‰¹é‡è®­ç»ƒæœ‰è‡ªèº«çš„å±€é™æ€§ï¼šé¢å¯¹é«˜ç»´é«˜æ•°æ®é‡çš„æ—¶å€™ï¼Œæ‰¹é‡å¤„ç†çš„æ–¹å¼å°±æ˜¾å¾—ç¬¨é‡å’Œä¸å¤Ÿé«˜æ•ˆã€‚å› æ­¤éœ€è¦æœ‰åœ¨çº¿å¤„ç†çš„æ–¹æ³•(Online)æ¥è§£å†³ç›¸åŒçš„é—®é¢˜ã€‚åœ¨çº¿å­¦ä¹ ç®—æ³•çš„ç‰¹ç‚¹æ˜¯ï¼šæ¯æ¥ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå°±ç”¨è¯¥æ ·æœ¬äº§ç”Ÿçš„losså’Œæ¢¯åº¦å¯¹æ¨¡å‹è¿­ä»£ä¸€æ¬¡ï¼Œä¸€ä¸ªä¸€ä¸ªæ•°æ®åœ°è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å¯ä»¥å¤„ç†å¤§æ•°æ®é‡è®­ç»ƒå’Œåœ¨çº¿è®­ç»ƒã€‚

### 3.1 Truncated Gradient

#### 3.1.1 L1æ­£åˆ™æ³•

$$
w^{(t+1)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} - \eta^{(t)} \cdot \lambda \cdot sgn( w^{(t)} )
\tag{3.1-1}
$$

å…¶ä¸­,
- $\lambda \in \mathbb{R} $æ˜¯ä¸€ä¸ªæ ‡é‡,ä¸”$\lambda \ge 0$,ä¸ºL1æ­£åˆ™åŒ–å‚æ•°ï¼›
- $sgn(v)$ä¸ºç¬¦å·å‡½æ•°ï¼›
- $\eta^{(t)}$ä¸ºå­¦ä¹ ç‡,é€šå¸¸å°†å…¶è®¾ç½®ä¸º$1/\sqrt{t}$çš„å‡½æ•°ï¼›
- $G^{(t)} = \nabla\_w f(w^{(t)}, z^{(t)}) $ä»£è¡¨äº†ç¬¬$t$æ¬¡è¿­ä»£ä¸­æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼›

#### 3.1.2 ç®€å•æˆªæ–­æ³•

ä»¥$k$ä¸ºçª—å£,å½“$t/k$ä¸ä¸ºæ•´æ•°æ—¶é‡‡ç”¨æ ‡å‡†SGDè¿›è¡Œè¿­ä»£ï¼›å½“$t/k$ä¸ºæ•´æ•°æ—¶,é‡‡ç”¨å¦‚ä¸‹æƒé‡æ›´æ–°æ–¹å¼ï¼›

$$
w^{(t+1)} = T_0 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \theta \right)
$$

$$
T_0 (v_i, \theta) = 
\begin{cases}
0 & \quad \mid v_i \mid \le \theta \\
v_i & \quad otherwise
\end{cases}
\tag{3.1-2}
$$

$\theta \in \mathbb{R}$æ˜¯ä¸€ä¸ªæ ‡é‡,ä¸”$\theta \ge 0$ï¼›

#### 3.1.3 æˆªæ–­æ¢¯åº¦æ³•(TG)

ç®€å•æˆªæ–­æ³•å¤ªè¿‡æ¿€è¿›,å› æ­¤TGåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼š

$$
w^{(t+1)} = T_1 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \quad \eta^{(t)} \lambda^{(t)}, \quad \theta \right)
$$

$$
T_1 (v_i, \alpha, \theta) = 
\begin{cases}
max(0, \quad v_i - \alpha ) & \quad v_i \in [0, \theta] \\
max(0, \quad v_i + \alpha ) & \quad v_i \in [- \theta, 0 ] \\
v_i & \quad otherwise
\end{cases}
\tag{3.1-3}
$$

- å…¶ä¸­$\lambda^{(t)} \in \mathbb{R}$,ä¸”$\lambda^{(t)} \ge 0$ï¼›
- TG åŒæ ·æ˜¯ä»¥$k$ä¸ºçª—å£,æ¯$k$æ­¥è¿›è¡Œä¸€æ¬¡æˆªæ–­ã€‚
  - å½“$t/k$ä¸ä¸ºæ•´æ•°æ—¶,$\lambda^{(t)} = 0$ï¼›
  - å½“$t/k$ä¸ºæ•´æ•°æ—¶,$\lambda^{(t)} = k \cdot \lambda$ã€‚
- ä»å…¬å¼$(3.1-3)$å¯ä»¥çœ‹å‡º,è¶…å‚æ•°$\lambda$å’Œ$\theta$å†³å®šäº†$w$çš„ç¨€ç–ç¨‹åº¦,è¿™ä¸¤ä¸ªå€¼è¶Šå¤§,åˆ™ç¨€ç–æ€§è¶Šå¼ºï¼›å°¤å…¶ä»¤$\lambda = \theta$æ—¶,åªéœ€è¦é€šè¿‡è°ƒèŠ‚ä¸€ä¸ªå‚æ•°å°±èƒ½æ§åˆ¶ç¨€ç–æ€§ã€‚

æ ¹æ®å…¬å¼$(3.1-3)$ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å†™å‡º TG çš„ç®—æ³•é€»è¾‘:

![TG](/posts_res/2019-06-05-Optimization-Method/5.png)


#### 3.1.4 æˆªæ–­å…¬å¼å¯¹æ¯”

![compare](/posts_res/2019-06-05-Optimization-Method/1.png)

å…¶ä¸­å·¦ä¾§æ˜¯ç®€å•æˆªæ–­æ³•çš„æˆªæ–­å…¬å¼ï¼Œå³ä¾§æ˜¯æˆªæ–­æ¢¯åº¦çš„æˆªæ–­å…¬å¼ã€‚å…¬å¼$(3.1-3)$è¿›è¡Œæ”¹å†™ï¼Œæè¿°ç‰¹å¾æƒé‡æ¯ä¸ªç»´åº¦çš„æ›´æ–°æ–¹å¼:

$$
w^{(t+1)}_i = 
\begin{cases}
Trnc \{ ( w^{(t)}_i - \eta^{(t)} g^{(t)}_i ), \lambda^{(t)}_{TG}, \theta \} & if \quad mod(t,k) = 0 \\
w^{(t)}_i - \eta^{(t)} g^{(t)}_i & otherwise
\end{cases}
$$

$$
\lambda^{(t)}_{TG} = \eta^{(t)} \lambda k
\tag{3.1-4}
$$

$$
Trnc( w, \lambda^{(t)}_{TG}, \theta ) = 
\begin{cases}
0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\
w - \lambda^{(t)}_{TG} sgn(w) & if \quad \lambda^{(t)}_{TG} \le \mid w \mid \le 0 \\
w & otherwise
\end{cases}
$$

å¦‚æœä»¤$\lambda^{(t)}\_{TG} = \theta$, æˆªæ–­å…¬å¼$Trnc(w, \lambda^{(t)}\_{TG}, \theta)$å˜æˆ:

$$
Trnc(w, \theta, \theta) = 
\begin{cases}
0 & if \quad \mid w \mid \le 0 \\
w & otherwise
\end{cases}
$$

**æ­¤æ—¶$TG$é€€åŒ–æˆç®€å•æˆªæ–­æ³•**ã€‚

å¦‚æœä»¤$\theta = \infty$æˆªæ–­å…¬å¼$Trnc(w, \lambda^{(t)}_{TG}, \theta)$å˜æˆ:

$$
Trnc(w, \lambda^{(t)}_{TG}, \infty) = 
\begin{cases}
0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\
w & otherwise
\end{cases}
$$

å¦‚æœå†ä»¤$k=1$,é‚£ä¹ˆç‰¹å¾æƒé‡ç»´åº¦æ›´æ–°å…¬å¼å˜æˆ:

$$
\begin{align*}
w^{(t+1)}_i &= Trnc\{ (w^{(t)}_i - \eta^{(t)} g^{(t)}_i), \eta^{(t)} \lambda, \infty \} \\
&= w^{(t)}_i - \eta^{(t)} g^{(t)}_i - \eta^{(t)} \cdot \lambda \cdot sgn(w^{(t)}_i) 
\end{align*}
$$

**æ­¤æ—¶ $TG$ é€€åŒ–æˆ L1æ­£åˆ™åŒ–æ³•**ã€‚


### 3.2 FOBOSå‰å‘åå‘åˆ‡åˆ†

#### 3.2.1 FOBOSç®—æ³•åŸç†

åœ¨ FOBOS ä¸­, å°†æƒé‡çš„æ›´æ–°åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤:

$$
w^{(t+0.5)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} \\
w^{(t+1)} = argmin_w \{ \frac{1}{2} \| w - w^{(t+0.5)} \|^2 + \eta^{(t+0.5)} \cdot \psi (w) \}
\tag{3.2-1}
$$

- å‰ä¸€ä¸ªæ­¥éª¤å®é™…ä¸Šæ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ¢¯åº¦ä¸‹é™æ­¥éª¤;
- åä¸€ä¸ªæ­¥éª¤å¯ä»¥ç†è§£ä¸ºå¯¹æ¢¯åº¦ä¸‹é™çš„ç»“æœè¿›è¡Œå¾®è°ƒ;
  - å‰ä¸€éƒ¨åˆ†ä¿è¯å¾®è°ƒå‘ç”Ÿåœ¨æ¢¯åº¦ä¸‹é™ç»“æœçš„é™„è¿‘
  - åä¸€éƒ¨åˆ†åˆ™ç”¨äºå¤„ç†æ­£åˆ™åŒ–ï¼Œäº§ç”Ÿç¨€ç–æ€§

å¦‚æœå°†å…¬å¼ $(3.2-1)$ ä¸­çš„ä¸¤ä¸ªæ­¥éª¤åˆäºŒä¸ºä¸€, å³å°† $w^{(t+0.5)}$ çš„è®¡ç®—å¸¦å…¥åˆ° $w^{(t+1)}$ ä¸­, æœ‰:

$$
w^{(t+1)} = argmin _w \{ \frac{1}{2} \| w - w^{(t)} + \eta^{(t)} G^{(t)} \|^2 + \eta^{(t+0.5)} \psi (w) \}
$$

ä»¤$F(w) = \frac{1}{2} || w - w^{(t)} + \eta^{(t)} G^{(t)} ||^2 + \eta^{(t+0.5)} \psi (w)$, 
å¦‚æœ$w^{(t+1)}$å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜è§£, é‚£ä¹ˆå°±å¯ä»¥æ¨æ–­$0$å‘é‡ä¸€å®šå±äº$F(w)$çš„æ¬¡æ¢¯åº¦é›†åˆ:
$$
0 \in \partial F(w) = w - w^{(t)} + \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w)
$$

ç”±äº$w^{(t+1)} = argmin_w F(w)$, é‚£ä¹ˆæœ‰:
$$
0 = \{ w - w^{(t)} - \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w) \} \mid_{w = w^{(t+1)}}
$$

ä¸Šå¼å®é™…ä¸Šç»™å‡ºäº† FOBOS ä¸­æƒé‡æ›´æ–°çš„å¦ä¸€ç§å½¢å¼:
$$
w^{(t+1)} = w^{(t)} - \eta^{(t)} G^{(t)} - \eta^{(t+0.5)} \partial \psi (w^{(t+1)})
$$

æˆ‘ä»¬è¿™é‡Œå¯ä»¥çœ‹åˆ°, $w^{(t+1)}$ä¸ä»…ä»…ä¸è¿­ä»£å‰çš„çŠ¶æ€$w^{(t)}$æœ‰å…³ï¼Œè€Œä¸”ä¸è¿­ä»£åçš„$\psi (w^{(t+1)})$æœ‰å…³ã€‚


#### 3.2.2 L1-FOBOS

åœ¨ L1 æ­£åˆ™åŒ–ä¸‹ï¼Œæœ‰$\psi (w) = \lambda || w ||\_1$, 
ä¸ºäº†ç®€åŒ–æè¿°, ç”¨å‘é‡$v= \[ v\_1, v\_2, \cdots, v\_N \] \in \mathbb{R}^N$ æ¥è¡¨ç¤º$w^{(t+0.5)}$, 
ç”¨æ ‡é‡$\hat{\lambda} \in \mathbb{R}$ æ¥è¡¨ç¤º $\eta^{(t+0.5)} \lambda$, å¹¶å°†å…¬å¼$(3.2-1)$ç­‰å·å³è¾¹æŒ‰ç»´åº¦å±•å¼€:
$$
w^{(t+1)} = argmin_w \sum_{i=1}^N ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )
\tag{3.2-2}
$$

å¯ä»¥çœ‹åˆ°,åœ¨æ±‚å’Œå…¬å¼$\sum_{i=1}^N \( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid \)$ ä¸­çš„æ¯ä¸€é¡¹éƒ½æ˜¯å¤§äºç­‰äº $0$ çš„,
æ‰€ä»¥å…¬å¼$(3.2-2)$å¯ä»¥æ‹†è§£æˆå¯¹ç‰¹å¾æƒé‡$w$æ¯ä¸€ç»´åº¦å•ç‹¬æ±‚è§£:
$$
w^{(t+1)}_i = argmin_{w_i} ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )
$$

![derived](/posts_res/2019-06-05-Optimization-Method/6.png)

![derived](/posts_res/2019-06-05-Optimization-Method/7.png)

å› æ­¤, ç»¼åˆä¸Šé¢çš„åˆ†æå¾—åˆ°åœ¨ FOBOS åœ¨ L1 æ­£åˆ™åŒ–æ¡ä»¶ä¸‹ï¼Œç‰¹å¾æƒé‡çš„å„ä¸ªç»´åº¦æ›´æ–°çš„æ–¹å¼ä¸º:
$$
\begin{align*}
w^{(t+1)}_i &= sgn(v_i) max (0, \mid v_i \mid - \hat{\lambda}) \\
&= sgn(w_i^{(t)} - \eta^{(t)} g^{(t)}_i ) max \{ 0, \mid w^{(t)}_i - \eta_t \cdot g_i^{(t)} \mid - \eta^{(t+0.5)} \lambda \}
\end{align*}
\tag{3.2-3}
$$

å…¶ä¸­, $g_i^{(t)}$ ä¸ºæ¢¯åº¦ $G^{(t)}$ åœ¨ç»´åº¦ $i$ ä¸Šçš„å–å€¼ã€‚

æ ¹æ®å…¬å¼$(3.2-3)$ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å°±å¯ä»¥è®¾è®¡å‡º L1-FOBOS çš„ç®—æ³•é€»è¾‘:

![fobos](/posts_res/2019-06-05-Optimization-Method/8.png)

#### 3.2.3 L1-FOBOSä¸TGçš„å…³ç³»

å¯¹äº L1-FOBOS ç‰¹å¾æƒé‡çš„å„ä¸ªç»´åº¦æ›´æ–°å…¬å¼$(3.2-3)$ï¼Œä¹Ÿå¯ä»¥å†™ä½œå¦‚ä¸‹å½¢å¼:

$$
w^{(t+1)}_i = 
\begin{cases}
0 & \quad \mid w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \mid \le \eta^{(t+0.5)} \lambda \\
\left( w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \right) - \eta^{(t+0.5)} \cdot \lambda \cdot sgn(w^{(t)}_i - \eta^{(t)} g^{(t)}_i) & \quad otherwise
\end{cases}
$$

ä¸Šå¼æˆªæ–­çš„å«ä¹‰æ˜¯ï¼šå½“ä¸€æ¡æ ·æœ¬äº§ç”Ÿçš„æ¢¯åº¦ä¸è¶³ä»¥ä»¤å¯¹åº”ç»´åº¦ä¸Šçš„æƒé‡å€¼å‘ç”Ÿè¶³å¤Ÿå¤§çš„å˜åŒ– $( \eta^{(t+0.5)} \cdot \lambda )$, 
åˆ™è®¤ä¸ºåœ¨æœ¬æ¬¡æ›´æ–°è¿‡ç¨‹ä¸­è¯¥ç»´åº¦ä¸å¤Ÿé‡è¦ï¼Œåº”å½“ä»¤å…¶æƒé‡ä¸º$0$ã€‚

åŒæ—¶ä¸Šå¼ä¸TGçš„ç‰¹å¾æƒé‡æ›´æ–°å…¬å¼$(3.1-4)$å¯¹æ¯”, å‘ç°å¦‚æœä»¤$ \theta = \infty, k = 1, \lambda^{(t)}_{TG} = \eta^{(t+0.5} \lambda $,åˆ™L1-FOBOSä¸TGå®Œå…¨ä¸€è‡´ï¼Œå› æ­¤å¯ä»¥è®¤ä¸ºL1-FOBOSæ˜¯TGåœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ç‰¹æ®Šå½¢å¼ã€‚

### 3.3 RDAæ­£åˆ™å¯¹å¶å¹³å‡

ç®€å•æˆªæ–­ã€TGã€FOBOS éƒ½æ˜¯å»ºç«‹åœ¨ SGD çš„åŸºç¡€ä¹‹ä¸Šçš„ï¼Œå±äºæ¢¯åº¦ä¸‹é™ç±»å‹çš„æ–¹æ³•ï¼Œè¿™ç±»å‹æ–¹æ³•çš„ä¼˜ç‚¹å°±æ˜¯**ç²¾åº¦æ¯”è¾ƒé«˜**ï¼Œ
å¹¶ä¸”TGã€FOBOSä¹Ÿéƒ½èƒ½åœ¨ç¨€ç–æ€§ä¸Šå¾—åˆ°æå‡ã€‚ä½†æ˜¯æœ‰äº›å…¶å®ƒç±»å‹çš„ç®—æ³•ï¼Œä¾‹å¦‚RDAï¼Œæ˜¯ä»å¦ä¸€ä¸ªæ–¹é¢æ¥æ±‚è§£ Online Optimization,
å¹¶ä¸”æ›´æœ‰æ•ˆåœ°æå‡äº†ç‰¹å¾æƒé‡çš„ç¨€ç–æ€§ã€‚

#### 3.3.1 RDAç®—æ³•åŸç†

åœ¨RDAä¸­ï¼Œç‰¹å¾æƒé‡çš„æ›´æ–°ç­–ç•¥ä¸º:
$$
w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \psi (w) + \frac{\beta^{(t)}}{t} h(w) \rbrace
\tag{3.3-1}
$$

å…¶ä¸­ï¼Œ$< G^{(r)}, w >$è¡¨ç¤ºæ¢¯åº¦ $G^{(r)}$å¯¹ $w$ çš„ç§¯åˆ†å¹³å‡å€¼(ç§¯åˆ†ä¸­å€¼); $\psi (w)$ ä¸ºæ­£åˆ™é¡¹ï¼›$h(w)$ä¸ºä¸€ä¸ªè¾…åŠ©çš„ä¸¥æ ¼å‡¸å‡½æ•°ï¼›
$ \lbrace \beta^{(t)} | t \ge 1 \rbrace $æ˜¯ä¸€ä¸ªéè´Ÿä¸”éè‡ªå‡åºåˆ—ã€‚æœ¬è´¨ä¸Šï¼Œå…¬å¼$(3.3-1)$ä¸­åŒ…å«äº†3ä¸ªéƒ¨åˆ†ï¼š
- çº¿æ€§å‡½æ•° $ \frac{1}{t} \sum_{r=1}^t < G^{(t)}, w > $, åŒ…å«äº†ä¹‹å‰æ‰€æœ‰æ¢¯åº¦(æˆ–æ¬¡æ¢¯åº¦)çš„å¹³å‡å€¼(dual average);
- æ­£åˆ™é¡¹ $\psi (w)$;
- é¢å¤–æ­£åˆ™é¡¹ $\frac{\beta^{(t)}}{t} h(w)$, è¿™æ˜¯ä¸ªä¸¥æ ¼å‡¸å‡½æ•°;

#### 3.3.2 L1-RDA

ä»¤$\psi (w) = \lambda || w ||\_1$ï¼Œå¹¶ä¸”ç”±äº$h(w)$æ˜¯ä¸€ä¸ªå…³äº$w$çš„ä¸¥æ ¼å‡¸å‡½æ•°ï¼Œä¸å¦¨ä»¤ $h(w) = \frac{1}{2} || w ||\_2^2 $,
æ­¤å¤–ï¼Œå°†éè´Ÿéè‡ªå‡åºåˆ— $ \lbrace \beta^{(t)} | t \ge 1 \rbrace $ å®šä¹‰ä¸º $ \beta^{(t)} = \gamma \sqrt{t} $ï¼Œå°†L1æ­£åˆ™åŒ–ä»£å…¥å…¬å¼$(3.3-1)$æœ‰:
$$
w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \lambda \| w \|_1 + \frac{\gamma}{2 \sqrt{t}} \| w \| _2^2 \rbrace
\tag{3.3-2}
$$

é’ˆå¯¹ç‰¹å¾æƒé‡çš„å„ä¸ªç»´åº¦å°†å…¶æ‹†è§£æˆ N ä¸ªç‹¬ç«‹çš„æ ‡é‡æœ€å°åŒ–é—®é¢˜:
$$
minimize_{w_i \in \mathbb{R}} \lbrace \bar{g}_i^{(t)} w_i + \lambda \mid w_i \mid + \frac{\gamma}{2\sqrt{t}} w_i^2 \rbrace
\tag{3.3-3}
$$

è¿™é‡Œ $ \lambda > 0, \frac{\gamma}{\sqrt{t}} > 0, \bar{g}\_i^{(t)} = \frac{1}{t} \sum\_{r=1}^t g\_i^{(r)} $, 
å…¬å¼$(3.3-3)$å°±æ˜¯ä¸€ä¸ªæ— çº¦æŸçš„éå¹³æ»‘æœ€ä¼˜åŒ–é—®é¢˜ã€‚å…¶ä¸­ç¬¬2é¡¹ $\lambda | w_i |$ åœ¨ $w_i$ å¤„ä¸å¯å¯¼ã€‚

å‡è®¾ $w\_i^{\ast}$ æ˜¯å…¶æœ€ä¼˜è§£ï¼Œå¹¶ä¸”å®šä¹‰ $\xi \in \partial | w\_i^{\ast} $ ä¸º $ | w\_i | $ åœ¨ $w\_i^{\ast}$ çš„æ¬¡å¯¼æ•°ï¼Œé‚£ä¹ˆæœ‰ï¼š

![derived](/posts_res/2019-06-05-Optimization-Method/9.png)

ä¹‹åå¯ä»¥å¾—åˆ°L1-RDAç‰¹å¾æƒé‡çš„å„ä¸ªç»´åº¦æ›´æ–°çš„æ–¹å¼ä¸ºï¼š
$$
w_i^{(t+1)} = 
\begin{cases}
0 & \quad if \quad \mid \bar{g}_i^{(t)} \mid < \lambda \\
-\frac{\sqrt{t}}{\gamma} ( \bar{g}_i^{(t)} - \lambda \cdot sgn( \bar{g}_i^{(t)} ) ) & \quad otherwise
\end{cases}
\tag{3.3-6}
$$

è¿™é‡Œå‘ç°ï¼Œå½“æŸä¸ªç»´åº¦ä¸Šç´¯ç§¯æ¢¯åº¦å¹³å‡å€¼çš„ç»å¯¹å€¼ $ | g_i^{(t)} | $ å°äºé˜ˆå€¼ğœ†çš„æ—¶å€™ï¼Œè¯¥ç»´åº¦æƒé‡å°†è¢«ç½® $0$ï¼Œç‰¹å¾æƒé‡çš„ç¨€ç–æ€§ç”±æ­¤äº§ç”Ÿã€‚

æ ¹æ®å…¬å¼ $(3.3-6)$ï¼Œå¯ä»¥è®¾è®¡å‡º L1-RDA çš„ç®—æ³•é€»è¾‘:

![rda](/posts_res/2019-06-05-Optimization-Method/10.png)


#### 3.3.3 L1-RDAä¸L1-FOBOSçš„æ¯”è¾ƒ

åœ¨ $3.2.2$ ä¸­æˆ‘ä»¬çœ‹åˆ°äº† L1-FOBOS å®é™…ä¸Šæ˜¯ TG çš„ä¸€ç§ç‰¹æ®Šå½¢å¼, åœ¨ L1-FOBOS ä¸­ï¼Œè¿›è¡Œ **æˆªæ–­** çš„åˆ¤å®šæ¡ä»¶æ˜¯
$ | w\_i^{(t)} - \eta^{(t)} g\_i^{(t)} | \le \lambda^{(t)}\_{TG} = \eta^{(t+0.5)} \lambda $ ã€‚
é€šå¸¸ä¼šå®šä¹‰ $\eta$ ä¸ºä¸ $1$ æ­£ç›¸å…³çš„å‡½æ•° $(\eta = \Theta (\frac{1}{\sqrt{t}}))$, 
å› æ­¤ L1-FOBOS çš„**æˆªæ–­é˜ˆå€¼**ä¸º $\Theta (\frac{1}{\sqrt{t}})) \lambda $, éšç€ $t$ çš„å¢åŠ , è¿™ä¸ªé˜ˆå€¼ä¼šé€æ¸é™ä½ã€‚

ç›¸æ¯”è¾ƒè€Œè¨€ï¼Œä»$(3.3-6)$å¯ä»¥çœ‹å‡º, L1-RDA çš„**æˆªæ–­é˜ˆå€¼**ä¸º $\lambda$, æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¹¶ä¸éšç€ $t$ è€Œå˜åŒ–, 
å› æ­¤å¯ä»¥è®¤ä¸º L1-RDA æ¯” L1-FOBOS åœ¨æˆªæ–­åˆ¤å®šä¸Šæ›´åŠ æ¿€è¿›, è¿™ç§æ€§è´¨ä½¿å¾— L1-RDA æ›´å®¹æ˜“äº§ç”Ÿç¨€ç–æ€§;
æ­¤å¤–ï¼ŒRDA ä¸­åˆ¤å®šå¯¹è±¡æ˜¯æ¢¯åº¦çš„ç´¯åŠ å¹³å‡å€¼ $\bar{g}\_i^{(t)}$, ä¸åŒäº TG æˆ– L1-FOBOS ä¸­é’ˆå¯¹å•æ¬¡æ¢¯åº¦è®¡ç®—çš„ç»“æœè¿›è¡Œåˆ¤å®š,
é¿å…äº†ç”±äºæŸäº›ç»´åº¦ç”±äºè®­ç»ƒä¸è¶³å¯¼è‡´æˆªæ–­çš„é—®é¢˜ã€‚å¹¶ä¸”é€šè¿‡è°ƒèŠ‚ $\lambda$ ä¸€ä¸ªå‚æ•°ï¼Œå¾ˆå®¹æ˜“åœ¨ç²¾åº¦å’Œç¨€ç–æ€§ä¸Šè¿›è¡Œæƒè¡¡


### 3.4 FTRL

FTRLç»¼åˆäº†L1-FOBOSåŸºäºæ¢¯åº¦ä¸‹é™æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„ç²¾åº¦ã€L1-RDAèƒ½åœ¨æŸå¤±ä¸€å®šç²¾åº¦çš„æƒ…å†µä¸‹äº§ç”Ÿæ›´å¥½çš„ç¨€ç–æ€§ã€‚

#### 3.4.1 L1-FOBOSå’ŒL1-RDAåœ¨å½¢å¼ä¸Šçš„ç»Ÿä¸€

L1-FOBOSåœ¨å½¢å¼ä¸Šï¼Œä»¤ $ \eta^{(t+0.5)} = \eta^{(t)} = \Theta ( \frac{1}{\sqrt{t}} ) $ æ˜¯ä¸€ä¸ªéš $t$å˜åŒ–çš„éå¢æ­£åºåˆ—, 
æ¯æ¬¡è¿­ä»£éƒ½å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$$
w^{(t+0.5)} = w^{(t)} - \eta^{(t)} G^{(t)}
$$

$$
w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t + 0.5)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace
$$

æŠŠè¿™ä¸¤ä¸ªå…¬å¼åˆå¹¶åˆ°ä¸€èµ·ï¼Œæœ‰:
$$
w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t)} + \eta^{(t)} G^{(t)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace
$$

é€šè¿‡è¿™ä¸ªå…¬å¼å¾ˆéš¾ç›´æ¥æ±‚å‡º $w^{(t+1)}$ çš„è§£æè§£ï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥æŒ‰ç»´åº¦å°†å…¶åˆ†è§£ä¸º N ä¸ªç‹¬ç«‹çš„æœ€ä¼˜åŒ–æ­¥éª¤:
$$
\begin{align*}
æœ€ä¼˜åŒ– &= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)} + \eta^{(t)} g_i^{(t)} )^2 + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\
&= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)})^2 + \frac{1}{2} ( \eta^{(t)} g_i^{(t)} )^2 + w_i \eta^{(t)} g_i^{(t)} + w_i^{(t)} \eta^{(t)} g_i^{(t)} + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\
&= minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid + \frac{1}{2} \eta^{(t)} (w_i - w_i^{(t)})^2 + \lbrack \frac{ \eta^{(t)} }{2} (g_i^{(t)})^2 + w_i^{(t)} g_i^{(t)} \rbrack  \rbrace
\end{align*}
$$

ç”±äº $\frac{ \eta^{(t)} }{2} (g\_i^{(t)})^2 + w\_i^{(t)} g\_i^{(t)}$ ä¸å˜é‡ $ w_i $ æ— å…³ï¼Œå› æ­¤ä¸Šå¼å¯ä»¥ç­‰ä»·äº:
$$
minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid \frac{1}{2 \eta^{(t)} (w_i - w_i^{(t)})^2 }  \rbrace
$$

å†å°†è¿™ N ä¸ªç‹¬ç«‹æœ€ä¼˜åŒ–å­æ­¥éª¤åˆå¹¶ï¼Œé‚£ä¹ˆ L1-FOBOS å¯ä»¥å†™ä½œ:
$$
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2 \eta^{(t)}} | w - w^{(t)} |_2^2 \rbrace
$$

è€Œå¯¹äº L1-RDA çš„å…¬å¼$(3.3.2-1)$ï¼Œæˆ‘ä»¬å¯ä»¥å†™ä½œ:
$$
w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + t \lambda |w|_1 +  \frac{1}{2 \eta^{(t)}} | w - 0 |_2^2 \rbrace
$$

è¿™é‡Œ $G^{(1:t)} = \sum_{s=1}^t G^{(s)}$; å¦‚æœä»¤ $ \sigma^{(s)} = \frac{1}{\eta^{(s)}} - \frac{1}{\eta^{(s-1)}}, \sigma^{(1:t)} = \frac{1}{\eta^{(t)}} $, ä¸Šé¢ä¸¤ä¸ªå¼å­å¯ä»¥å†™åš:
$$
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- w^{(t)} | _2^2  \rbrace
\tag{3.4.1-1}
$$

$$
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + t \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- 0 | _2^2  \rbrace
\tag{3.4.1-2}
$$

æ¯”è¾ƒ$(3.4.1-1)$å’Œ$(3.4.1-2)$è¿™ä¸¤ä¸ªå…¬å¼ï¼Œå¯ä»¥çœ‹å‡º L1-FOBOS å’Œ L1-RDA çš„åŒºåˆ«åœ¨äº:
- (1) å‰è€…å¯¹è®¡ç®—çš„æ˜¯ç´¯åŠ æ¢¯åº¦ä»¥åŠ L1 æ­£åˆ™é¡¹åªè€ƒè™‘å½“å‰æ¨¡çš„è´¡çŒ®ï¼Œè€Œåè€…é‡‡ç”¨äº†ç´¯åŠ çš„å¤„ç†æ–¹å¼;
- (2) å‰è€…çš„ç¬¬ä¸‰é¡¹é™åˆ¶$ w $çš„å˜åŒ–ä¸èƒ½ç¦»å·²è¿­ä»£è¿‡çš„è§£å¤ªè¿œï¼Œè€Œåè€…åˆ™é™åˆ¶ $w$ ä¸èƒ½ç¦» 0 ç‚¹å¤ªè¿œ;

#### 3.4.2 FTRLç®—æ³•åŸç†

FTRL ç»¼åˆè€ƒè™‘äº† FOBOS å’Œ RDA å¯¹äºæ­£åˆ™é¡¹å’Œ$w$é™åˆ¶çš„åŒºåˆ«ï¼Œå…¶ç‰¹å¾æƒé‡çš„æ›´æ–°å…¬å¼ä¸º:
$$
w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + \lambda_1 |w|_1 + \lambda_2 |w|_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} | w - w^{(s)} |_2^2 \rbrace
\tag{3.4.2-1}
$$

æ³¨æ„ï¼Œå…¬å¼ $(3.4.2-1)$ ä¸­å‡ºç°äº†L2æ­£åˆ™é¡¹ $ \frac{1}{2} || w ||_2^2 $ï¼Œ
L2æ­£åˆ™é¡¹çš„å¼•å…¥ä»…ä»…ç›¸å½“äºå¯¹æœ€ä¼˜åŒ–è¿‡ç¨‹å¤šäº†ä¸€ä¸ªçº¦æŸï¼Œä½¿å¾—ç»“æœæ±‚è§£ç»“æœæ›´åŠ â€œå¹³æ»‘â€ã€‚

å…¬å¼$(3.4.2-1)$çœ‹ä¸Šå»å¾ˆå¤æ‚ï¼Œæ›´æ–°ç‰¹å¾æƒé‡è²Œä¼¼éå¸¸å›°éš¾çš„æ ·å­ã€‚ä¸å¦¨å°†å…¶è¿›è¡Œæ”¹å†™ï¼Œå°†æœ€åä¸€é¡¹å±•å¼€ï¼Œç­‰ä»·äºæ±‚ä¸‹é¢è¿™æ ·ä¸€ä¸ªæœ€ä¼˜åŒ–é—®é¢˜:
$$
w^{(t+1)} = argmin_w \lbrace (G^{(1:t)} - \sum_{s=1}^t \sigma^{(s)} w^{(s)} ) \cdot w + \lambda_1 |w|_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) \cdot | w |_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} |w^{(s)}|_2^2 \rbrace
$$

ç”±äº $\frac{1}{2} \sum\_{s=1}^t \sigma^{(s)} || w^{(s)} ||\_2^2$ ç›¸å¯¹äº $w$ æ¥è¯´æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œå¹¶ä¸”ä»¤ $ z^{(t)} = G^{(1:t)} - \sum\_{s=1}^t \sigma^{(s)} w^{(s)} $, ä¸Šå¼ç­‰ä»·äº:
$$
w^{(t+1)} = argmin_w \lbrace z^{(t)} \cdot w + \lambda_1 |w|_1 + \frac{1}{2} (\lambda_2 + \sum_{s=1}^t \sigma^{(s)} )  |w|_2^2 \rbrace
$$

é’ˆå¯¹ç‰¹å¾æƒé‡çš„å„ä¸ªç»´åº¦å°†å…¶æ‹†è§£æˆNä¸ªç‹¬ç«‹çš„æ ‡é‡æœ€å°åŒ–é—®é¢˜:
$$
minimize_{w_i \in \mathbb{R}} \lbrace z_i^{(t)} w_i + \lambda_1 \mid w \mid_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) w_i^2 \rbrace
$$

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸å¼$(3.3.2-2)$ç±»ä¼¼çš„ä¼˜åŒ–é—®é¢˜ï¼Œç”¨ç›¸åŒçš„åˆ†ææ–¹æ³•å¯ä»¥å¾—åˆ°:
$$
w_i^{t+1} = 
\begin{cases}
0 & \quad if \mid z_i^{(t)} \mid < \lambda \\
-( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} )^{-1} ( z_i^{(t)} - \lambda_1 sng( z_i^{(t)} ) ) & \quad otherwise
\end{cases}
\tag{3.4.2-2}
$$

ä»å¼ $(3.4.2-2)$ å¯ä»¥çœ‹å‡ºï¼Œå¼•å…¥ L2 æ­£åˆ™åŒ–å¹¶æ²¡æœ‰å¯¹ FTRL ç»“æœçš„ç¨€ç–æ€§äº§ç”Ÿä»»ä½•å½±å“ã€‚


#### 3.4.3 Per-Coordinate Learning Rates

å‰é¢ä»‹ç»äº† FTRL çš„åŸºæœ¬æ¨å¯¼ï¼Œä½†æ˜¯è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯ä¸€ç›´æ²¡æœ‰è¢«è®¨è®ºåˆ°çš„:å…³äºå­¦ä¹ ç‡ $\eta^{(t)}$ çš„é€‰æ‹©å’Œè®¡ç®—ã€‚
äº‹å®ä¸Šåœ¨ FTRL ä¸­ï¼Œæ¯ä¸ªç»´åº¦ä¸Šçš„å­¦ä¹ ç‡éƒ½æ˜¯å•ç‹¬è€ƒè™‘çš„ (Per-Coordinate Learning Rates)ã€‚

åœ¨ä¸€ä¸ªæ ‡å‡†çš„OGDé‡Œé¢ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå…¨å±€çš„å­¦ä¹ ç‡ç­–ç•¥$\eta^{(t)} = \frac{1}{\sqrt{t}}$ï¼Œè¿™ä¸ªç­–ç•¥ä¿è¯äº†å­¦ä¹ ç‡æ˜¯ä¸€ä¸ªæ­£çš„éå¢é•¿åºåˆ—ï¼Œ
å¯¹äºæ¯ä¸€ä¸ªç‰¹å¾ç»´åº¦éƒ½æ˜¯ä¸€æ ·çš„ã€‚è€ƒè™‘ç‰¹å¾ç»´åº¦çš„å˜åŒ–ç‡: å¦‚æœç‰¹å¾1 æ¯”ç‰¹å¾2 çš„å˜åŒ–æ›´å¿«ï¼Œé‚£ä¹ˆåœ¨ç»´åº¦1 ä¸Šçš„å­¦ä¹ ç‡åº”è¯¥ä¸‹é™å¾—æ›´å¿«ã€‚
æˆ‘ä»¬å¾ˆå®¹æ˜“å°±å¯ä»¥æƒ³åˆ°å¯ä»¥ç”¨æŸä¸ªç»´åº¦ä¸Šæ¢¯åº¦åˆ†é‡æ¥åæ˜ è¿™ç§å˜åŒ–ç‡ã€‚åœ¨ FTRL ä¸­ï¼Œç»´åº¦ $i$ ä¸Šçš„å­¦ä¹ ç‡æ˜¯è¿™æ ·è®¡ç®—çš„:
$$
\eta^{(t)}_i = \frac{\alpha}{ \beta + \sqrt{ \sum_{s=1}^t (g_i^{(s)})^2 } }
\tag{3.4.3-1}
$$

ç”±äº $\sigma^{(1:t)} = \frac{1}{\eta^{(t)}}$ï¼Œæ‰€ä»¥å…¬å¼$(3.4.2-2)$ä¸­ $\sum\_{s=1}^t \sigma^{(s)} = \frac{1}{\eta^{(t)}} = (\beta + \sqrt{ \sum\_{s=1}^t (g\_i^{(s)})^2 }) / \alpha$ã€‚
è¿™é‡Œçš„ $\alpha$ å’Œ $\beta$ æ˜¯éœ€è¦è¾“å…¥çš„å‚æ•°, å…¬å¼ $(3.4.2-2)$ ä¸­å­¦ä¹ ç‡å†™æˆç´¯åŠ çš„å½¢å¼ï¼Œæ˜¯ä¸ºäº†æ–¹ä¾¿ç†è§£åé¢ FTRL çš„è¿­ä»£è®¡ç®—é€»è¾‘ã€‚


#### 3.4.4 FTRLç®—æ³•é€»è¾‘

åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº† FTRL çš„ç‰¹å¾æƒé‡ç»´åº¦çš„æ›´æ–°æ–¹æ³•$å…¬å¼(3.4.2-2)$ï¼Œæ¯ä¸ªç‰¹å¾ç»´åº¦çš„å­¦ä¹ ç‡è®¡ç®—æ–¹æ³•å…¬å¼$(3.4.3-1)$ï¼Œ
é‚£ä¹ˆå¾ˆå®¹æ˜“å†™å‡º FTRL çš„ç®—æ³•é€»è¾‘, è¿™é‡Œæ˜¯æ ¹æ®$(3.4.2-2)$ å’Œ$(3.4.3-1)$ å†™çš„ L1&L2-FTRL æ±‚è§£æœ€ä¼˜åŒ–çš„ç®—æ³•é€»è¾‘ï¼Œå¦‚ä¸‹ï¼š

![fengyang](/posts_res/2019-06-05-Optimization-Method/2.png)

è€Œè®ºæ–‡`Ad Click Prediction: a View from the Trenches`ä¸­ Algorithm 1 ç»™å‡ºçš„æ˜¯ L1&L2-FTRL é’ˆå¯¹ Logistic Regression çš„ç®—æ³•é€»è¾‘:

![Ad Click Prediction](/posts_res/2019-06-05-Optimization-Method/3.png)


### 3.5 Onlineæ€»ç»“

ä»ç±»å‹ä¸Šæ¥çœ‹ï¼Œç®€å•æˆªæ–­æ³•ã€TGã€FOBOS å±äºåŒä¸€ç±»ï¼Œéƒ½æ˜¯æ¢¯åº¦ä¸‹é™ç±»çš„ç®—æ³•,å¹¶ä¸”TGåœ¨ç‰¹å®šæ¡ä»¶å¯ä»¥è½¬æ¢æˆç®€å•æˆªæ–­æ³•å’ŒFOBOS;
RDAå±äºç®€å•å¯¹å¶å¹³å‡çš„æ‰©å±•åº”ç”¨;FTRL å¯ä»¥è§†ä½œ RDA å’Œ FOBOS çš„ç»“åˆï¼ŒåŒæ—¶å…·å¤‡äºŒè€…çš„ä¼˜ç‚¹ã€‚
ç›®å‰æ¥çœ‹ï¼Œ RDA å’Œ FTRL æ˜¯æœ€å¥½çš„ç¨€ç–æ¨¡å‹ Online Training çš„ç®—æ³•ã€‚

è°ˆåˆ°é«˜ç»´é«˜æ•°æ®é‡çš„æœ€ä¼˜åŒ–æ±‚è§£ï¼Œä¸å¯é¿å…çš„è¦æ¶‰åŠåˆ°å¹¶è¡Œè®¡ç®—çš„é—®é¢˜, [å†¯æ‰¬(8119)çš„åšå®¢](http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html)è®¨è®ºäº† batch æ¨¡å¼ä¸‹çš„å¹¶è¡Œé€»è¾‘å›å½’ï¼Œå…¶å®åªè¦ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œå°±å¯ä»¥ç”¨äºå…¶å®ƒé—®é¢˜çš„æœ€ä¼˜åŒ–æ±‚è§£ã€‚
å¦å¤–ï¼Œå¯¹äº Online ä¸‹ï¼Œ[Parallelized Stochastic Gradient Descent](http://martin.zinkevich.org/publications/nips2010.pdf)ç»™å‡ºäº†ä¸€ç§å¾ˆç›´è§‚çš„æ–¹æ³•:

![Parallelized Stochastic Gradient Descent](/posts_res/2019-06-05-Optimization-Method/4.png)

å¯¹äº Online æ¨¡å¼çš„å¹¶è¡ŒåŒ–è®¡ç®—ï¼Œä¸€æ–¹é¢å¯ä»¥å‚è€ƒ ParallelSGD çš„æ€è·¯ï¼Œå¦ä¸€æ–¹é¢ä¹Ÿå¯ä»¥å€Ÿé‰´ batch æ¨¡å¼ä¸‹å¯¹é«˜ç»´å‘é‡ç‚¹ä¹˜ä»¥åŠæ¢¯åº¦åˆ†é‡å¹¶è¡Œè®¡ç®—çš„æ€è·¯ã€‚
æ€»ä¹‹ï¼Œåœ¨ç†è§£ç®—æ³•åŸç†çš„ åŸºç¡€ä¸Šå°†è®¡ç®—æ­¥éª¤è¿›è¡Œæ‹†è§£ï¼Œä½¿å¾—å„èŠ‚ç‚¹èƒ½ç‹¬è‡ªæ— å…³åœ°å®Œæˆè®¡ç®—æœ€åæ±‡æ€»ç»“æœå³å¯ã€‚


------------

> - [ä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adam](https://zhuanlan.zhihu.com/p/32230623)
> - [SGDã€Momentumã€RMSpropã€AdamåŒºåˆ«ä¸è”ç³»](https://zhuanlan.zhihu.com/p/32488889)
> - [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
> - [åœ¨çº¿æœ€ä¼˜åŒ–æ±‚è§£(Online Optimization)-å†¯æ‰¬](/posts_res/2019-06-05-Optimization-Method/online_optimization_fengyang.pdf)
> - [æ¯”Momentumæ›´å¿«ï¼šæ­å¼€Nesterov Accelerated Gradientçš„çœŸé¢ç›®](https://zhuanlan.zhihu.com/p/22810533)
> - [æ·±åº¦å­¦ä¹ æœ€å…¨ä¼˜åŒ–æ–¹æ³•æ€»ç»“æ¯”è¾ƒï¼ˆSGD,Adagrad,Adadelta,Adam,Adamax,Nadamï¼‰](https://zhuanlan.zhihu.com/p/22252270)
> - [Deep Learning æœ€ä¼˜åŒ–æ–¹æ³•ä¹‹AdaGrad](https://zhuanlan.zhihu.com/p/29920135)
> - [Ad_Click_Prediction_a_View_from_the_Trenches](/posts_res/2019-06-05-Optimization-Method/Ad Click Prediction- a View from the Trenches.pdf)
> - [å„å¤§å…¬å¸å¹¿æ³›ä½¿ç”¨çš„åœ¨çº¿å­¦ä¹ ç®—æ³•FTRLè¯¦è§£](https://www.cnblogs.com/EE-NovRain/p/3810737.html)

