<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Good Project To Learn]]></title>
    <url>%2F2038%2F10%2F10%2FGood-Project-To-Learn%2F</url>
    <content type="text"><![CDATA[看透论文的整个流程TO Learn CMake入门实战 google-research/bert brightmart/albert_zh facebookresearch/dlrm shenweichen/DeepCTR ddbourgin/numpy-ml hexiangnan/neural_factorization_machine ChenglongChen/tensorflow-DeepFM suvigyavijay/DeepRecommender HIT-SCIR/ELMoForManyLangs robi56/Deep-Learning-for-Recommendation-Systems alibaba/x-deeplearning LLSean/data-mining yumeng5/Spherical-Text-Embedding xuwei517/FlinkExample Johnson0722/CTR_Prediction jdwittenauer/ipython-notebooks Summary Projects weiguozhao/LeetCodes 自己刷的LeetCode LeetCode大佬的azl397985856/leetcode wzhe06/Reco-papers 推荐系统相关经典论文、资源等 princewen/tensorflow_practice TF实战练习，包括强化学习、推荐系统、NLP等 kon9chunkit/GitHub-Chinese-Top-Charts GitHub中文排行榜 IBBD技术博客 涉及大数据、前后端、后台、其他工具等 Recommendation System Factorization Machines fm_tensorflow_code Natural Language Processing DSSM论文阅读与总结 微软深度网络的语义模型 CV ImageNet Classification with Deep Convolutional Neural Networks 经典CNN论文 Machine Learning danielsabinasz/TensorSlow Python实现的TensorFlow Distribute System dmlc/ps-lite ps-lite架构的分布式 Tools &amp; Corpus fighting41love/funNLP NLP相关的很多语料库和工具 apachecn/sklearn-doc-zh 翻译的scikit-learns Contest Experiences guoday/Tencent2019_Preliminary_Rank1st 2019腾讯广告大赛冠军Code 12345 _ _ _ _ __ __ _ _ | | | | ___| | | __\ \ / /__ _ __| | __| || |_| |/ _ \ | |/ _ \ \ /\ / / _ \| &apos;__| |/ _` || _ | __/ | | (_) \ V V / (_) | | | | (_| ||_| |_|\___|_|_|\___/ \_/\_/ \___/|_| |_|\__,_|]]></content>
      <categories>
        <category>TODO</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于推荐中重排序的思考]]></title>
    <url>%2F2020%2F12%2F18%2F%E5%85%B3%E4%BA%8E%E6%8E%A8%E8%8D%90%E4%B8%AD%E9%87%8D%E6%8E%92%E5%BA%8F%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[整理自个人工作经历和相关博客、论文。1. 重排的问题定义工业推荐系统中重排的主要任务和其中存在的四个主要挑战，分别为上下文感知、排列特异性、复杂度和业务要求，分析这四个特性和达到序列收益最优的关系。 通过融入更多的信息，来修正并且得到每个商品更加准确的预估分数，并且采用基于贪婪的策略来进行排序，以期用户能够尽早地与他更感兴趣的商品进行交互。 然而，现有的基于贪婪的策略的重排方法忽略了最终推荐列表之间的上下文关系，因此不能保证其达到序列最优。 上下文感知 重排的任务设定是从上游给定的输入商品列表选择固定个数并排好序的最终推荐列表。我们把这个任务视为由多次决策顺序组成，其中每次决策是从输入商品列表中挑选一个商品放到当前位置。为了真正地形成一个上下文感知的最终推荐列表，在每次决策时，我们应当考虑到两方面的影响： 上文:当前决策应该考虑到上文对此次决策影响来做出最合理的决策。如上文推荐了一个大概率会交互的商品，是否应该出异类目的商品来缓解他对前序类目及推荐结果的疲劳度，还是让他接着浏览他感兴趣的同类目商品。 下文:当前决策应当考虑到对下文产生的深远影响。如当前决策推荐了一个他不感兴趣的商品，应该考虑到他可能因为此次决策而降低用户体验甚至停止继续浏览带来的收益损失。 排列特异性 基于上下文感知的考虑，即使是两条由同样商品集合组成的最终推荐列表，由于其排列方式导致的序列整体收益的差异我们称为排列特异性。找到最优的排列需要考虑到这种特性，然而，基于贪婪的策略陷入了无限循环的问题：上下文感知的模型为输入商品列表列表中的每个商品预估交互概率，然后基于这个分数贪婪排序得到一个新的排列，又由于排列特异性导致新的排列中每个商品的预期交互概率改变而又需要重新进行预估。无法证明这样一个循环能达到最优排列，因而破除排列特异性成为了转化上下文感知模型为生产力的关键。 复杂度 长久以来，重排的复杂度被以一个单点预估的角度来看待。考虑到排列特异性，潜在的推荐列表往往在Anm(≈mn)的量级（m、n 分别是输入商品列表和最终推荐列表的长度，如 1000、10），我们最后要挑选出的只是其中的一条序列。显然，将所有可能的候选序列都用上下文感知的模型评估是不可能的事情。如何设计快速且有效的算法和系统并兼顾上下文感知和排列特异性的特点是一个极大的挑战。 业务要求 典型业务要求的考虑，我们主要分为两部分： 打散策略-最常见的如对类目、店家等维度的打散策略，来保证最终推荐列表有较好的用户体验。因此我们需要在重排时考虑这些策略，否则即使是效率最优的最终推荐列表也不能被透出； 性能要求-云端部署的推荐系统对性能往往有很高要求，串行一个超过 10 毫秒的新链路会导致超时率上升。同时我们应该考虑，复杂的算法是否还有合适的优化方式或者链路。 2. 常见的模型类型 Point-wise 模型：和经典的 CTR 模型基本结构类似，如 DNN [8]， WDL [9] 和 DeepFM [10]。和排序相比优势主要在于实时更新的模型、特征和调控权重。LTR 对于调控权重的能力还是基于单点的，没有考虑到商品之间互相的影响，因此并不能保证整个序列收益最优。随着工程能力的升级，ODL [11] 和实时特征逐渐合并到排序阶段并且取得了较大提升。 Pair-wise 模型：通过 pair-wise 损失函数来比较商品对之间的相对关系。具体来说，RankSVM [12], GBRank [13] 和 RankNet [2] 分别使用了 SVM、GBT 和 DNN。但是，pair-wise 模型忽略了列表的全局信息，而且极大地增加了模型训练和预估的复杂度。 List-wise 模型：建模商品序列的整体信息和对比信息，并通过 list-wise 损失函数来比较序列商品之间的关系。LambdaMart [14]、MIDNN [3]、DLCM[6]、PRM [5] 和 SetRank [4] 分别通过 GBT、DNN、RNN、Self-attention 和 Induced self-attention 来提取这些信息。但是由于最终还是通过贪婪策略进行排序，还是不能真正做到考虑到排列特异性的上下文感知。随着工程能力的升级，输入序列的信息和对比关系也可以在排序阶段中提取出来。 Generative 模型：主要分为两种，一种如考虑了前序信息的，如 MIRNN [3] 和 Seq2Slate [15] 都通过 RNN 来提取前序信息，再通过 DNN 或者 Pointer-network 来从输入商品列表中一步步地生成最终推荐列表。最近的组合优化工作 Exact-K [16] 注重于直接对序列整体收益进行建模，设计了两段式结构，一个用来预测整体收益以指导另一个生成最终推荐列表。通过我们的讨论，仅考虑了前序信息是不完整的，应当全面地考虑到上下文的影响。 Diversity 模型：最近有很多工作考虑最终推荐列表里的相关性和多样性达到平衡，如 [17~20]。我们的工作区别在于，我们并不会去优化多样性指标，最终推荐列表是否表现出多样性全由效率指标决定。 3. 一些工作的总结记录feed_documents.zip detailed_documents.zip 压缩 1tar -czvf - file | openssl des3 -salt -k password -out file.tar.gz 解压 1openssl des3 -d -k password -salt -in file.tar.gz | tar xzf - 序列检索系统在淘宝首页信息流重排中的实践]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>推荐系统</tag>
        <tag>重排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gauc&timeauc]]></title>
    <url>%2F2020%2F11%2F27%2Fgauc-timeauc%2F</url>
    <content type="text"><![CDATA[1. AUCAUC是评估模型对pair数据，将正样本的预测分数大于负样本的预测分数的能力；计算方式，scala写的：123456789101112131415161718192021222324252627// 预测值 + 标签case class LabeledPred(predict: Double, label: Int)def auc(points: Seq[LabeledPred]) = &#123; val posNum = points.count(_.label &gt; 0) val negNum = points.length - posNum if (posNum == 0 || negNum == 0) &#123; println("Error: Lables of all samples are the same.") 0.0 &#125; else &#123; val sorted = points.sortBy(_.predict) var negSum = 0 // pos greater than neg var posGTNeg = 0 for (p &lt;- sorted) &#123; if (p.label &gt; 0) &#123; posGTNeg = posGTNeg + negSum &#125; else &#123; negSum = negSum + 1 &#125; &#125; posGTNeg.toDouble/(posNum * negNum).toDouble &#125;&#125; 上面表达的是按预测分升序排列(负样本在前，正样本在后)后遍历， 每遇到一个正样本，将在它之前的负样本数量加上(表示模型预测对的pair数量)， 每遇到一个负样本，则加1， 最后用预测对的pair对数量 处以 总的区分正负样本的pair对的数量，即得auc。 2. GAUC在auc的基础上，按照用户、刷次等进行分组，剔除整组都是正样本或整组都是负样本的数据， 按照展现的样本数量比例为权重，计算得到模型的结果。 1234567891011121314151617181920212223242526// 预测值 + 标签 + 用户组 case class LabeledPredDev(device: String, labeledPred: LabeledPred) /** * @param points * @return */ def gAuc(points: Seq[LabeledPredDev]) = &#123; val userMap = points.groupBy(_.device) .filterNot(_._2.forall(_.labeledPred.label &gt; 0)) // 去掉全部记录为正例的用户 .filterNot(_._2.forall(_.labeledPred.label &lt; 1)) // 去掉全部记录为负例的用户 val userCntMap = userMap.mapValues(_.size) val userAucMap = userMap.mapValues(tmp =&gt; auc(tmp.map(_.labeledPred))) val cntAucPairs = for (key &lt;- userCntMap.keys.toSeq) yield (userCntMap.getOrElse(key, 0), userAucMap.getOrElse(key, 0.0)) val sumImprs = cntAucPairs.map(_._1).sum val weightedAuc = cntAucPairs.foldLeft(0.0)&#123; (sum, pair) =&gt; &#123; pair._1 * pair._2 + sum &#125; &#125; weightedAuc.toDouble/sumImprs.toDouble &#125; 3. 逆序数其实上面auc的计算，体现的也是逆序数的逻辑。 正序数、逆序数可作为模型效果的衡量指标。当样本的之间存在序关系时，由样本间两两组成的，若模型预测结果的序关系与之间的序关系相同，称为正序；若模型预测结果的序关系与之间的序关系相反，称为逆序。当正序数量越多、逆序数量越少时，表明模型对序关系的刻画越准确，模型效果越好。正逆序即为正序数量与逆序数量的比值。 逆序数的计算，可以参考LeetCode上的这道题：计算右侧小于当前元素的个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution: def countSmallerTL(self, nums: List[int]) -&gt; List[int]: """ 暴力，O(N^2) """ length = len(nums) count = [0 for _ in range(length)] for i in range(length): if i == length - 1: break for n in nums[i+1:]: if n &lt; nums[i]: count[i] += 1 return count def countSmaller(self, nums: List[int]) -&gt; List[int]: """ 归并排序，O(NlogN) """ res = [0] * len(nums) length = len(nums) index_nums = list(zip(range(length), nums)) def merge_sort(arr): if len(arr) &lt;= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) right = merge_sort(arr[mid:]) return merge(left, right) def merge(left, right): temp = [] i, j = 0, 0 while i &lt; len(left) or j &lt; len(right): if j == len(right) or i &lt; len(left) and left[i][1] &lt;= right[j][1]: temp.append(left[i]) res[left[i][0]] += j i += 1 else: temp.append(right[j]) j+= 1 return temp merge_sort(index_nums) return res 4. TimeAUC对于点击，我们可以直接使用auc、gauc来进行计算，衡量模型的排序能力。 对于时长目标的时候，我们同样可以参考逆序数和auc的计算逻辑，构造一个timeauc的指标衡量模型对有点击行为的时长预测的能力。 计算有点击的时长的时候，需要将时长等于0的样本过滤掉， timeAuc: 计算逆序数累加和(模型预测错误的数量) 1.0 - 逆序数累加和 / pair总数 其实在真正计算逆序率的时候，是根据肯德尔相关系数来计算的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import numpy as npfrom scipy.stats._stats import _kendall_disdef inverse_ratio(x, y): """ x = y_true, duration y = y_pred, prob_dur """ n = len(x) tot = n * (n - 1) // 2 perm = np.argsort(y) # sort on y and convert y to dense ranks x, y = x[perm], y[perm] y = np.r_[True, y[1:] != y[:-1]].cumsum(dtype=np.intp) # stable sort on x and convert x to dense ranks perm = np.argsort(x, kind='mergesort') x, y = x[perm], y[perm] x = np.r_[True, x[1:] != x[:-1]].cumsum(dtype=np.intp) dis = _kendall_dis(x, y) obs = np.r_[True, (x[1:] != x[:-1]) | (y[1:] != y[:-1]), True] cnt = np.diff(np.nonzero(obs)[0]).astype('int64', copy=False) ntie = (cnt * (cnt - 1) // 2).sum() # joint ties xtie = count_rank_tie(x) # ties in x, stats ytie = count_rank_tie(y) # ties in y, stats if xtie == tot or ytie == tot: return np.nan # Note that tot = con + dis + (xtie - ntie) + (ytie - ntie) + ntie # = con + dis + xtie + ytie - ntie # con_minus_dis = tot - xtie - ytie + ntie - 2 * dis # tau = con_minus_dis / np.sqrt(tot - xtie) / np.sqrt(tot - ytie) # Limit range to fix computational errors # tau = min(1., max(-1., tau)) con = tot - dis - xtie - ytie + ntie inv_ratio = dis / (con + dis) if con + dis == 0: inv_ratio = np.nan # tau = np.nan return inv_ratiodef count_rank_tie(ranks): cnt = np.bincount(ranks).astype('int64', copy=False) cnt = cnt[cnt &gt; 1] return (cnt * (cnt - 1) // 2).sum() 同样的也可以计算GroupTimeAUC AUC的理解与计算 模型排序中的逆序对计算]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>GAUC</tag>
        <tag>TimeAUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformer学习]]></title>
    <url>%2F2020%2F09%2F18%2FTransformer%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[视频教程： Transformer-李宏毅老师 转自： 10分钟带你深入理解Transformer原理及实现 0. 模型架构今天的示例任务为中译英: 中文输入为 我爱你，通过 Transformer 翻译为 I Love You。Transformer 中对应的超参数包括： $N$ $d_{model}$ $d_{ff}$ $h$ $d_k$ $d_v$ $P_{drop}$ $\epsilon_{ls}$ $trainSteps$ 6 512 2048 8 64 64 0.1 0.1 100K 这些也是函数 make_model(src_vocal, tgt_vocab, N=6, d_model=512, d_ff = 2048, h=8, dropout=0.1) 使用的超参数。 整个架构猛一看是挺吓人的，首先还是需要将整个 Transformer 拆分进行描述： Embedding 部分 Encoder 部分 Decoder 部分 1. 对 Input 和 Output 进行 representation1.1 对 Input 的 represent首先用常用来表达 categorical 特征的方法即one-hot encoding 对句子进行表达。 one-hot 指的是一个向量只有一个元素是1，其余的都为0。 很直接的，vector 的长度就是由词汇表 vocabulary 的长度决定的。 如果想要表达10000个word，那么就需要10000维的向量。 1.2 word embedding但我们不直接给 Transformer 输入简单的one-hot vector，原因包括这种表达方式的结果非常稀疏，非常大，且不能表达 word 与 word 之间的特征。 所以这里对词进行 embedding，用较短的向量表达这个 word 的属性。 一般在 Pytorch/tensorflow 中，我们都是用 nn.Embedding 来做，或者直接用 one-hot vector 与权重矩阵 $W$ 相乘得到。 nn.Embedding 包含一个权重矩阵 $W$，对应的 shape 为 ( num_embeddings，embedding_dim )。 num_embeddings 指的是词汇量，即想要翻译的 vocabulary 的长度。 embedding_dim 指的是想用多长的 vector 来表达一个词，可以任意选择，比如64，128，256，512等。 在 Transformer 论文中选择的是512(即 d_model = 512)。 其实可以形象地将 nn.Embedding 理解成一个 lookup table，里面对每一个 word 都存了向量 vector 。 给任意一个 word，都可以从表中查出对应的结果。 处理 nn.Embedding 权重矩阵有两种选择： 使用 pre-trained 的 embeddings 并固化，这种情况下实际就是一个 lookup table。 对其进行随机初始化(当然也可以选择 pre-trained 的结果)，但设为 trainable。这样在 training 过程中不断地对 embeddings 进行改进。 Transformer 选择的是后者。 在 Annotated Transformer 中，class “Embeddings“ 用来生成 word 的embeddings，其中用到 nn.Embedding。具体实现见下： 12345678class Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) 1.3 Positional Embedding我们对每一个 word 进行 embedding 作为 input 表达。 但是还有问题，embedding 本身不包含在句子中的相对位置信息。 那 RNN 为什么在任何地方都可以对同一个 word 使用同样的向量呢？ 因为 RNN 是按顺序对句子进行处理的，一次一个 word。 但是在 Transformer 中，输入句子的所有 word 是同时处理的，没有考虑词的排序和位置信息。 对此，Transformer 的作者提出了加入 positional encoding 的方法来解决这个问题。 positional encoding 使得 Transformer 可以衡量 word 位置有关的信息。 positional encoding 与 word embedding 相加就得到 embedding with position。 那么具体 positional encoding 怎么做？为什么能表达位置信息呢？ 作者探索了两种创建 positional encoding 的方法： 通过训练学习 positional encoding 向量 使用公式来计算 positional encoding向量 试验后发现两种选择的结果是相似的，所以采用了第2种方法，优点是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用。 计算 positional encoding 的公式为： 在这个公式中: pos 指的是这个 word 在这个句子中的位置 i指的是 embedding 维度。比如选择 d_model=512，那么i就从1数到512 为什么选择 sin 和 cos ？positional encoding 的每一个维度都对应着一个正弦曲线，作者假设这样可以让模型相对轻松地通过对应位置来学习。 在 Annotated Transformer 中，使用 class Positional Encoding 来创建 positional encoding 并加入到 word embedding 中： 12345678910111213141516171819class PositionalEncoding(nn.Module): "Implement the PE function." def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer('pe', pe) def forward(self, x): x = x + Variable(self.pe[:, :x.size(1)],requires_grad=False) return self.dropout(x) 波的频率和偏移对于每个维度是不同的： 1.4 Input 小总结经过 word embedding 和 positional embedding 后可以得到一个句子的 representation， 比如”我爱你“这个句子，就被转换成了三个向量，每个向量都包含 word 的特征和 word 在句子中的位置信息： 我们对输出的结果做同样的操作，这里即中英翻译的结果 I Love You。 使用word embedding 和 positional encoding 对其进行表示。 Input Tensor 的 size 为 [nbatches, L, 512]： nbatches 指的是定义的 batch_size L 指的是 sequence 的长度,(比如“我爱你”，L = 3) 512 指的是 embedding 的 dimension 目前完成了模型架构的底层的部分： 2. EncoderEncoder 相对 Decoder 会稍微麻烦一些。 Encoder 由 6 个相乘的 Layer 堆叠而成（6并不是固定的，可以基于实际情况修改），看起来像这样： 每个 Layer 包含 2 个 sub-layer： 第一个是 multi-head self-attention mechanism 第二个是 simple，position-wise fully connected feed-forward network Annotated Transformer 中的 Encoder 实现代码： 123456789101112131415161718192021222324252627class Encoder(nn.Module): "Core encoder is a stack of N layers" def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): "Pass the input (and mask) through each layer in turn." for layer in self.layers: x = layer(x, mask) return self.norm(x)class EncoderLayer(nn.Module): "Encoder is made up of self-attn and feed forward (defined below)" def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): "Follow Figure 1 (left) for connections." x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) class Encoder 将 $layer$ 堆叠 N 次。是 class EncoderLayer 的实例。 EncoderLayer 初始化需要指定 size, self_attn, feed _forward, dropout size 对应 $d_model$，论文中为512 self_attn 是 class MultiHeadedAttention 的实例，对应sub-layer 1 feed_forward 是 class PositionwiseFeedForward 的实例，对应sub-layer 2 dropout 对应 dropout rate 2.1 Encoder Sub-layer 1: Multi-Head Attention Mechanism理解 Multi-Head Attention 机制对于理解 Transformer 特别重要，并且在 Encoder 和 Decoder 中都有用到。 概述： 我们把 attention 机制的输入定义为 $x$。$x$ 在 Encoder 的不同位置，含义有所不同。 在 Encoder 的开始，$x$ 的含义是句子的 representation。 在 EncoderLayer 的各层中间，$x$ 代表前一层 EncoderLayer 的输出。 使用不同的 linear layers 基于 $x$ 来计算 keys，queries和values： key = linear_k(x) query = linear_q(x) value = linear_v(x) linear_k, linear_q, linear_v 是相互独立、权重不同的。 计算得到 keys(K), queries(Q)和values(V) 值之后，按论文中如下公式计算 Attention： Attention(Q, K, V) = softmax ( \frac{QK^T}{\sqrt(d_k)} ) V矩阵乘法表示： 这里的比较奇怪的地方是为啥要除以 $\sqrt(d_k)$ 对吧？ 作者的解释是说防止 $d_k$ 增大时，$QK^T$ 点积值过大，所以用 $\sqrt(d_k)$ 对其进行缩放。引用一下原文: We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 对 $\frac{QK^T}{\sqrt(d_k)}$ 取 softmax 之后值都介于0到1之间，可以理解成得到了 attention weights。 然后基于这个 attention weights 对 $V$ 求 weighted sum 值 $Attention(Q, K, V)$。 详细解释： Annotated Transformer 中 Multi-Headed attention 的实现为 1234567891011121314151617181920212223242526272829303132class MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): "Take in model size and number of heads." super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): "Implements Figure 2" if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) Do all the linear projections in batch from d_model =&gt; h x d_k query, key, value = \ [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))] # 2) Apply attention on all the projected vectors in batch. x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # 3) "Concat" using a view and apply a final linear. x = x.transpose(1, 2).contiguous() \ .view(nbatches, -1, self.h * self.d_k) return self.linears[-1](x) 这个 class 进行实例化时需要指定: h = 8，即 heads 的数目。在 Transformer 的 base model 中有 8 heads d_model = 512 dropout = dropoutRate = 0.1 keys 的维度 $d_k$ 是基于 $d_{model} / k$ 计算来的。在上面的例子中 $d_k = 512 / 8 = 64$。 下面分3步详细介绍一下 MultiHeadedAttention 的 forward() 函数： 从上面的代码看出，forward 的 input 包括：query，key，values和mask。 这里先暂时忽略 mask。 query，key和value 是哪来的？ 实际上他们是 $x$ 重复了三次得来的，$x$ 或者是初始的句子embedding或者是前一个 EncoderLayer 的输出， 见 EncoderLayer 的代码红色方框部分，self.self_atttn 是 MultiHeadedAttention 的一个实例化： query 的 shape 为 [nbatches, L, 512] ,其中: nbatches 对应 batch size L 对应 sequence length ，512 对应 d_mode key 和 value 的 shape 也为 [nbatches， L， 512] Step 1 对 “query”，“key”和“value”进行 linear transform ，他们的 shape 依然是[nbatches， L， 512]。 对其通过 view() 进行 reshape，shape 变成 [nbatches, L, 8, 64]。这里的h=8对应 heads 的数目，d_k=64 是 key 的维度。 transpose 交换 dimension1和2，shape 变成 [nbatches， 8, L 64]。 Step 2前面提到我们计算 attention 的公式： Attention(Q, K, V) = softmax ( \frac{QK^T}{\sqrt(d_k)} ) VAnnotated Transformer 中的 attention() 代码为： 1234567891011def attention(query, key, value, mask=None, dropout=None): "Compute 'Scaled Dot Product Attention'" d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) \ / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = F.softmax(scores, dim = -1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn query 和 key.transpose(-2, -1) 相乘，两者分别对应的 shape 为 [nbatches, 8, L 64] 和 [nbatches， 8， 64， L]。 这样相乘得到的结果 scores 的 shape为[nbatches, 8, L, L]。 对 scores 进行 softmax，所以 p_attn 的 shape 为 [nbatches, 8, L, L]。 values的 shape 为 [nbatches, 8, L, 64]。 所以最后 p_attn 与 values 相乘输出的 result 的 shape 为 [nbatches, 8, L, 64]。 在我们的输入与输出中，有8个 heads 即 Tensor 中的 dimension 1，[ nbatches, 8, L, 64 ]。 8个 heads 都进行了不同的矩阵乘法，这样就得到了不同的 representation subspace。 这就是 multi-headed attention 的意义。 Step 3x的初始shape为 [ nbatches, 8, L, 64 ]，x.transpose(1,2) 得到 [ nbatches，L, 8,64 ]。 然后使用 view 进行 reshape 得到 [ nbatches, L, 512 ]。可以理解为8个heads结果的 concatenate 。 最后使用 last linear layer 进行转换。shape仍为 [ nbatches, L, 512 ]。与input时的shape是完全一致的。 可视化见论文中的图例： 2.2 Encoder Sub-layer 2: Position-Wise fully connected feed-forward networkSubLayer-2 只是一个 feed-forward network。比较简单。 FFN(x) = max (0, x W_1 + b_1) W_2 + b_2在 Annotated Transformer 中对应的实现为： 12345678910class PositionwiseFeedForward(nn.Module): "Implements FFN equation." def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(F.relu(self.w_1(x)))) 2.3 Encoder short summaryEncoder 总共包含6个 EncoderLayers 。每一个 EncoderLayer 包含2个 SubLayer： SubLayer-1 做 Multi-Headed Attention SubLayer-2 做 feedforward neural network 3. The Decoder Encoder 与 Decoder 的交互方式可以理解为： Decoder 也是N层堆叠的结构。被分为3个 SubLayer，可以看出 Encoder 与 Decoder 三大主要的不同： Diff_1：Decoder SubLayer-1 使用的是 “masked” Multi-Headed Attention 机制，防止为了模型看到要预测的数据，防止泄露。 Diff_2：SubLayer-2 是一个 encoder-decoder multi-head attention。 Diff_3：LinearLayer 和 SoftmaxLayer 作用于 SubLayer-3 的输出后面，来预测对应的 word 的 probabilities 。 3.1 Diff_1 : “masked” Multi-Headed Attentionmask 的目标在于防止 decoder “seeing the future”，就像防止考生偷看考试答案一样。mask包含1和0： Attention 中使用 mask 的代码中： 12if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) 引用作者的话说 We […] modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Diff_2 : encoder-decoder multi-head attentionAnnotated Transformer 中的 DecoderLayer 的实现为： 123456789101112131415class DecoderLayer(nn.Module): "Decoder is made of self-attn, src-attn, and feed forward (defined below)" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) 重点在于 x = self.sublayer1 self.src_attn 是 MultiHeadedAttention 的一个实例。 query = x，key = m, value = m, mask = src_mask，这里x来自上一个 DecoderLayer，m来自 Encoder的输出。 到这里 Transformer 中三种不同的 Attention 都已经集齐了： 3.3 Diff_3 : Linear and Softmax to Produce Output Probabilities最后的 linear layer 将 decoder 的输出扩展到与 vocabulary size 一样的维度上。 经过 softmax 后，选择概率最高的一个 word 作为预测结果。 假设我们有一个已经训练好的网络，在做预测时，步骤如下： 给 decoder 输入 encoder 对整个句子 embedding 的结果 和一个特殊的开始符号 &lt;/s&gt;。decoder 将产生预测，在我们的例子中应该是 I。 给 decoder 输入 encoder 的 embedding 结果和 &lt;/s&gt;I，在这一步 decoder 应该产生预测 Love。 给 decoder 输入 encoder 的 embedding 结果和 &lt;/s&gt;I Love，在这一步 decoder 应该产生预测 China。 给 decoder 输入 encoder 的 embedding 结果和 &lt;/s&gt;I Love China, decoder应该生成句子结尾的标记，decoder 应该输出 &lt;/eos&gt;。 然后 decoder 生成了 &lt;/eos&gt;，翻译完成。 但是在训练过程中，decoder 没有那么好时，预测产生的词很可能不是我们想要的。 这个时候如果再把错误的数据再输给 decoder，就会越跑越偏： 这里在训练过程中要使用到 teacher forcing。利用我们知道他实际应该预测的 word 是什么，在这个时候喂给他一个正确的结果作为输入。 相对于选择最高的词 (greedy search)，还有其他选择是比如 beam search，可以保留多个预测的 word。 Beam Search 方法不再是只得到一个输出放到下一步去训练了，我们可以设定一个值，拿多个值放到下一步去训练， 这条路径的概率等于每一步输出的概率的乘积，具体可以参考李宏毅老师的课程。 或者 “Scheduled Sampling”：一开始我们只用真实的句子序列进行训练，而随着训练过程的进行，我们开始慢慢加入模型的输出作为训练的输入这一过程。 这部分对应 Annotated Transformer 中的实现为： 12345678class Generator(nn.Module): "Define standard linear + softmax generation step." def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) The Annotated Transformer Attention is All You Need The Illustrated Transformer]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>Attention</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LearnToRank的由来和分类]]></title>
    <url>%2F2020%2F09%2F18%2FLearnToRank%E7%9A%84%E7%94%B1%E6%9D%A5%E5%92%8C%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[学习排序 Learning to Rank：从 pointwise 和 pairwise 到 listwise，经典模型与优缺点 推荐系统中的排序学习 Ranking 是信息检索领域的基本问题，也是搜索引擎背后的重要组成模块。本文将对结合机器学习的 ranking 技术 — Learning2Rank 做个系统整理，包括 pointwise、pairwise、listwise 三大类型，它们的经典模型，解决了什么问题，仍存在什么缺陷。本文主要参考刘铁岩老师的《Learning to Rank for Information Retrieval》和李航老师的《Learning to rank for information retrieval and natural language processing》。 1. 概述1.1 RankingRanking 模型可以粗略分为基于相关度和基于重要性进行排序的两大类。 早期基于相关度的模型，通常利用 query 和 doc 之间的词共现特性（如布尔模型）、VSM（如 TFIDF、LSI 等）、概率排序思想（BM25、LMIR 等）等方式。 基于重要性的模型，利用的是 doc 本身的重要性，如 PageRank、TrustRank 等。 这里我们关注基于相关度的 ranking。 相关度的标注 最流行也相对好实现的一样方式时，人工标注 MOS，即相关度等级。 其次是，人工标注 pairwise preference，即一个 doc 是否相对另一个 doc 与该 query 更相关。 最 costly 的方式是，人工标注 docs 与 query 的整体相关度排序。 评估指标 即评估 query 与 docs 之间的真实排序与预测排序的差异。 大部分评估指标都是针对每组 query-docs 进行定义，然后再在所有组上进行平均。 常用的基于度量的 ranking 错误率如下 MAP NDCG 各种评价指标有各种指标详细说明 可以发现，这些评估指标具备两大特性： 基于 query ，即不管一个 query 对应的 docs 排序有多糟糕，也不会严重影响整体的评价过程，因为每组 query-docs 对平均指标都是相同的贡献。 基于 position ，即显式的利用了排序列表中的位置信息，这个特性的副作用就是上述指标是离散不可微的。 一方面，这些指标离散不可微，从而没法应用到某些学习算法模型上；另一方面，这些评估指标较为权威，通常用来评估基于各类方式训练出来的 ranking 模型。因此，即使某些模型提出新颖的损失函数构造方式，也要受这些指标启发，符合上述两个特性才可以。这些细节在后面会慢慢体会到。 1.2 Learning to RankLearning2Rank 即将 ML 技术应用到 ranking 问题，训练 ranking 模型。通常这里应用的是判别式监督 ML 算法。经典L2R框架如下： 特征向量 x 反映的是某 query 及其对应的某 doc 之间的相关性，通常前面提到的传统 ranking 相关度模型都可以用来作为一个维度使用。 L2R 中使用的监督机器学习方法主要是判别式类。 根据上图基本元素（输入空间、假设空间、输出空间、损失函数）方面的差异，L2R可以分为三大类，pointwise 类，pairwise 类，listwise 类。 总结如下: 2 Pointwise Approach2.1 特点Pointwise 类方法，其 L2R 框架具有以下特征： 输入空间中样本是单个 doc（和对应 query）构成的特征向量； 输出空间中样本是单个 doc（和对应 query）的相关度； 假设空间中样本是打分函数； 损失函数评估单个 doc 的预测得分和真实得分之间差异。 这里讨论下，关于人工标注标签怎么转换到 pointwise 类方法的输出空间： 如果标注直接是相关度 $s_j$，则 $doc_{x_j}$ 的真实标签定义为 $y_j = s_j$ 如果标注是 pairwise preference $s_{u,v}$，则 $doc_{x_j}$ 的真实标签可以利用该 doc 击败了其他 docs 的频次 如果标注是整体排序 $\pi$，则 $doc_{x_j}$ 的真实标签可以利用映射函数，如将 doc 的排序位置序号当作真实标签 根据使用的 ML 方法不同，pointwise 类可以进一步分成三类：基于回归的算法、基于分类的算法，基于有序回归的算法。 2.2 基于回归的算法此时，输出空间包含的是实值相关度得分。 采用 ML 中传统的回归方法即可。 2.3 基于分类的算法此时，输出空间包含的是无序类别。 对于二分类，SVM、LR 等均可；对于多分类，提升树等均可。 2.4 基于有序回归的算法此时，输出空间包含的是有序类别。 通常是找到一个打分函数，然后用一系列阈值对得分进行分割，得到有序类别。采用 PRanking、基于 margin 的方法都可以。 2.5 缺陷回顾概述中提到的评估指标应该基于 query 和 position， ranking 追求的是排序结果，并不要求精确打分，只要有相对打分即可。 pointwise 类方法并没有考虑同一个 query 对应的 docs 间的内部依赖性。一方面，导致输入空间内的样本不是 IID 的，违反了 ML 的基本假设，另一方面，没有充分利用这种样本间的结构性。其次，当不同 query 对应不同数量的 docs 时，整体 loss 将会被对应 docs 数量大的 query 组所支配，前面说过应该每组 query 都是等价的。 损失函数也没有 model 到预测排序中的位置信息。因此，损失函数可能无意的过多强调那些不重要的 docs，即那些排序在后面对用户体验影响小的 doc。 2.6 改进Pointwise 类算法也可以再改进，比如在 loss 中引入基于 query 的正则化因子的 RankCosine 方法。 3. Pairwise Approach3.1 特点Pairwise 类方法，其 L2R 框架具有以下特征： 输入空间中样本是（同一 query 对应的）两个 doc（和对应 query）构成的两个特征向量； 输出空间中样本是 pairwise preference； 假设空间中样本是二变量函数； 损失函数评估 doc pair 的预测 preference 和真实 preference 之间差异。 这里讨论下，关于人工标注标签怎么转换到 pairwise 类方法的输出空间： 如果标注直接是相关度 $s_j$，则 $doc_{pair(x_u,x_v)}$ 的真实标签定义为 $y_{u,v} = 2 * I_{s_u &gt; s_v} - 1$ 如果标注是 pairwise preference $s_{u,v}$，则 $doc_{pair(x_u,x_v)}$ 的真实标签定义为 $y_{u,v} = s_{u,v}$ 如果标注是整体排序 $\pi$，则 $doc_{pair(x_u,x_v)}$ 的真实标签定义为 $y_{u,v} = 2 * I_{π_u, π_v} - 1$ 3.2 基于二分类的算法pairwise 类方法基本就是使用二分类算法即可。 经典的算法有 基于 NN 的 SortNet，基于 NN 的 RankNet，基于 fidelity loss 的 FRank，基于 AdaBoost 的 RankBoost，基于 SVM 的 RankingSVM，基于提升树的 GBRank。 3.3 缺陷虽然 pairwise 类相较 pointwise 类 model 到一些 doc pair 间的相对顺序信息，但还是存在不少问题，回顾概述中提到的评估指标应该基于 query 和 position， 如果人工标注给定的是第一种和第三种，即已包含多有序类别，那么转化成 pairwise preference 时必定会损失掉一些更细粒度的相关度标注信息。 doc pair 的数量将是 doc 数量的二次，从而 pointwise 类方法就存在的 query 间 doc 数量的不平衡性将在 pairwise 类方法中进一步放大。 pairwise 类方法相对 pointwise 类方法对噪声标注更敏感，即一个错误标注会引起多个 doc pair 标注错误。 pairwise 类方法仅考虑了 doc pair 的相对位置，损失函数还是没有 model 到预测排序中的位置信息。 pairwise 类方法也没有考虑同一个 query 对应的 doc pair 间的内部依赖性，即输入空间内的样本并不是 IID 的，违反了 ML 的基本假设，并且也没有充分利用这种样本间的结构性。 3.4 改进pairwise 类方法也有一些尝试，去一定程度解决上述缺陷，比如： Multiple hyperplane ranker，主要针对前述第一个缺陷 magnitude-preserving ranking，主要针对前述第一个缺陷 IRSVM，主要针对前述第二个缺陷 采用 Sigmoid 进行改进的 pairwise 方法，主要针对前述第三个缺陷 P-norm push，主要针对前述第四个缺陷 Ordered weighted average ranking，主要针对前述第四个缺陷 LambdaRank，主要针对前述第四个缺陷 Sparse ranker，主要针对前述第四个缺陷 4. Listwise Approach4.1 特点Listwise 类方法，其 L2R 框架具有以下特征： 输入空间中样本是（同一 query 对应的）所有 doc（与对应的 query）构成的多个特征向量（列表）； 输出空间中样本是这些 doc（和对应 query）的相关度排序列表或者排列； 假设空间中样本是多变量函数，对于 docs 得到其排列，实践中，通常是一个打分函数，根据打分函数对所有 docs 的打分进行排序得到 docs 相关度的排列； 损失函数分成两类，一类是直接和评价指标相关的，还有一类不是直接相关的。具体后面介绍。 这里讨论下，关于人工标注标签怎么转换到 listwise 类方法的输出空间： 如果标注直接是相关度 s_j，则 doc set 的真实标签可以利用相关度 s_j 进行比较构造出排列 如果标注是 pairwise preference s{u,v}，则 doc set 的真实标签也可以利用所有 s{u,v} 进行比较构造出排列 如果标注是整体排序 π，则 doc set 则可以直接得到真实标签 根据损失函数构造方式的不同，listwise 类可以分成两类直接基于评价指标的算法，间接基于评价指标的算法。 4.2 直接基于评价指标的算法直接取优化 ranking 的评价指标，也算是 listwise 中最直观的方法。但这并不简单，因为前面说过评价指标都是离散不可微的，具体处理方式有这么几种： 优化基于评价指标的 ranking error 的连续可微的近似，这种方法就可以直接应用已有的优化方法，如SoftRank，ApproximateRank，SmoothRank 优化基于评价指标的 ranking error 的连续可微的上界，如 SVM-MAP，SVM-NDCG，PermuRank 使用可以优化非平滑目标函数的优化技术，如 AdaRank，RankGP 上述方法的优化目标都是直接和 ranking 的评价指标有关。现在来考虑一个概念，informativeness。通常认为一个更有信息量的指标，可以产生更有效的排序模型。而多层评价指标（NDCG）相较二元评价（AP）指标通常更富信息量。因此，有时虽然使用信息量更少的指标来评估模型，但仍然可以使用更富信息量的指标来作为 loss 进行模型训练。 4.3 非直接基于评价指标的算法这里，不再使用和评价指标相关的 loss 来优化模型，而是设计能衡量模型输出与真实排列之间差异的 loss，如此获得的模型在评价指标上也能获得不错的性能。 经典的如 ，ListNet，ListMLE，StructRank，BoltzRank。 4.4 缺陷listwise 类相较 pointwise、pairwise 对 ranking 的 model 更自然，解决了 ranking 应该基于 query 和 position 问题。 listwise 类存在的主要缺陷是：一些 ranking 算法需要基于排列来计算 loss，从而使得训练复杂度较高，如 ListNet和 BoltzRank。此外，位置信息并没有在 loss 中得到充分利用，可以考虑在 ListNet 和 ListMLE 的 loss 中引入位置折扣因子。 5. 总结实际上，前面介绍完，可以看出来，这三大类方法主要区别在于损失函数。不同的损失函数指引了不同的模型学习过程和输入输出空间。]]></content>
      <categories>
        <category>算法常识</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交叉熵损失函数区别及应用场景]]></title>
    <url>%2F2020%2F09%2F18%2F%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8C%BA%E5%88%AB%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[对于机器学习中常用的损失函数 tf.nn.sigmoid_cross_entropy_with_logits (Binary Cross Entropy, BCE) 和 tf.nn.softmax_cross_entropy_with_logits_v2(Cross Entropy, CE)， 想必有过时间的同学都已经能都熟练描述出他们的运行原理。但是在什么场景，用哪个损失函数呢？在不同的场景，这两个交叉熵损失又有哪些区别和联系呢？ 下面先附上 tensorflow v1.15 中的函数原型： tf.nn.sigmoid_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, name=None) tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits, axis=None, name=None, dim=None) **重点理解交叉熵计算方式，BE和BCE的计算上的区别，根据业务场景、框架api实现方式等综合考虑选择loss计算方式，不要拘泥于本文中矛盾的地方** 1. 基本概念和公式 信息量 I(x) = - log f(x) 其中 $f(x)$ 是预测为正例的概率 熵 H(x) = - \sum_{x \in X} f(x) log f(x) 其中 $f(x)$ 是预测为正例的概率，熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 相对熵（Relative Entropy，又称为KL散度Kullback-Leibler divergence） 是两个随机分布间距离的度量。记为 $DKL(p||q)$。它度量当真实分布为p时，假设分布q的无效性。 交叉熵（Cross-Entropy） CE(x) = - \sum_{i=1}^C [ y_i log f_i(x) + (1 - y_i) log (1 - f_i(x))] 其中 $x$ 表示输入样本, $C$ 为待分类的类别总数, 以手写数字识别任务(MNIST)为例, 其输入出的类别数为10, 对应的C=10，$y_i$ 为第 $i$ 个类别对应的真实标签, $f_i(x)$ 为对应的模型输出值. Binary-Cross-Entropy BCE(x)_i = - [ y_i log f_i(x) + (1 - y_i) log(1 - f_i(x)) ] 其中 $ i \in [1, C ]$，即每个类别输出节点都对应一个BCE的值 所以其实 BCE 只是 CE 的一种当label是二分类的时候的一种特殊形式，个人认为是因为在实际的场景中由于二分类的任务比较多，为了方便和效率考虑，单独将 BCE 从 CE 中抽出来另外多实现了一个。 2. 应用场景分析着重讨论以下三种场景：二分类、单标签多分类、多标签多分类（multi-label）。 2.1 二分类首先在二分类的场景下，我们只有一个输出节点，其输出值 $f(x) \in \lbrace 0, 1 \rbrace$ 。 那么按照约定俗成的观点，应该使用sigmoid + BCE作为最后的输出层配置。 BCE(x)_i = - [ y_i log f_i(x) + (1 - y_i) log(1 - f_i(x)) ]由于只有一个分类输出，上式中的 $i$ 可以忽略。 那如果使用CE会怎样呢？由于输出类别数 $C = 1$，所以CE中的求和可以忽略： $$ CE(x) = -y_i log f_i(x) $$ 可以看到，在这种情况下，两者都具有相同的部分，BCE仅在样本标签 $ y = 0 $ 时多了一个反向的损失函数因子。 这里存疑？ 因此，只有对错分的正样本，CE损失函数的值才大于0，此时的网络权值才会得到调整，最终的结果是正样本预测精度会很高，但负样本基本上相当于随机猜测。而对应BCE而言，对错分的正负样本都会产生梯度，网络权值都会进行调整。所以，从直觉上来看，BCE的收敛会更快，学习出的weight也会更合理。 事实上，在keras的train模块中对categorical_crossentropy的使用进行了强制限制，如果输出标签维度为1，只能使用binary_crossentropy，否则程序会报错。为了验证上述推论，我们使用keras自带的imdb二分类例子进行试验，由于输出维度为1的情况下不能直接使用categorical_crossentropy，我们修改例子的代码，通过在自定义loss函数中直接调用backend.categorical_crossentropy函数的方法实验。 运行200个step后，binary_crossentropy已经明显收敛： 20000/25000 [==&gt;……] - ETA: 1:53 - loss: 0.5104 - acc: 0.7282 而categorical_crossentropy却收敛缓慢： 20000/25000 [==&gt;……] - ETA: 1:58 - loss: 5.9557e-08 - acc: 0.5005 可以看到，CE损失函数工作得很差，这与我们的推论相符。所以keras明确禁止了在这种情况下categorical_crossentropy的使用，现在看来也是很合理的。 2.2 单标签多分类按约定俗成的观点，应该使用 softmax+CE 的方案，我们同样先把对应的两种损失函数写出来， \begin{align*} BCE(x) &= \frac{\sum_{i=1}^C BCE(x)_i}{C} \\ &= \frac{- sum _{i=1}^C [ y_i log f_i(x) + (1 - y_i) log(1 - f_i(x)) ]}{C} \end{align*}同样的，由于只有一个类别的真实标签 $ y_i = 1 $ ,对CE来说，可以将前边的求和符号去掉： $$ CE(x) = -y_i log f_i (x) $$ 看到这里，大家可能会有个疑问，在CE中我们同样只计算了某个类别标签为1时的loss，而没有计算它为0时的loss，会不会也像二分类的场景那样，导致模型收敛缓慢？ 但在这里答案是否定的，原因就在于前边的softmax函数具有“排它”性质，某一个输出增大，必然导致其它类别的输出减小，因为其进行了归一化操作，使得每个类别的预测输出概率加和必须为1。但好奇的读者可能又要问了，那使用BCE应该也可以吧？没错！理论上确实是可以。当然可以，因为二者本身就是一家人 下面我们使用keras自带的mnist多分类例子进行实验： 运行100个step后，binary_crossentropy的结果如下： 100/600 [=&gt;…………………….] - ETA: 19:58 - loss: 0.0945 - categorical_accuracy: 0.8137 categorical_crossentropy的结果如下： 100/600 [=&gt;…………………….] - ETA: 18:36 - loss: 0.6024 - acc: 0.8107 可以看到，两者并没有太大差距，binary_crossentropy效果反而略好于categorical_crossentropy。注意这里的acc为训练集上的精度，训练步数也仅有100个step，读者如有兴趣，可以深入分析。但这里至少说明了一点，在单标签多分类的情况下BCE同样是适用的。 2.3 多标签多分类多标签多分类（multi-label）由于假设每个标签的输出是相互独立的，因此常用配置是 sigmoid + BCE， 其中每个类别输出对应一个sigmoid。 如果读者仔细看了前面两个小节，相信不用分析，也可以自行得出结果，即这种场景下使用CE将难以收敛， 原因跟2.1中的分析类似—-我们只计算了某个类别标签为1时的loss及梯度，而忽略了为0时的loss，而每个输出又相互独立，不像softmax函数那样有归一化的限制。 所以multi-label是一定不能使用CE作为loss函数的。 3. 其他“CE用于多分类, BCE适用于二分类”其实大部分都是正确的， 唯一有待商榷的部分在于多分类（单标签）其实也可以使用BCE，而对于multi-label的多分类，则不能使用CE。 另外有同学提到，BCE是CE的一种特殊情况， 对于正例，则 $loss = -y_postive log f(x)$， 其中 $f(x)$ 是预测为正例的概率； 对于负例，则 $loss = -(1 - y_negative)log (1 - f(x))$， 其中 $f(x)$ 还是预测为正例的概率，$1 - f(x)$ 则是预测为负例的概率； 所以，交叉熵(Cross Entropy)一直都是 $-y log f(x)$，其中 $f(x)$ 是预测为正例的概率； keras中两种交叉熵损失函数的探讨 TensorFlow：交叉熵损失函数 AI 多类分类(multi-class) and 多标签分类(mulit-label) and 多输出-多分类-多标签classification Tensorflow 交叉熵（Cross-Entropy） TensorFlow四种Cross Entropy算法实现和应用]]></content>
      <categories>
        <category>损失函数</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[肯德尔Rank相关系数(Kendall rank correlation coefficient)]]></title>
    <url>%2F2020%2F05%2F21%2F%E8%82%AF%E5%BE%B7%E5%B0%94Rank%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[肯德尔Rank相关系数(Kendall rank correlation coefficient)， 又叫 Kendall&#39;s tau 是数学统计中一个常用的系数，用来描述两个序列的相关系数。 如果两个序列完全一致，则 Kendall&#39;s tau 值为1，两个毫不相关的序列的 Kendall&#39;s tau 值为0， 而两个互逆的序列的 Kendall&#39;s tau 系数为-1。 具体的计算方式为: $1 - 2 \cdot symDif / (n \cdot (n - 1))$, 其中 $n$ 为排列的长度(两个序列的长度相同)，$symDif$为对称距离。 对称距离的计算方式如下: 对于两个给定的序列 $S1 = \lbrace a, b, c, d \rbrace; S2 = \lbrace a, c, b, d \rbrace $ 分别找出两个序列的二元约束集。 $S1$的所有二元约束集为 $\lbrace (a,b), (a,c), (a,d), (b,c), (b,d), (c,d) \rbrace$, $S2$ 的所有二元约束集为 $\lbrace (a,c), (a,b), (a,d), (c,b), (c,d), (b,d) \rbrace $， 比较两个二元约束集，其中不同的二元约束有两个$(b,c)$和$(c,b)$,所以对称距离为$2$。 代入上面的计算公式可以得到这两个序列的相关系数为: $ 1 - 2 \cdot 2 / (4 \cdot 3) = 2 / 3 = 0.667 $ 这是一个很有用的参数，可以用来比较两个序列的相似性，例如可以用于搜索引擎的排序结果的好坏。 比较一个序列与一个类似标准答案的排序序列的相似性（人工评价），得出排序序列的有效性。 计算的代码如下： 12345678910111213141516171819def cal_kendall_tau(list_1 , list_2): length = len(list_1) if length != len(list_2): return -1 set_1 = set() set_2 = set() for i in range(length): for j in range(i+1,length): set_1.add( (list_1[i],list_1[j]) ) set_2.add( (list_2[i],list_2[j]) ) count = len(set_1 &amp; set_2) return float(count)*2 / ((length-1)*length)if __name__ == '__main__': list_1 = ['a','b','c','d'] list_2 = ['c','b','a','d'] list_3 = list_1[:] list_3.reverse() print('sim of 1&amp;2 : %s' % cal_kendall_tau(list_1,list_2))]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>评估方法</tag>
        <tag>RankEvaluate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH远程执行任务]]></title>
    <url>%2F2020%2F05%2F18%2FSSH%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[转自: SSH远程执行任务 SSH 是 Linux 下进行远程连接的基本工具，但是如果仅仅用它来登录那可是太浪费啦！ SSH 命令可是完成远程操作的神器啊，借助它我们可以把很多的远程操作自动化掉！ 下面就对 SSH 的远程操作功能进行一个小小的总结。1. 远程执行命令 如果我们要查看一下某台主机的磁盘使用情况，是不是必须要登录到目标主机上才能执行 df 命令呢？ 当然不是的，我们可以使用 ssh 命令在远程的主机上执行 df 命令，然后直接把结果显示出来。 整个过程就像是在本地执行了一条命令一样： 1$ ssh nick@xxx.xxx.xxx.xxx "df -h" 那么如何一次执行多条命令呢？其实也很简单，使用分号把不同的命令隔起来就可以了。 1$ ssh nick@xxx.xxx.xxx.xxx "pwd; cat hello.txt" 第一条命令返回的结果： /home/nick 这说明用这种方式执行命令时的当前目录就是登陆用户的家目录。第二条命令返回 hello.txt 文件的内容。 注意，当命令多于一个时最好用引号括起来，否则在有的系统中除了第一个命令，其它都是在本地执行的。 2. 执行需要交互的命令有时候我们需要远程执行一些有交互操作的命令。 12$ ssh nick@xxx.xxx.xxx.xxx "sudo ls /root"$ ssh nick@xxx.xxx.xxx.xxx "top" 这两条命令虽然提示的失败原因不同，但它们有一个共同点：都需要与用户交互(需要 TTY)。所以它们失败的原因也是相同的： 默认情况下，当你执行不带命令的 ssh 连接时，会为你分配一个 TTY。因为此时你应该是想要运行一个 shell 会话。 但是当你通过 ssh 在远程主机上执行命令时，并不会为这个远程会话分配 TTY。此时 ssh 会立即退出远程主机， 所以需要交互的命令也随之结束。好在我们可以通过 -t 参数显式的告诉 ssh，我们需要一个 TTY 远程 shell 进行交互！ 添加 -t 参数后，ssh 会保持登录状态，直到你退出需要交互的命令。 -t 参数的官方解释： “Force pseudo-terminal allocation. This can be used to execute arbitrary screen-based programs on a remote machine, which can be very useful, e.g. when implementing menu services. Multiple -t options force tty allocation, even if ssh has no local tty.” 更强悍的是我们居然可以指定多个 -t 参数！ 3. 执行多行的命令有时候我们可能需要随手写几行简单的逻辑，这也没有问题，ssh 能轻松搞定！ 你可以用单引号或双引号开头，然后写上几行命令，最后再用相同的引号来结束。 那么如果需要在命令中使用引号该怎么办？其实针对类似的情况有一条比较通用的规则， 就是混合使用单双引号。这条规则在这里也是适用的： 当我们在命令中引用了变量时会怎么样呢？ 请注意上图中的最后一行，并没有输出我们期望的 nick。 这里多少有些诡异，因为如果变量没有被解释的话，输出的应该是 $name 才对。但是这里却什么都没有输出。 对于引用变量的写法，可以通过下面的方式保证变量被正确解释： 注意，我们在上图的命令中为 bash 指定了 -c 参数。 4. 远程执行脚本对于要完成一些复杂功能的场景，如果是仅仅能执行几个命令的话，简直是弱爆了。 我们可能需要写长篇累牍的 shell 脚本去完成某项使命！ 此时 SSH 依然是不辱使命的好帮手。 4.1 执行本地的脚本我们在本地创建一个脚本文件 test.sh，内容为： 12lspwd 然后运行下面的命令： 1$ ssh nick@xxx.xxx.xxx.xxx &lt; test.sh 通过重定向 stdin，本地的脚本 test.sh 在远程服务器上被执行。 接下来我们我期望能为脚本 test.sh 传递一个参数，为了验证传入的参数，在 test.sh 文件的末尾添加两行： 12echo $0echo $1 然后尝试执行下面的命令： 12$ ssh nick@xxx.xxx.xxx.xxx &lt; test.sh helloworld$ ssh nick@xxx.xxx.xxx.xxx &lt; "test.sh helloworld" 下图显示了执行的结果： 看来上面的方法都无法为脚本传递参数。 要想在这种情况下(远程执行本地的脚本)执行带有参数的脚本，需要为 bash 指定 -s 参数： 1$ ssh nick@xxx.xxx.xxx.xxx 'bash -s' &lt; test.sh helloworld 在上图的最后两行，输出的是 “bash” 和 “helloworld” 分别对应 $0 和 $1。 4.2 执行远程服务器上的脚本除了执行本地的脚本，还有一种情况是脚本文件存放在远程服务器上，而我们需要远程的执行它！ 此时在远程服务器上用户 nick 的家目录中有一个脚本 test.sh。文件的内容如下： 12lspwd 执行下面的命令： 1$ ssh nick@xxx.xxx.xxx.xxx "/home/nick/test.sh" 注意，此时需要指定脚本的绝对路径！ 下面我们也尝试为脚本传递参数。在远程主机上的 test.sh 文件的末尾添加两行： 12echo $0echo $1 然后尝试执行下面的命令： 1$ ssh nick@xxx.xxx.xxx.xxx /home/nick/test.sh helloworld 最后两行 “/home/nick/test.sh” 和 “helloworld” 分别对应 $0 和 $1。]]></content>
      <categories>
        <category>小工具</category>
      </categories>
      <tags>
        <tag>小工具</tag>
        <tag>Shell</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python终端调试]]></title>
    <url>%2F2020%2F05%2F17%2FPython%E7%BB%88%E7%AB%AF%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[运行 python -m pdb test.py，会自动停在第一行，等待调试，这时你可以看看帮助1(Pdb) h同时可以在所需要调试的程序开头中 import pdb，并在需要调试的代码处加入断点 pdb.set_trace()1. 关键命令1.1 断点设置1234(Pdb)b 10 #断点设置在本py的第10行或(Pdb)b ots.py:20 #断点设置到 ots.py第20行删除断点（Pdb）b #查看断点编号(Pdb)cl 2 #删除第2个断点 1.2 运行123(Pdb)n #单步运行(Pdb)s #细点运行 也就是会下到，方法(Pdb)c #跳到下个断点 1.3 查看123(Pdb)p param #查看当前 变量值(Pdb)l #查看运行到某处代码(Pdb)a #查看全部栈内变量 2.命令集合 命令 解释 break 或 b 设置断点 continue 或 c 继续执行程序 list 或 l 查看当前行的代码段 step 或 s 进入函数 return 或 r 执行代码直到从当前函数返回 exit 或 q 中止并退出 next 或 n 执行下一行 pp 打印变量的值 help 帮助 Python单步调试 Python如何实现单步调试]]></content>
      <categories>
        <category>小工具</category>
      </categories>
      <tags>
        <tag>调试工具</tag>
        <tag>Debugger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[DMR]Deep Match to Rank Model]]></title>
    <url>%2F2020%2F05%2F14%2FDMR-Deep-Match-to-Rank-Model%2F</url>
    <content type="text"><![CDATA[1. 背景推荐系统通常分为两个阶段，即召回和排序阶段。在召回阶段会对用户和物品进行匹配，得到较小的一部分候选集进入到排序阶段。 在召回阶段，协同过滤方法是最常用来计算用户和物品相关性的方法。 在排序阶段，排序模型会对候选集的每个物品进行打分，然后选取得分最高的N个物品推荐给用户。 而打分最为常用的方式是预测用户对物品的点击率。 因此，点击率预估也受到了学术界和工业界众多研究者的关注。而本文也重点关注点击率预估问题。 对于点击率预估问题，个性化是提升其效果的一个重要的方面。个性化的一个重要方面就是对用户兴趣的刻画，如之前提到过的DIN、DIEN、DSIN等模型。 但是这些模型忽略了建模用户和物品之间的相关性。用户和物品之间的相关性，可以直接衡量用户对目标商品的偏好强度。 表征user2item的相关性，主要有基于矩阵分解和基于深度学习的方法。 基于深度学习的方法，如比较熟悉的Youtube的DNN召回模型。 2. DMR模型 2.1 特征表示输入可以分为四部分: 用户特征(UserProfile): 用户ID、消费等级等，用 $x_p$ 表示。 用户行为特征(UserBehavior): 用户交互过的物品集合，每个物品包括其物品ID、品类ID，用 $x_b = [ e_1, e_2, \cdots, e_T ]$ 表示；其中$e_1$即用户交互过的第一个物品的向量，不同用户的行为长度通常不同，因此往往对所有物品对应的向量进行pooling操作。 目标物品特征(TargetItem): 物品ID、品类ID，用 $x_t$ 表示。 上下文特征(Context): 时间、召回方式、对应的召回评分，用 $x_c$ 表示。 大多数输入特征是离散特征，通过Embedding的方式转换成对应的嵌入向量。 其中用户行为和目标物品共用同一组Embedding来节省存储空间(后面会介绍有特别的一个Embedding矩阵)。 随后，将 $x = [ x_p, x_b, x_t, x_c ]$ 输入到多层全连接网络中， 中间层使用 $pRelu$ 激活函数，最后一层使用 $sigmoid$ 激活函数，得到点击率预估值。 损失函数使用交叉熵损失： L_{target} = - \frac{1}{N} \sum_{(x,y) \in D} ylog(f(x)) + (1 - y) log(1 - f(x))2.2 User-to-Item Netword User-to-Item Netword类似于YoutubeDNN的结构：即基于用户历史行为得到用户向量表示，然后与目标物品对应的向量求内积，来表征相关性。 目标物品的向量，即物品ID对应的Embedding，并且单独使用了另一组物品ID的Embedding。 这里需要注意的是，除了User-to-Item Network中目标物品ID的Embedding外，其他输入部分的物品ID的Embedding都是同一组Embedding。后续实验也表明使用两组Embedding尽管增大了存储空间，但模型的表征能力更强，同时实验效果也好于仅仅把Embedding的长度扩大两倍。 用户的向量表示u仍是其历史行为中物品向量的加权求和，但与之前不同的是，这里计算相关性并没有用到目标物品向量。 这里的加权个人认为更多取决于偏置向量，如时间因素等，那么有可能学到的事近期发生的行为具有更大的权重，时间较远的行为权重较小， 这和我们的直觉是一致的。 \begin{align*} a_t &= z^T tanh (W_p p_t + W_e e_t + b) \\ \alpha_t &= \frac{exp(a_t)}{\sum_{i=1}^T exp(a_i)} \\ u &= g(\sum_{t=1}^T (\alpha_t e_t)) = g( \sum_{t=1}^T (h_t) ) \end{align*}随后，用户和物品的相关性通过内积的方式得到： r = u^T \cdot v_{\cdot}'我们希望$r$越大，代表用户与目标物品的相关性更高，但是仅仅通过反向传播方式去训练并不容易实现这一目标， 同时目标物品的另一组Embedding(单独的那个Embedding矩阵)也学习的不充分。因此在上述的基础上增加了辅助网络(类似于DIEN)。 辅助网络的任务是基于用户的前 T-1 次行为，来预测用户第 T 次交互的物品。 基于用户的向量表示u，我们很容易得到用户前T-1次行为后的向量表示 $u_{T-1}$，即拿前T-1次行为重复上述用户向量表示的计算过程即可。那么用户第T次交互的物品为j的概率为： p_j = \frac{exp(u^T_{T-1} v_j')}{\sum_{i=1}^K exp(u^T_{T-1} v_i')}可以看到，辅助网络是一个多分类网络，为了减少计算复杂性，进行了负采样操作，因此辅助网络的损失为： L_{NS} = - \frac{1}{N} \sum_{i=1}^N (log(\sigma(u^T_{T-1} v_o')) + \sum_{j=1}^k log(\sigma(-u^T_{T-1} v_j')))那么此时总的损失计算如下： L_{final} = L_{target} + \beta L_{NS}这里增加 User-to-Item Network 网络，个人感觉还带来了另一个好处，可以将该模型应用于召回阶段做召回， 然后保存预测值，这样就可以直接作为一维特征用于DMR模型中，减少了DMR模型计算的耗时。 2.3 Item-to-Item Network Item-to-Item Network的思路基本跟DIN是相同的，即首先计算用户历史行为中每个物品跟目标物品之间的相关性得分，然后基于该相关性得分对历史行为物品向量进行加权平均。 相关性得分计算公式如下： \hat{a_t} = \hat{z}^T tanh (\hat{W}_c e_c + \hat{W}_p p_t + \hat{W}_e e_t + \hat{b})这里输入主要有三部分，分别是历史行为中第t个物品对应的向量$e_t$、第t个物品的偏置向量$p_t$，以及目标物品向量$e_c$。 在计算相似性之后，有三部分输出到concat层： 对历史行为中物品向量的加权求和： \begin{align*} \hat{\alpha_t} = \frac{exp(\hat{a}_t) }{ \sum_{i=1}^T exp(\hat{a}_i)} \\ \hat{u} = \sum_{t=1}^T ( \hat{\alpha}_t \hat{e}_t ) \end{align*} 所有历史行为中物品与目标物品相关性评分的求和，注意这里是softmax之前的得分，从另一个角度刻画了用户与目标物品的相关性： \hat{r} = \sum_{t=1}^T \hat{a}_{t\cdot} 目标物品向量。 3. 实验结果见论文。 Deep Match to Rank Model for Personalized Click-Through Rate Prediction lvze92/DMR]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>DIN</tag>
        <tag>Youtube</tag>
        <tag>CTR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell常用实例]]></title>
    <url>%2F2020%2F05%2F13%2FShell%E5%B8%B8%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1. 判断文件/文件夹是否存在123456789101112131415161718#shell判断文件,目录是否存在或者具有权限folder="/var/www/"file="/var/www/log"# -x 参数判断 $folder 是否存在并且是否具有可执行权限if [ ! -x "$folder"]; then mkdir "$folder"fi# -d 参数判断 $folder 是否存在if [ ! -d "$folder"]; then mkdir "$folder"fi# -f 参数判断 $file 是否存在if [ ! -f "$file" ]; then touch "$file"fi 2. 判断变量123456789101112# -n 判断一个变量是否有值if [ ! -n "$var" ]; then echo "$var is empty" exit 0fi# 判断两个变量是否相等if [ "$var1" = "$var2" ]; then echo '$var1 eq $var2'else echo '$var1 not eq $var2'fi]]></content>
      <categories>
        <category>小工具</category>
      </categories>
      <tags>
        <tag>常用实例</tag>
        <tag>Shell</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Murmur哈希]]></title>
    <url>%2F2020%2F05%2F11%2FMurmur%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[说到哈希算法，可能大部分人都会不自觉得想到 md 和 sha 系列，在这之前，我就是这样的，因为他们意味着流行安全和稳定。 但是，最近我知道了一款另类的流行的哈希函数，这款哈希函数广泛应用于分布式系统-Hadoop/Lucence等等， 原因就是因为它速度快而且散列效果好，这个哈希算法就是 MurmurHash。 哈希系列比较流行的有三个系列，分别是 MD/SHA 和 MAC 系列，但是这些系列都是比较安全， 虽然 MD5 和 SHA-1 已经被王小云教授碰撞了，但是相对我们平时使用的直接简单求模这种还是比较安全的， 相对安全带来的负面效果就是计算量还是挺大的，而且不保证哈希结果的均匀。 而在分布式环境下，为了资源的合理利用，我们需要的更多是均匀，因为是内部散列的作用， 所以哈希安全我们并不那么在乎，所以在这种情境下，2008 才被发明的 MurmurHash 成为了分布式中的宠儿，深受 Google 系的喜爱。 MurmurHash 当前最新的版本是 MurmurHash3，它能够产生出32-bit或128-bit哈希值。 除了我们能够猜到的不再使用的 mmh1 以及 还在使用的 mmh2 之外，还有好些变种，不过都是针对平台优化的。 Murmur哈希的算法确实比较简单，它的计算过程其实就是它的名字，MUltiply and Rotate， 因为它在哈希的过程要经过多次MUltiply and Rotate，所以就叫 MurMur 了。 具体算法就不介绍了，也不上伪代码和程序代码了，就以 Python 语言为例子，简单记录一下怎么使用 python 中有两个库可以使用，但是有一个是纯 cpp 迁移，所以比较少人用，另外一个比较多人使用，名字就是 mmh3，所以我们先安装一下它： 1pip install mmh3 mmh3 其实就只有4个函数，hash/hash64 和 hash128，这三个函数都是返回整数，所以如果需要的话， 我们都是要自己转换成十六进制的。此外，还有一个是返回字节的 hash_bytes,它就返回直接序列，我们可以直接使用： 123456import mmh3print(mmh3.hash('foo'))print(mmh3.hash64('foo'))print(mmh3.hash128('foo'))print(mmh3.hash_bytes('foor')) 可以看到结果是： 1234-156908512(-2129773440516405919, 9128664383759220103)168394135621993849475852668931176482145*\xc4\x9c\x94\x82\x07p\xb2\x9ci\xe1\xf6\xdd&#125;\xbf\x05 Murmur哈希 - 维基百科 mmh3 - pypi]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>Hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashEmbedding&QREmbedding]]></title>
    <url>%2F2020%2F05%2F09%2FHashEmbedding-QREmbedding%2F</url>
    <content type="text"><![CDATA[1. HashEmbedding paper: hash-embeddings-for-efficient-word-representations.pdf 1.1 简介Hash Embedding可以看作是普通word embedding和通过随机hash函数得到的词嵌入的interposition(插补)。 在Hash Embedding中，每个token由k个d维embedding vectors和一个k维weight vector表示，token的最终d维表示是两者的乘积。 实验表明，Hash Embedding可以轻松处理包含数百万个token的庞大词汇表。 使用Hash Embedding时，无需在训练前创建字典，也无需在训练后进行任何形式的词汇修剪。 使用Hash Embedding训练的模型表现出的性能至少与在各种任务中使用常规Embedding训练的模型具有相同的性能水平。 1.2 背景经典的word embedding表示方法存在的问题：词表过大时，极大的增加神经网络参数量。 传统解决词表过大的方法： 忽略低频词、停用词。问题：有些低频词或停用词在特定任务中可能是关键信息。例如“and”在一个逻辑相关的任务中是非常重要的。 Remove non-discriminative tokens after training：选取最有效的token。问题：性能下降、很多任务不合适 压缩词向量：有损压缩(量化等) 使用Hash Embedding能有效避免上述方法的问题，并且有很多优点： 使用Hash Embedding时，无需事先创建字典，并且该方法可以处理动态扩展的词汇表。 Hash Embedding具有能够隐式词汇修剪的机制。 Hash Embedding基于Hash，但具有可训练的机制，可以处理有问题的冲突。 Hash Embedding执行类似于乘积量化的操作， 但是不是所有token共享一个单一的小密码本，而是每个token都可以获取非常大的密码本中的一些元素。 1.3 流程 多个Hash函数从同一个Embedding Table中获取 embedding vector 将获取到的多个 embedding vector 进行加权求和 sum embedding vector (可选)将权重和 sum embedding vector进行连接，作为 final embedding vector 2. QREmbedding paper: Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems 2.1 流程 需要：一个整除Embedding Table，一个取余Embedding Table，超参数m 原始index分别做除法(地板除)、取余得到：j = index // m，k = index % m j从整除Embedding Table中获得 embedding vector vj k从取余Embedding Table中获得 embedding vector vk 将 vj 和 vk 进行element-wise reduce合并，可以是加、乘……]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[保序回归-IsotonicRegression]]></title>
    <url>%2F2020%2F03%2F19%2F%E4%BF%9D%E5%BA%8F%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 保序回归的数学定义定义：给定一个有限的实数集合 $Y=y_1, y_2, \cdots, y_n$ 代表观察到的响应，以及 $X = x_1, x_2, \cdots, x_n$ 代表未知的响应值，训练一个模型最小化下列方程： f(x) = \sum_{i = 1}^n w_i (y_i - x_i)^2 其中 $x_1 \le x_2 \cdots \le x_n$, $w_i$ 为权重是正值，其结果称之为保序回归，而且其解是唯一的。 保序回归的结果是分段函数。 2. 举例说明一般从元素的首元素向后观察，如果出现乱序现象 (当前元素大于后续元素) 时停止观察，并从乱序元素开始逐个向前吸收元素组成一个序列，直达该序列所有元素的平均值小于或是等于下一个待吸收的元素。 未发现乱序正常读取 12原始序列：&lt;9, 10, 14&gt;结果序列：&lt;9, 10, 14&gt; 分析：从9往后观察，到最后的元素14都未发现乱序情况，不用处理。 发现乱序，平均值大于后面小于前面 12原始序列：&lt;9, 14, 10&gt;结果序列：&lt;9, 12, 12&gt; 分析：从9往后观察，观察到14时发生乱序 (14 &gt; 10)，停止该轮观察，转入吸收元素处理，从吸收元素10向前一个元素的子序列为 ，取该序列所有元素的平均值得12，故用序列 替代 。吸收10后已经到了最后的元素，处理操作完成。 发现乱序，平均值大于后面，大于前面 12原始序列：&lt;14, 9, 10, 15&gt;结果序列：&lt;11, 11, 11, 15&gt; 分析：从14往后观察时发生乱序 (14 &gt; 9)，停止该轮观察转入吸收元素处理，吸收元素9后子序列为 。求该序列所有元素的平均值得11.5，由于11.5大于下个待吸收的元素10，所以再吸收10得序列 。求该序列所有元素的平均值得11，由于11小于下个待吸收的元素15，所以停止吸收操作，用序列 替代 。 3. 官方代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Author: Nelle Varoquaux &lt;nelle.varoquaux@gmail.com&gt;# Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;# License: BSDimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.collections import LineCollectionfrom sklearn.linear_model import LinearRegressionfrom sklearn.isotonic import IsotonicRegressionfrom sklearn.utils import check_random_staten = 100x = np.arange(n)rs = check_random_state(0)y = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))# ############################################################################## Fit IsotonicRegression and LinearRegression modelsir = IsotonicRegression()y_ = ir.fit_transform(x, y)lr = LinearRegression()lr.fit(x[:, np.newaxis], y) # x needs to be 2d for LinearRegression# ############################################################################## Print dataprint("index\tbefore\tafter")for _x, _y, _y_ in zip(x, y, y_): print("&#123;&#125;\t&#123;&#125;\t&#123;&#125;".format(_x, _y, _y_))# ############################################################################## Plot resultsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]lc = LineCollection(segments, zorder=0)lc.set_array(np.ones(len(y)))lc.set_linewidths(np.full(n, 0.5))fig = plt.figure()plt.plot(x, y, 'r.', markersize=12)plt.plot(x, y_, 'b.-', markersize=12)plt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')plt.gca().add_collection(lc)plt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')plt.title('Isotonic regression')plt.show() 输出结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101index before after0 -6.0 -6.01 31.657359027997266 31.6573590279972662 68.93061443340548 68.930614433405483 86.31471805599453 77.45819571303414 97.47189562170502 77.45819571303415 48.587973461402754 77.45819571303416 130.29550745276566 100.376271134522817 74.97207708399179 100.376271134522818 95.86122886681098 100.376271134522819 152.1292546497023 142.7702873128463610 139.89476363991855 142.7702873128463611 162.24533248940003 142.7702873128463612 166.24746787307683 142.7702873128463613 93.9528664807629 142.7702873128463614 143.4025100551105 142.7702873128463615 153.62943611198907 142.7702873128463616 130.6606672028108 142.7702873128463617 181.51858789480823 159.4623258843472218 143.221948958322 159.4623258843472219 187.78661367769953 159.4623258843472220 183.22612188617114 159.4623258843472221 141.5521226679158 159.4623258843472222 131.7747107964575 159.4623258843472223 185.9026915173973 159.4623258843472224 182.94379124341003 159.4623258843472225 121.9048269010741 159.4623258843472226 134.79184330021644 159.4623258843472227 196.61022550876018 180.6462721245604828 187.3647914993237 180.6462721245604829 199.05986908310777 180.6462721245604830 168.6993602242573 180.6462721245604831 187.28679513998634 180.6462721245604832 206.825378073324 180.6462721245604833 225.31802623080807 180.6462721245604834 215.76740307447068 180.6462721245604835 178.1759469228055 180.6462721245604836 159.54589563221123 180.6462721245604837 150.87930798631928 180.6462721245604838 152.1780823064823 180.6462721245604839 148.4439727056968 180.6462721245604840 174.6786033352154 180.6462721245604841 168.88348091416842 180.6462721245604842 203.0600057846781 180.6462721245604843 148.20948169591304 180.6462721245604844 197.33312448851598 181.4241914661578545 173.43206982445474 181.4241914661578546 173.50738008550292 181.4241914661578547 217.56005054539455 183.500413899091348 167.59101490553132 183.500413899091349 180.6011502714073 183.500413899091350 221.59128163621628 183.500413899091351 202.56218592907138 183.500413899091352 176.5145956776061 183.500413899091353 183.44920232821372 183.500413899091354 150.36665926162357 183.500413899091355 151.26758453675748 183.500413899091356 188.1525633917275 183.66250148682157 206.02215052732095 183.66250148682158 158.87687219528598 183.66250148682159 192.71722811110502 183.66250148682160 172.54369320866556 183.66250148682161 235.35671925225458 194.1490568842443762 161.15673631957662 194.1490568842443763 199.94415416798358 194.1490568842443764 216.71936349478185 194.1490568842443765 190.48273710132125 194.1490568842443766 161.23463096954828 194.1490568842443767 225.97538525880535 204.2145544788009568 202.70532522986298 204.2145544788009569 219.42476210246798 204.2145544788009570 198.13399385206577 204.2145544788009571 174.83330595080275 204.2145544788009572 210.52297205741954 210.5229720574195473 247.2032546602085 212.951149611006274 256.8744056768155 212.951149611006275 166.53666701431655 212.951149611006276 181.1902710926842 212.951149611006277 266.8354413344796 220.2829058562866778 221.47239262335108 220.2829058562866779 181.10133173369405 220.2829058562866780 211.72245773362195 220.2829058562866781 254.33596236321267 222.2615724270055882 245.9420303898299 222.2615724270055883 239.54083994216566 222.2615724270055884 178.13256282451584 222.2615724270055885 240.71736481267536 222.2615724270055886 220.29540593272918 222.2615724270055887 176.86684072391034 222.2615724270055888 250.43181848660697 223.7369361678155189 226.99048351651325 223.7369361678155190 253.5429753258425 223.7369361678155191 191.08942885245202 223.7369361678155192 196.6299746576628 223.7369361678155193 276.1647391135002 232.6119832251233794 235.69384458002705 232.6119832251233795 201.21740957339182 232.6119832251233796 257.73554892516916 232.6119832251233797 192.24837393352863 232.6119832251233798 264.7559925067295 246.5072509030670599 228.25850929940458 246.50725090306705 4. 保序回归应用实例以某种药物的使用量为例，假设药物使用量为数组 $X=0, 1, 2, 3, \cdots, 99$，病人对药物的反应量为$Y = y_1, y_2, y_3, \cdots, y_{99}$ ，而由于个体的原因，$Y$不是一个单调函数(即：存在波动)，如果我们按照药物反应排序，对应的$X$就会成为乱序，失去了研究的意义。而我们的研究的目的是为了观察随着药物使用量的递增，病人的平均反应状况。在这种情况下，使用保序回归，即不改变$X$的排列顺序，又求的$Y$的平均值状况。 从上图中可以看出，最长的蓝线 $x$ 的取值约是$30$到$60$，在这个区间内，$Y$的平均值一样，那么从经济及病人抗药性等因素考虑，使用药量为30个单位是最理想的。 当前IT行业虚拟化比较流行，使用这种方式，找到合适的判断参数，就可以使用此算法使资源得到最大程度的合理利用。 Isotonic Regression-fa.bianp.net 保序回归 Isotonic Regression-Python Isotonic regression - wikipedia Isotonic Regression-sklearn]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thrift基础]]></title>
    <url>%2F2020%2F03%2F13%2Fthrift%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Thrift最初由Facebook研发，主要用于各个服务之间的RPC通信，支持跨语言， 常用的语言比如C++, Java, Python, PHP, Ruby等语言都支持。 Thrift是一个典型的CS（客户端/服务端）结构，客户端和服务端可以使用不同的语言开发。 在推荐系统进行Serving的时候，用到这个进行快速的交互。 本文是从Thrift Tutorial及网上的一些资料整理学习得到，版权归原作者。 1. Thrift中的类型1.1 基本类型thrift不支持无符号类型，因为很多编程语言不存在无符号类型。 byte: 有符号字节 i16: 16位有符号整数 i32: 32位有符号整数 i64: 64位有符号整数 double: 64位浮点数 string: 字符串 1.2 特殊类型binary: 无编码字节序列 1.3 结构体 Structs就像C语言一样，thrift也支持struct类型，目的就是将一些数据聚合在一起，方便传输管理。struct的定义形式如下： 123456struct Example &#123; 1:i32 number=10, 2:i64 bigNumber, 3:double decimals, 4:string name="thrifty"&#125; 1.4 容器类型 Containers集合中的元素可以是除了service之外的任何类型，包括exception。 list (类似c++ STL vector、Java ArrayList) set (类似STL set、Java HashSet)，PHP不支持set，所以PHP对待set就像是map map (类似STL map、Java HashMap) 1.5 异常 Exceptionsthrift支持自定义exception，规则和struct一样，如下： 1234exception InvalidOperation &#123;1: i32 what,2: string why&#125; 1.6 枚举 Enum枚举的定义形式和Java的Enum定义差不多，例如： 1234enum Sex &#123; MALE, FEMALE&#125; 1.7 服务 Servicethrift定义服务相当于Java中创建Interface一样，创建的service经过代码生成命令之后就会生成客户端和服务端的框架代码。定义形式如下： 12345service &lt;name&gt; &#123; &lt;returntype&gt; &lt;name&gt;(&lt;arguments&gt;) [throws (&lt;exceptions&gt;)] ...&#125; 具体例子如下： 12345service StringCache &#123; void set(1:i32 key, 2:string value), string get(1:i32 key) throws (1:KeyNotFound knf), void delete(1:i32 key)&#125; 2. Thrift中其他操作2.1 类型重定义 typedefthrift支持类似C++一样的typedef定义，比如： 12typedef i32 inttypedef i64 long NOTE: 末尾没有逗号或者分号 2.2 常量 constthrift也支持常量定义，使用const关键字，例如： 12const i32 MAX_RETRIES_TIME = 10const string NAME = "ME"; NOTE: 末尾的分号是可选的，可有可无，并且支持16进制赋值 2.3 命名空间thrift的命名空间相当于Java中的package的意思，主要目的是组织代码。thrift使用关键字namespace定义命名空间，例如： 123namespace java tutorialnamespace py tutorialnamespace cpp tutorial NOTE: 格式是：namespace 语言名 路径，注意末尾不能有分号，不同的语言可以设置不同的namespace 2.4 文件包含thrift也支持文件包含，相当于C/C++中的include，Java中的import。使用关键字include定义，例如： 1include "shared.thrift" 2.5 注释thrift注释方式支持shell风格的注释，支持C/C++风格的注释，即#和//开头的语句都单当做注释，/**/包裹的语句也是注释。 2.6 可选与必选thrift提供两个关键字required，optional，分别用于表示对应的字段时必填的还是可选的。例如： 1234struct People &#123; 1: required string name; 2: optional i32 age;&#125; 表示name是必填的，age是可选的。 3. 实例3.1 生成代码知道了怎么定义thrift文件之后，我们需要用定义好的thrift文件生成我们需要的目标语言的源码，本文以生成java源码为例。 假设现在定义了如下一个thrift文件，命名为Test.thrift： 12345678910111213141516171819202122namespace java com.winwill.thriftenum RequestType &#123; SAY_HELLO, //问好 QUERY_TIME, //询问时间&#125;struct Request &#123; 1: required RequestType type; // 请求的类型，必选 2: required string name; // 发起请求的人的名字，必选 3: optional i32 age; // 发起请求的人的年龄，可选&#125;exception RequestException &#123; 1: required i32 code; 2: optional string reason;&#125;// 服务名service HelloWordService &#123; string doAction(1: Request request) throws (1:RequestException qe); // 可能抛出异常。&#125; 在终端运行如下命令(前提是已经安装thrift)： 123thrift --gen java Test.thrift # 生成jave的代码thrift --gen py Test.thrift # 生成python的代码thrift --gen cpp Test.thrift # 生成cpp的代码 则在当前目录会生成一个gen-java/gen-py/gen-cpp目录，该目录下会按照namespace定义的路径名一次一层层生成文件夹，到gen-java/com/winwill/thrift/目录下可以看到生成的4个Java类。 可以看到，thrift文件中定义的enum，struct，exception，service都相应地生成了一个Java类，这就是能支持Java语言的基本的框架代码。 3.2 服务端实现-对thrift中定义的service的接口实现上面代码生成这一步已经将接口代码生成了，现在需要做的是实现HelloWordService的具体逻辑，实现的方式就是创建一个Java类， implements com.winwill.thrift.HelloWordService，例如： 123456789101112131415161718192021222324package com.winwill.thrift;import org.apache.commons.lang3.StringUtils;import org.apache.thrift.TException;import java.util.Date;// 实现接口public class HelloWordServiceImpl implements com.winwill.thrift.HelloWordService.Iface &#123; // 实现这个方法完成具体的逻辑。 public String doAction(com.winwill.thrift.Request request) throws com.winwill.thrift.RequestException, TException &#123; // 打印 System.out.println("Get request: " + request); if (StringUtils.isBlank(request.getName()) || request.getType() == null) &#123; throw new com.winwill.thrift.RequestException(); &#125; String result = "Hello, " + request.getName(); if (request.getType() == com.winwill.thrift.RequestType.SAY_HELLO) &#123; result += ", Welcome!"; &#125; else &#123; result += ", Now is " + new Date().toLocaleString(); &#125; return result; &#125;&#125; 3.3 启动服务-Server端上面这个就是服务端的具体实现类，现在需要启动这个服务，所以需要一个启动类，启动类的代码如下： 123456789101112131415161718package com.winwill.thrift;import org.apache.thrift.server.TServer;import org.apache.thrift.server.TSimpleServer;import org.apache.thrift.transport.TServerSocket;import java.net.ServerSocket;public class HelloWordServer &#123; public static void main(String[] args) throws Exception &#123; // 创建socket ServerSocket socket = new ServerSocket(7912); TServerSocket serverTransport = new TServerSocket(socket); com.winwill.thrift.HelloWordService.Processor processor = new com.winwill.thrift.HelloWordService.Processor(new HelloWordServiceImpl()); TServer server = new TSimpleServer(processor, serverTransport); System.out.println("Running server..."); server.serve(); &#125;&#125; 运行之后看到控制台的输出为： 1Running server... 3.4 客户端请求-Client端现在服务已经启动，可以通过客户端向服务端发送请求了，客户端的代码如下： 123456789101112131415161718192021222324252627282930package com.winwill.thrift;import org.apache.thrift.protocol.TBinaryProtocol;import org.apache.thrift.protocol.TProtocol;import org.apache.thrift.transport.TSocket;import org.apache.thrift.transport.TTransport;public class HelloWordClient &#123; public static void main(String[] args) throws Exception &#123; // 创建socket TTransport transport = new TSocket("localhost", 8888); // 包装协议 TProtocol protocol = new TBinaryProtocol(transport); // 创建client com.winwill.thrift.HelloWordService.Client client = new com.winwill.thrift.HelloWordService.Client(protocol); // 建立连接 transport.open(); // 第一种请求类型 com.winwill.thrift.Request request = new com.winwill.thrift.Request() .setType(com.winwill.thrift.RequestType.SAY_HELLO).setName("winwill2012").setAge(24); System.out.println(client.doAction(request)); // 第二种请求类型 request.setType(com.winwill.thrift.RequestType.QUERY_TIME).setName("winwill2012"); System.out.println(client.doAction(request)); transport.close(); // 请求结束，断开连接 &#125;&#125; 如果是Python代码的话，原生的socket太慢了，可以再包装一层buffer 1234567891011# Make sockettransport = TSocket.TSocket('localhost', 9090)# Buffering is critical. Raw sockets are very slowtransport = TTransport.TBufferedTransport(transport)# Wrap in a protocolprotocol = TBinaryProtocol.TBinaryProtocol(transport)# Create a client to use the protocol encoderclient = com.winwill.thrift.HelloWordService.Client(protocol) 运行客户端代码，得到结果： 12Hello, winwill2012, Welcome!Hello, winwill2012, Now is 2020-3-13 16:21:45 并且此时，服务端会有请求日志： 123Running server...Get request: Request(type:SAY_HELLO, name:winwill2012, age:24)Get request: Request(type:QUERY_TIME, name:winwill2012, age:24) 可以看到，客户端成功将请求发到了服务端，服务端成功地将请求结果返回给客户端，整个通信过程完成。 4. 延伸阅读一下gen-java中生成的代码 Thrift Tutorial thrift入门教程]]></content>
      <categories>
        <category>语言框架</category>
      </categories>
      <tags>
        <tag>Serving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解FTRL-Proximal]]></title>
    <url>%2F2020%2F03%2F03%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3FTRL-Proximal%2F</url>
    <content type="text"><![CDATA[1. 前言写这片post的目的是因为最近在实践中需要对不同的优化器进行调整， 突然发现自己对各种优化算法的了解都停留在一个相对比较浅的层面上， 虽然之前自己从各个渠道汇总整理了一份优化算法的总结性post——优化算法整理post, 但是都是浮于表面的总结，并没有沉淀下来成为自己的东西， 因此这篇post主要记录一下自己在看 Ad Click Prediction a View from the Trenches 这篇论文中得到和不理解的地方。 2. FTRL的背景点击率预估问题(CTR)是推荐系统(计算广告)中非常重要的模块，预估一个用户对广告/Item的点击概率，从而提升广告/Item效果。 LR模型是CTR问题中最经典的模型，而LR模型训练也是无约束最优化问题中经典案例。因此问题最终归约为如何有效优化LR模型； 同时还需要考虑模型稀疏性，模型稀疏性可能会和训练数据不足、模型复杂度过高相关，也是控制过拟合的有效手段。 LR模型需要解决问题是： 样本为正例的概率表示为: \begin{equation*} \begin{aligned} p_t &= \sigma(w_t \cdot x_t) \\ \sigma(a) &= \frac{1}{1 + exp(-a)} \end{aligned} \end{equation*}对应的log损失函数为: l(w_t) = -y_t log p_t - (1 - y_t) log(1 - p_t)批量GD算法能够保证精度，同时可以加上正则项，L1或者L2正则预防过拟合问题。 在线GD算法需要能够利用实时产生的正负样本，一定程度上能够优化模型效果。 在线优化算法需要特殊关注模型鲁棒性和稀疏性，由于样本是一个一个到来，即使加了L1或者L2正则也无法保证取得稀疏的模型。 因此在线GD会着重处理模型稀疏性，FTRL就是综合了RDA和FOBOS优点，既有RDA的精度，又有FOBOS的稀疏性。 3. FTRL的主要内容3.1 FTRL更新策略 w_{t+1}=argmin(g_{1:t}w+\frac12\sum_{s=1}^t\sigma_s||w-w_s||^2+\lambda_1||w||+\frac12 \lambda_2||w||^2)其中 $ g_{1:t}=\sum_{s=1}^t g_t $ ,相当于新产生的权重验证所有样本的梯度并且和历史权重不偏离太远。最后通过L1正则进行稀疏性约束。这样既保证了权重更新的精度有保证了稀疏性。 另外参数 $\sigma_s$ 是一个和学习率相关参数 $\sum_{s=1}^t \sigma_s=\frac{1}{\eta_t}$ 而 $\eta_t=\frac{1}{\sqrt{t}}$ 是一个非增序列 3.2 公式推导 \begin{equation*} \begin{aligned} F(w) &=g_{1:t}w+\frac12\sum_{s=1}^t\sigma_s||w-w_s||^2+\lambda_1||w||+\frac12 \lambda_2||w||^2 \\ &=g_{1:t}w+\frac12\sum_{s=1}^t\sigma_s(w^Tw-2w^Tw_s+w_s^Tw_s)+\lambda_1||w||+\frac12 \lambda_2||w||^2 \\ &=(g_{1:t}-\sum_{s=1}^t\sigma_s w_s) w+\frac12(\sum_{s=1}^t\sigma_s+\lambda_2)w^Tw+\lambda_1||w||+const \\ &=z_t^T w+\frac12(\frac 1 \eta_t+\lambda_2)w^Tw+\lambda_1||w||+const \end{aligned} \end{equation*}其中 $z_{t-1}=g_{1:t-1}-\sum_{s=1}^{t-1}\sigma_s w_s$，根据定义可以得到 $z_{t-1}=z_{t-1}+g_t-(\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}})w_t$ 对上式进行求导可以得到 z_t+(\frac 1 \eta_t+\lambda_2)w+\lambda_1 \partial|W|=0从上式中可以容易得到$w$和$z$必须是异号，否则等式不能成立。 note: 考虑凸函数 $f(x)=|x|$。在原点的次微分是区间[−1, 1]。$x0$，则是单元素集合 $\lbrace 1\rbrace$。 \partial |W| = \begin{cases} 0, & \text{if $-1< w < 1$ } \\ 1, & \text{if $ w > 1$ } \\ -1 , & \text{if $ w < -1$} \end{cases}因此根据L1正则的次导数和梯度公式，讨论如下（论文推导中没有带 $\lambda_2$正则系数，Algorithm1则带了）。 当 $|z_{t,i}| &lt; \lambda_1$ 时 $w_i &gt; 0$，则有 $w = \frac{-z_i - \lambda_1}{\frac{1}{\eta_t} + \lambda_2} &lt; 0$ 不成立 $w_i &lt; 0$，则有 $w = \frac{-z_i + \lambda_1}{\frac{1}{\eta_t} + \lambda_2} &gt; 0$ 不成立 $w_i=0$ 当$z_t &gt; \lambda_1$时 由于两者必须异号，此时有$w_i &lt; 0$ $w = \frac{-z_i + \lambda_1}{\frac{1}{\eta_t} + \lambda_2}$ 当$z_t &lt; − \lambda_1$时，同理 $w = \frac{-z_i + \lambda_1}{\frac{1}{\eta_t} + \lambda_2}$ 因此有 w_i = \begin{cases} 0, & \text{if $|z_i| \le \lambda_1$ } \\ \frac{-(z_i-sgn(z_i)\lambda_1)}{\frac 1 \eta_t+\lambda_2}, & \text{if $others$ } \end{cases} 3.3 算法流程(论文中的Algorithm1) 其中 $n_i$表示梯度的累加和，在论文3.1节中也说了可以对 $n_i^{0.5}$ 中的 $power=0.5$ 进行不同的尝试。 4. 工程上的tricks下列的这些方法都可以在实际的生产环境中进行实验，包括word2vec的采样等操作，都是实际工作中值得学习，尝试的方法。 4.1 Per-Coordinate Learning Rates即各个维度采用不同的学习率: \eta_{t_i}=\frac {\alpha}{\beta+\sqrt \sum_{s=1}^tg^2_{s_i}}对于不同的维度的特征的出现次数是不一样的，因此更新的步长(学习率)也应该是不一样的。 出现次数多的特征应该学习率变化的次数也要多一些，收缩的速度(次数)也要快一些，这个维度的学习率绝对值learning_rate也应该是相对较小的； 出现次数少的特征应该学习率变化的次数相对少一些，收缩的速度(次数)也要相对慢一些，这个维度的学习率绝对值learninng_rate也应该是相对较大的，保证快速的更新。 4.2 Probabilistic Feature Inclusion背景：在训练数据中存在着大量的稀疏特征，如何将一些用不到的稀疏特征过滤掉是这一小节讨论的问题。 方法： L1正则的方式，将稀疏特征的维度系数更新为零；会导致不可接受的精度损失； hash的方式，并没有什么收益； 概率特征引入，可以通过之前的数据预先统计，然后应用在线上； Poisson Inclusion: 假设特征是随机到达的，也就是说应该符合Poisson分布，在一个特征未被引入之前，以概率$p$进行添加，一旦添加后序和OGD的流程一致； Bloom Filter Inclusion: 使用Bloom Filter对特征出现次数统计，超过一定次数$n$加入到训练中，但布隆过滤器并非精确的，而是false positive的，因为产生冲突时，可能会加入还没有满$n$次的特征来进行训练，实际中我们并不要求完全精确的筛选。在实验中，Bloom Filter表现出更高的RAM节省及更少的AucLoss，更推荐。 4.3 Encoding Values with Fewer Bits论文中指出几乎所有的有效系数都落在 $(-2, +2)$这个范围里，因此使用float32或者float64是完全没有必要的。 这里提出了新的编码方式q2.13。 q2.13中2位为整数部分，13位为小数部分，另外有1位符号位。 4.4 Training Many Similar Models在做特征筛选或者设置优化时，基于已有的训练数据，控制变量，生成多个不同的变体来进行对比实验是一个不错的方法，但这样做比较耗费资源。 之前有一个比较省资源的方法是基于一个先验模型，使用其他模型来预测残差(应该是类似boosting的思想)，但是这样不能进行特征移除和特征替换的效果。 这里Google提出了一个方法，由于每个模型变体的相似度是很高的，一部分特征是变体之间共享的，一部分特征是变体独有的。 如果采用一张hashtable来存储所有变体的模型参数，就可以摊销各个模型都进行独立存储key的内存消耗，同时也会降低看网络带宽、CPU、磁盘的消耗。 A Single Value Structure 这个是指不同模型变体之间共享一个特征参数，而不是每一个模型变体存储单独的一个，用一个位字段来标识这个特征被哪些变体使用。 更新的方式如下： 每一个模型变体去进行prediction并计算loss； 对于每一个特征给出一个迭代后的参数值； 对所有的更新后的参数值进行平均并进行更新存储； 后序这个更新的参数值会供所有的变体共享； 4.5 Computing Learning Rates with Counts学习率更新优化通过正负样本个数进行优化。 假设有 $P$ 个正样本，$N$ 个负样本，那么预测正样本的概率应该是 $p = P / (P + N)$， 假设使用 Logistic Regression模型的话，那么正样本的梯度就是 $p - 1$，负样本的梯度就是 $p$，因此梯度平方和就可以表示为： \begin{equation*} \begin{aligned} \sum g^2_{t,i} &= \sum_{positive\_events} (1 - p_t)^2 + \sum_{negative\_events} p_t^2 \\ &\approx P(1 - \frac{P}{N+P})^2 + N (\frac{P}{N+P})^2 \\ &= \frac{PN}{N+P} \end{aligned} \end{equation*}4.6 Subsampling Training Data对于CTR模型，负样本数量特别多，因此正样本更有价值，考虑采用负样本采样，采样方式如下： 正样本全部保留 负样本赋值一个分数 $r \in (0, 1]$ 如果只按照上面的采样方式进行训练的话，会带来严重的偏差预测，因此需要对样本按照分数和label赋值一个重要性权重 $weight$； w_t = \begin{cases} & 1 \quad \quad \text{event } t \text{ is in a clicked query} \\ & \frac{1}{r} \quad \quad \text{event } t \text{ is in a query with no clicks} \end{cases}同时相当于梯度也会乘上该系数，不会改变最终的损失。 定义 $s_t$ 表示event t被采样的概率($1$ or $r$)，同时 $s_t = 1 / w_t$，因此有 El(w_t) = s_tw_tl(w_t)+(1-s_t)0 = s_t \frac{1}{s_t} l_t (w_t) = l(w_t)所以线性期望证明了 $weight$ 加权的目标函数等价于原始数据集的目标函数。 5. 模型效果评估评测模型效果一般基于历史数据，综合很多指标来进行评估，比如AucLoss，LogLoss，SquaredError等。下面5.1 &amp; 5.2是Google进行模型评估时的尝试： 5.1 Progressive Validation常规操作一般是从训练集里抽一部分出来做test，但Google认为都不如online test可以反馈在最近的query数据上的效果，可以使用100%的数据做测试和训练。 由于不同的国家，不同的时间点，不同的query下，指标的benchmark是不同的，而相对指标变化是稳定的，应该关注指标的相对值。同时也说明观察模型效果要对数据进行切片观察。 5.2 Deep Understanding through Visualization我们看到的模型整体提升是一个聚合结果，可能只发生在某一个slice上的效果提升了，其他slice并没有或者发生下降。 我们可以利用天生对图像的敏感进行不同维度切分下指标的可视化来表达模型的表现。 于是，Google开发了一个web可视化工具GridViz，可以可视化不同变体在不同slice上的指标变化。 通过不同的冷暖色，不同的透明程度来表达升高降低，以及对应的幅度信息。 5.3 置信度评估前面说了CTR预测是怎么发生的，但在实际场景中只预测click发生的概率是不够的，还要对这个预测进行一个置信度评估， 来达到explore/exploit E&amp;E问题的一个tradeoff。一方面做出精准的预测，把比较好的广告符合用户兴趣的广告展示出来， 但是这类数据总归是少的；公司总是要赚钱的，而且应广告主的需求，也需要将一些展示次数比较少能获得较高收益的广告也展示出来。 传统标准的置信度评估方法不适合这个场景，原因在于： 标准方法需要完全收敛的、没有正则批处理模型，数据要IID的。不符合； 标准方法需要NxN的矩阵，在Google场景下N的量级是billion，无奈放不下；不符合； 实际场景中，计算置信度的时间应该与做ctr判定在一个数量级上； Google提出了一个uncertainty score来进行评估： 核心思想在于对每一个特征的频次进行计数，该频次决定这学习率，学习率越小则该特征更可信，与我们的直觉是相符的。 为了简单，考虑将 $\lambda_1 = \lambda_2 = 0$，所以FTRL-Proximal等价于在线梯度下降算法OGD，令 $n_{t,i} = \beta + \sum_{s=1}^t g_{s,i}^2$，则得到： \begin{equation*} \begin{aligned} | x \cdot w_t - x \cdot w_{t+1} | &= \sum_{i:|x_i| > 0} \eta_{t,i} | g_{t,i} | \\ &\le \alpha \sum_{i:|x_i| > 0} \frac{x_{t,i}}{\sqrt{n_{t,i}}} \\ &= \alpha \boldsymbol{\eta} \cdot x \\ &\equiv u(x) \end{aligned} \end{equation*}因此定义 uncertainty score 为 $u(x) = \alpha \boldsymbol{\eta} \cdot x$ 的上界值，这个值可以通过点积快速计算。 6. 失败的尝试 对特征使用哈希技巧一些文献声称的feature hashing（用于节省内存）的方式在试验中无效。因此保存可解释（即non-hashed）的特征数值向量。 Dropout没有增强泛化性能反而损害了精度。我们认为这是在深度学习领域(比如图像识别)中特征是稠密的，所以Dropout可以大放异彩，我们的数据本来就很稀疏，再Dropout就没了。 特征bagging不知道为什么反正就是结果并不好。 特征标准化normalize可能是由于学习速度和正规化的相互作用，特征归一化之后效果并不好 Ad Click Prediction a View from the Trenches 【每周一文】Ad Click Prediction: a View from the Trenches(2013) CTR预测算法之FTRL-Proximal]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
        <tag>在线学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python整数&字符串&字节串相互转换]]></title>
    <url>%2F2020%2F03%2F01%2FPython%E6%95%B4%E6%95%B0-%E5%AD%97%E7%AC%A6%E4%B8%B2-%E5%AD%97%E8%8A%82%E4%B8%B2%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[转载自python整数、字符串、字节串相互转换-Lixingcong的博客 导航 数字 字符串 字节码 到数字 进制转换 字符转整数 字节串转整数 到字符串 str() 字符串编码解码 decode(‘hex’) 到字节码 数字转字符串 字符串转字节串 🈚 还有常见的单个字符转换 函数 功能 记忆口诀 例子 备注 chr 数字转成对应的ascii字符 chr长得很像char，因此转成char chr(65) ==&gt; A 输入参数有效范围为0~255 ord 单个字符转对应ascii序号 d为digit最后一个字母，因此转成数字 ord(&#39;A&#39;) ==&gt; 65 进制转换10进制转16进制: hex(16) ==&gt; 0x10 16进制转10进制: int(STRING,BASE)将字符串STRING转成十进制int，其中STRING的基是base。该函数的第一个参数是字符串 int(&#39;0x10&#39;, 16) ==&gt; 16 类似的还有八进制oct()， 二进制bin() 16进制字符串转成二进制 hex_str=&#39;00fe&#39; bin(int(&#39;1&#39;+hex_str, 16))[3:] #含有前导0 # 结果 &#39;0000000011111110&#39; bin(int(hex_str, 16))[2:] #忽略前导0 # 结果 &#39;11111110&#39; 二进制字符串转成16进制字符串 bin_str=&#39;0b0111000011001100&#39; hex(int(bin_str,2)) # 结果 &#39;0x70cc&#39; 字符to整数10进制字符串: int(&#39;10&#39;) ==&gt; 10 16进制字符串: int(&#39;10&#39;, 16) ==&gt; 16 # 或者 int(&#39;0x10&#39;, 16) ==&gt; 16 字节串to整数使用网络数据包常用的struct，兼容C语言的数据结构 struct中支持的格式如下表 Format C-Type Python-Type 字节数 备注 x pad byte no value 1 c char string of length 1 1 b signed char integer 1 B unsigned char integer 1 ? _Bool bool 1 h short integer 2 H unsigned short integer 2 i int integer 4 I unsigned int integer or long 4 l long integer 4 L unsigned long long 4 q long long long 8 仅支持64bit机器 Q unsigned long long long 8 仅支持64bit机器 f float float 4 d double float 8 s char[] string 1 p char[] string 1(与机器有关) 作为指针 P void * long 4 作为指针 对齐方式：放在第一个fmt位置 CHARACTER BYTE ORDER SIZE ALIGNMENT @ native native native = native standard none &lt; little-endian standard none &gt; big-endian standard none ! network (= big-endian) standard none 转义为short型整数: struct.unpack(&#39;&lt;hh&#39;, bytes(b&#39;\x01\x00\x00\x00&#39;)) ==&gt; (1, 0) 转义为long型整数: struct.unpack(&#39;&lt;L&#39;, bytes(b&#39;\x01\x00\x00\x00&#39;)) ==&gt; (1,) 整数to字节串转为两个字节: struct.pack(&#39;&lt;HH&#39;, 1,2) ==&gt; b&#39;\x01\x00\x02\x00&#39; 转为四个字节: struct.pack(&#39;&lt;LL&#39;, 1,2) ==&gt; b&#39;\x01\x00\x00\x00\x02\x00\x00\x00&#39; 整数to字符串直接用函数 str(100) 字符串to字节串我用c++实现的encode(hex)和decode(hex) decode和encode区别 decode函数是重新解码，把CT字符串所显示的69dda8455c7dd425【每隔两个字符】解码成十六进制字符\x69\xdd\xa8\x45\x5c\x7d\xd4\x25 CT=&#39;69dda8455c7dd425&#39; print &quot;%r&quot;%CT.decode(&#39;hex&#39;) # 输出结果 &#39;i\xdd\xa8E\\}\xd4%&#39; encode函数是重新编码，把CT字符串所显示的69dda8455c7dd425【每个字符】编码成acsii值，ascii值为十六进制显示，占两位。执行下列结果显示36396464613834353563376464343235等价于将CT第一个字符’6’编码为0x36h 第二个字符’9’编码为0x39h CT=&#39;69dda8455c7dd425&#39; print &quot;%r&quot;%CT.encode(&#39;hex&#39;) # 输出结果 &#39;36396464613834353563376464343235&#39; 可以理解为：decode解码，字符串变短一半，encode编码，字符串变为两倍长度 decode(‘ascii’)解码为字符串Unicode格式。输出带有’u’ encode(‘ascii’)，编码为Unicode格式，其实python默认处理字符串存储就是Unicode，输出结果估计和原来的字符串一样。 字符串编码为字节码: &#39;12abc&#39;.encode(&#39;ascii&#39;) ==&gt; b&#39;12abc&#39; 数字或字符数组: bytes([1,2, ord(&#39;1&#39;),ord(&#39;2&#39;)]) ==&gt; b&#39;\x01\x0212&#39; 16进制字符串: bytes().fromhex(&#39;010210&#39;) ==&gt; b&#39;\x01\x02\x10&#39; 16进制字符串: bytes(map(ord, &#39;\x01\x02\x31\x32&#39;)) ==&gt; b&#39;\x01\x0212&#39; 16进制数组: bytes([0x01,0x02,0x31,0x32]) ==&gt; b&#39;\x01\x0212&#39; 字节串to字符串字节码解码为字符串: bytes(b&#39;\x31\x32\x61\x62&#39;).decode(&#39;ascii&#39;) ==&gt; 12ab 字节串转16进制表示,夹带ascii: str(bytes(b&#39;\x01\x0212&#39;))[2:-1] ==&gt; \x01\x0212 字节串转16进制表示,固定两个字符表示: str(binascii.b2a_hex(b&#39;\x01\x0212&#39;))[2:-1] ==&gt; 01023132 字节串转16进制数组: [hex(x) for x in bytes(b&#39;\x01\x0212&#39;)] ==&gt; [&#39;0x1&#39;, &#39;0x2&#39;, &#39;0x31&#39;, &#39;0x32&#39;] 问题：什么时候字符串前面加上’r’、’b’、’r’，其实官方文档有写。我认为在Python2中，r和b是等效的。 The Python 2.x documentation: A prefix of ‘b’ or ‘B’ is ignored in Python 2; it indicates that the literal should become a bytes literal in Python 3 (e.g. when code is automatically converted with 2to3). A ‘u’ or ‘b’ prefix may be followed by an ‘r’ prefix. ‘b’字符加在字符串前面，对于python2会被忽略。加上’b’目的仅仅为了兼容python3，让python3以bytes数据类型(0~255)存放这个字符、字符串。 The Python 3.3 documentation states: Bytes literals are always prefixed with ‘b’ or ‘B’; they produce an instance of the bytes type instead of the str type. They may only contain ASCII characters; bytes with a numeric value of 128 or greater must be expressed with escapes. 数据类型byte总是以’b’为前缀，该数据类型仅为ascii。 下面是stackflow上面一个回答。我觉得不错，拿出来跟大家分享 In Python 2.x Pre-3.0 versions of Python lacked this kind of distinction between text and binary data. Instead, there was: unicode = u’…’ literals = sequence of Unicode characters = 3.x str str = ‘…’ literals = sequences of confounded bytes/characters Usually text, encoded in some unspecified encoding. But also used to represent binary data like struct.pack output. Python 3.x makes a clear distinction between the types: str = ‘…’ literals = a sequence of Unicode characters (UTF-16 or UTF-32, depending on how Python was compiled) bytes = b’…’ literals = a sequence of octets (integers between 0 and 255) CPP实现encode就是做个笔记，毕竟在做题Cryptography时候用c++写字符串的处理很蛋疼！为了防止再次造轮子，记下来。 #include &lt;cstring&gt; //用到strlen函数 static unsigned char ByteMap[] = { &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;,&#39;9&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39; }; unsigned char hex_2_dec(unsigned char c){ if(c &gt;= &#39;0&#39; &amp;&amp; c &lt;= &#39;9&#39;) return c - &#39;0&#39;; if(c &gt;= &#39;a&#39; &amp;&amp; c &lt;= &#39;f&#39;) return c - &#39;a&#39; + 10; } void str_encode(unsigned char *src, unsigned char *dest, int len_of_src) { // 使用注意：dest_len &gt;= 2*len_src +1，最后一位是存放&#39;\0&#39;。 int t1; for (int i = 0; i &lt; len_of_src; ++i) { t1 = (int) src[i]; dest[2 * i] = ByteMap[t1 / 16]; dest[2 * i + 1] = ByteMap[t1 % 16]; } dest[2 * len_of_src] = 0; //必须填充最后一个为&#39;\0&#39; } void str_decode(unsigned char *src,unsigned char *dest){ int len_of_src=strlen((char *)src); unsigned char t1; for(int i=1;i&lt;=len_of_src;i+=2){ t1=hex_2_dec(src[i-1]); t1= 16*t1 + hex_2_dec(src[i]); dest[i/2]=t1; } }]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Interest Network]]></title>
    <url>%2F2020%2F01%2F21%2FDeep-Interest-Network%2F</url>
    <content type="text"><![CDATA[1. 问题在推荐系统领域中，通常的做法是将UserProfile、UserBehaviors、CandidateItem、ContextFeatures分别通过Embedding之后，从高维稀疏特征转化为低维稠密特征， 然后通过神经网络对这些特征进行学习，输出对CandidateItem的CTR。通常的推荐系统神经网络模型结构如下图所示： 上述传统做法的一大缺点是将用户所有的行为记录UserBehaviors都平等地对待，对应的模型中就是 average poolingn 或者 sum pooling，将用户交互过的Item的embedding vector进行 简单的平均或者加和来表示这个用户历史行为的 vector，可能稍微加点trick的话，对不同时间的行为加上一个 time decay 系数，对兴趣进行时间衰减。 但是，通过我们的一系列行为中，有一部分是无效或者叫暂时兴趣，只是在当时的那个时刻存在之后就消失的兴趣，在上述传统 average pooling 中也将这部分兴趣点平等对待地考虑了进来， 这无疑对用户的兴趣点捕捉是存在问题的，因此这篇论文提出了将 Attention机制 应用于推荐系统领域中。主要解决的是：让模型对不同时刻的交互行为学习不同的权重，而不是平等对待每一个历史交互行为 如上图所示，用户历史的购买鞋子行为对推荐大衣的广告的影响很小。 2. Attention机制Paper中提出了新的模型 Deep Interest Network, DIN 用来解决传统对用户行为一概而论的缺点。模型结构如下： 其中不同的地方主要集中在对用户历史行为的学习部分，因此这里不再赘述其他部分，单独只说对用户历史行为的学习部分(引入Attention的部分)。 Embedding层都是一样的，Item和其Categroy都是单独进行Embedding，然后进行Concat，这样每个 ItemVector 都是由其本身的ID和cate组合而成 CandidateItem，即待评估ctr的Item同其他Item的Embedding操作是一样的，共用相同的Embedding Matrix，生成 CandidateVector 将 ItemVector 和 CandidateVector 一起传入到一个 Activate Unit 中，学习历史交互Item和CandidateItem之间的关联性weight(归一化到[0, 1])，即Item这个历史行为对推荐CandiateItem的影响权重有多少 这里的 Activate Unit 其实就是 Attention机制，里面具体的操作在后续 5.代码逻辑 中再详细描述 将学习到的Item 和 CandidateItem之间的weight同 ItemVector进行 element-wise 乘 所有的历史交互Item进行 sum pooling，得到一个同 ItemVector 相同纬度的 BehaviorsVector 来表示用户历史兴趣对待推荐 CandidateItem 的分布 将 UserProfilesFeatures、BehaviorsVector、CandidateVector、ContextFeatures 一起再输入到神经网路中学习一些特征交叉、特征组合，最终输出预测的ctr 论文中公式表述Attention的公式(3) 如下： v_U(A) = f(v_A, e_1, e_2, .., e_H) = \sum_{j=1}^H a(e_j, v_A) \cdot e_j = \sum_{j=1}^H w_j e_j其中: ${ e_1, e_2, …, e_H }$ 是长度为 H 的用户 U 历史行为embedding vector列表；$v_A$是Item A的embedding vector，$v_U(A)$表示在候选Item是 A 的时候，对用户历史学习到的用户表示向量。 3. Metrics AUC = \frac{\sum_{i=1}^n \sharp impression_i \times AUC_i}{\sum_{i=1}^n \sharp impression_i}其中 $n$ 是用户数量，$ \sharp impression_i$ 和 $AUC_i$ 是第 $i$ 个用户的impression和$AUC$ 4. 激活函数无论是PReLU还是ReLU的控制函数都是一个阶跃函数，其变化点在 $0$ 处，意味着面对不同的输入这个变化点是不变的。 实际上神经元的输出分布是不同的，面对不同的数据分布采用同样的策略可能是不合理的，因此提出的 Dice 中改进了这个控制函数， 让它根据数据的分布来调整，这里我们选择了统计神经元输出的均值和方差来描述数据的分布： f(s) = p(s) \cdot s + (1 - p(s)) \cdot \alpha s, \quad p(s) = \frac{1}{1 + exp( - \frac{s - E[s]}{ \sqrt( Var[s] + \epsilon ) } )}其中：训练阶段$E[s]$ 和 $Var[s]$ 是每个mini-batch的均值和方差；测试阶段 $E[s]$ 和 $Var[s]$ is calculated by moving averages E[s] and Var[s] over data. $\epsilon$是一个很小的常数，论文中设置为 $10^{-8}$。 Ps：测试阶段这一句没理解 5. 代码逻辑先说格式： train set format: list((用户ID, 历史购买序列, 再次购买的商品, label)) test set format: list((用户ID, 历史购买序列, (购买商品, 没购买商品))) 其中的各种ID都已经映射到了index，历史购买序列是一个index序列，label非0即1 Attention机制部分： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def attention(queries, keys, keys_length): ''' queries: [B, H] (batchSize, 32) 待测商品和其类别的emb concat keys: [B, T, H] (batchSize, sequenceLength, 32) 购买序列的商品和其类别的emb concat keys_length: [B] (batchSize, sequenceLength) batch中每个样本的购买序列长度 ''' # 32 queries_hidden_units = queries.get_shape().as_list()[-1] # 把query复制sequenceLength次 queries = tf.tile(queries, [1, tf.shape(keys)[1]]) # 转化为和keys相同的shape queries = tf.reshape(queries, [-1, tf.shape(keys)[1], queries_hidden_units]) # Activation Unit 增加了 `queries - keys` 的部分 din_all = tf.concat([queries, keys, queries - keys, queries * keys], axis=-1) # B*T*4H # 三层全链接，学习两个 商品 之间的 Activation Weight d_layer_1_all = tf.layers.dense(din_all, 80, activation=tf.nn.sigmoid, name='f1_att') d_layer_2_all = tf.layers.dense(d_layer_1_all, 40, activation=tf.nn.sigmoid, name='f2_att') d_layer_3_all = tf.layers.dense(d_layer_2_all, 1, activation=None, name='f3_att') # B*T*1 # outputs 的结果等价于 Q * K，(batchSize, 1, sequenceLength) outputs = tf.reshape(d_layer_3_all, [-1, 1, tf.shape(keys)[1]]) # B*1*T # Mask key_masks = tf.sequence_mask(keys_length, tf.shape(keys)[1]) # (batchSize, 1, sequenceLength) key_masks = tf.expand_dims(key_masks, 1) # B*1*T # # 在补足的地方附上一个很小的值而不是0, 为了softmax之后的值接近为0 paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) # 将每个样本序列中空缺的商品都赋值为(-2 ** 32 + 1) outputs = tf.where(key_masks, outputs, paddings) # B * 1 * T # Scale, 做 Q*K / sqrt(dim) outputs = outputs / (keys.get_shape().as_list()[-1] ** 0.5) # Activation, 归一化到 [0, 1] # 这里的output是attention计算出来的权重，即论文公式(3)里的w outputs = tf.nn.softmax(outputs) # B * 1 * T # Weighted Sum, attention结束 softmax&#123; Q * K / sqrt(dim) &#125; * V # 三维矩阵相乘，相乘发生在后两维，即 B * (( 1 * T ) * ( T * H )) outputs = tf.matmul(outputs, keys) # B * 1 * H # 返回attention之后的history sequence length的值, # 即论文中 Goods x Goods_Weight 并SUM Pooling后的结果 return outputs Ref Paper: Deep Interest Network for Click-Through Rate Prediction Code: zhougr1993/DeepInterestNetwork 推荐系统中的注意力机制——阿里深度兴趣网络（DIN） 实战DIN]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>排序</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow Serving入门]]></title>
    <url>%2F2020%2F01%2F19%2FTensorFlow-Serving%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. 环境 MacOs=10.15.2 python=3.7.6 tensorflow=1.15.0 tensorflow-serving-api=1.15.0 docker=19.03.5 2. tf-serving介绍大家习惯使用TensorFlow进行模型的训练、验证和预测，但模型完善之后的生产上线流程就见仁见智了，针对这种情况Google提供了TensorFlow Servering，可以将训练好的模型直接上线并提供服务。 Tf-Serving的工作流程主要分为以下几个步骤： Source会针对需要进行加载的模型创建一个Loader，Loader中会包含要加载模型的全部信息； Source通知Manager有新的模型需要进行加载； Manager通过版本管理策略（Version Policy）来确定哪些模型需要被下架，哪些模型需要被加载； Manger在确认需要加载的模型符合加载策略，便通知Loader来加载最新的模型； 客户端像服务端请求模型结果时，可以指定模型的版本，也可以使用最新模型的结果； Tf-Serving客户端和服务端的通信方式有两种(gRPC、RESTfull API) 3. 准备环境和样例 准备 tf-serving 的Docker环境 1docker pull tensorflow/serving 下载官方示例代码 123mkdir -p /tmp/tfservingcd /tmp/tfservinggit clone https://github.com/tensorflow/serving 4. 启动RESTfull API形式的tf-serving 运行 tf-serving 12345docker run -p 8501:8501 \--mount type=bind,\source=/tmp/tfserving/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,\target=/models/half_plus_two \-e MODEL_NAME=half_plus_two -t tensorflow/serving &amp; a. -p 8501:8501是将外部端口8501和docker端口8501进行绑定 b. source指的是外部物理机上模型的路径，target指的是docker内部的模型路径；上面的命令将外部的模型路径和docker内部的路径绑定 c. -e设置docker内部环境变量 d. -t分配一个伪终端(Allocate a pseudo-TTY) i. 较早的docker版本没有 --mount 选项，如遇到提示无此选项，请升级docker版本 ii. 如遇到直接exit 125的情况，把相对路径换成绝对路径，去掉 \ 命令换行 client端调用tf-serving验证 1234# 上面的模型是 y = 0.5 * x + 2curl -d '&#123;"instances": [1.0, 2.0, 5.0]&#125;' -X POST http://localhost:8501/v1/models/half_plus_two:predict# 输出为：# &#123; "predictions": [2.5, 3.0, 4.5] &#125; 5. 启动gRPC形式的tf-serving 编译模型 1python /tmp/tfserving/serving/tensorflow_serving/example/mnist_saved_model.py /tmp/tfserving/models/mnist a. 更多关于提供模型看Serving a TensorFlow Model 运行 tf-serving 123docker run -p 8500:8500 \--mount type=bind,source=/tmp/tfserving/models/mnist,target=/models/mnist \-e MODEL_NAME=mnist -t tensorflow/serving client端调用tf-serving验证 123python /tmp/tfserving/serving/tensorflow_serving/example/mnist_client.py --num_tests=1000 --server=127.0.0.1:8500# 输出为：# Inference error rate: 10.4% a. 如果直接运行mnist_client.py出现找不到 tensorflow_serving 的问题，请手动安装 pip install tensorflow-serving-api==1.15.0 6. 其他关于 tf-serving 的资料 tensorflow/serving tensorflow/serving with docker tensorflow serving官网文档 TensorFlow Serving-简书 by EddyLiu2017 一个简单的TensorFlow-Serving例子]]></content>
      <categories>
        <category>语言框架</category>
      </categories>
      <tags>
        <tag>tf-serving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网搭建方法]]></title>
    <url>%2F2020%2F01%2F08%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E6%90%AD%E5%BB%BA%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. 自建ss服务器教程自建ss/ssr教程很简单，整个教程分三步： 购买VPS服务器 一键部署VPS服务器 一键加速VPS服务器 1.1 购买VPS服务器VPS服务器需要选择国外的，首选国际知名的vultr，速度不错、稳定且性价比高，按小时计费，能够随时开通和删除服务器，新服务器即是新ip。vultr官网：https://my.vultr.com/ 1.2 一键部署VPS服务器 点击图中的CentOS几个字，会弹出centos6，然后选中centos6！之后部署服务器。 使用ssh命令连接服务器。 安装ShadowsocksR一键部署管理脚本 123yum -y install wgetwget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.shchmod +x ssr.sh &amp;&amp; bash ssr.sh 启动脚本安装SSR服务端 选择安装脚本 设置端口和密码 设置加密方式10，选择兼容原版 设置协议插件，协议origin，混淆plain 不做任何限制，一路回车 等待部署完成 1.3 一键加速VPS服务器 先更换服务器内核 123yum -y install wgetwget --no-check-certificate https://blog.asuhu.com/sh/ruisu.shbash ruisu.sh 一键安装锐速 12wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.shbash serverspeeder-all.sh 提示没有完全匹配的内核,随便选一个内核就行,出现running字样即可! 1.4 客户端 Windows SSR客户端 Mac SSR客户端 Linux客户端一键安装配置使用脚本 (使用方法见注释) 安卓SSR客户端 iOS-ShadowrocketIPA文件下载及教程地址 2. 自建v2ray服务器教程详见各部分标题的超链接，或最全的引用链接。 Alvin9999/new-pac]]></content>
      <categories>
        <category>小工具</category>
      </categories>
      <tags>
        <tag>小工具</tag>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BatchSize在训练过程中的影响]]></title>
    <url>%2F2020%2F01%2F07%2FBatchSize%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[先看两个问题： 深度学习中batch size的大小对训练过程的影响是什么样的？ 有些时候不可避免地要用超大batch，比如人脸识别，可能每个batch要有几万甚至几十万张人脸图像，训练过程中超大batch有什么优缺点，如何尽可能地避免超大batch带来的负面影响？ 不考虑Batch Normalization 1. 面试版回答 Solution1: 不考虑bn的情况下，batch size的大小决定了深度学习训练过程中的完成每个epoch所需的时间和每次迭代(iteration)之间梯度的平滑程度。batch size只能说影响完成每个epoch所需要的时间，决定也算不上吧。根本原因还是CPU，GPU算力吧。瓶颈如果在CPU，例如随机数据增强，batch size越大有时候计算的越慢。 对于一个大小为N的训练集，如果每个epoch中mini-batch的采样方法采用最常规的N个样本每个都采样一次，设mini-batch大小为b，那么每个epoch所需的迭代次数(正向+反向)为 N/b, 因此完成每个epoch所需的时间大致也随着迭代次数的增加而增加。 由于目前主流深度学习框架处理mini-batch的反向传播时，默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以b的大小决定了相邻迭代之间的梯度平滑程度，b太小，相邻mini-batch间的差异相对过大，那么相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；b越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来。 总结下来： batch size过小，花费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。 Solution2(个人猜想，未证实) 如果硬件资源允许，想要追求训练速度使用超大batch，可以采用一次正向+多次反向的方法，避免模型陷入局部最小值。即使用超大epoch做正向传播，在反向传播的时候，分批次做多次反向转播，比如将一个batch size为64的batch，一次正向传播得到结果，instance级别求loss（先不平均），得到64个loss结果；反向传播的过程中，分四次进行反向传播，每次取16个instance的loss求平均，然后进行反向传播，这样可以做到在节约一定的训练时间，利用起硬件资源的优势的情况下，避免模型训练陷入局部最小值。 2. 通俗版回答把第一个问题简化为一个小时候经常玩的游戏： 深度学习训练过程：贴鼻子 训练样本：负责指挥的小朋友们(观察角度各不一样) 模型：负责贴的小朋友 模型衡量指标：最终贴的位置和真实位置之间的距离大小 由于每个小朋友站的位置各不一样，所以他们对鼻子位置的观察也各不一样(训练样本的差异性)。这时候假设小明是负责贴鼻子的小朋友，小朋友A、B、C、D、E是负责指挥的同学(A, B站在图的右边，C，D，E站在左边)，这时候小明如果采用： 每次随机询问一个同学，那么很容易出现，先询问到了A，A说向左2cm，再问C，C说向右5cm，然后B，B说向左4cm，D说向右3cm，这样每次指挥的差异都比较大，结果调过来调过去，没什么进步。 每次随机询问两个同学，每次取询问的意见的平均，比如先问到了(A, C)，A说向左2cm，C说向右5cm，那就取个均值，向右1.5cm。然后再问（B, D），这样的话减少了极端情况（前后两次迭代差异巨大）这种情况的发生，能更好更快的完成游戏。 每次全问一遍，然后取均值，这样每次移动的方向都是所有人决定的均值，这样的话，最后就是哪边的小朋友多最终结果就被很快的拉向哪边了。(梯度方向不变，限于极小值) 3. 科学版回答使用MINST进行实验。详细实验代码、流程、结果等见参考 由于现在绝大多数的框架在进行mini-batch的反向传播的时候，默认都是将batch中每个instance的loss平均化之后在进行反向传播，所以相对大一点的batch size能够防止loss震荡的情况发生。从这两张图中可以看出batch size越小，相邻iter之间的loss震荡就越厉害，相应的，反传回去的梯度的变化也就越大，也就越不利于收敛。同时很有意思的一个现象，batch size为1的时候，loss到后期会发生爆炸，这主要是lr=0.02设置太大，所以某个异常值的出现会严重扰动到训练过程。这也是为什么对于较小的batchsize，要设置小lr的原因之一，避免异常值对结果造成的扰巨大扰动。而对于较大的batchsize，要设置大一点的lr的原因则是大batch每次迭代的梯度方向相对固定，大lr可以加速其收敛过程。 深度学习 | Batch Size大小对训练过程的影响]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式TensorFlow模式]]></title>
    <url>%2F2020%2F01%2F05%2F%E5%88%86%E5%B8%83%E5%BC%8FTensorFlow%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. 简介Tensorflow API提供了Cluster、 Server以及 Supervisor来支持模型的分布式训练。 关于Tensorflow的分布式训练介绍可以参考 Distributed Tensorflow。简单的概括说明如下： Tensorflow分布式Cluster由多个Task组成，每个Task对应一个tf.train.Server实例, 作为Cluster的一个单独节点； 多个相同作用的Task可以被划分为一个job，例如ps job作为参数服务器只保存Tensorflow model的参数，而worker job则作为计算节点只执行计算密集型的Graph计算。 Cluster中的Task会相对进行通信，以便进行状态同步、参数更新等操作。 Tensorflow分布式集群的所有节点执行的代码是相同的。分布式任务代码具有固定的模式： 1234567891011121314151617181920212223# 第1步：命令行参数解析，获取集群的信息ps_hosts和worker_hosts，以及当前节点的角色信息job_name和task_index# 第2步：创建当前task结点的Servercluster = tf.train.ClusterSpec(&#123;"ps": ps_hosts, "worker": worker_hosts&#125;)server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)# 第3步：如果当前节点是ps，则调用server.join()无休止等待；如果是worker，则执行第4步。if FLAGS.job_name == "ps": server.join()# 第4步：则构建要训练的模型# build tensorflow graph model# 第5步：创建tf.train.Supervisor来管理模型的训练过程# Create a "supervisor", which oversees the training process.sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir="/tmp/train_logs")# The supervisor takes care of session initialization and restoring from a checkpoint.sess = sv.prepare_or_wait_for_session(server.target)# Loop until the supervisor shuts downwhile not sv.should_stop() # train model 2. Tensorflow分布式训练代码框架根据上面说到的Tensorflow分布式训练代码固定模式，如果要编写一个分布式的Tensorlfow代码，其框架如下所示: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import tensorflow as tf# Flags for defining the tf.train.ClusterSpectf.app.flags.DEFINE_string("ps_hosts", "", "Comma-separated list of hostname:port pairs")tf.app.flags.DEFINE_string("worker_hosts", "", "Comma-separated list of hostname:port pairs")# Flags for defining the tf.train.Servertf.app.flags.DEFINE_string("job_name", "", "One of 'ps', 'worker'")tf.app.flags.DEFINE_integer("task_index", 0, "Index of task within the job")FLAGS = tf.app.flags.FLAGSdef main(_): ps_hosts = FLAGS.ps_hosts.split(",") worker_hosts = FLAGS.worker_hosts(",") # Create a cluster from the parameter server and worker hosts. cluster = tf.train.ClusterSpec(&#123;"ps": ps_hosts, "worker": worker_hosts&#125;) # Create and start a server for the local task. server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index) if FLAGS.job_name == "ps": server.join() elif FLAGS.job_name == "worker": # Assigns ops to the local worker by default. with tf.device(tf.train.replica_device_setter( worker_device="/job:worker/task:%d" % FLAGS.task_index, cluster=cluster)): # Build model... loss = ... global_step = tf.Variable(0) train_op = tf.train.AdagradOptimizer(0.01).minimize( loss, global_step=global_step) saver = tf.train.Saver() summary_op = tf.merge_all_summaries() init_op = tf.initialize_all_variables() # Create a "supervisor", which oversees the training process. sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir="/tmp/train_logs", init_op=init_op, summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600) # The supervisor takes care of session initialization and restoring from # a checkpoint. sess = sv.prepare_or_wait_for_session(server.target) # Start queue runners for the input pipelines (if any). sv.start_queue_runners(sess) # Loop until the supervisor shuts down (or 1000000 steps have completed). step = 0 while not sv.should_stop() and step &lt; 1000000: # Run a training step asynchronously. # See `tf.train.SyncReplicasOptimizer` for additional details on how to # perform *synchronous* training. _, step = sess.run([train_op, global_step])if __name__ == "__main__": tf.app.run() 对于所有Tensorflow分布式代码，可变的只有两点： 构建tensorflow graph模型代码； 每一步执行训练的代码； 3. 分布式MNIST任务我们通过修改tensorflow/tensorflow提供的mnist_softmax.py来构造分布式的MNIST样例来进行验证。 修改后的代码请参考如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tftf.app.flags.DEFINE_string("ps_hosts", "", "Comma-separated list of hostname:port pairs")tf.app.flags.DEFINE_string("worker_hosts", "", "Comma-separated list of hostname:port pairs")tf.app.flags.DEFINE_string("job_name", "", "One of 'ps', 'worker'")tf.app.flags.DEFINE_integer("task_index", 0, "Index of task within the job")FLAGS = tf.app.flags.FLAGSdef main(_): ps_hosts = FLAGS.ps_hosts.split(",") worker_hosts = FLAGS.worker_hosts.split(",") # Create a cluster from the parameter server and worker hosts. cluster = tf.train.ClusterSpec(&#123;"ps": ps_hosts, "worker": worker_hosts&#125;) # Create and start a server for the local task. server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index) print("Cluster job: %s, task_index: %d, target: %s" % (FLAGS.job_name, FLAGS.task_index, server.target)) if FLAGS.job_name == "ps": server.join() elif FLAGS.job_name == "worker": # Assigns ops to the local worker by default. with tf.device(tf.train.replica_device_setter( worker_device="/job:worker/task:%d" % FLAGS.task_index, cluster=cluster)): # Build model ... mnist = input_data.read_data_sets("data", one_hot=True) # Create the model x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) y = tf.matmul(x, W) + b # Define loss and optimizer y_ = tf.placeholder(tf.float32, [None, 10]) cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_)) global_step = tf.Variable(0) train_op = tf.train.AdagradOptimizer(0.01).minimize( cross_entropy, global_step=global_step) # Test trained model correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) saver = tf.train.Saver() summary_op = tf.merge_all_summaries() init_op = tf.initialize_all_variables() # Create a "Supervisor", which oversees the training process. sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir="/opt/tensor", init_op=init_op, summary_op=summary_op, saver = saver, global_step=global_step, save_model_secs=600) # The supervisor takes care of session initialization and restoring from # a checkpoint. sess = sv.prepare_or_wait_for_session(server.target) # Start queue runners for the input pipelines (if ang). sv.start_queue_runners(sess) # Loop until the supervisor shuts down (or 2000 steps have completed). step = 0 while not sv.should_stop() and step &lt; 2000: batch_xs, batch_ys = mnist.train.next_batch(100) _, step = sess.run([train_op, global_step], feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) print("Step %d in task %d" % (step, FLAGS.task_index)) print("done.") if FLAGS.task_index != 0: print("accuracy: %f" % sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))if __name__ == "__main__": tf.app.run() 我们同样通过tensorlfow的Docker image来启动一个容器来进行验证。 1$ docker run -d -v /path/to/your/code:/tensorflow/mnist --name tensorflow tensorflow/tensorflow 启动tensorflow之后，启动4个Terminal，然后通过下面命令进入tensorflow容器，切换到/tensorflow/mnist目录下 12$ docker exec -ti tensorflow /bin/bash$ cd /tensorflow/mnist 然后在四个Terminal中分别执行下面一个命令来启动Tensorflow cluster的一个task节点， 1234567891011# Start ps 0python mnist_dist.py --ps_hosts=localhost:2221,localhost:2222 --worker_hosts=localhost:2223,localhost:2224 --job_name=ps --task_index=0# Start ps 1python mnist_dist.py --ps_hosts=localhost:2221,localhost:2222 --worker_hosts=localhost:2223,localhost:2224 --job_name=ps --task_index=1# Start worker 0python mnist_dist.py --ps_hosts=localhost:2221,localhost:2222 --worker_hosts=localhost:2223,localhost:2224 --job_name=worker --task_index=0# Start worker 1python mnist_dist.py --ps_hosts=localhost:2221,localhost:2222 --worker_hosts=localhost:2223,localhost:2224 --job_name=worker --task_index=1 Tensorflow学习笔记4：分布式Tensorflow]]></content>
      <categories>
        <category>语言框架</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的损失函数]]></title>
    <url>%2F2020%2F01%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. 平方损失误差平方误差损失(也称为L2 Loss)是实际值和预测值之差的平方 L = (y - f(x))^2MSE损失函数通过平方误差来惩罚模型犯的大错误。把一个比较大的数平方会使它变得更大。这个属性使MSE成本函数对异常值的健壮性降低。因此，如果数据容易出现许多的异常值，则不应使用这个它。 2. 绝对误差损失绝对误差是预测值和实际值之间的距离，与符号无关。绝对误差也称为L1 Loss L = | y - f(x) |与MSE相比，MAE成本对异常值更加健壮。但是，在数学方程中处理绝对值或模数运算符并不容易。我们可以认为这是MAE的缺点。 3. Huber损失Huber损失结合了MSE和MAE的最佳特性。对于较小的误差，它是二次的，否则是线性的(对于其梯度也是如此)。Huber损失需要确定 $\delta$ 参数： L(\delta) = \begin{cases} \frac{1}{2} (y - f(x)) ^2 &\quad if | y - f(x) | \le \delta \\ \delta | y - f(x) | - \frac{1}{2} \delta^2 &\quad otherwise \end{cases}Huber损失对于异常值比MSE更强。它用于稳健回归(robust regression)，M估计法(M-estimator)和可加模型(additive model)。Huber损失的变体也可以用于分类。 4. 交叉熵损失损失函数4.1 二分类交叉熵 L = -y \times log(p) - (1-y) \times log(1 -p)其中 $p = sigmoid(f(x)) = 1 / (1 + e^{-f(x)})$。 4.2 多分类交叉熵 L(X_i, Y_i) = - \sum_{j=1}^c y_{i,j} \times log(p_{i,j})其中 $Y_i$ 是one-hot形式的label，$y_{i,j}$非$0$即$1$，$p_{i,j}$是sigmoid后的概率值，介于 [$0, 1$]。 5. Hinge损失函数Hinge损失主要用于带有类标签-1和1的支持向量机(SVM)。因此，请确保将数据集中”恶性”类的标签从0更改为-1。 Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。 L = max(0, \quad 1 - y \times f(x))Hinge损失简化了SVM的数学运算，同时最大化了损失(与对数损失(Log-Loss)相比)。当我们想要做实时决策而不是高度关注准确性时，就可以使用它。 6. KL散度KL散度通常不用来做损失函数，其转化后的形式(交叉熵)经常应用于分类问题做损失函数。 KL散度概率分布与另一个概率分布区别的度量。KL散度为零表示分布相同。 假定有两个概率分布 $P$ 和 $Q$，则 $P$ 和 $Q$ 之间的KL散度定义为： D_{KL}(P \parallel Q) = \begin{cases} -\sum_x P(x) \cdot log \frac{Q(x)}{P(x)} = \sum_x P(x) \cdot log \frac{P(x)}{Q(x)}, &\quad 对离散分布 \\ -\int P(x) \cdot log \frac{Q(x)}{P(x)} \cdot dx = \int P(x) \cdot log \frac{P(x)}{Q(x)} \cdot dx, &\quad 对连续分布 \end{cases}Note: 发散函数不对称。 与多分类分类相比，KL散度更常用于逼近复杂函数。我们在使用变分自动编码器(VAE)等深度生成模型时经常使用KL散度。 机器学习中的 7 大损失函数实战总结（附Python演练）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[resume-2020]]></title>
    <url>%2F2020%2F01%2F01%2Fresume-2020%2F</url>
    <content type="text"><![CDATA[目录 沉浸式视频排序 新闻Push推荐排序 简历在老地方1. 沉浸式视频排序沉浸式视频(背景是黑色的)具有自动播放、自动下滑的特点，而且沉浸式视频的视频长度跨度很大，短则10s+，长则2h+。如何合理的划分正样本和负样本是特征、模型优化的前提！原始正负样本比例大概 5:1。 主要有以下操作： 根据正样本的停留时长dwell、正样本的视频长度video_length，设计一个公式动态计算完成率阈值finish_ratio_threshold，然后将 finish_ratio &lt; finish_ratio_threshold的正样本进行丢弃；(也尝试过将正样本转化成负样本，效果不好)； 考虑到沉浸式视频是一个相对独立的场景，因此衡量用户的短期兴趣是十分有必要的，因此将用户最近click的docid、最近view的docid加入到训练中，用以充当用户的短期兴趣； 其他的特征进行统计候选值的个数，例如只在oppo平台上进行实验，U_OS的候选值只有android，所以这个特征没什么用 FM模型/DNN(full-connect)模型，用embedding矩阵代表系数的思想 2. 新闻Push推荐排序编辑推送每天候选用于排序的文章只有200～300篇文章，机器推送数量会比较多一些。 同时编辑推送的文章直接用于排序，没有召回阶段，机器推送的文章是通过用户的tag标签进行召回的。 因此在排序的过程中要加入一些提高相关性的操作： 原始训练数据正负比例 1:6，对负样本进行欠采样到正负比例 1:3 在训练过程中参考DSSM中，伪造一些负样本加入到训练中（原理下面解释） 添加match特征，例：matchprofile U_CT D_CT A_CT，意思是将U_CT和D_CT中的值取交集，以新的facet A_CT加入到训练中 同时使用信息流数据进行warmup(可以认为使用信息流模型的参数来完成初始化Push模型) 伪造负样本的原理 前提假设： 历史将推荐给用户的Push文章都是模型认为和用户强相关的文章。 原理解释： 数据中存在着正样本和负样本，但是这些样本都是和用户强相关的，都是模型认为用户感兴趣的，但是却返回了不同的行为。 此时用这样的数据训练模型很难学习到哪些是和用户相关的、哪些是和用户不相关的，所以需要伪造一些和用户不相关的样本数据加入到训练中。 实现原理： 训练过程中，存在几个数据结构： LRUCache：统计正样本中的文章出现的次数 sample_table：根据LRUCache中统计的文章出现次数，参考word2vec的采样表计算方式计算采样表 流程 1.训练数据流中来到一个正样本 (userId, docId, label=1.0) 2.LRUCache中docId出现的次数加一 3.每隔60秒根据LRUCache中的值重新计算一次采样表 4.从采样表中随机采样docIdFake，组装伪造负样本 (userId, docIdFake, label=0.0) 5.将伪造负样本加入到训练中 3. 总结 跳槽找工作刷题很重要，要熟练掌握基本的数据结构和算法，所以不能停止刷LeetCode 对自己做的项目要深入，讲给别人听的时候，要让别人觉得确实如此 对于自己使用的一些技术，要知道其来源和适用场景 对数据的规模，调整等要清楚，对模型的实现细节，整个流程要搞明白，知道不同的模型适用什么样的场景 一些常用的调参手段、案例要能讲清楚，评估指标，计算方式要掌握 对GBDT、XGboost的工业级训练、Serve最好能有了解，以及树模型的优缺点，输出特征重要性、原理等 深入了解Python、C++ 以上就是本人 2020年，也是人生中第一次社招的经验总结。完结，撒花！]]></content>
      <categories>
        <category>个人小结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow-Embedding笔记]]></title>
    <url>%2F2019%2F10%2F12%2Ftensorflow-embedding%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[embedding中牢记feature_batch中的value表示的都是embedding矩阵中的index1. tf 1.x中的Embedding实现使用embedding_lookup函数来实现Emedding，如下：123456789101112131415161718192021222324# embedding matrix 4x4embedding = tf.constant( [ [0.21, 0.41, 0.51, 0.11], [0.22, 0.42, 0.52, 0.12], [0.23, 0.43, 0.53, 0.13], [0.24, 0.44, 0.54, 0.14] ], dtype=tf.float32)feature_batch = tf.constant([2, 3, 1, 0])# 相当于把行号变化，原来index=2的变为index=0，index=3-&gt;1, index=1-&gt;2, index=3-&gt;0get_embedding1 = tf.nn.embedding_lookup(embedding, feature_batch)# 生成 4x4 的one-hot矩阵，1的位置有feature_batch中的值决定feature_batch_one_hot = tf.one_hot(feature_batch, depth=4)# embedding层其实是一个全连接神经网络层，那么其过程等价于：# 矩阵乘法，描述了 embedding_lookup 的原理get_embedding2 = tf.matmul(feature_batch_one_hot, embedding)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) embedding1, embedding2, batchOneHot = sess.run([get_embedding1, get_embedding2, feature_batch_one_hot]) print("embedding1\n", embedding1) print("embedding2\n", embedding2) print("feature_batch_one_hot\n", batchOneHot) 代码运行结果： 123456789101112131415embedding1 [[0.23 0.43 0.53 0.13] [0.24 0.44 0.54 0.14] [0.22 0.42 0.52 0.12] [0.21 0.41 0.51 0.11]]embedding2 [[0.23 0.43 0.53 0.13] [0.24 0.44 0.54 0.14] [0.22 0.42 0.52 0.12] [0.21 0.41 0.51 0.11]]feature_batch_one_hot [[0. 0. 1. 0.] [0. 0. 0. 1.] [0. 1. 0. 0.] [1. 0. 0. 0.]] 2. tf 1.x中与Embedding类似操作 - 单维索引123456789101112131415161718192021222324# 单维索引embedding = tf.constant( [ [0.21, 0.41, 0.51, 0.11], [0.22, 0.42, 0.52, 0.12], [0.23, 0.43, 0.53, 0.13], [0.24, 0.44, 0.54, 0.14] ], dtype=tf.float32)index_a = tf.Variable([2, 3, 1, 0])gather_a = tf.gather(embedding, index_a)# axis=0: 按行取， axis=1：按列取gather_a_axis1 = tf.gather(embedding, index_a, axis=1)b = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])index_b = tf.Variable([2, 4, 6, 8])gather_b = tf.gather(b, index_b)# 一维gather直接使用 tf.gatherwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) gather_a, gather_b, gather_a_axis1 = sess.run([gather_a, gather_b, gather_a_axis1]) print("gather_a\n", gather_a) print("gather_b\n", gather_b) print("gather_a_axis1\n", gather_a_axis1) 代码运行结果： 123456789101112gather_a [[0.23 0.43 0.53 0.13] [0.24 0.44 0.54 0.14] [0.22 0.42 0.52 0.12] [0.21 0.41 0.51 0.11]]gather_b [3 5 7 9]gather_a_axis1 [[0.51 0.11 0.41 0.21] [0.52 0.12 0.42 0.22] [0.53 0.13 0.43 0.23] [0.54 0.14 0.44 0.24]] 3. tf 1.x中与Embedding类似操作 - 多维索引123456789101112131415161718# 多维索引a = tf.Variable([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]])index_a = tf.Variable([2])b = tf.get_variable(name='b', shape=[3, 3, 2], initializer=tf.random_normal_initializer)# [0, 1, 1]表示第一维选择index=0，第二维选择index=1，第三维选择index=1index_b = tf.Variable([[0, 1, 1], [2, 2, 0]])# 注意多维gather要使用 tf.gather_ndwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("tf.gather_nd(a, index_a)\n", sess.run(tf.gather_nd(a, index_a))) print("b\n", sess.run(b)) print("tf.gather_nd(b, index_b)\n", sess.run(tf.gather_nd(b, index_b))) 代码运行结果： 12345678910111213141516tf.gather_nd(a, index_a) [11 12 13 14 15]b [[[-3.9612162e-01 2.6312229e-04] [-1.5676118e-01 5.1959008e-01] [ 2.1008211e-01 1.3654137e+00]] [[ 3.5585640e-03 -1.0942048e+00] [-1.1834130e+00 -8.3026028e-01] [-5.5713791e-01 -1.7461585e-01]] [[-7.2083569e-01 -4.4790068e-01] [ 4.5505306e-01 -1.4471538e-01] [ 1.2432091e+00 -8.3164376e-01]]]tf.gather_nd(b, index_b) [0.5195901 1.2432091] 4. tf 1.x中与Embedding类似操作 - 稀疏表示的Embedding12345678910111213141516171819202122232425262728293031323334# sparse embedding 稀疏表示的embeddinga = tf.SparseTensor(indices=[[0, 0], [1, 2], [1, 3]], values=[1, 2, 3], dense_shape=[2, 4])b = tf.sparse_tensor_to_dense(a)embedding = tf.constant( [ [0.21, 0.41, 0.51, 0.11], [0.22, 0.42, 0.52, 0.12], [0.23, 0.43, 0.53, 0.13], [0.24, 0.44, 0.54, 0.14] ], dtype=tf.float32)# b# [[1 0 0 0]# [0 0 2 3]]# 使用 embedding_lookup_sparse，而不是 embedding_lookup# [1 0 0 0] 表示将index=1的那一行拿出来# [0 0 2 3] 表示将index=2和index=3的那两行拿出来，对应位置做 combinerembedding_sparse = tf.nn.embedding_lookup_sparse(embedding, sp_ids=a, sp_weights=None, combiner='mean')c = tf.matmul(tf.cast(b, tf.float32), embedding)e = tf.reduce_sum(b, axis=1, keepdims=True)l = tf.div(c, tf.cast(e, tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print("a\n", sess.run(a)) print("embedding_sparse\n", sess.run(embedding_sparse)) print("b\n", sess.run(b)) print("c\n", sess.run(c)) print("e\n", sess.run(e)) print("l\n", sess.run(l)) 代码运行结果： 12345678910111213141516171819a SparseTensorValue(indices=array([[0, 0], [1, 2], [1, 3]]), values=array([1, 2, 3], dtype=int32), dense_shape=array([2, 4]))embedding_sparse [[0.22 0.42 0.52 0.12 ] [0.235 0.435 0.53499997 0.13499999]]b [[1 0 0 0] [0 0 2 3]]c [[0.21 0.41 0.51 0.11 ] [1.18 2.1799998 2.68 0.68 ]]e [[1] [5]]l [[0.21 0.41 0.51 0.11 ] [0.23599999 0.43599996 0.536 0.136 ]] 5. tf 2.0中Embedding实现在tf2.0中，embedding同样可以通过embedding_lookup来实现，不过不同的是，我们不需要通过sess.run来获取结果了，可以直接运行结果，并转换为numpy。 12345678910111213141516embedding = tf.constant( [ [0.21, 0.41, 0.51, 0.11], [0.22, 0.42, 0.52, 0.12], [0.23, 0.43, 0.53, 0.13], [0.24, 0.44, 0.54, 0.14] ], dtype=tf.float32)feature_batch = tf.constant([2, 3, 1, 0])get_embedding1 = tf.nn.embedding_lookup(embedding, feature_batch)feature_batch_one_hot = tf.one_hot(feature_batch, depth=4)get_embedding2 = tf.matmul(feature_batch_one_hot, embedding)print(get_embedding1.numpy().tolist())print(get_embedding2.numpy().tolist()) 代码运行结果： 12345678910embedding1 [[0.23000000417232513, 0.4300000071525574, 0.5299999713897705, 0.12999999523162842], [0.23999999463558197, 0.4399999976158142, 0.5400000214576721, 0.14000000059604645], [0.2199999988079071, 0.41999998688697815, 0.5199999809265137, 0.11999999731779099], [0.20999999344348907, 0.4099999964237213, 0.5099999904632568, 0.10999999940395355]]embedding2 [[0.23000000417232513, 0.4300000071525574, 0.5299999713897705, 0.12999999523162842], [0.23999999463558197, 0.4399999976158142, 0.5400000214576721, 0.14000000059604645], [0.2199999988079071, 0.41999998688697815, 0.5199999809265137, 0.11999999731779099], [0.20999999344348907, 0.4099999964237213, 0.5099999904632568, 0.10999999940395355]] 神经网络中使用embedding层，推荐使用Keras： 12345678910111213141516171819from tensorflow.keras import layersnum_classes=10input_x = tf.keras.Input(shape=(None,),)embedding_x = layers.Embedding(num_classes, 10)(input_x)hidden1 = layers.Dense(50,activation='relu')(embedding_x)output = layers.Dense(2,activation='softmax')(hidden1)x_train = [2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7,2,3,4,5,8,1,6,7]y_train = [0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,1]model2 = tf.keras.Model(inputs = input_x,outputs = output)model2.compile(optimizer=tf.keras.optimizers.Adam(0.001), #loss=tf.keras.losses.SparseCategoricalCrossentropy(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])history = model2.fit(x_train, y_train, batch_size=4, epochs=1000, verbose=0) tensorflow中的Embedding操作详解]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Wide-Deep Learning for Recommender Systems笔记]]></title>
    <url>%2F2019%2F10%2F08%2FWide-Deep-Learning-for-Recommender-Systems%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Google Play 用的深度神经网络推荐系统，主要思路是将 Memorization(Wide Model) 和 Generalization(Deep Model) 取长补短相结合。1. Overview of System先来看一下推荐系统的整体架构，由两个部分组成，检索系统(或者说候选生成系统） 和 排序系统(排序网络)。首先，用 检索(retrieval) 的方法对大数据集进行初步筛选，返回最匹配 query 的一部分物品列表，这里的检索通常会结合采用 机器学习模型(machine-learned models) 和 人工定义规则(human-defined rules) 两种方法。从大规模样本中召回最佳候选集之后，再使用 排序系统 对每个物品进行算分、排序，分数 $P(y|x)$，$y$ 是用户采取的行动(比如说下载行为)，$x$ 是特征，包括 User features e.g., country, language, demographics Contextual features e.g., device, hour of the day, day of the week Impression features e.g., app age, historical statistics of an app WDL 就是用在排序系统中。 2. Wide and Deep Learning人脑就是一个不断记忆（memorization）并且归纳（generalization）的过程，而这篇论文的思想，就是将宽线性模型（Wide Model，用于记忆，下图左侧）和深度神经网络模型（Deep Model，用于归纳，下图右侧）结合，汲取各自优势形成了 Wide &amp; Deep 模型用于推荐排序（下图中间）。 2.1 Wide Model Memorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data. 要理解的概念是 Memorization，主要是学习特征的共性或者说相关性，产生的推荐是和已经有用户行为的物品直接相关的物品。 用的模型是 逻辑回归(logistic regression, LR)，LR 的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable)。 LR 的特征往往是二值且稀疏的(binary and sparse)，这里同样采用 one-hot 编码，如 “user_installed_app=netflix”，如果用户安装了 Netflix，这个特征的值为 1，否则为 0。 为了达到 Memorization，我们对稀疏的特征采取特征交叉 (cross-product transformation)， 比如说 AND(user_installed_app=netflix, impression_app=pandora”) 这个特征，只有 Netflix 和 Pandora 两个条件都达到了，值才为 1， 这类 feature 解释了共现(co-occurrence) 和 target label 之间的关系。 一个 cross-product transformation 的局限在于，对于在训练集里没有出现过的 query-item pair，它不能进行泛化(Generalization) 到此，总结一下，wide模型的输入是用户安装应用(installation)和为用户展示（impression）的应用间的向量积（叉乘），模型通常训练 one-hot 编码后的二值特征，这种操作不会归纳出训练集中未出现的特征对。 Linear model y = w^T x + b$x = [ x_1, x_2, \cdots, x_d ]$ 是包含了 d个特征的向量，$w = [ w_1, w_2, \cdots, w_d]$ 是模型参数，b 是偏置。 特征包括了原始的输入特征以及 cross-product transformation 特征，cross-product transformation 的式子如下： \phi_k(x) = \prod_{i=1}^d x^{c_{ki}}_i$c_{kj}$是一个布尔变量，如果第 $i$ 个特征是第 $k$ 个 transformation $\phi_k$ 的一部分，那么值就为 1，否则为 0，作用： This captures the interactions between the binary features, and adds nonlinearity to the generalized linear model. 2.2 Deep Model Generalization is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past. 要理解的概念是 Generalization，可以理解为相关性的传递(transitivity)，会学习新的特征组合，来提高推荐物品的多样性，或者说提供泛化能力(Generalization) 泛化往往是通过学习低维稠密特征 (low-dimensional dense embeddings) 来探索过去从未或很少出现的新的特征组合来实现的， 通常的 embedding-based model 有 Factorization Machines(FM) 和 Deep Neural Networks(DNN)。 特殊兴趣或者小众爱好的用户，query-item matrix 非常稀疏，很难学习；然而 dense embedding 的方法还是可以得到对所有 query-item pair 非零的预测，这就会导致 over-fiting，推荐不怎么相关的物品。 这点和 LR 正好互补，因为 LR 只能记住很少的特征组合。 为了达到 Generalization，我们会引入新的小颗粒特征，如类别特征（安装了视频类应用，展示的是音乐类应用，等等，AND(user_installed_category=video, impression_category=music)，这些高维稀疏的类别特征（如人口学特征和设备类别）映射为低纬稠密的向量后，与其他连续特征（用户年龄、应用安装数等）拼接在一起，输入 MLP 中，最后输入逻辑输出单元。 一开始嵌入向量(embedding vectors)被随机初始化，然后训练过程中通过最小化损失函数来优化模型。每一个隐层(hidden-layer)做这样的计算： a^{(l+1)} = f(W^{(l)} a^{(l)} + b^{(l)})$f$ 是激活函数(通常用 ReLU)，l是层数。 总结一下，基于 embedding 的深度模型的输入是 类别特征(产生embedding) + 连续特征。 2.3 Joint Training对两个模型的输出算 log odds ratio 然后加权求和，作为预测。 Joint Training vs Ensemble Joint Training 同时训练 wide &amp; deep 模型，优化的参数包括两个模型各自的参数以及 weights of sum Ensemble 中的模型是分别独立训练的，互不干扰，只有在预测时才会联系在一起 用 mini-batch stochastic optimization 来进行训练，可以看下这篇论文Efficient Mini-batch Training for Stochastic Optimization。 在论文提到的实验中，训练时 Wide Model 部分用了 Follow-the-regularized-learder(FTRL)+ L1 正则， Deep Model 用了 AdaGrad，对于逻辑回归，模型预测如下： P(Y=1 | x ) = \sigma ( w^T_{wide}[ x, \phi(x) ] + w^T_{deep} a^{(l_f)} + b )其中 $Y$非0即1，$\sigma$是sigmoid函数， $\phi(x)$ 是 $x$ 的交叉特征(cross product transformations)，$b$ 是偏置项。$w_{wide}$是wide部分的权重，$w_{deep}$ 是应用在最终 $a^{(l_f)}$上的权重。 2.4 System Implementation整个pipline如下图所示： 2.4.1 Data Generation Label: 标准是 app acquisition，用户下载为 1，否则为 0 Vocabularies: 将类别特征(categorical features)映射为整型的 id，连续的实值先用累计分布函数CDF归一化到[0, 1]，再划档离散化。 Continuous real-valued features are normalized to [0, 1] by mapping a feature value x to its cumulative distribution function P(X ≤ x), divided into nq quantiles. The normalized value is i−1nq−1for values in the i-th quantiles. 2.4.2 Model Training训练数据有 500 billion examples，输入层(Input layer) 会同时产生稀疏(sparse)的和稠密(dense)的特征，具体的 Model 上面已经讨论过了。 需要注意的是，当新的训练数据来临的时候，我们用的是热启动(warm-starting)方式，也就是从之前的模型中读取 embeddings 以及 linear model weights 来初始化一个新模型，而不是全部推倒重新训练。 2.4.3 Model Serving当模型训练并且优化好之后，我们将它载入服务器，对每一个 request，排序系统从检索系统接收候选列表以及用户特征，来为每一个 app 算分排序， 分数就是前向传播的值(forward inference)啦，可以并行训练提高 performance。 论文笔记 - Wide and Deep Learning for Recommender Systems 《Wide &amp; Deep Learning for Recommender Systems 》笔记 深度学习第二课：个性化推荐]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Factorization Machines for Sparse Predictive Analytics笔记]]></title>
    <url>%2F2019%2F10%2F07%2FNeural-Factorization-Machines-for-Sparse-Predictive-Analytics%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[NFM充分结合了FM提取的二阶线性特征与神经网络提取的高阶非线性特征。总得来说，FM可以被看作一个没有隐含层的NFM，故NFM肯定比FM更具表现力。FM预测公式： \hat{y}(\mathbf{x})_{FM} = w_0 + \sum_{i=1}^{n}w_i x_i + \frac{1}{2} \sum_{f=1}^{k} ((\sum_{i=1}^{n}v_{i,f} x_i)^2-\sum_{i=1}^{n}v_{i,f}^2 x_i^2) NFM预测公式： \hat{y}(\mathbf{x})_{NFM} = w_0 + \sum_{i=1}^{n}w_i x_i + f(\mathbf{x})其中第$1$项与第$2$项是与FM相似的线性回归部分，第3项是NFM的核心部分，它由一个如下图所示的网络结构组成： Embedding Layer 该层是一个全连接层，将稀疏的向量给压缩表示。假设我们有 $v_i \in R^k$ 为第 $i$ 个特征的embedding向量，那么在经过该层之后，我们得到的输出为 $ \lbrace x_1 v_1, \cdots, x_n v_n \rbrace$, 注意，该层本质上是一个全连接层，不是简单的embedding lookup。 Bi-Interaction Layer 上层得到的输出是一个特征向量的embedding的集合，本层本质上是做一个pooling的操作，让这个embedding向量集合变为一个向量，公式如下： f_{BI} (\mathcal{V}_x) = \sum_{i=1}^n \sum_{j=i+1}^n x_i v_i \bigodot x_j v_j 其中 $\bigodot$ 代表两个向量对应的元素相乘。显然，该层的输出向量为 $k$ 维，本层采用的pooling方式与传统的max pool和average pool一样都是线性复杂度的，上式可以变换为: f_{BI}(\mathcal{V}_x) = \frac{1}{2} \lbrack (\sum_{i=1}^n x_i v_i)^2 - \sum_{i=1}^n (x_i v_i)^2 \rbrack 上式中用 $v_2$ 来表示 $v \bigodot v$ ，其实本层本质上就是一个FM算法。 Hidden Layer 普通的全连接层。 Prediction Layer 将Hidden Layer的输出经过全连接层，得到最终的Score。 NFM_tensorflow实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# this is code# Set graph level random seedtf.set_random_seed(self.random_seed)# Input data.self.train_features = tf.placeholder(tf.int32, shape=[None, None]) # None * features_Mself.train_labels = tf.placeholder(tf.float32, shape=[None, 1]) # None * 1self.dropout_keep = tf.placeholder(tf.float32, shape=[None])self.train_phase = tf.placeholder(tf.bool)# Variables.self.weights = self._initialize_weights()# Model.# _________ sum_square part _____________# get the summed up embeddings of features.nonzero_embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'], self.train_features)self.summed_features_emb = tf.reduce_sum(nonzero_embeddings, 1) # None * K# get the element-multiplicationself.summed_features_emb_square = tf.square(self.summed_features_emb) # None * K# _________ square_sum part _____________self.squared_features_emb = tf.square(nonzero_embeddings)self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1) # None * K# ________ FM __________self.FM = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb) # None * Kif self.batch_norm: self.FM = self.batch_norm_layer(self.FM, train_phase=self.train_phase, scope_bn='bn_fm')self.FM = tf.nn.dropout(self.FM, self.dropout_keep[-1]) # dropout at the bilinear interactin layer# ________ Deep Layers __________for i in range(0, len(self.layers)): self.FM = tf.add(tf.matmul(self.FM, self.weights['layer_%d' % i]), self.weights['bias_%d' % i]) # None * layer[i] * 1 if self.batch_norm: self.FM = self.batch_norm_layer(self.FM, train_phase=self.train_phase, scope_bn='bn_%d' % i) # None * layer[i] * 1 self.FM = self.activation_function(self.FM) self.FM = tf.nn.dropout(self.FM, self.dropout_keep[i]) # dropout at each Deep layerself.FM = tf.matmul(self.FM, self.weights['prediction']) # None * 1# _________out _________Bilinear = tf.reduce_sum(self.FM, 1, keep_dims=True) # None * 1self.Feature_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.weights['feature_bias'], self.train_features), 1) # None * 1Bias = self.weights['bias'] * tf.ones_like(self.train_labels) # None * 1self.out = tf.add_n([Bilinear, self.Feature_bias, Bias]) # None * 1# Compute the loss.if self.loss_type == 'square_loss': if self.lambda_bilinear &gt; 0: self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out)) + tf.contrib.layers.l2_regularizer( self.lambda_bilinear)(self.weights['feature_embeddings']) # regulizer else: self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out))elif self.loss_type == 'log_loss': self.out = tf.sigmoid(self.out) if self.lambda_bilinear &gt; 0: self.loss = tf.contrib.losses.log_loss(self.out, self.train_labels, weight=1.0, epsilon=1e-07, scope=None) + tf.contrib.layers.l2_regularizer( self.lambda_bilinear)(self.weights['feature_embeddings']) # regulizer else: self.loss = tf.contrib.losses.log_loss(self.out, self.train_labels, weight=1.0, epsilon=1e-07, scope=None)# Optimizer.if self.optimizer_type == 'AdamOptimizer': self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(self.loss)elif self.optimizer_type == 'AdagradOptimizer': self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=1e-8).minimize(self.loss)elif self.optimizer_type == 'GradientDescentOptimizer': self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)elif self.optimizer_type == 'MomentumOptimizer': self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize( self.loss) Neural Factorization Machines for Sparse Predictive Analytics 论文笔记《Neural Factorization Machines for Sparse Predictive Analytics》 hexiangnan/neural_factorization_machine]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Field-aware Factorization Machines for CTR Prediction笔记]]></title>
    <url>%2F2019%2F10%2F06%2FField-aware-Factorization-Machines-for-CTR-Prediction%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[FFM(Field Factorization Machine)是在FM的基础上引入了场（Field）的概念而形成的新模型。 在FM中计算特征 $x_i$ 与其他特征的交叉影响时, 使用的都是同一个隐向量 $v_i$ 。 而FFM将特征按照事先的规则分为多个场(Field), 特征 $x_i$ 属于某个特定的场$f$。 每个特征将被映射为多个隐向量 $v_{i,1}, v_{i,2}, \cdots, v_{i,f}$ , 每个隐向量对应一个场。 当两个特征 $x_i, x_j$ 组合时, 用对方对应的场对应的隐向量做内积: w_{i,j} = v_{i, f_j} \cdot v_{j, f_i}FFM 由于引入了场, 使得每两组特征交叉的隐向量都是独立的, 可以取得更好的组合效果, FM 可以看做只有一个场的 FFM。 在FFM中，每一维特征 $x_i$，针对其它特征的每一种field $f_j$，都会学习一个隐向量 $v_{i, f_j}$。 这比FM增加了隐向量的数量。 FFM 预测公式： \hat{y}(\mathbf{x}) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i} \rangle x_i x_j其中，$f_j$ 是第 $j$ 个特征所属的field。如果隐向量的长度为 $k$，那么FFM的二次参数有 $nfk$ 个，远多于FM模型的 $nk$ 个。 此外，由于隐向量与field相关，FFM二次项并不能够化简，其预测复杂度是 $O(kn^2)$。 Yu-Chin Juan实现了一个C++版的FFM模型，源码可从Github下载。这个版本的FFM省略了常数项和一次项，模型方程如下: \phi(\mathbf{w}, \mathbf{x}) = \sum_{j_1, j_2 \in \mathcal{C}_2} \langle \mathbf{v}_{j_1, f_2}, \mathbf{v}_{j_2, f_1} \rangle \cdot x_{j_1} x_{j_2}其中，$\mathcal{C}_2$是非零特征的二元组合，$j_1$ 是特征，属于field $f_1$，$w_{j_1,f_2}$ 是特征 $j_1$ 对field $f_2$ 的隐向量。 此FFM模型采用logistic loss作为损失函数，和L2惩罚项，因此只能用于二元分类问题。 \min_{\mathbf{w}} \sum_{i=1}^L \log \big( 1 + \exp\{ -y_i \phi (\mathbf{w}, \mathbf{x}_i ) \} \big) + \frac{\lambda}{2} \| \mathbf{w} \|^2其中，$y_i \in {−1,1}$ 是第 $i$ 个样本的label，$L$ 是训练样本数量，$\lambda$ 是惩罚项系数。 FFM_TensorFlow实现代码: 1234567891011121314151617181920212223# this is codewith tf.variable_scope('linear_layer'): w0 = tf.get_variable('w0', shape=[self.num_classes], initializer=tf.zeros_initializer()) self.w = tf.get_variable('w', shape=[self.p, num_classes], initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) self.linear_terms = tf.add(tf.matmul(self.X, self.w), w0) with tf.variable_scope('interaction_layer'): self.v = tf.get_variable('v', shape=[self.p, self.field, self.k], initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) self.interaction_terms = tf.constant(0, dtype='float32') for i in range(self.p): for j in range(i + 1, self.p): self.interaction_terms += tf.multiply( tf.reduce_sum(tf.multiply(self.v[i, self.feature2field[i]], self.v[j, self.feature2field[j]])), tf.multiply(self.X[:, i], self.X[:, j])) self.interaction_terms = tf.reshape(self.interaction_terms, [-1, 1]) self.y_out = tf.math.add(self.linear_terms, self.interaction_terms) if self.num_classes == 2: self.y_out_prob = tf.nn.sigmoid(self.y_out) elif self.num_classes &gt; 2: self.y_out_prob = tf.nn.softmax(self.y_out) 算法流程及工程实现的trick，详细阅读 参考文献[1]。 深入FFM原理与实践 Field-aware Factorization Machine LLSean/data-mining]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Factorization Machines笔记]]></title>
    <url>%2F2019%2F10%2F05%2FFactorization-Machines%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[FM预测公式： \hat{y}(x) = w_0 + \sum_{i=1}^n w_i x_i + \sum_{i=1}^n \sum_{j=i+1}^n \cdot x_i x_y其中， = \sum_{f=1}^{k} v_{i,f} \cdot v_{j,f} 二阶交叉部分可以通过数学转化，降低计算复杂度： \begin{aligned} pair\_interactions &= \sum_{i=1}^n \sum_{j=i+1}^n x_i x_j \\ &= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n x_i x_j - \frac{1}{2} \sum_{i=1}^n x_i x_i \\ &= \frac{1}{2} \lgroup \sum_{i=1}^n \sum_{j=1}^n \sum_{f=1}^k v_{i, f} v_{j,f} x_i x_j - \sum_{i=1}^n \sum_{f=1}^k v_{i,f} v_{i,f} x_i x_i \rgroup \\ &= \frac{1}{2} \sum_{f=1}^k \lgroup \lgroup \sum_{i=1}^n v_{i,f} x_i \rgroup \lgroup \sum_{j=1}^n v_{j,f} x_j \rgroup - \sum_{i=1}^n v_{i,f}^2 x_i^2 \rgroup \\ &= \frac{1}{2} \sum_{f=1}^k \lgroup \lgroup \sum_{i=1}^n v_{i,f} x_i \rgroup^2 - \sum_{i=1}^n v_{i,f}^2 x_i^2 \rgroup \end{aligned}最终FM的预测公式为： \hat{y}(\mathbf{x}) = w_0 + \sum_{i=1}^{n}w_i x_i + \frac{1}{2} \sum_{f=1}^{k} ((\sum_{i=1}^{n}v_{i,f} x_i)^2-\sum_{i=1}^{n}v_{i,f}^2 x_i^2)FM的训练复杂度，利用SGD（Stochastic Gradient Descent）训练模型。模型各个参数的梯度如下: \frac{\partial}{\partial\theta} y (\mathbf{x}) = \begin{cases} 1, & if \quad \theta \quad is \quad w_0 \\ x_i, & if \quad \theta \quad is \quad w_i \\ x_i \sum_{j=1}^n v_{j, f} x_j - v_{i, f} x_i^2, & if \quad \theta \quad v_{i, f} \end{cases}其中, $v_{j,f}$ 是隐向量 $v_j$ 的第 $f$ 个元素。 由于 $\sum_{j=1}^n v_{j,f} x_j$ 只与 $f$ 有关，而与 $i$ 无关，在每次迭代过程中，只需计算一次所有 $f$ 的 $\sum_{j=1}^n v_{j,f} x_j$, 就能够方便地得到所有 $v_{i,f}$ 的梯度。显然，计算所有 $f$ 的 $\sum_{j=1}^n v_{j,f} x_j$ 的复杂度是 $O(kn)$； 已知 $\sum_{j=1}^n v_{j,f} x_j$ 时，计算每个参数梯度的复杂度是 $O(1)$；得到梯度后，更新每个参数的复杂度是 $O(1)$； 模型参数一共有 $nk + n + 1$ 个。因此，FM参数训练的复杂度也是 $O(kn)$。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。 MSE为： L = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda_w ||W||^2 + \lambda_v ||V||^2FM_TensorFlow为： 其中p为特征维度，k为$v$的维度，label是one-hot形式的 12345678910111213141516171819202122232425262728293031"""y'(x) = w0 + sum( wi * xi ) + 0.5 * sum( (vi xi)**2 - vi**2 * xi**2 )"""with tf.variable_scope('linear_layer'): # 单独的全局bias w0 = tf.get_variable(name='w0', shape=[self.num_classes], initializer=tf.zeros_initializer()) # 线性乘积部分 self.w = tf.get_variable(name='w', shape=[self.p, num_classes], initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) # [n, feature_num] * [feature_num, num_classes] -&gt; [n, num_classes] # [n, num_classes] + [feature_num] -&gt; [n, num_classes] self.linear_terms = tf.add(tf.matmul(self.X, self.w), w0) with tf.variable_scope('interaction_layer'): # 特征交叉部分 self.v = tf.get_variable(name='v', shape=[self.p, self.k], initializer=tf.truncated_normal_initializer(mean=0, stddev=0.01)) self.interaction_terms = tf.multiply(0.5, tf.reduce_mean(tf.subtract(tf.pow(tf.matmul(self.X, self.v), 2), tf.matmul(self.X, tf.pow(self.v, 2))), 1, keep_dims=True)) with tf.name_scope("predict_layer"): self.y_out = tf.add(self.linear_terms, self.interaction_terms) if self.num_classes == 2: self.y_out_prob = tf.nn.sigmoid(self.y_out) elif self.num_classes &gt; 2: self.y_out_prob = tf.nn.softmax(self.y_out) 论文及工程地址： Factorization Machines fm_tensorflow LLSean/data-mining]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java修饰符总结]]></title>
    <url>%2F2019%2F09%2F21%2FJava%E4%BF%AE%E9%A5%B0%E7%AC%A6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[复制粘贴自: Java修饰符详解 1. 访问修饰符 default(默认的): 在同一包内可见，不使用任何修饰符; private(私有的): 在同一类内可见; public(共有的): 对所有类可见; protected(受保护的): 对同一包内的类和所有子类可见; 1.1 default接口里的变量都隐式声明为public static final, 而接口里的方法默认情况下访问权限为public。 1234String version = "1.5.1";boolean processOrder() &#123; return true;&#125; 1.2 private私有访问修饰符是最严格的访问级别, 被声明为 private 的方法、变量和构造方法只能被所属类访问， 并且类和接口不能声明为 private, 声明为私有访问类型的变量只能通过类中公共的 getter/setter 方法被外部类访问。 Private 访问修饰符的使用主要用来隐藏类的实现细节和保护类的数据。 123456789public class Logger &#123; private String format; public String getFormat() &#123; return this.format; &#125; public void setFormat(String format) &#123; this.format = format; &#125;&#125; 1.3 public如果几个相互访问的 public 类分布在不同的包中, 则需要导入相应 public 类所在的包。 由于类的继承性, 类所有的公有方法和变量都能被其子类继承。 Java 程序的 main() 方法必须设置成公有的, 否则, Java 解释器将不能运行该类。 123public static void main(String[] arguments) &#123; // ...&#125; 1.4 protectedprotected 访问修饰符不能修饰类和接口，方法和成员变量能够声明为 protected, 但是接口的成员变量和成员方法不能声明为 protected。 子类能访问 protected 修饰符声明的方法和变量，这样就能保护不相关的类使用这些方法和变量。 下面的父类 AudioPlayer 使用了 protected 访问修饰符，子类重载了父类的 openSpeaker() 方法, 子类的访问修饰符要小于等于父类，这里可以用protected/public。 123456789101112class AudioPlayer &#123; protected boolean openSpeaker(Speaker sp) &#123; // 实现细节 &#125;&#125;class StreamingAudioPlayer extends AudioPlayer &#123; @Override protected boolean openSpeaker(Speaker sp) &#123; // 实现细节 &#125;&#125; 方法继承的规则 父类中声明为 public 的方法在子类中也必须为 public。 父类中声明为 protected 的方法在子类中要么声明为 protected，要么声明为 public，不能声明为 private, 也不能是default。 父类中声明为 private 的方法，不能够被继承。 2. 非访问修饰符 static: 用来创建类方法和类变量; final: 用来修饰类、方法和变量。final 修饰的类不能够被继承，修饰的方法不能被继承类重新定义，修饰的变量为常量，是不可修改的; abstract: 用来创建抽象类和抽象方法; synchronized: 主要用于线程的编程; transient: 部分数据不需要序列化控制; volatile: 主要用于线程的编程; 2.1 static 静态变量 static 关键字用来声明独立于对象的静态变量, 无论一个类实例化多少对象, 它的静态变量只有一份拷贝。静态变量也被成为类变量, 局部变量不能被声明 static 变量。 静态方法 Static 关键字用来声明独立于对象的静态方法。静态方法不能使用类的非静态变量。静态方法从参数列表得到数据，然后计算这些数据。对类变量和方法的访问可以直接使用 classname.variablename 和 classname.methodname 的方式访问。 1234567891011121314151617181920212223public class InstanceCounter &#123; private static int numInstances = 0; protected static int getCount() &#123; return numInstances; &#125; private static void addInstance() &#123; numInstances++; &#125; InstanceCounter() &#123; InstanceCounter.addInstance(); &#125; public static void main(String[] args) &#123; System.out.println("Starting with " + InstanceCounter.getCount() + " instances"); for (int i = 0; i &lt; 500; ++i) &#123; new InstanceCounter(); &#125; System.out.println("Created " + InstanceCounter.getCount() + " instances"); &#125;&#125; 12Starting with 0 instancesCreated 500 instances 2.2 final2.2.1 final变量final 变量能被显式地初始化并且只能初始化一次。被声明为 final 的对象的引用不能指向不同的对象。 但是 final 对象里的数据可以被改变, 也就是说 final 对象的引用不能改变，但是里面的值可以改变。 final 修饰符通常和 static 修饰符一起使用来创建类常量。 1234567891011public class Test &#123; final int value = 10; // 声明常量的实例 public static final int BOXWIDTH = 6; static final String TITLE = "Manager"; public void changeValue() &#123; value = 12; //将输出一个错误 &#125;&#125; 2.2.2 final方法类中的 final 方法可以被子类继承，但是不能被子类修改。 声明 final 方法的主要目的是防止该方法的内容被修改。 12345public class Test&#123; public final void changeName() &#123; // 方法实现 &#125;&#125; 2.2.3 final类final 类不能被继承，没有类能够继承 final 类的任何特性。 123public final class Test &#123; // 类体&#125; 2.3 abstract2.3.1 抽象类抽象类不能用来实例化对象，声明抽象类的唯一目的是为了将来对该类进行扩充。 一个类不能同时被 abstract 和 final 修饰。如果一个类包含抽象方法，那么该类一定要声明为抽象类，否则将出现编译错误。 抽象类可以包含抽象方法和非抽象方法。 12345678abstract class Caravan &#123; private double price; private String model; private String year; //抽象方法 public abstract void goFast(); public abstract void changeColor();&#125; 2.3.2 抽象方法 抽象方法是一种没有任何实现的方法，该方法的的具体实现由子类提供; 抽象方法不能被声明成 final 和 static; 任何继承抽象类的子类必须实现父类的所有抽象方法，除非该子类也是抽象类; 如果一个类包含若干个抽象方法，那么该类必须声明为抽象类。抽象类可以不包含抽象方法; 抽象方法的声明以分号结尾，例如：public abstract sample(); 123456789101112// 抽象类public abstract class SuperClass &#123; //抽象方法 abstract void m();&#125;class SubClass extends SuperClass &#123; //实现抽象方法 void m()&#123; //......... &#125;&#125; 2.4 synchronized synchronized 关键字声明的方法同一时间只能被一个线程访问; synchronized 修饰符可以应用于四个访问修饰符; 123public synchronized void showDetails() &#123; //.......&#125; 2.5 transient序列化的对象包含被 transient 修饰的实例变量时，java 虚拟机(JVM)跳过该特定的变量。 该修饰符包含在定义变量的语句中，用来预处理类和变量的数据类型。 一旦变量被transient修饰, 变量将不再是对象持久化的一部分, 该变量内容在序列化后无法获得访问; transient关键字只能修饰变量，而不能修饰方法和类; 注意本地变量是不能被transient关键字修饰的; 变量如果是用户自定义类变量，则该类需要实现Serializable接口; 被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化; 1234// 不会持久化public transient int limit = 55;// 持久化public int b; 2.5 volatilevolatile 修饰的成员变量在每次被线程访问时，都强制从共享内存中重新读取该成员变量的值。 而且当成员变量发生变化时，会强制线程将变化值回写到共享内存。 这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。 一个 volatile 对象引用可能是 null。 12345678910111213public class MyRunnable implements Runnable &#123; private volatile boolean active; public void run() &#123; active = true; while (active) &#123; // 第一行 // 代码 &#125; &#125; public void stop() &#123; active = false; // 第二行 &#125;&#125; 通常情况下，在一个线程调用 run() 方法 (在 Runnable 开启的线程), 在另一个线程调用 stop() 方法。 如果 第一行 中缓冲区的 active 值被使用，那么在 第二行 的 active 值为 false 时循环不会停止。 但是以上代码中我们使用了 volatile 修饰 active，所以该循环会停止。]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化算法]]></title>
    <url>%2F2019%2F06%2F05%2FOptimization-Method%2F</url>
    <content type="text"><![CDATA[本文是从各个论文、博客、专栏等学习整理所得,如有任何错误疏漏等问题,欢迎评论或邮箱提出,大家一起学习进步！0 优化算法框架 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{0-1} 根据历史梯度计算一阶动量和二阶动量 m_t = \phi (g_1, g_2, \cdots, g_t) \tag{0-2} V_t = \psi (g_1, g_2, \cdots, g_t) \tag{0-3} 计算当前时刻的下降梯度 \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t \tag{0-4} 根据下降梯度进行更新 w_{t+1} = w_t - \eta_t \tag{0-5} 核心区别是第3步执行的下降方向,在这个式子中,前半部分是实际的学习率（也即下降步长）,后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将重要的地方标红显示 1. Gradient Descent Variants1.1 Batch Gradient Descent 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{1.1-1} 根据历史梯度计算一阶动量和二阶动量 \color{red} {m_t = g_t} \tag{1.1-2} \color{red} {V_t = 1} \tag{1.1-3} 计算当前时刻的下降梯度 \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t \tag{1.1-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t \end{align*} \tag{1.1-5} 1.2 Stochastic Gradient Descent 计算目标函数关于当前参数的梯度 \color{red} { g_t = \nabla f(w_t; x^{(i)}, y^{(i)}) } \tag{1.2-1}其中$(x^{(i)}, y^{(i)})$表示第$i$个样本； 根据历史梯度计算一阶动量和二阶动量 \color{red} {m_t = g_t} \tag{1.2-2} \color{red} {V_t = 1} \tag{1.2-3} 计算当前时刻的下降梯度 \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t \tag{1.2-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t \end{align*} \tag{1.2-5} 1.3 Mini-Bach Gradient Descent 计算目标函数关于当前参数的梯度 \color{red} { g_t = \nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } \tag{1.3-1}其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本,$n$表示mini-batch的大小； 根据历史梯度计算一阶动量和二阶动量 \color{red} {m_t = g_t} \tag{1.3-2} \color{red} {V_t = 1} \tag{1.3-3} 计算当前时刻的下降梯度 \eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t \tag{1.3-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t \end{align*} \tag{1.3-5} 上述算法存在的问题： 很难调整出一个合适的learning_rate learning_rate的变化规则往往是预定义的,很难适应不同的数据 所有的特征共享相同的learning_rate 局部最有解的问题 2. Gradient Descent Optimization Algorithm2.1 Gradient Descent with Momentum 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{2.1-1} 根据历史梯度计算一阶动量和二阶动量,计算当前时刻下降梯度(将框架的第2步和第3步合并) \color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t } \tag{2.1-2&3} \color{red} { \eta_t = m_t } \tag{2.1-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t ) \end{align*} \tag{2.1-5} 一阶动量是移动平均值,这里 $\gamma $ 的经验值为0.9。 以历史动量的和为主,适当加上一点当前的偏移,即考虑惯性的因素。 2.2 Nesterov Accelerated Gradient 计算目标函数关于当前参数+下次变化的梯度 \color{red} { g_t = \nabla f(w_t - \gamma m_{t-1}) } \tag{2.2-1} 根据历史梯度计算一阶动量和二阶动量,计算当前时刻下降梯度(将框架的第2步和第3步合并) \color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t } \tag{2.2-2&3} \color{red} { \eta_t = m_t } \tag{2.2-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t ) \end{align*} \tag{2.2-5} 这里 $\gamma $ 的经验值为0.9,在对参数求导时,不再和之前方法一直,而是对下次参数求导,即多向前看一步可以用来指导当前怎么走。 2.3 AdaGrad 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{2.3-1} 根据历史梯度计算一阶动量和二阶动量 m_t = g_t \tag{2.3-2} \color{red} {V_t = \sum_{\tau = 1}^t g_{\tau}^2 } \tag{2.3-3} 计算当前时刻的下降梯度 \begin{align*} \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t \end{align*} \tag{2.3-4}这里$\epsilon$是为了避免分母为$0$,通常设置为$1e-8$。 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t \end{align*} \tag{2.3-5} 不同的参数具有不同的梯度,从而达到了不同参数具有不同学习率的目的,这样梯度更新快的参数,学习率(步长)会渐渐变小。 同样最终会面临学习率过小,模型无法继续学习的问题。 2.4 AdaDelta首先定义动态平均值 $\color{red}{ E[ g^2 ]_t = \gamma E[ g^2 ]_{t-1} + (1 - \gamma) g_t^2 }$,该值仅取决于当前梯度值与上一时刻的动态平均值,其中$\gamma$通常设置成$0.9$。 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{2.4-1} 根据历史梯度计算一阶动量和二阶动量 m_t = g_t \tag{2.4-2} \color{red} {V_t = E[ g^2 ]_t } \tag{2.4-3} 计算当前时刻的下降梯度 \begin{align*} \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ E[ g^2 ]_t + \epsilon }} \cdot m_t \end{align*} \tag{2.4-4.1}这里$\epsilon$是为了避免分母为$0$。将分母$ \sqrt{ E[ g^2 ]_t + \epsilon} $ 记为 $\color{red}{ RMS[g]_t} $,定义 E[ \Delta g^2 ] _t = \gamma E[ \Delta g^2 ] _{t-1} + (1 - \gamma) \Delta g _t^2则： RMS[\Delta g] _t = \sqrt{ E[ \Delta g^2 ] _t + \epsilon }用$RMS[\Delta g]_{t-1}$代替学习率$\alpha$,则式$(2.4-4.1)$可以转化为: \begin{align*} \eta_t &= \frac{\alpha}{RMS[g] _t} \cdot g_t \\ &= \frac{ RMS[\Delta g] _{t-1} }{ RMS[g] _t } \cdot g _t \end{align*} \tag{2.4-4.2} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{RMS[\Delta g] _{t-1}}{RMS[g] _t} \cdot g_t \end{align*} \tag{2.4-5} $\color{red}{为什么}$用$RMS[\Delta g]_{t-1}$代替学习率$\alpha$?? 2.5 RMSpropRMSprop是AdaDelta算法的一个特例。 E[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t w_{t+1} = w_t - \frac{\alpha}{ \sqrt{ E[g^2] _t + \epsilon } } g_tHinton建议$\gamma$设置成$0.9$,学习率设置成$0.001$。 2.6 Adam 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{2.6-1} 根据历史梯度计算一阶动量和二阶动量 \color{red}{ \hat{m_t} = \beta_1 \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\ \color{red}{ m_t = \frac{\hat{m_t}}{1 - \beta_1^t} } \tag{2.6-2} \color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\ \color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } } \tag{2.6-3}其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量； 计算当前时刻的下降梯度 \begin{align*} \eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \end{align*} \tag{2.6-4}其中增加的$\epsilon$为了防止分母等于$0$； 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \end{align*} \tag{2.6-5}作者建议默认值$\beta_1 = 0.9$,$\beta_2 = 0.999$,$\epsilon = 1e-8$。 2.7 AdaMaxAdamax是Adam的一种变体,此方法对学习率的上限提供了一个更简单的范围。 计算目标函数关于当前参数的梯度 g_t = \nabla f(w_t) \tag{2.7-1} 根据历史梯度计算一阶动量和二阶动量 \color{red}{ m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t } \tag{2.7-2} \color{red}{ \begin{align*} V_t &= \beta_2^{\infty} V_{t-1} + (1 - \beta_2^{\infty}) \| g_t \| ^{\infty} \\ &= max( \beta_2 \cdot V_{t-1}, \| g_t \| ) \end{align*} } \tag{2.7-3}其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量； 计算当前时刻的下降梯度 \color{red}{ \begin{align*} \eta_t &= \frac{\alpha}{V_t} \cdot m_t \\ &= \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \} \end{align*} } \tag{2.7-4} 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \} \end{align*} \tag{2.7-5}论文说合适的默认值为$\alpha = 0.002, \beta_1 = 0.9, \beta_2 = 0.999$。 2.8 Nadam 计算目标函数关于当前参数的梯度 \color{red}{ g_t = \nabla f(w_t - \frac{\alpha}{\sqrt{V_t}} \cdot m_{t-1}) } \tag{2.8-1} 根据历史梯度计算一阶动量和二阶动量 \color{red}{ \hat{m_t} = \gamma \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\ \color{red}{ m_t = \frac{\hat{m_t}}{ 1 - \beta_1^t } } \tag{2.8-2} \color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\ \color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } } \tag{2.8-3}其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量； 计算当前时刻的下降梯度 \begin{align*} \eta_t &= \frac{\alpha}{\sqrt{V_t} + \epsilon } \cdot m_t \end{align*} \tag{2.8-4}其中增加的$\epsilon$为了防止分母等于$0$； 根据下降梯度进行更新 \begin{align*} w_{t+1} &= w_t - \eta_t \end{align*} \tag{2.8-5} 3. Online Optimization Algorithm上面描述的主要是批量训练的优化方法，批量训练有自身的局限性：面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效。因此需要有在线处理的方法(Online)来解决相同的问题。在线学习算法的特点是：每来一个训练样本，就用该样本产生的loss和梯度对模型迭代一次，一个一个数据地进行训练，因此可以处理大数据量训练和在线训练。 3.1 Truncated Gradient3.1.1 L1正则法 w^{(t+1)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} - \eta^{(t)} \cdot \lambda \cdot sgn( w^{(t)} ) \tag{3.1-1}其中, $\lambda \in \mathbb{R} $是一个标量,且$\lambda \ge 0$,为L1正则化参数； $sgn(v)$为符号函数； $\eta^{(t)}$为学习率,通常将其设置为$1/\sqrt{t}$的函数； $G^{(t)} = \nabla_w f(w^{(t)}, z^{(t)}) $代表了第$t$次迭代中损失函数的梯度； 3.1.2 简单截断法以$k$为窗口,当$t/k$不为整数时采用标准SGD进行迭代；当$t/k$为整数时,采用如下权重更新方式； w^{(t+1)} = T_0 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \theta \right) T_0 (v_i, \theta) = \begin{cases} 0 & \quad \mid v_i \mid \le \theta \\ v_i & \quad otherwise \end{cases} \tag{3.1-2}$\theta \in \mathbb{R}$是一个标量,且$\theta \ge 0$； 3.1.3 截断梯度法(TG)简单截断法太过激进,因此TG在此基础上进行了改进： w^{(t+1)} = T_1 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \quad \eta^{(t)} \lambda^{(t)}, \quad \theta \right) T_1 (v_i, \alpha, \theta) = \begin{cases} max(0, \quad v_i - \alpha ) & \quad v_i \in [0, \theta] \\ max(0, \quad v_i + \alpha ) & \quad v_i \in [- \theta, 0 ] \\ v_i & \quad otherwise \end{cases} \tag{3.1-3} 其中$\lambda^{(t)} \in \mathbb{R}$,且$\lambda^{(t)} \ge 0$； TG 同样是以$k$为窗口,每$k$步进行一次截断。 当$t/k$不为整数时,$\lambda^{(t)} = 0$； 当$t/k$为整数时,$\lambda^{(t)} = k \cdot \lambda$。 从公式$(3.1-3)$可以看出,超参数$\lambda$和$\theta$决定了$w$的稀疏程度,这两个值越大,则稀疏性越强；尤其令$\lambda = \theta$时,只需要通过调节一个参数就能控制稀疏性。 根据公式$(3.1-3)$，我们很容易写出 TG 的算法逻辑: 3.1.4 截断公式对比 其中左侧是简单截断法的截断公式，右侧是截断梯度的截断公式。公式$(3.1-3)$进行改写，描述特征权重每个维度的更新方式: w^{(t+1)}_i = \begin{cases} Trnc \{ ( w^{(t)}_i - \eta^{(t)} g^{(t)}_i ), \lambda^{(t)}_{TG}, \theta \} & if \quad mod(t,k) = 0 \\ w^{(t)}_i - \eta^{(t)} g^{(t)}_i & otherwise \end{cases} \lambda^{(t)}_{TG} = \eta^{(t)} \lambda k \tag{3.1-4} Trnc( w, \lambda^{(t)}_{TG}, \theta ) = \begin{cases} 0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\ w - \lambda^{(t)}_{TG} sgn(w) & if \quad \lambda^{(t)}_{TG} \le \mid w \mid \le 0 \\ w & otherwise \end{cases}如果令$\lambda^{(t)}_{TG} = \theta$, 截断公式$Trnc(w, \lambda^{(t)}_{TG}, \theta)$变成: Trnc(w, \theta, \theta) = \begin{cases} 0 & if \quad \mid w \mid \le 0 \\ w & otherwise \end{cases}此时$TG$退化成简单截断法。 如果令$\theta = \infty$截断公式$Trnc(w, \lambda^{(t)}_{TG}, \theta)$变成: Trnc(w, \lambda^{(t)}_{TG}, \infty) = \begin{cases} 0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\ w & otherwise \end{cases}如果再令$k=1$,那么特征权重维度更新公式变成: \begin{align*} w^{(t+1)}_i &= Trnc\{ (w^{(t)}_i - \eta^{(t)} g^{(t)}_i), \eta^{(t)} \lambda, \infty \} \\ &= w^{(t)}_i - \eta^{(t)} g^{(t)}_i - \eta^{(t)} \cdot \lambda \cdot sgn(w^{(t)}_i) \end{align*}此时 $TG$ 退化成 L1正则化法。 3.2 FOBOS前向后向切分3.2.1 FOBOS算法原理在 FOBOS 中, 将权重的更新分为两个步骤: w^{(t+0.5)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} \\ w^{(t+1)} = argmin_w \{ \frac{1}{2} \| w - w^{(t+0.5)} \|^2 + \eta^{(t+0.5)} \cdot \psi (w) \} \tag{3.2-1} 前一个步骤实际上是一个标准的梯度下降步骤; 后一个步骤可以理解为对梯度下降的结果进行微调; 前一部分保证微调发生在梯度下降结果的附近 后一部分则用于处理正则化，产生稀疏性 如果将公式 $(3.2-1)$ 中的两个步骤合二为一, 即将 $w^{(t+0.5)}$ 的计算带入到 $w^{(t+1)}$ 中, 有: w^{(t+1)} = argmin _w \{ \frac{1}{2} \| w - w^{(t)} + \eta^{(t)} G^{(t)} \|^2 + \eta^{(t+0.5)} \psi (w) \}令$F(w) = \frac{1}{2} || w - w^{(t)} + \eta^{(t)} G^{(t)} ||^2 + \eta^{(t+0.5)} \psi (w)$, 如果$w^{(t+1)}$存在一个最优解, 那么就可以推断$0$向量一定属于$F(w)$的次梯度集合: 0 \in \partial F(w) = w - w^{(t)} + \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w)由于$w^{(t+1)} = argmin_w F(w)$, 那么有: 0 = \{ w - w^{(t)} - \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w) \} \mid_{w = w^{(t+1)}}上式实际上给出了 FOBOS 中权重更新的另一种形式: w^{(t+1)} = w^{(t)} - \eta^{(t)} G^{(t)} - \eta^{(t+0.5)} \partial \psi (w^{(t+1)})我们这里可以看到, $w^{(t+1)}$不仅仅与迭代前的状态$w^{(t)}$有关，而且与迭代后的$\psi (w^{(t+1)})$有关。 3.2.2 L1-FOBOS在 L1 正则化下，有$\psi (w) = \lambda || w ||_1$, 为了简化描述, 用向量$v= [ v_1, v_2, \cdots, v_N ] \in \mathbb{R}^N$ 来表示$w^{(t+0.5)}$, 用标量$\hat{\lambda} \in \mathbb{R}$ 来表示 $\eta^{(t+0.5)} \lambda$, 并将公式$(3.2-1)$等号右边按维度展开: w^{(t+1)} = argmin_w \sum_{i=1}^N ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid ) \tag{3.2-2}可以看到,在求和公式$\sum_{i=1}^N ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )$ 中的每一项都是大于等于 $0$ 的, 所以公式$(3.2-2)$可以拆解成对特征权重$w$每一维度单独求解: w^{(t+1)}_i = argmin_{w_i} ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid ) 因此, 综合上面的分析得到在 FOBOS 在 L1 正则化条件下，特征权重的各个维度更新的方式为: \begin{align*} w^{(t+1)}_i &= sgn(v_i) max (0, \mid v_i \mid - \hat{\lambda}) \\ &= sgn(w_i^{(t)} - \eta^{(t)} g^{(t)}_i ) max \{ 0, \mid w^{(t)}_i - \eta_t \cdot g_i^{(t)} \mid - \eta^{(t+0.5)} \lambda \} \end{align*} \tag{3.2-3}其中, $g_i^{(t)}$ 为梯度 $G^{(t)}$ 在维度 $i$ 上的取值。 根据公式$(3.2-3)$，我们很容易就可以设计出 L1-FOBOS 的算法逻辑: 3.2.3 L1-FOBOS与TG的关系对于 L1-FOBOS 特征权重的各个维度更新公式$(3.2-3)$，也可以写作如下形式: w^{(t+1)}_i = \begin{cases} 0 & \quad \mid w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \mid \le \eta^{(t+0.5)} \lambda \\ \left( w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \right) - \eta^{(t+0.5)} \cdot \lambda \cdot sgn(w^{(t)}_i - \eta^{(t)} g^{(t)}_i) & \quad otherwise \end{cases}上式截断的含义是：当一条样本产生的梯度不足以令对应维度上的权重值发生足够大的变化 $( \eta^{(t+0.5)} \cdot \lambda )$, 则认为在本次更新过程中该维度不够重要，应当令其权重为$0$。 同时上式与TG的特征权重更新公式$(3.1-4)$对比, 发现如果令$ \theta = \infty, k = 1, \lambda^{(t)}_{TG} = \eta^{(t+0.5} \lambda $,则L1-FOBOS与TG完全一致，因此可以认为L1-FOBOS是TG在特定条件下的特殊形式。 3.3 RDA正则对偶平均简单截断、TG、FOBOS 都是建立在 SGD 的基础之上的，属于梯度下降类型的方法，这类型方法的优点就是精度比较高， 并且TG、FOBOS也都能在稀疏性上得到提升。但是有些其它类型的算法，例如RDA，是从另一个方面来求解 Online Optimization, 并且更有效地提升了特征权重的稀疏性。 3.3.1 RDA算法原理在RDA中，特征权重的更新策略为: w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \psi (w) + \frac{\beta^{(t)}}{t} h(w) \rbrace \tag{3.3-1}其中，$&lt; G^{(r)}, w &gt;$表示梯度 $G^{(r)}$对 $w$ 的积分平均值(积分中值); $\psi (w)$ 为正则项；$h(w)$为一个辅助的严格凸函数； $ \lbrace \beta^{(t)} | t \ge 1 \rbrace $是一个非负且非自减序列。本质上，公式$(3.3-1)$中包含了3个部分： 线性函数 $ \frac{1}{t} \sum_{r=1}^t &lt; G^{(t)}, w &gt; $, 包含了之前所有梯度(或次梯度)的平均值(dual average); 正则项 $\psi (w)$; 额外正则项 $\frac{\beta^{(t)}}{t} h(w)$, 这是个严格凸函数; 3.3.2 L1-RDA令$\psi (w) = \lambda || w ||_1$，并且由于$h(w)$是一个关于$w$的严格凸函数，不妨令 $h(w) = \frac{1}{2} || w ||_2^2 $, 此外，将非负非自减序列 $ \lbrace \beta^{(t)} | t \ge 1 \rbrace $ 定义为 $ \beta^{(t)} = \gamma \sqrt{t} $，将L1正则化代入公式$(3.3-1)$有: w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \lambda \| w \|_1 + \frac{\gamma}{2 \sqrt{t}} \| w \| _2^2 \rbrace \tag{3.3-2}针对特征权重的各个维度将其拆解成 N 个独立的标量最小化问题: minimize_{w_i \in \mathbb{R}} \lbrace \bar{g}_i^{(t)} w_i + \lambda \mid w_i \mid + \frac{\gamma}{2\sqrt{t}} w_i^2 \rbrace \tag{3.3-3}这里 $ \lambda &gt; 0, \frac{\gamma}{\sqrt{t}} &gt; 0, \bar{g}_i^{(t)} = \frac{1}{t} \sum_{r=1}^t g_i^{(r)} $, 公式$(3.3-3)$就是一个无约束的非平滑最优化问题。其中第2项 $\lambda | w_i |$ 在 $w_i$ 处不可导。 假设 $w_i^{\ast}$ 是其最优解，并且定义 $\xi \in \partial | w_i^{\ast} $ 为 $ | w_i | $ 在 $w_i^{\ast}$ 的次导数，那么有： 之后可以得到L1-RDA特征权重的各个维度更新的方式为： w_i^{(t+1)} = \begin{cases} 0 & \quad if \quad \mid \bar{g}_i^{(t)} \mid < \lambda \\ -\frac{\sqrt{t}}{\gamma} ( \bar{g}_i^{(t)} - \lambda \cdot sgn( \bar{g}_i^{(t)} ) ) & \quad otherwise \end{cases} \tag{3.3-6}这里发现，当某个维度上累积梯度平均值的绝对值 $ | g_i^{(t)} | $ 小于阈值𝜆的时候，该维度权重将被置 $0$，特征权重的稀疏性由此产生。 根据公式 $(3.3-6)$，可以设计出 L1-RDA 的算法逻辑: 3.3.3 L1-RDA与L1-FOBOS的比较在 $3.2.2$ 中我们看到了 L1-FOBOS 实际上是 TG 的一种特殊形式, 在 L1-FOBOS 中，进行 截断 的判定条件是 $ | w_i^{(t)} - \eta^{(t)} g_i^{(t)} | \le \lambda^{(t)}_{TG} = \eta^{(t+0.5)} \lambda $ 。 通常会定义 $\eta$ 为与 $1$ 正相关的函数 $(\eta = \Theta (\frac{1}{\sqrt{t}}))$, 因此 L1-FOBOS 的截断阈值为 $\Theta (\frac{1}{\sqrt{t}})) \lambda $, 随着 $t$ 的增加, 这个阈值会逐渐降低。 相比较而言，从$(3.3-6)$可以看出, L1-RDA 的截断阈值为 $\lambda$, 是一个常数，并不随着 $t$ 而变化, 因此可以认为 L1-RDA 比 L1-FOBOS 在截断判定上更加激进, 这种性质使得 L1-RDA 更容易产生稀疏性; 此外，RDA 中判定对象是梯度的累加平均值 $\bar{g}_i^{(t)}$, 不同于 TG 或 L1-FOBOS 中针对单次梯度计算的结果进行判定, 避免了由于某些维度由于训练不足导致截断的问题。并且通过调节 $\lambda$ 一个参数，很容易在精度和稀疏性上进行权衡 3.4 FTRLFTRL综合了L1-FOBOS基于梯度下降方法具有较高的精度、L1-RDA能在损失一定精度的情况下产生更好的稀疏性。 3.4.1 L1-FOBOS和L1-RDA在形式上的统一L1-FOBOS在形式上，令 $ \eta^{(t+0.5)} = \eta^{(t)} = \Theta ( \frac{1}{\sqrt{t}} ) $ 是一个随 $t$变化的非增正序列, 每次迭代都可以表示为： w^{(t+0.5)} = w^{(t)} - \eta^{(t)} G^{(t)} w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t + 0.5)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace把这两个公式合并到一起，有: w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t)} + \eta^{(t)} G^{(t)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace通过这个公式很难直接求出 $w^{(t+1)}$ 的解析解，但是我们可以按维度将其分解为 N 个独立的最优化步骤: \begin{align*} 最优化 &= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)} + \eta^{(t)} g_i^{(t)} )^2 + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\ &= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)})^2 + \frac{1}{2} ( \eta^{(t)} g_i^{(t)} )^2 + w_i \eta^{(t)} g_i^{(t)} + w_i^{(t)} \eta^{(t)} g_i^{(t)} + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\ &= minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid + \frac{1}{2} \eta^{(t)} (w_i - w_i^{(t)})^2 + \lbrack \frac{ \eta^{(t)} }{2} (g_i^{(t)})^2 + w_i^{(t)} g_i^{(t)} \rbrack \rbrace \end{align*}由于 $\frac{ \eta^{(t)} }{2} (g_i^{(t)})^2 + w_i^{(t)} g_i^{(t)}$ 与变量 $ w_i $ 无关，因此上式可以等价于: minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid \frac{1}{2 \eta^{(t)} (w_i - w_i^{(t)})^2 } \rbrace再将这 N 个独立最优化子步骤合并，那么 L1-FOBOS 可以写作: w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2 \eta^{(t)}} | w - w^{(t)} |_2^2 \rbrace而对于 L1-RDA 的公式$(3.3.2-1)$，我们可以写作: w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + t \lambda |w|_1 + \frac{1}{2 \eta^{(t)}} | w - 0 |_2^2 \rbrace这里 $G^{(1:t)} = \sum_{s=1}^t G^{(s)}$; 如果令 $ \sigma^{(s)} = \frac{1}{\eta^{(s)}} - \frac{1}{\eta^{(s-1)}}, \sigma^{(1:t)} = \frac{1}{\eta^{(t)}} $, 上面两个式子可以写做: w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- w^{(t)} | _2^2 \rbrace \tag{3.4.1-1} w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + t \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- 0 | _2^2 \rbrace \tag{3.4.1-2}比较$(3.4.1-1)$和$(3.4.1-2)$这两个公式，可以看出 L1-FOBOS 和 L1-RDA 的区别在于: (1) 前者对计算的是累加梯度以及 L1 正则项只考虑当前模的贡献，而后者采用了累加的处理方式; (2) 前者的第三项限制$ w $的变化不能离已迭代过的解太远，而后者则限制 $w$ 不能离 0 点太远; 3.4.2 FTRL算法原理FTRL 综合考虑了 FOBOS 和 RDA 对于正则项和$w$限制的区别，其特征权重的更新公式为: w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + \lambda_1 |w|_1 + \lambda_2 |w|_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} | w - w^{(s)} |_2^2 \rbrace \tag{3.4.2-1}注意，公式 $(3.4.2-1)$ 中出现了L2正则项 $ \frac{1}{2} || w ||_2^2 $， L2正则项的引入仅仅相当于对最优化过程多了一个约束，使得结果求解结果更加“平滑”。 公式$(3.4.2-1)$看上去很复杂，更新特征权重貌似非常困难的样子。不妨将其进行改写，将最后一项展开，等价于求下面这样一个最优化问题: w^{(t+1)} = argmin_w \lbrace (G^{(1:t)} - \sum_{s=1}^t \sigma^{(s)} w^{(s)} ) \cdot w + \lambda_1 |w|_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) \cdot | w |_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} |w^{(s)}|_2^2 \rbrace由于 $\frac{1}{2} \sum_{s=1}^t \sigma^{(s)} || w^{(s)} ||_2^2$ 相对于 $w$ 来说是一个常数，并且令 $ z^{(t)} = G^{(1:t)} - \sum_{s=1}^t \sigma^{(s)} w^{(s)} $, 上式等价于: w^{(t+1)} = argmin_w \lbrace z^{(t)} \cdot w + \lambda_1 |w|_1 + \frac{1}{2} (\lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) |w|_2^2 \rbrace针对特征权重的各个维度将其拆解成N个独立的标量最小化问题: minimize_{w_i \in \mathbb{R}} \lbrace z_i^{(t)} w_i + \lambda_1 \mid w \mid_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) w_i^2 \rbrace到这里，我们遇到了与式$(3.3.2-2)$类似的优化问题，用相同的分析方法可以得到: w_i^{t+1} = \begin{cases} 0 & \quad if \mid z_i^{(t)} \mid < \lambda \\ -( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} )^{-1} ( z_i^{(t)} - \lambda_1 sng( z_i^{(t)} ) ) & \quad otherwise \end{cases} \tag{3.4.2-2}从式 $(3.4.2-2)$ 可以看出，引入 L2 正则化并没有对 FTRL 结果的稀疏性产生任何影响。 3.4.3 Per-Coordinate Learning Rates前面介绍了 FTRL 的基本推导，但是这里还有一个问题是一直没有被讨论到的:关于学习率 $\eta^{(t)}$ 的选择和计算。 事实上在 FTRL 中，每个维度上的学习率都是单独考虑的 (Per-Coordinate Learning Rates)。 在一个标准的OGD里面使用的是一个全局的学习率策略$\eta^{(t)} = \frac{1}{\sqrt{t}}$，这个策略保证了学习率是一个正的非增长序列， 对于每一个特征维度都是一样的。考虑特征维度的变化率: 如果特征1 比特征2 的变化更快，那么在维度1 上的学习率应该下降得更快。 我们很容易就可以想到可以用某个维度上梯度分量来反映这种变化率。在 FTRL 中，维度 $i$ 上的学习率是这样计算的: \eta^{(t)}_i = \frac{\alpha}{ \beta + \sqrt{ \sum_{s=1}^t (g_i^{(s)})^2 } } \tag{3.4.3-1}由于 $\sigma^{(1:t)} = \frac{1}{\eta^{(t)}}$，所以公式$(3.4.2-2)$中 $\sum_{s=1}^t \sigma^{(s)} = \frac{1}{\eta^{(t)}} = (\beta + \sqrt{ \sum_{s=1}^t (g_i^{(s)})^2 }) / \alpha$。 这里的 $\alpha$ 和 $\beta$ 是需要输入的参数, 公式 $(3.4.2-2)$ 中学习率写成累加的形式，是为了方便理解后面 FTRL 的迭代计算逻辑。 3.4.4 FTRL算法逻辑到现在为止，我们已经得到了 FTRL 的特征权重维度的更新方法$公式(3.4.2-2)$，每个特征维度的学习率计算方法公式$(3.4.3-1)$， 那么很容易写出 FTRL 的算法逻辑, 这里是根据$(3.4.2-2)$ 和$(3.4.3-1)$ 写的 L1&amp;L2-FTRL 求解最优化的算法逻辑，如下： 而论文Ad Click Prediction: a View from the Trenches中 Algorithm 1 给出的是 L1&amp;L2-FTRL 针对 Logistic Regression 的算法逻辑: 3.5 Online总结从类型上来看，简单截断法、TG、FOBOS 属于同一类，都是梯度下降类的算法,并且TG在特定条件可以转换成简单截断法和FOBOS; RDA属于简单对偶平均的扩展应用;FTRL 可以视作 RDA 和 FOBOS 的结合，同时具备二者的优点。 目前来看， RDA 和 FTRL 是最好的稀疏模型 Online Training 的算法。 谈到高维高数据量的最优化求解，不可避免的要涉及到并行计算的问题, 冯扬(8119)的博客讨论了 batch 模式下的并行逻辑回归，其实只要修改损失函数，就可以用于其它问题的最优化求解。 另外，对于 Online 下，Parallelized Stochastic Gradient Descent给出了一种很直观的方法: 对于 Online 模式的并行化计算，一方面可以参考 ParallelSGD 的思路，另一方面也可以借鉴 batch 模式下对高维向量点乘以及梯度分量并行计算的思路。 总之，在理解算法原理的 基础上将计算步骤进行拆解，使得各节点能独自无关地完成计算最后汇总结果即可。 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam SGD、Momentum、RMSprop、Adam区别与联系 An overview of gradient descent optimization algorithms 在线最优化求解(Online Optimization)-冯扬 比Momentum更快：揭开Nesterov Accelerated Gradient的真面目 深度学习最全优化方法总结比较（SGD,Adagrad,Adadelta,Adam,Adamax,Nadam） Deep Learning 最优化方法之AdaGrad Ad_Click_Prediction_a_View_from_the_Trenches 各大公司广泛使用的在线学习算法FTRL详解 FTRL 不太简短之介绍 这个文章讲得也挺好]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pymongo基础]]></title>
    <url>%2F2019%2F04%2F24%2Fpymongo%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[pymongo基础这篇教程将为我们讲解如何通过pymongo操作MongoDB。准备首先，需要安装PyMongo，安装成功之后，就可以正常导入了： 1&gt;&gt;&gt; import pymongo我们也假设你的开发环境已经安装好了MongoDB，并且运行在默认的主机与端口上。运行mongo的命令如下： 1$ mongod 创建MongoClient使用PyMongo的第一步就是创建一个MongoClient来运行mongod实例： 12&gt;&gt;&gt; from pymongo import MongoClient&gt;&gt;&gt; client = MongoClient() 上面的代码会连接到MongoDB的默认主机与端口，当然，也可以明确指定主机与端口： 1&gt;&gt;&gt; client = MongoClient('localhost', 27017) 或者使用URL格式： 1&gt;&gt;&gt; client = MongoClient('mongodb://localhost:27017/') 获取数据库一个MongoDB的实例可以操作多个独立的数据库，当我们使用PyMongo的时候可以通过MongoClient的属性来获取不同的数据库。 1&gt;&gt;&gt; db = client.test_database 如果数据库名比较特殊，直接使用属性不能获取到，比如”test-databse”, 则可以通过字典形式来获取： 1&gt;&gt;&gt; db = client("test-database") 获取集合MongoDB中的集合用来保存一组文档，相当于关系型数据库中的数据表，获取集合的方式如下： 1&gt;&gt;&gt; collection = db.test_collection 或者： 1&gt;&gt;&gt; collection = db["test_collection"] 要注意的是，数据库以及集合都是延迟创建的，也就是说执行上面的命令实际上不会在MongoDB的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。 文档MongoDB中的数据采用BSON格式来表示和存储，BSON格式是一种二进制的JSON格式。在PyMongo中采用字典来表示文档，例如，下面的字典可以表示一篇博客信息： 12345&gt;&gt;&gt; import datetime&gt;&gt;&gt; post = &#123;"author": "Mike",... "text": "My first blog post!",... "tags": ["mongodb", "python", "pymongo"],... "date": datetime.datetime.utcnow()&#125; 注意文档中可以包含一些Python原生的数据类型，比如datetime.datetime对象，它将会被转换成合适的BSON类型。 插入文档我们使用insert_one()方法来插入文档： 1234&gt;&gt;&gt; posts = db.posts&gt;&gt;&gt; post_id = posts.insert_one(post).inserted_id&gt;&gt;&gt; post_idObjectId('...') 如果文档中没有_id这个属性，那么当它插入集合中的时候，会自动给它赋予这样一个属性，同一个集合中每个文档的_id属性值必须具有唯一性。insert_one()返回一个InsertOneResutl实例。 插入第一个文档之后，集合posts就被创建了，我们可以查看数据库中已经创建好的集合： 12&gt;&gt;&gt; db.collection_names(include_system_collections=False)[u'posts'] 使用find_one()获取第一个匹配的文档MongoDB中最常用的基本操作是find_one(), 这个方法返回查询匹配到的第一个文档，如果没有则返回None。下面我们使用find_one()来获取posts集合中的第一个文档： 12&gt;&gt;&gt; posts.find_one()&#123;u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']&#125; 查询结果是一个字典，内容正是我们之前插入的博客内容。 我们也可以使用find_one()来查找满足匹配要求的文档。例如，下面的例子查找作者名为”Mike”的文档。 12&gt;&gt;&gt; posts.find_one(&#123;"author": "Mike"&#125;)&#123;u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']&#125; 如果我们尝试配置作者名”Eliot”，返回结果为空： 12&gt;&gt;&gt; posts.find_one(&#123;"author": "Eliot"&#125;)&gt;&gt;&gt; 通过ObjectId进行查询我们也可以通过ObjectId属性来进行查找，例如： 1234&gt;&gt;&gt; post_idObjectId(...)&gt;&gt;&gt; posts.find_one(&#123;"_id": post_id&#125;)&#123;u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']&#125; 注意ObjectId与它的字符串表示形式是不一样的： 123&gt;&gt;&gt; post_id_as_str = str(post_id)&gt;&gt;&gt; posts.find_one(&#123;"_id": post_id_as_str&#125;) # No result&gt;&gt;&gt; 在Web应用开发过程中，经常面临的一个任务就是从URL请求中获取ObjectId，然后根据这个ID查找匹配的文档。所以有必要在把ID传递给find_one()之前，对于字符串形式的id做一个转换，将其转换成ObjectID对象。 123456from bson.objectid import ObjectId# The web framework gets post_id from the URL and passes it as a stringdef get(post_id): # Convert from string to ObjectId: document = client.db.collection.find_one(&#123;'_id': ObjectId(post_id)&#125;) 关于Unicode字符串你可能注意到我们从服务器端获取的数据与我们之前定义的数据格式并不一样， 例如存到数据库之前为”Mike”,从服务器获取之后，结果为”u’Mike”,这是为什么？ 这是因为，MongoDB采用BSON格式来存储数据，BSON是采用utf-8进行编码的，所以PyMongo必须确保要存储的字符串是合法的utf-8字符串，正常的字符串一般都可存储，但是Unicode字符串不行，所以Unicode字符串在存储之前都必须先转码成utf-8格式，PyMongo在获取数据之后，会将utf-8格式的数据转码成unicode字符串。这也就是为什么会出现编码不一样的问题， 批量插入除了插入单个文档，我们还可以批量插入多个文档，通过给insert_many()方法传递一个列表，它会插入列表中的每个文档。 1234567891011&gt;&gt;&gt; new_posts = [&#123;"author": "Mike",... "text": "Another post!",... "tags": ["bulk", "insert"],... "date": datetime.datetime(2009, 11, 12, 11, 14)&#125;,... &#123;"author": "Eliot",... "title": "MongoDB is fun",... "text": "and pretty easy too!",... "date": datetime.datetime(2009, 11, 10, 10, 45)&#125;]&gt;&gt;&gt; result = posts.insert_many(new_posts)&gt;&gt;&gt; result.inserted_ids[ObjectId('...'), ObjectId('...')] 查询多个文档为了查询多个文档，我们可以使用find()方法，find()方法返回一个Cursor对象，使用这个对象可以遍历所有匹配的文档。例如下面的例子： 123456&gt;&gt;&gt; for post in posts.find():... post...&#123;u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']&#125;&#123;u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']&#125;&#123;u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'&#125; 跟find_one()一样，我们也可以给find()方法设置匹配条件，例如，查询作者名为”Mike”的文档： 12345&gt;&gt;&gt; for post in posts.find(&#123;"author": "Mike"&#125;):... post...&#123;u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']&#125;&#123;u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']&#125; 统计如果我们想获取匹配到文档个数，那么可以使用count()方法，使用这个方法还可以获取集合中文档的个数： 12&gt;&gt;&gt; posts.count()3 获取获取匹配到的文档个数： 12&gt;&gt;&gt; posts.find(&#123;"author": "Mike"&#125;).count()2 范围查询MongoDB支持多种高级查询，比如范围查询，请看下面这个例子： 123456&gt;&gt;&gt; d = datetime.datetime(2009, 11, 12, 12)&gt;&gt;&gt; for post in posts.find(&#123;"date": &#123;"$lt": d&#125;&#125;).sort("author"):... print post...&#123;u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'&#125;&#123;u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']&#125; 这个我们使用了$lt操作符实现范围查询，还使用sort()方法对查询到的结果进行排序。 索引添加索引可以加快查询的速度，在这个例子中，将展示如何创建一个唯一索引。 首先，创建索引: 1234&gt;&gt;&gt; result = db.profiles.create_index([('user_id', pymongo.ASCENDING)],... unique=True)&gt;&gt;&gt; list(db.profiles.index_information())[u'user_id_1', u'_id_'] 注意我们现在有两个索引了，一个在_id上，它由MongoDB自动创建，另外一个就是刚刚创建的索引了。 然后添加几个文档： 1234&gt;&gt;&gt; user_profiles = [... &#123;'user_id': 211, 'name': 'Luke'&#125;,... &#123;'user_id': 212, 'name': 'Ziltoid'&#125;]&gt;&gt;&gt; result = db.profiles.insert_many(user_profiles) 这个索引会阻止我们插入已经存在的user_id： 123456&gt;&gt;&gt; new_profile = &#123;'user_id': 213, 'name': 'Drew'&#125;&gt;&gt;&gt; duplicate_profile = &#123;'user_id': 212, 'name': 'Tommy'&#125;&gt;&gt;&gt; result = db.profiles.insert_one(new_profile) # This is fine.&gt;&gt;&gt; result = db.profiles.insert_one(duplicate_profile)Traceback (most recent call last):pymongo.errors.DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: &#123; : 212 &#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基础]]></title>
    <url>%2F2019%2F03%2F29%2Fgit%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[learn from: 廖雪峰的官方网站-Git教程 1. 创建版本库 进入到准备当作Project的目录下，执行git init，生成隐藏文件夹.git即创建成功。 git add &lt;file&gt;：用来将file添加到暂存区Stage中。 git commit -m &quot;&lt;some messages&gt;&quot;：用来将暂存区Stage中的修改内容提交到版本库Version中。 2. 版本控制 git log：用来从最近到最远的commit日志。 git log --pretty=oneline：增加参数pretty，用来简洁输出commit日志。 git reset --hard HEAD^：用来将版本库中的master分支退回到上一个版本。 git reflog：显示所有记录的命令，可以用来回到任意版本 git reset --hard &lt;commit_id&gt;。 工作区、暂存区、版本库关系如下： git diff应用场景、管理修改。 git reset HEAD &lt;file&gt;：用来将暂存区的修改回退到工作区。 git checkout -- &lt;file&gt;：用来丢弃工作区的修改，即用版本库中的版本替换工作区的版本。 123456789场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 &quot;git checkout -- &lt;file&gt;&quot;。场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步:a) 第一步用命令 &quot;git reset HEAD &lt;file&gt;&quot;;b) 就回到了场景1，第二步按场景1操作。场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，&quot;git reset --hard &lt;commit_id&gt;&quot;回到任意版本，不过前提是没有推送到远程库。 当删除了工作区的文件后，有两种情况： 确实要删除文件 git rm &lt;file&gt;：用来删除版本库中的文件，要commit才行，实际可以理解为在暂存区记录了要删除版本库中文件的命令。 git commit -m &quot;&lt;some messages&gt;&quot;：用来提交删除到版本库中。 注意：先手动删除文件，然后使用git rm 和git add效果是一样的。 误删了文件要还原 git checkout -- &lt;file&gt;：用来将版本库中的文件覆盖工作区的文件 3. 远程仓库 先有本地库，后有远程库 登陆GitHub，然后，在右上角找到”Create a new repo”按钮，创建一个新的仓库; 在”Repository name”填入learngit，其他保持默认设置，点击”Create repository”按钮，就成功地创建了一个新的Git仓库; 在GitHub上的这个learngit仓库还是空的，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库; git remote add origin git@github.com:&lt;your_github_username&gt;/learngit.git,将工作区内容推到暂存区；如果出现”fatal: remote origin already exists”的问题，尝试先执行git remote rm origin后，再执行add; git push -u origin master，将本地仓库推送到Github。 查看一下Github中是不是已经有了本地的内容。 远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库 “-u”参数：表示把本地的master分支内容推送的远程新的master分支，并把本地的master分支和远程的master分支关联起来，以后的推送或者拉取时就可以简化命令 从现在起，只要本地作了提交，就可以通过命令：git push origin master, 把本地master分支的最新修改推送至GitHub。 先有远程库，后有本地库 git clone &lt;repo_ssh_url&gt;：用来克隆一个远程库到本地。 在克隆的仓库中进行工作，然后add, commit, push这个流程就可以更新远程库了。 4. 分支管理4.1 创建合并分支 git branch：用于查看当前分支，会列出所有分支，当前分支前面有*。 git branch &lt;dev&gt;：创建dev分支。 git checkout &lt;dev&gt;：切换到dev分支。 git checkout -b &lt;dev&gt;：git checkout命令加上-b参数表示创建并切换。 git merge &lt;dev&gt;：用于合并dev分支到当前分支。 git branch -d &lt;dev&gt;：用于删除dev分支。 4.2 解决冲突当不同的分支都对同一个文件做了不同的修改以后，在master分支执行git merge &lt;another_branch&gt;会发生冲突， 这个时候可以通过git status可以告诉我们冲突的文件，之后通过git diff &lt;file_name&gt;查看不同分支修改的内容， 也可以直接打开&lt;file_name&gt;文件查看，其中Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容。 之后直接修改文件&lt;file_name&gt;决定如何合并，然后add，commit，就算合并结束了…… 之后可以通过带参数的git log查看分支的合并情况：git log --graph --pretty=oneline --abbrev-commit， 结果类似下面的图形，不同的分支会用不同的颜色表示出来。 1234567891011121314$ git log --graph --pretty=oneline --abbrev-commit* cf810e4 (HEAD -&gt; master) conflict fixed|\ | * 14096d0 (feature1) AND simple* | 5dc6824 &amp; simple|/ * b17d20e branch test* d46f35e (origin/master) remove test.txt* b84166e add test.txt* 519219b git tracks changes* e43a48b understand how stage works* 1094adb append GPL* e475afc add distributed* eaadf4e wrote a readme file 4.3 分支管理策略合并分支的时候，Git会尽可能地使用Fast forward模式，这种模式下删除分支会丢失分支信息。 使用--no-ff参数强制禁用Fast forward模式 这个时候，Git就会在merge时生成一个新的commit，从分支历史上就可以看出分支信息。 例如：执行git merge --no-ff -m &quot;merge with no-ff&quot; dev后，使用git log --graph --pretty=oneline --abbrev-commit查看分支历史, 123456* e1e9c68 (HEAD -&gt; master) merge with no-ff|\ | * f52c633 (dev) add merge|/ * cf810e4 conflict fixed... 在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 所以，团队合作的分支看起来就像下图这样…… 4.4 Bug分支&amp;feature分支在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 修复Bug的具体流程如下： git stash：保存dev分支的工作区现场，便于之后恢复现场继续开发工作； 切换到带有Bug的那个分支，假设是master分支上有Bug，那么执行git checkout master切换到master分支； 在master分支上执行git checkout -b issue-101创建并切换到新的分支issue-101，开始修复Bug； Bug修复完成后执行git add &lt;file&gt;, git commit -m &quot;&lt;fixed bug 101&gt;&quot;, 将修复提交； git checkout master切换回master分支，git merge --no-ff -m &quot;&lt;merge bug fixed 101&gt;&quot; issue-101合并修复； git branch -d issue-101删除issue-101分支； git checkout dev切换到dev分支，继续之前的开发工作； git stash list：用于列出之前保存的所有工作区现场； 恢复工作区现场，一种可以用git stash apply &lt;one_stash_id&gt;，恢复后stash内容不删除，要用git stash drop &lt;one_stash_id&gt;进行手动删除；另一种用git stash pop，恢复的同时吧stash内容删除； 举例： 12345678910设A为游戏软件1、master 上面发布的是A的1.0版本2、dev 上开发的是A的2.0版本3、这时，用户反映 1.0版本存在漏洞，有人利用这个漏洞开外挂4、需要从dev切换到master去填这个漏洞，正常必须先提交dev目前的工作，才能切换。5、而dev的工作还未完成，不想提交，所以先把dev的工作stash一下。然后切换到master6、在master建立分支issue101并切换.7、在issue101上修复漏洞。8、修复后，在master上合并issue1019、切回dev，恢复原本工作，继续工作。 添加一个新功能时，肯定不希望因为一些实验性质的代码，把master分支搞乱了，所以每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，删除该feature分支。 it checkout -b feature-vulcan：创建并切换到新的分支； 开发新的功能……，git add &lt;dev_files&gt;, git commit -m &quot;add feature vulan&quot;； git checkout dev，切换会dev分支，准备合并； 之后和Bug分支的类似…… 假如由于某些原因，这个分支不允许合并要就地销毁了，执行git branch -d feature-vulcan会提示分支没有合并，如果删除，将丢失掉修改； 执行git branch -D feature-vulcan进行强制删除。 4.5 多人协作 git remote：用于查看远程库的信息；git remote -v：用于查看更详细的信息； git push origin master：把本地库当前分支的内容推送到远程库master分支；git push origin dev：把本地库当前分支的内容推送到远程库dev分支； 12345但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？1. master分支是主分支，因此要时刻与远程同步；2. dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；3. bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；4. feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。 流程如下： git clone &lt;url&gt;：克隆远程库的master分支； git branch -b dev origin/dev：创建本地库dev分支，并和远程库dev分支链接起来； 在本地库dev分支进行开发…… git push origin dev：试图将本地库dev分支的开发内容推送到远程库中； 如果推送失败，则因为远程分支比本地库更新，需要先用git pull更新本地库内容，之后试图合并； 如果合并有冲突，则解决冲突（上面详见4.2节），并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin &lt;branch-name&gt;推送就能成功； 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;链接。 4.6 Rebase没看懂，可以再看看下面这两个讲解。 廖雪峰的官方网站-分支管理-Rebase Git Book 中文版-rebase 5. 标签管理发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。 将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。 5.1 创建标签默认标签是打在最新提交的commit上的 切换到要打标签的分支上：git checkout master； git tag &lt;name&gt;：就可以打一个标签，默认打在HEAD指针上； git tag：查看所有标签； git tag &lt;name&gt; &lt;commit_id&gt;：对某一次提交打标签； git show &lt;name&gt;：查看标签信息； git tag -a &lt;name&gt; -m &quot;&lt;introduction_tag&gt;&quot; &lt;commit_id&gt;：创建带有说明的标签，用-a指定标签名，-m指定说明文字； 5.2 操作标签 git tag -d &lt;name&gt;：删除本地标签； git push origin &lt;name&gt;：推送某个标签到远程库； git push origin --tags：推送全部尚未推送到远程库的本地标签； 删除远程库的标签要删除本地标签，之后git push origin :refs/tags/&lt;name&gt;删除远程库的标签； 5. 使用Github 访问项目的主页，然后Fork这个项目； 从自己的账户下克隆这个项目； 对克隆的项目进行修改； 在Github上发起pull request。 Bootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样: 6. 自定义Git6.1 忽略特征文件有些时候必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件等等， 每次git status都会显示Untracked files …，有强迫症的童鞋心里肯定不爽； 好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件， 然后把要忽略的文件名填进去，Git就会自动忽略这些文件； 不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。 所有配置文件可以直接在线浏览：https://github.com/github/gitignore 忽略文件的原则是： 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件; git add -f &lt;file&gt;：强制添加文件； git check-ignore：检查.gitignore文件的规则； git check-ignore -v App.class： Git会告诉我们.gitignore文件的哪一行规则忽略了该文件； 6.2 配置别名--global：针对当前用户起作用的，如果不加，那只针对当前的仓库起作用； git config --global alias.st status：配置 git st 等价于 git status git config --global alias.co checkout：配置 git co 等价于 git checkout git config --global alias.ci commit：配置 git ci 等价于 git commit git config --global alias.br branch：配置 git br 等价于 git branch git config --global alias.unstage &#39;reset HEAD&#39;：配置 git unstage 等价于 git reset HEAD git config --global alias.last &#39;log -1&#39;：配置 git last 等价于 git log -l git config --global alias.lg &quot;log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit&quot;：配置 git lg 等价于 git log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit 每个仓库的Git配置文件都放在.git/config文件中 123456789101112131415[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[remote &quot;origin&quot;] url = git@github.com:weiguozhao/learngit.git fetch = +refs/heads/*:refs/remotes/origin/*[branch &quot;master&quot;] remote = origin merge = refs/heads/master[alias] last = log -1 当前用户的Git配置文件放在用户主目录下的一个隐藏文件.gitconfig中 12345678[alias] co = checkout ci = commit br = branch st = status[user] name = your_name email = your@email.com 题外话如果clone Github上面的仓库速度比较缓慢的话，可以尝试一下命令： 前提是有科学上网工具，并且使用Git的http进行clone，其中127.0.0.1后面是科学上网监听的端口，我的SSR是socker 1086。 加速： 12git config --global http.https://github.com.proxy socks5://127.0.0.1:1086git config --global https.https://github.com.proxy socks5://127.0.0.1:1086 删除： 12git config --global --unset https.https://github.com.proxygit config --global --unset http.https://github.com.proxy]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hexo-NexT的Local Search转圈问题解决记录]]></title>
    <url>%2F2019%2F03%2F28%2Fhexo-NexT%E7%9A%84Local-Search%E8%BD%AC%E5%9C%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[cite: https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/ 有时候更新完文章之后，莫名其妙地Local Search不能用了，这是大部分是因为文章中有非法字符bsStep1. 检查问题来源由于使用的是localSearch，在使用hexo g的时候，会在public里面生成search.xml作为搜索主体， 之后使用一些在线验证XML的网站取检查，把search.xml的内容全部拿过去检查，这里可以用来检查。 一般都会出来一个这个问题： 总之，问题就是因为多了这个backspace字符！ Step2. 显示看不到的backspace字符我是使用VSCODE，开启方式是到在设定中选中renderControlCharacters 打开之后你就可以看到那个backspace字符了…… Step3. 搜索并替换backspace的unicode是\u0008， 而VSCODE的搜索正则表达式使用的是Rust要輸入\x{0008}才可以， 其实直接把那个很小的bs框起來复制放到搜索框中即可！！！ Step4. 重新生成部署重新在小站目录下执行hexo d -g，然后看看线上的Local Search吧！]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vim Tutorial]]></title>
    <url>%2F2019%2F03%2F27%2FVim-Tutorial%2F</url>
    <content type="text"><![CDATA[Vim自带的Tutorial:在终端下输入vimtutor即可打开vim自带的教程。 cite: https://irvingzhang0512.github.io/2018/06/09/vim-tutor/ 1. 总结1.1. 光标移动 普通移动：h j k l。 到下一个单词开头：w，可以添加数字2w。 到下一个单词结尾：e，可以添加数字2e。 移动到当前行开头：0。 移动到当前行结尾：$。 移动光标到文件底部：G。 移动光标到文件顶部：gg。 移动到指定的行：#G，其中#表示数字。 1.2. 删除 删除一个字符：命令模式，x。 删除从当前位置，当下一个单词开头的所有字符：dw。 删除当前行从当前位置开始，到结尾的字符：d$。 删除当前位置，到当前单词结尾的字符：de。 可以在dw或de中间添加数字，如d2w，d5e。 删除整行：dd，并将数据保存到buffer。 删除多行：在#dd前添加数字，#表示数字。 1.3. 查询与替换 查询：在命令模式中输入/以及需要查询的内容，回车。 输入n查询下一个匹配。 输入N查询上一个匹配。 如果要反向查询，则使用?而不是/。 选项： 查询时不区分大小写：:set ic, :set noic。 查询结果高亮：:set hls，:nohlsearch。 使用%查询其对应的字符，如(), {}, []。 替换光标指向的一个字符：r（只能替换一个字符）。 在命令模式输入r，再输入需要替换的字符，之后就再次进入命令模式。 替换多个字符：R。 替换字符串： :s/old/new：只替换一个记录。 :s/old/new/g：替换当前行所有匹配记录。 :#,#s/old/new/g：替换指定行中所有匹配的记录。 :%s/old/new/g：替换文件中所有匹配记录。 :%s/old/new/gc：替换文件中所有匹配记录，在替换前会询问。 1.4. 复制粘贴 删除的内容是保存在缓存中，可以通过p黏贴。 复制：使用v，并移动光标，选择需要复制的文本，使用y进行复制。 粘贴：p。 获取其他文本中的内容，并复制到本地：:r TEST。 也可以复制命令结果，如:r !ls。 1.5. 添加操作 插入字符（进入编辑模式）：i。 追加字符 在光标指向的位置后添加内容，进入编辑模式：a。 在本行末尾添加内容，进入编辑模式：A。 change operator：删除指定位置的数据，并进入编辑模式。 ce：删除光标到当前单词末尾的数据，并进入编辑模式。 c与d的用法类似，也可以使用e w $以及数字。 1.6. 其他操作 退出vim：命令模式，输入:q!（退出不保存），输入:wq（保存并退出）。 查看当前处于文件中的位置：CTRL-G。 撤销：u撤销一个操作，U撤销当前行的之前的操作。 撤销的撤销：CTRL-R。 返回到之前光标的位置：CTRL-O，其反操作是CTRL-I。 2. 分课程总结2.1. 第一课 移动光标：命令模式，h j k l。 退出vim：命令模式，输入:q!（退出不保存），输入:wq（保存并退出）。 删除一个字符：命令模式，x。 插入字符（进入编辑模式）, i。 追加字符 在光标指向的位置后添加内容，进入编辑模式：a。 在本行末尾添加内容，进入编辑模式：A。 执行shell命令：:!ls。 保存到某文件：:w TEST。 2.2. 第二课 删除从当前位置，当下一个单词开头的所有字符：dw。 删除当前行从当前位置开始，到结尾的字符：d$。 删除当前位置，到当前单词结尾的字符：de。 移动光标： 到下一个单词开头：w。 到下一个单词结尾：e。 移动到当前行开头：0。 可以在w或e前，添加数字。 可以在dw或de中间添加数字，如d2w，d5e。 删除整行：dd，并将数据保存到buffer。 删除多行：在dd前添加数字。 撤销：u撤销一个操作，U撤销当前行的之前的操作。 撤销的撤销：CTRL-R。 2.3. 第三课 删除的内容是保存在缓存中，可以通过p黏贴。 替换光标指向的一个字符：r（只能替换一个字符）。 在命令模式输入r，再输入需要替换的字符，之后就再次进入命令模式。 change operator：删除指定位置的数据，并进入编辑模式。 ce：删除光标到当前单词末尾的数据，并进入编辑模式。 c与d的用法类似，也可以使用e w $以及数字。 2.4. 第四课 查看当前处于文件中的位置：CTRL-G。 移动光标到文件底部：G。 移动光标到文件顶部：gg。 移动到指定的行：#G，其中#表示数字。 查询：在命令模式中输入/以及需要查询的内容。 输入n查询下一个匹配。 输入N查询上一个匹配。 如果要反向查询，则使用?而不是/。 返回到之前光标的位置：CTRL-O，其反操作是CTRL-I。 使用%查询其对应的字符，如(), {}, []。 替换字符串： :s/old/new：只替换一个记录。 :s/old/new/g：替换当前行所有匹配记录。 :#,#s/old/new/g：替换指定行中所有匹配的记录。 :%s/old/new/g：替换文件中所有匹配记录。 :%s/old/new/gc：替换文件中所有匹配记录，在替换前会询问。 2.5. 第五课 执行shell命令：:!ls。 保存到某文件：:w TEST。 选择文本：使用v，之后移动光标，就可以选择一段文本。 之后若使用:w TEST命令，可以将选中的文本保存到指定文件中 获取其他文本中的内容，并复制到本地：:r TEST。 也可以复制命令结果，如:r !ls。 2.6. 第六课 添加行：o光标下添加行，O光标上添加行。 替换多个字符：R。 替换一个字符：r。 复制黏贴： 复制：使用v，并移动光标，选择需要复制的文本，使用y进行复制。 粘贴：p。 选项： 查询时不区分大小写：:set ic, :set noic。 查询结果高亮：:set hls，:nohlsearch]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python常用实例]]></title>
    <url>%2F2019%2F03%2F13%2Fpython%E5%B8%B8%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[求两个时间之间的差123456from datetime import datetime as dtinsert = dt.strptime('2018-03-22 10:00:00', "%Y-%m-%d %H:%M:%S")current = dt.strptime(str(dt.now().strftime('%Y-%m-%d')), '%Y-%m-%d')print (current-insert).days # 天print (current-insert).seconds # 秒print (current-insert).microseconds # 毫秒]]></content>
      <categories>
        <category>小工具</category>
      </categories>
      <tags>
        <tag>常用实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark基础]]></title>
    <url>%2F2019%2F03%2F07%2FSpark%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[0. 多说几句写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。Spark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录， 大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。 现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑， 网络上的主要学习来源是子雨大数据之Spark入门教程，这个教程真的只是入门。 以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述 1. RDD编程 基本RDD“转换”运算 map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集 filter(func)：筛选出满足函数func的元素，并返回一个新的数据集 flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果 distinct（去重运算） randomSplit（根据指定的比例随机分为N各RDD） reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合 groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集 union（两个RDD取并集） intersection（两个RDD取交集） subtract（两个RDD取差集） cartesian（两个RDD进行笛卡尔积运算） 基本RDD“动作”运算 count() 返回数据集中的元素个数 collect() 以数组的形式返回数据集中的所有元素 first() 返回数据集中的第一个元素 take(n) 以数组的形式返回数据集中的前n个元素 reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素 foreach(func) 将数据集中的每个元素传递到函数func中运行 takeOrdered（排序后取前N条数据） Key-Value形式 RDD“转换”运算 filter（过滤符合条件的数据） mapValues（对value值进行转换） sortByKey（根据key值进行排序） reduceByKey（合并相同key值的数据） join（内连接两个KDD） leftOuterJoin（左外连接两个KDD） rightOuterJoin（右外连接两个RDD） subtractByKey（相当于key值得差集运算） Key-Value形式 RDD“动作”运算 first（取第一条数据） take（取前几条数据） countByKey（根据key值分组统计） lookup（根据key值查找value值） RDD持久化 persist用于对RDD进行持久化 unpersist取消RDD的持久化，注意持久化的存储等级 2. text文本数据(需要自己解析)spark 1.6.1 12345val sparkConf = new SparkConf()sparkConf.setAppName("...")sparkConf.set("key", "value")sparkContext = new SparkContext(sparkConf)sparkContext.textFile("text_path").map(...) 读取文本数据后，使用map进行自定义的解析 spark 2.1.0 1234567891011val warehouseLocation = "spark-warehouse"val sparkConf = new SparkConf()sparkConf.setAppName("...")sparkConf.set("key", "value")val sparkSession = SparkSession .builder() .appName("Spark Hive Example") .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate()sparkSession.read.textFile("text_path").map(...) 3. 读Hive表数据spark 1.6.1 123456val sparkConf = new SparkConf()sparkConf.setAppName("...")sparkConf.set("key", "value")sparkContext = new SparkContext(sparkConf)hiveContext = new HiveContext(sparkContext)hiveContext.sql("select * from table where condition").map(...) spark 2.1.0 1234567891011val warehouseLocation = "spark-warehouse"val sparkConf = new SparkConf()sparkConf.setAppName("...")sparkConf.set("key", "value")val sparkSession = SparkSession .builder() .appName("Spark Hive Example") .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate()sparkSession.sql("select * from table where condition").map(...) 4. 提交到集群中运行pyspark 123456789101112131415#!/usr/bin/env bashspark_submit=$&#123;spark_bin&#125;/spark-submitspark_master=yarn-clienttime \$spark_submit \ --queue whichQueue \ --master $spark_master \ --driver-memory 10g \ --executor-memory 40g \ --num-executors 24 \ --executor-cores 8 \ --jars SomeThirdPartyJarFile.jar \ --conf spark.yarn.executor.memoryOverhead=600 \ your_script.py params scala 2.11 spark 2.1.0 123456789101112131415161718#!/usr/bin/env bashspark_submit=$&#123;spark_bin&#125;/spark-submitjar_file=packageAccordingMavenOrSbt.jartime \$spark_submit \ --class "package.className" \ --master yarn \ --deploy-mode client \ --num-executors 8 \ --executor-cores 3 \ --driver-memory 8g \ --executor-memory 24g \ --queue whichQueue \ --conf spark.classpath.userClassPathFirst=true \ $&#123;jar_file&#125; \ --param $&#123;param&#125; scala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析 5. Spark多路输出 version:spark 2.1.0scala 2.11.8 1234567891011121314151617181920212223242526272829303132333435363738class RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] &#123; // 不输出Key override def generateActualKey(key: Any, value: Any): Any = NullWritable.get() // 文件名用Key表示 override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = key.toString&#125;object MultiOutputPathTest &#123; val warehouseLocation = "spark-warehouse" def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() sparkConf.setAppName("MultiOutputPathTest") sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") sparkConf.set("spark.yarn.executor.memoryOverhead", "800") sparkConf.set("spark.sql.hive.convertMetastoreParquet", "false") // sparkConf.setMaster("local[*]") val sparkSession = SparkSession .builder() .config(sparkConf) .config("spark.sql.warehouse.dir", warehouseLocation) .enableHiveSupport() .getOrCreate() // 定义数据 val data = sparkSession.parallelize(List(("w", "www"), ("b", "blog"), ("c", "com"), ("w", "bt"))) // 将同一个Key的数据聚合到一起 .reduceByKey((x, y) =&gt; x + "\n" + y) // 重新划分partition数量，3表示按照key的数量分成相同的partition数量 .partitionBy(new HashPartitioner(3)) // 多路输出，第一个参数为输出路径，第二个参数为Key的类型，第三个参数为Value的类型，第四个参数是输出格式类 .saveAsHadoopFile(output_path, classOf[String], classOf[String], classOf[RDDMultipleTextOutputFormat]) &#125;&#125; 结果应该是在output_path路径下生成3个文件w, b, c。其中文件w中的内容为www\nbt, 文件b中的内容为blog, 文件c中的内容为com。 6. PySpark中自定义环境6.1 背景不同用户的应用程序使用的Python版本及需要的Python依赖可能存在不同，若每次缺少依赖都请 op 去update所有节点，对于大集群的维护来说相对比较困难，而且走变更流程等还影响 Spark 用户的工作效率。 为解决上述问题，我们探索一种用户可自定义Python环境的方法，用户可依据自己的需要，定制自己的Python并在spark-submit时做简单指定即可。 6.2 具体操作流程下面以构造一个Python 3.6.10的依赖包为例 6.2.1 构建Python如果系统中没有安装 wget 的话，请先安装，具体方法自行搜索。 123456# 下载Python源码wget https://www.python.org/ftp/python/3.6.10/Python-3.6.10.tgz# 解压tar -zxvf Python-3.6.10.tgz# 编译 Python --prefix指定编译输出路径./Python-3.6.10/configure --prefix=/search/odin/zhaoweiguo/temp/python-3.6.10 &amp;&amp; make &amp;&amp; make install 6.2.2 安装依赖根据个人需要安装Spark环境中需要的依赖包，例如 pyspark、google等。其中google包主要是因为在Spark中需要解析protobuf。 1234# install pyspark package，-t是指定安装路径/search/odin/zhaoweiguo/temp/python-3.6.10/bin/pip3 install -t /search/odin/zhaoweiguo/temp/python-3.6.10/lib/python3.6/site-packages pyspark# install google package/search/odin/zhaoweiguo/temp/python-3.6.10/bin/pip3 install -t /search/odin/zhaoweiguo/temp/python-3.6.10/lib/python3.6/site-packages google 6.2.3 打包Python环境123# 进入python根目录，注意打包路径决定下面的spark配制方式cd /search/odin/zhaoweiguo/temp/python-3.6.10tar -zcf python-3.6.10.tgz * 6.2.4 其他打包方法也可以不是用编译Python的方式进行操作得到Python打包环境，上面的打包方式了解以后，conda创建的方式大同小异。具体： 使用conda创建python虚拟环境，对应 6.2.1 进入创建的虚拟环境，安装需要第三方库，对应 6.2.2 打包整个虚拟环境，对应6.2.3 6.3 Spark配置使用因客户机存在公用的可能，且每个应用程序的需求不同，为了降低不同用户之间的影响，我们推荐在提交命令中配制的作法。 client模式 Driver在用户提交宿主机运行，提交机和线上集群环境可能存在差异，因此，区分配制两端的 Python 环境 (若相同只须指定“spark.pyspark.python”即可)。 1234567891011time spark-submit \ --conf ...其他配置... \ # 上传该tgz压缩文件, 压缩的tgz文件会被提取到executor的工作目录下, 后面到#python-3.6.10表示压缩文件被解压成到文件名称 --conf spark.yarn.dist.archives=file:///search/odin/zhaoweiguo/temp/python-3.6.10/python-3.6.10.tgz#python-3.6.10 \ # 指定driver的工作环境 # spark-submit执行的及其就是driver机器，直接选择python在本机的绝对路径 --conf spark.pyspark.driver.python=/search/odin/zhaoweiguo/temp/python-3.6.10/bin/python3.6 \ # 指定executor的工作Python环境 # ./python-3.6.10指的是executor的工作目录下上面python-3.6.10.tgz解压的python-3.6.10文件夹 --conf spark.pyspark.python=./python-3.6.10/bin/python3.6 \ script.py $&#123;params&#125; cluster模式 Driver运行在ApplicationMaster, 而ApplicationMaster运行在Executor(container)中，因此，可视为Driver和Executor环境统一，只需要配制spark.pyspark.python即可。 12345678time spark-submit \ --conf ...其他配置... \ # 上传该tgz压缩文件, 压缩的tgz文件会被提取到executor的工作目录下, 后面到#python-3.6.10表示压缩文件被解压成到文件名称 --conf spark.yarn.dist.archives=file:///search/odin/zhaoweiguo/temp/python-3.6.10/python-3.6.10.tgz#python-3.6.10 \ # 指定executor的工作Python环境 # ./python-3.6.10指的是executor的工作目录下上面python-3.6.10.tgz解压的python-3.6.10文件夹 --conf spark.pyspark.python=./python-3.6.10/bin/python3.6 \ script.py $&#123;params&#125; 6.4 缺点上述方案虽可实现用户自定义python环境，但执行过程中每个Executor从HDFS下载一次python环境，增加RPC等开销，在开启动态资源伸缩功能时，下载次数会更多…… Spark on Yarn 之Python环境定制 spark-python版本依赖与三方模块方案 7. Spark的一些技巧技巧包括： broadcast map_join 大表 join 小表 局部聚合+全局聚合 …… wwcom614/Spark]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pig基础]]></title>
    <url>%2F2019%2F02%2F14%2Fpig%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Pig基础知识 数据模型 包（bag）是元组（tuple）的集合 元组（tuple）是有序的字段（field）集 字段（field）是一段数据 语句 每个语句以分号（；）结尾 只有illustrate、dump、store等操作的时候，才会执行MapReduce 注释 单行注释 -- 多行注释 /* code */ 数据类型 int long float double chararray Bytearray Boolean Datetime Biginteger Bigdecimal Tuple Bag Map NULL值（上述类型都可以为NULL值，等同与Python中的None） Pig运算符加载数据1relation = load 'data_path' using PigStorage() as (name:chararray, gpa:double); 其中的PigStorage()为加载或存储结构化数据是使用，还有一些其他的函数参考这里，as 后面跟的是数据的模式，样例中假设了数据模式为name,gpa，并需要指定每个字段的类型。 读取hive表时使用org.apache.hive.hcatalog.pig.HCatLoader()，不需要指定模式。 存储数据1store relation into 'data_path' [ using PigStorage() ]; 同加载数据时类似，这里不需要再指定数据的模式了。 Dump运算符1dump relation; 用于将关系数据输出到终端中，通常用于调试。调用该命令可以执行MapReduce。 Describe运算符1describe relation; 输出relation的模型，调用该命令不会执行MapReduce。 Explain运算符1explain relation; 显示relation的逻辑，物理结构以及MapReduce执行计划。调用该命令不会执行MapReduce。 Illustrate运算符1illustrate relation; 提供了一系列语句的逐步执行。调用该命令会执行MapReduce。 分组和连接1Group_data = GROUP Relation_name BY age; 通常是一个关系自我进行分组，可以指定一个、多个或者ALL来分组（当指定为ALL的时候，不需要使用BY） 1Cogroup_data = COGROUP Relation1 by age, Relation2 by age; 和group具有相同的功能，区别是cogroup通常用于多个关系进行分组。 1Join_data = Join customers by id, orders by customer_id; Join操作和SQL的Join基本一致，也分self-join, left-join, right-join, full-outer-join 合并和拆分1Union_data = UNION relation1, relation2; 将多个关系上下罗列，注意关系只见的模式（schema）要相同。 1SPLIT relation1 into relation2 if (condition), relation3 (condition2); 过滤1Relation2_name = FILTER Relation1_name BY (condition); 1Relation_name2 = DISTINCT Relatin_name1; 去重，通常会花费比较多的时间 1Relation_name2 = FOREACH Relatin_name1 GENERATE (required data); 通常用完filter、group、join等操作后，紧接着使用foreach 排序12Relation_name2 = ORDER Relatin_name1 BY (ASC|DESC);` 1Result = LIMIT Relation_name （required number of tuples）; 限制记录的数量 其他内置函数 eval函数 AVG() 计算包内数值的平均值 BagToString() 将包的元素连接成字符串 CONCAT() 连接两个或多个相同类型的表达式 COUNT() 获取包中元素的数量，同时计算包中元组的数量 COUNT_STAR() 计算包中的元素数量 DIFF() 比较元组中的两个包(字段) isEmpty() 检查包或映射是否为空 MAX() 计算单列包中的列(数值或字符)的最大值 MIN() 计算单列包中的列(数值或字符)的最小值 PluckTuple() 可定义Prefix，并过滤以给定Prefix开头的关系中的列 SIZE() 基于任何Pig数据类型计算元素的数量 SUBtTRACT(A，B) 两个包的差，结果为在A不在B种的元组的包 SUM() 计算单列包中的某列的数值总和 TOKENIZE() 要在单个元组中拆分字符串(其中包含一组字)，病返回包含拆分操作的输出的包 load函数 / store函数 PigStorage() 加载和存储结构化文件 TextLoader() 将非结构化数据加载到Pig中 BinStorage() 使用机器可读格式将数据加载并存储到Pig中 Handling Compression 加载和存储压缩数据 bag函数 / tuple函数 TOBAG() 将两个或多个表达式转换为包 TOP() 获取关系的顶部N个元组 TOTUPLE() 将一个或多个表达式转换为元祖 TOMAP() 将key-value对转换为Map stirng函数 date函数 math函数 注释 / multi-lines code / -- single line code 具体使用方法及其他的一些内置函数，常用函数，可以参考w3cschool 写Pig脚本，重点是UDF的编写，这里我写的jython_UDF比较多（其实就是按照Python写的）， 需要注意的是要定义好outputSchema，并在UDF函数头部加上修饰符@ 12345678910111213141516171819#!/use/bin/python# -*- coding:utf-8 -*-import sysreload(sys)sys.setdefaultencoding('utf-8')def outputSchema(schema_def): def decorate(func): func.outputSchema = schema_def return func return decorate@outputSchema('newDate:chararray')def dateChangeFormat(thisDate): try: return thisDate.strip().split()[0]; except: return '2000-01-01' 题外话：现在Spark是主流，建议转Spark Pig的一些实例 差集的计算 12345A = load 'input1' as (x, y);B = load 'input2' as (u, v);C = cogroup A by x, B by u;D = filter C by IsEmpty(B);E = foreach D generate flatten(A); 解释：A和B分组后，如果B中为空，那么说明与之对应的A元素仅在A中出现，即在A不在B，也即为A-B的差集 传递参数 1pig -param date='2014-05-17' example.pig pig脚本中用$date调用该值 FLATTEN关键字 可以消除嵌套 项目相关 1%default PDAY '2019-02-02' 在脚本前面使用该命令设定脚本传递参数的默认值 1register xxx.jar as xxx 在脚本的最前面注册使用jar文件，其中as重命名不是必要的 统计行数 1234A = LOAD '1.txt' USING PigStorage (' ‘) AS(col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double); B = GROUP A all;C = FOREACH B GENERATE COUNT(col2);DUMP C; 加载多个文件 12LOAD '/data/201&#123;8,9&#125;'load/data/2018 /data/2019两个目录下的数据 其他常用的实例 参考pig实战 pig常用语法总结，教你快速入门——算法篇]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>语言基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo博客同步管理及迁移]]></title>
    <url>%2F2018%2F11%2F28%2Fhexo%E5%8D%9A%E5%AE%A2%E5%90%8C%E6%AD%A5%E7%AE%A1%E7%90%86%E5%8F%8A%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"><![CDATA[转自：使用hexo，如果换了电脑怎么更新博客？ - 容与的回答 - 知乎其他hexo自定义教程: 打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化背景：一台电脑上已有一个在用的博客，又新用了一台电脑，实现原电脑和新电脑都可以提交更新博客，实现同步或者说博客的版本管理。 原电脑操作 在原电脑上操作，给 username.github.io 博客仓库创建 hexo 分支，并设为默认分支。 在Github的username.github.io仓库上新建一个hexo分支，并切换到该分支； 并在该仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为hexo，update更新保存； 如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。 （如何创建和添加参考 github help ） 随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。 在username.github.io目录使用Git Bash执行git branch命令查看当前所在分支，应为新建的分支hexo 显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。 命令行 cd 到 clone 的仓库（username.github.io）执行下列命令 123git add -Agit commit -m "--"git push origin hexo 把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。 将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地username.github.io目录删掉了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。 将博客目录下 themes 文件夹下每个主题文件夹里面的 .git目录和 .gitignore文件删掉。 命令行 cd 到博客目录，执行下列命令 123git add -Agit commit -m "--"git push origin hexo 将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git目录和 .gitignore文件，主题文件夹下内容将传不上去。 至此原电脑上的操作结束。 新电脑操作 先把环境安装好 node.js git hexo ssh key 也创建看这里和添加看这里好。 创建添加ssh 命令行进入用户主目录,比如在我的电脑下：c:\users\administrator。 命令行运行ssh-keygen -t rsa -C &quot;username@example.com&quot;。它会提示你输入邮箱，输入完成以后一路回车就行。完成这一步，找到该目录下的.ssh文件夹，里面多了两个文件：id_rsa和id_rsa.pub分别是你这台电脑的ssh key的私钥和公钥。用文本打开公钥文件，复制里面的所有内容备用。 登录github后，进入 settings -&gt; SSH and GPG keys。点击 New SSH key ,然后title随便填，比如可以是你的电脑名称，把刚才复制的公钥内容黏贴到key中，最后点击 Add SHH key 就完成了 选好博客安装的目录，clone仓库。 1git clone git@github.com:username/username.github.io.git cd 到博客目录，执行下列命令 12npm installhexo g &amp;&amp; hexo s 安装依赖，生成和启动博客服务。 正常的话，浏览器打开 localhost:4000 可以看到博客了。 至此新电脑操作完毕。 更新源文件、部署静态文件以后无论在哪台电脑上，更新以及提交博客，依次执行。 1234git add -Agit commit -m "--"git push origin hexohexo clean &amp;&amp; hexo g -d 即可备份源文件到hexo分支，部署静态文件到master分支。]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[推荐系统HandBook Chapter3 基于内容的推荐系统-前沿和趋势]]></title>
    <url>%2F2018%2F11%2F28%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FHandBook-Chapter3%2F</url>
    <content type="text"><![CDATA[物品推荐的问题已经有广泛的研究，现有两类主要方式。基于内容的推荐系统试图推荐给定用户过去喜欢的相似物品， 而协同过滤推荐方式的系统识别出拥有相同爱好的用户，并推荐他们喜欢过的物品。基于内容的推荐系统的基础一个基于内容的推荐系统的高层次结构下图所示。推荐的过程包含有三个阶段，每一个阶段都由独立的部件控制。 内容分析器 该部件的主要功能是将来自信息源的对象的内容表示成恰当的格式，以便于下一阶段的处理。 信息学习器 该部件收集用户偏好的数据特征，并试图通过机器学习技术泛化这些数据，从而构建用户特征信息。 过滤组件 该部件将用户与物品在表示空间中进行匹配，之后生成一个潜在感兴趣物品的排名清单。 基于内容的推荐系统的推荐流程 根据用户u对物品i的评分r，信息学习器通过监督学习算法，为该用户生成偏好特征； 用户偏好特征存储在信息资源库里，并被接下来的过滤组件使用； 比较用户偏好特征和新物品的特征，过滤组件预测用户对新物品的感兴趣程度； 过滤组件生成一个推荐列表给用户。 用户偏好会不断变化，上述步骤不断迭代。 基于内容过滤的优缺点基于内容的推荐与基于协同过滤的推荐相比有一下优点： 用户独立性 只使用当前用户提供的评分构建个人信息 协同过滤使用近邻用户，只有用户近邻喜欢的物品才有可能被推荐 透明度 通过显式列出推荐列表中的物品特征，可以对推荐进行解释 协同过滤是黑盒子，解释仅来自相似的未知用户 新物品 新物品没有用户评分也能被推荐 协同过滤只有被一系列用户评分后，才有可能被推荐 缺点： 可分析的内容有限 受特征数量、类型以及领域知识的限制 过度特化 无法获得出人意料的推荐。（不能发现用户的潜在兴趣） 新用户 必须收集到足够的用户评分后，才能理解用户偏好并给予推荐 基于内容的推荐系统的现状基于关键词系统概述 Web推荐系统领域 Letizia Personal Web Watcher Syskill&amp;Webert ifWeb Amalthea WebMate 新闻过滤领域 NewT PSUN INFOrmer NewsDude Daily Learner YourNews 音乐领域 Last.fm使用协同过滤 MyStrands使用协同过滤 Pandora使用人工基于内容的描述 融合协同过滤和基于内容的方法的混合推荐系统 Fab WebWacther ProfBuilder PTV Content-boosted Collaborative Filtering CinemaScreen 基于本体的语义分析 SiteIF 多语言新闻网站的个性化工具 第一个采用基于感知的文档表示来对用户兴趣建模 词领域消歧 ITR, Item Recommender 词感知消歧 SEWeP 利用日志和Web站点内容的语义实现个性化 Quickstep 在线学术研究论文的推荐系统 informed Recommender 使用消费者产品评价给出推荐建议 系统通过作为知识表示和共享的翻译本体，把用户观点转换成一个结构化形式 News@hand 采用基于本体表示物品特征和用户偏好来推荐新闻的系统 趋势和未来研究略]]></content>
      <categories>
        <category>推荐系统HandBook读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统HandBook Chapter2 推荐系统中的数据挖掘方法]]></title>
    <url>%2F2018%2F11%2F27%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FHandBook-Chapter2%2F</url>
    <content type="text"><![CDATA[数据挖掘的过程一般由三个连续执行的步骤组成：数据预处理、数据分析和结果解释。数据预处理相似度度量方法 欧几里德距离 d(x,y) = \sqrt{\sum_{k=1}^n (x_k - y_k)^2} 马氏距离 d(x,y) = \sqrt{(x-y) \sigma^{-1} (x-y)^T} 其中$ \sigma $是数据的协方差矩阵。 余弦度量 cos(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} 皮卡逊相关性系数 Person(x,y) = \frac{\sum (x, y)}{\sigma_x \times \sigma_y}抽样根据抽样是否放回，可以分为： 有放回抽样 无放回抽样 根据抽样方法，可以分为： 简单随机抽样 每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。 系统(等距)抽样 将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。 分层抽样 将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。 整群抽样 将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。 抽样-维基百科 除非数据集足够大，否则交叉验证可能不可信 降维推荐系统反复出现的问题： 稀疏：只有几个有限的特征有值，其他大部分为零； 维度灾难：高维空间并没有给结果带来正相关； 推荐系统中最相关的降维算法，这些技术可以单独作为推荐方法使用，或者作为其他技术预处理的步骤。 主成份分析(PCA) 获得一组有序的成分列表，其根据最小平方误差计算出变化最大的值； 列表中第一个成分代表的变化量要比第二个成分所代表的变化量大，以此类推； 通过忽略变化贡献较小的成分来降低维度； 奇异值分解(SVD) 一大优势是有增量算法来计算近似的分解； 具体降维算法详细内容见—&gt;这里的学习小结 去噪去噪是预处理步骤中非常重要的一步，其目的是在最大化信息量时去除掉不必要的影响。 噪声的分类： 自然的噪声 用户在选择偏好反馈时无意产生的 恶意的噪声 用户为了偏离结果在系统中故意引入的 通过预处理步骤来提高精确度能够比复杂的算法优化效果要好得多 分类最近邻给出一个要分类的点，kNN分类器能够从训练记录中发现k个最近的点，然后按照它最近邻的类标签类确定所属类标签。 选择k值的问题： k值太小，分类可能对噪声点太敏感； k值太大，近邻范围可能会包含其他类中太多的点； 决策树决策树的优点：构建代价比较小并且在分类未知的对象方面速度比较快；在维持精度的同时，能够容易解释。 推荐系统中使用决策树： 用在基于模型的方法中； 作为物品排序的工具。 基于规则的分类规则的产生途径： 从数据中直接抽取规则，例子为RIPPER或CN2 从其他分类模型中间接抽取，例如决策树模型或神经模型 不流行、很难建立完整的规则 贝叶斯分类器朴素贝叶斯收孤立噪声点和不相关属性的影响小，且在概率估算期间可以通过忽略实例来处理缺失值。 但是独立性假设对一些相互关联的属性来说可能不成立，通常的解决办法是使用贝叶斯信念网络。 具体算法详细内容见—&gt;这里 分类器的集成从训练数据构造一系列的分类器，并通过聚集预测值(或中间特征权重)来预测类标签。 Bagging Boosting Stacking 评估分类器一些常用的评估指标见—&gt;这里 聚类分析K-Means聚类算法(分块方法)一开始会随机选择k个中心点，所有物品都会被分配到它们最靠近的中心节点的类中。 由于聚类新添加或移出物品，新聚类的中心节点需要更新，聚类的成员关系也需要更新。 这个操作持续下去，知道再没有物品改变它们的聚类成员关系。 算法第一次迭代时，大部分的聚类的最终位置就会发生，因此，跳出迭代的条件一般改变成“知道相对少的点改变聚类”来提高效率。 优点 简单 有效 缺点 k值确定需要先验的数据知识 最终聚类对初始中心点敏感 可能产生空聚类 具体算法详细见—&gt;这里 改进的K-Means聚类DBSCAN(基于密度)通过建立密度定义作为在一定范围内的点的数量。 DBSCAN定义了三种点： 核心点：在给定距离内拥有超过一定数量邻居的点； 边界点：没有超过指定数量的邻居但属于核心点邻居； 噪声点：既不是核心点也不是边界点； 其他改进的聚类算法： 消息传递的聚类算法（基于图聚类） 分层聚类按照层级树的结构产生一系列嵌套聚类 注意 改进的k-means算法并没有应用于推荐算法中。k-means算法的简单和效率要优于它的替代算法。 关联规则挖掘给定物品的频繁度称为支持量（比如（牛奶，啤酒，尿布）=131）类比频次 物品集的支持度是包含它的事务的比例（比如（牛奶，啤酒，尿布）=0.12）类比频率 关联规则挖掘的两步方法： 产生了所有支持度大于等于最小支持度的物品集（频繁项集生成） 从每一频繁物品集中产生高置信规则（规则生成） 具体可以参考Apriori算法。]]></content>
      <categories>
        <category>推荐系统HandBook读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统HandBook Chapter1 概述]]></title>
    <url>%2F2018%2F11%2F26%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FHandBook-Chapter1%2F</url>
    <content type="text"><![CDATA[推荐系统是一种软件工具和技术方法，它可以向用户建议有用的物品，其是一种解决信息过载的有效工具。推荐系统的功能 服务提供商采用这种技术的原因： 提高转化率，即接受推荐并消费物品的用户数量比上仅浏览了这些信息的普通访客数量； 出售更多种类的物品； 增加用户满意度； 更好地了解用户需求； 推荐系统分类 基于内容(content-based) 系统为用户推荐与他们过去的兴趣类似的物品。 物品间的相似性基于被比较的物品的特征来计算。 协同过滤(collaborative filtering) 找到与用户有相同品味的用户，然后将相似用户过去喜欢的物品推荐给用户。 两用户间的相似偏好通过计算用户历史评分记录相似度得到。 最流行&amp;最广泛实现的技术。 基于人口统计学(demographic) 基于用户的语言、国籍等人口统计学信息进行个性化推荐。 推荐系统对这方面的研究较少。 基于知识(knowledge-based) 根据特定领域知识推荐物品。 这些知识是关于如何确定物品的哪些特征能够满足用户需要和偏好，以及最终如何确定物品对用户有用。 这些系统中，相似函数用来估算用户需求(问题描述)与推荐(解决问题)的匹配度。 基于社区(community-based，社会化推荐) 依赖用户朋友的偏好。 关注度越来越高。 混合推荐系统(hybird recommender system) 综合上述的技术进行推荐。 应用与评价系统设计阶段首先要考虑的是推荐系统应用的领域。 娱乐： 电影、音乐和IPTV的推荐。 内容： 个性化新闻报纸、文件推荐、网页推荐、电子学习程序和电子邮件过滤等。 电子商务： 为消费者提供需购买的产品，如书籍、电脑等。 服务： 旅游服务推荐、专家咨询推荐、租房推荐或者中介服务。 另外一个重要的问题是推荐系统的评测。出于各种各样的目的，推荐系统生命周期的不同阶段都要进行评测。 推荐解释在推荐系统中扮演的七大作用： 透明度：说明系统是如何工作的； 可反馈性：允许用户告诉系统有错误； 信任：增加用户对系统的信心； 有效性：帮助用户做出好的决定； 说服力：说服用户去尝试或购买； 高效性：帮助用户快速抉择； 满意度：增加用户舒适度或乐趣。 推荐系统的挑战本书没有涵盖，但是对推荐系统的研究发展比较重要的挑战。 大的真实数据集背景下的算法扩展性 较小的离线测试后，该方法在非常大的数据集上可能失效或完全不适应，需要重新评估研究 主动推荐系统 虽然没有明确的请求，推荐系统也产生推荐 需要预测推荐什么，什么时候，如何推荐 推荐系统中的用户隐私保护 简约明智地使用用户信息，同时避免被恶意用户获得 推荐给用户的物品多样性 推荐列表中包含的物品具有多样性，则用户更可能找到合适的物品 在建立推荐列表的过程中整合用户的长期和短期偏好 通常要使用混合模型来正确地整合用户的长期和短期偏好 通用的用户模型和交叉领域推荐系统能够在不同的系统和应用领域调配用户数据 在开放式网络中运行的分布式推荐系统 最优化推荐序列的推荐系统 移动上下文的推荐系统]]></content>
      <categories>
        <category>推荐系统HandBook读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令]]></title>
    <url>%2F2018%2F11%2F26%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[配置环境安装git和node.js安装hexo123npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化简写12345hexo n "我的博客" == hexo new "我的博客" #新建文章hexo p == hexo publishhexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署 安装插件123安装插件：npm install 插件名 –save卸载插件：npm uninstall 插件名更新插件和博客框架：npm update 服务器12345678hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo g #生成静态网页hexo d #开始部署 监视文件变动12hexo generate #使用 Hexo 生成静态文件快速而且简单hexo generate --watch #监视文件变动 完成后部署123两个命令的作用是相同的:hexo generate --deploy == hexo g -dhexo deploy --generate == hexo d -g 草稿1hexo publish [layout] &lt;title&gt; 模版12345678hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #将.deploy目录部署到GitHubhexo new [layout] &lt;title&gt;hexo new draft "My Draft" 推送到服务器上123hexo n #写文章hexo g #生成hexo d #部署 #可与hexo g合并为 hexo d -g 部署类型设置git站点配置文件中 123456# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:***/***.github.io.git branch: master]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[t-SNE降维]]></title>
    <url>%2F2018%2F11%2F12%2FtSNE%2F</url>
    <content type="text"><![CDATA[转载自：t-SNE完整笔记t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，是由Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。 此外，t-SNE是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。 t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。 我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。 1.SNESNE是通过仿射(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤： SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。 SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。 我们看到t-SNE模型是非监督的降维，它跟k-means等不同，它不能通过训练得到一些东西之后再用于其它数据 (比如k-means可以通过训练得到 k 个点，再用于其它数据集，而 t-SNE 只能单独的对数据做操作，也就是说他只有 fit_transform，而没有fit操作） SNE是先将欧氏距离转换为条件概率来表达点与点之间的相似度。具体来说，给定 N 个高维的数据 $x_1, …, x_N$ (注意 N 不是维度), t-SNE首先是计算概率$p_{ij}$，正比于$x_i$和$x_j$之间的相似度(这种概率是我们自主构建的)，即： p_{j∣i} = \frac{exp(− ∣∣ x_i − x_j ∣∣ ^2 / (2 \sigma ^2_i))}{ \sum_{k \not= i} exp(− ∣∣ x_i − x_k ∣∣ ^2 / (2 \sigma ^2_i))}这里的有一个参数是$ \sigma_i $，对于不同的点 $x_i$ 取值不一样，后续会讨论如何设置。此外设置 $p_{x∣x} = 0$, 因为我们关注的是两两之间的相似度。 那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\frac{1}{\sqrt{2}}$，因此它们之间的相似度如下: q_{j∣i} = \frac{ exp(− ∣∣ x_i − x_j ∣∣ ^2)}{ \sum_{k\not=i} exp(−∣∣ x_i − x_k ∣∣ ^2)}同样，设定$q_{i∣i} = 0$。 如果降维的效果比较好，局部特征保留完整，那么 $p_{i∣j} = q_{i∣j}$, 因此我们优化两个分布之间的距离 - KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下: C = \sum_{i} KL( P_i ∣∣ Q_i ) = \sum_i \sum_j p_{j∣i} log \frac{p_{j∣i}}{q_{j∣i}}这里的 $P_i$ 表示了给定点 $x_i$ 下，其他所有数据点的条件概率分布。需要注意的是KL散度具有不对称性，在低维映射中不同的距离对应的惩罚权重是不同的， 具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小 (注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q_{j∣i} = 0.2$ 来建模较大的 $p_{j∣i}=0.8$, $cost = p log(\frac{p}{q}) = 1.11$, 同样用较大的 $q_{j∣i}=0.8$ 来建模较大的 $p_{j∣i}=0.2$, $cost = -0.277$, 因此，SNE会倾向于保留数据中的局部特征。 下面我们开始正式的推导SNE。首先不同的点具有不同的 $\sigma_i$，$P_i$ 的熵(entropy)会随着 $\sigma_i$ 的增加而增加。 SNE使用困惑度(perplexity)的概念，用二分搜索的方式来寻找一个最佳的 $\sigma$。其中困惑度指: Perp(P_i) = 2^{H(P_i)}这里的 $H(P_i)$是 $P_i$ 的熵，即: H(P_i) = − \sum_j p_{j∣i} log_2 p_{j∣i}困惑度可以解释为一个点附近的有效近邻点个数。SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间，给定之后，使用二分搜索的方式寻找合适的$\sigma$。 那么核心问题是如何求解梯度了，目标函数等价于 $\sum \sum −p log(q) $这个式子与softmax非常的类似，我们知道softmax的目标函数是 $ \sum −y log p$ ，对应的梯度是$y−p$ (注：这里的softmax中 $y$ 表示label，$p$ 表示预估值)。同样我们可以推导SNE的目标函数中的$i$在$j$下的条件概率情况的梯度是$2 (p_{i∣j} − q_{i∣j})(y_i − y_j) $， 同样$j$在$i$下的条件概率的梯度是 $2 ( p_{j∣i} − q_{j∣i})(y_i − y_j)$，最后得到完整的梯度公式如下: \frac{\partial C}{\partial y_i} = 2 \sum_j (p_{j∣i} − q_{j∣i} + p_{i∣j} − q_{i∣j})(y_i − y_j)在初始化中，可以用较小的 $\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。 即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下: Y^{(t)} = Y^{(t−1)} + \eta \frac{\partial C}{ \partial Y} + \alpha (t)( Y^{(t−1)} − Y^{(t−2)})这里的$Y^{(t)}$表示迭代$t$次的解，$\eta$表示学习速率 ，$\alpha (t)$ 表示迭代$t$次的动量。 此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。 因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。 2.t-SNE尽管SNE提供了很好的可视化方法，但是它很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下: 使用对称版的SNE，简化梯度公式 低维空间下，使用t分布替代高斯分布表达两点之间的相似度 t-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。 2.1 对称版SNE优化$p_{i∣j}$和$q_{i∣j}$的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即$P$是高维空间里各个点的联合概率分布，$Q$是低维空间下的，目标函数为: C = KL( P ∣∣ Q) = \sum_i \sum_j p_{i,j} log p_{ij} q_{ij}这里的$p_{ii},q_{ii}$为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意$i, p_{ij} = p_{ji}, q_{ij} = q_{ji} $，因此概率分布可以改写为: p_{ij} = \frac{ exp(−∣∣ x_i − x_j ∣∣ ^2 / 2 \sigma^2)}{\sum_{k\not=l} exp(−∣∣ x_k − x_l ∣∣ ^2 / 2 \sigma^2)} \quad \quad q_{ij} = \frac{exp(−∣∣ y_i − y_j ∣∣^2)}{ \sum_{k≠l} exp(−∣∣ y_k − y_l ∣∣^2)}这种表达方式，使得整体简洁了很多。但是会引入异常值的问题。 比如$x_i$是异常值，那么$∣∣ x_i − x_j ∣∣^2$会很大，对应的所有的$j, p_{ij}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对cost影响很小。 为了解决这个问题，我们将联合概率分布定义修正为: $p_{ij} = (p_{i∣j} + p_{j∣i}) / 2 $, 这保证了 $\sum_j p_{ij} &gt; \frac{1}{2n}$, 使得每个点对于cost都会有一定的贡献。 对称SNE的最大优点是梯度计算变得简单了，如下: \frac{\partial C}{ \partial y_i} = 4 \sum_j (p_{ij} − q_{ij})(y_i − y_j)实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。 2.2 Crowding问题拥挤问题就是说各个簇聚集在一起，无法区分。 比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射， 比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。 进一步的说明，假设一个以数据点$x_i$为中心，半径为$r$的$m$维球(三维空间就是球)，其体积是按$r^m$增长的， 假设数据点是在$m$维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。 从上图可以看到，随着维度的增大，大部分数据点都聚集在$m$维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。 怎么解决crowding问题呢？ Cook et al.(2007) 提出一种slight repulsion的方式，在基线概率分布(uniform background)中引入一个较小的混合因子 $\rho$， 这样$q_{ij}$就永远不会小于 $ \frac{2 \rho}{n(n-1)}$ (因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q_{ij}$总是会比$p_{ij}$大一点。 这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\rho$为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\rho$。 直接优化UNI-SNE是不行的(即一开始$\rho$不为0)，因为距离较远的两个点基本是一样的$q_{ij}$(等于基线分布), 即使$p_{ij}$很大，一些距离变化很难在$q_{ij}$中产生作用。 也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。 2.3 t-SNE对称SNE实际上在高维度下另外一种减轻”拥挤问题”的方法： 在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。 我们对比一下高斯分布和t分布(如上图)，t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。 使用了t分布之后的q变化，如下: q_{ij} = \frac{(1+∣∣ y_i − y_j ∣∣ ^2)^{−1}}{\sum_{k\not=l} (1 + ∣∣ y_i − y_j ∣∣ ^2)^{−1}}此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下: \frac{\partial C}{\partial y_i} = 4 \sum_j ( p_{ij} − q_{ij} ) (y_i − y_j) ( 1 + ∣∣ y_i − y_j ∣∣^2)^{−1}t-SNE的有效性，也可以从上图中看到： 横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点； 而对于低相似度的点，t分布在低维空间中的距离需要更远。 这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。 总结一下，t-SNE的梯度更新有两大优势： 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。 这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。 2.4 算法过程算法详细过程如下： Data: $X = x_1, …, x_n$ 计算cost function的参数：困惑度Perp 优化参数: 设置迭代次数T，学习速率$\eta$, 动量$\alpha(t)$ 目标结果是低维数据表示 $Y^T = y_1, …, y_n$ 开始优化 计算在给定Perp下的条件概率$p_{j∣i}$ (参见上面公式) 令 $p_{ij} = (p_{j∣i} + p_{i∣j}) / 2n $ 用 $N(0, 10^{−4}I)$ 随机初始化 Y 迭代，从 t = 1 到 T， 做如下操作: 计算低维度下的 $q_{ij}$ (参见上面的公式) 计算梯度（参见上面的公式） 更新 $Y^t = Y^{t−1} + \eta \frac{\partial C}{\partial Y} + \alpha (t)(Y^{t−1} − Y^{t−2}) $ 结束 结束 优化过程中可以尝试的两个trick: 提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。 提前夸大(early exaggeration)：在开始优化阶段，$p_{ij}$乘以一个大于1的数进行扩大，来避免因为$q_{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4。 2.5 不足主要不足有四个: 主要用于可视化，很难用于其他目的。 比如测试集合降维，因为它没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。 t-SNE倾向于保存局部特征。 对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间。 t-SNE没有唯一最优解，且没有预估部分。 如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。 训练太慢。有很多基于树的算法在t-SNE上做一些改进 2.6 sklearn中TSNE参数函数参数表： 返回对象的属性表：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归神经网络 - GRU]]></title>
    <url>%2F2018%2F10%2F17%2Frnn2%2F</url>
    <content type="text"><![CDATA[GRU神经单元GRU神经单元与LSTM神经单元及其相似，可以二者相互对比着学习。 GRU可以看成是LSTM的变种，GRU把LSTM中的遗忘门(forget gate)和输入门(input gate)用更新门(update gate)来替代。 把cell state$C_t$和隐状态$h_t$进行合并，在计算当前时刻新信息的方法和LSTM有所不同。 重置门 r_t = \sigma (W_r x_t + U_t h_{t-1} )更新门 z_t = \sigma ( W_z x_t + U_z h_{t-1} )更新状态 计算候选隐藏层(candidate hidden layer)$\hat{h}_t$，这个时候候选隐藏层和LSTM中的$\hat{C}_t$是类似的。 可以看成是当前时刻的新信息，其中$r_t$用来控制需要保留多少之前的记忆，如果$r_t$为$0$，那么$\hat{h}_t$只包含当前词的信息。 \hat{h}_t = tanh ( W x_t + U (r_t \odot h_{t-1} )最后$z_t$控制需要从前一时刻的隐藏层$h_{t−1}$中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$\hat{h}_t$，最后得到$h_t$，直接得到最后输出的隐藏层信息，这里与LSTM的区别是GRU中没有output gate。 h_t = (1-z_t) h_{t-1} + z_t \hat{h}_y如果reset gate接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关的信息； update gate控制当前时刻的隐藏层输出$h_t$需要保留多少之前的隐藏层信息，若$z_t$接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。 一般来说那些具有短距离依赖的单元reset gate比较活跃（如果$r_t$为1，而$z_t$为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元update gate比较活跃。 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-Means聚类]]></title>
    <url>%2F2018%2F10%2F09%2Fkmeans%2F</url>
    <content type="text"><![CDATA[1. 距离量度1.1 两点之间的距离 欧式距离 d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} 曼哈顿距离 d = | x_1 - x_1 | + | y_1 - y_2 | 切比雪夫距离 d = max(| x_1 - x_2 |, | y_1 - y_2 |) 余弦距离 cos \theta = \frac{x_1 x_2 + y_1 y_2}{\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}} Jaccard相似系数 J(A,B) = \frac{| A \cap B |}{| A \cup B |} 相关系数 \rho_{XY} = \frac{Cov(X, Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E((X-EX)(Y-EY))}{\sqrt{D(X)} \sqrt{D(Y)}} 1.2 两个类别之间的距离 单连接聚类 Single-linkage clustering 一个类的所有成员到另一个类的所有成员之间的最短两个之间的距离 全连接聚类 Complete-linkage clustering 两个类中最远的两个点之间的距离 平均连接聚类 Average-linkage clustering 两个类中的点两两的距离求平均 2. K-Means聚类算法思想： 选择K个点作为初始质心 repeat 将每个点指派到最近的质心，形成K个簇 重新计算每个簇的质心 until 簇不发生变化或达到最大迭代次数 这里的重新计算每个簇的质心，如何计算是根据目标函数得来的，因此在开始时需要考虑距离度量和目标函数。 考虑欧几里得距离的数据，使用误差平方和（Sum of the Squared Error, SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。 SSE = \sum_{i=1}^K \sum_{x \in c_i} dist(c_i, x)^2其中$K$表示$K$个聚类中心，$c_i$表示第几个中心，$dist$表示的是欧式距离。 在更新质心时使用所有点的平均值，为什么呢？这里是由SSE决定的。 对第$k$个质心$c_k$求解，最小化式(7)：即对SSE求导并令导数等于零，求解$c_k$，如下： \begin{equation} \begin{aligned} \frac{\partial}{\partial c_k} SSE &= \frac{\partial}{\partial c_k} \sum_{i=1}^K \sum_{x \in c_i} (c_i - x)^2 \\ &= \sum_{i=1}^K \sum_{x \in c_i} \frac{\partial}{\partial c_k} (c_i - x)^2 \\ &= \sum_{x \in c_k} 2 (c_k - x_k) \\ &= 0 \end{aligned} \end{equation} \sum_{x \in c_k} 2(c_k - x_k) = 0 \Rightarrow m_k c_k = \sum_{c \in c_k} x_k \Rightarrow c_k = \frac{1}{m_k} \sum_{x \in c_k} x_k这样，正如前面所述，簇的最小化SSE的最佳质心是簇中各点的均值。 3. K-Means算法的优缺点 优点 易实现 缺点 K值需要预先给定 对初始选取的聚类中心点敏感 可能收敛到局部最小值 在大规模数据收敛慢 进阶学习 bisecting K-means，DBSCAN 4. 算法实现kmeans.py 4.1 运行结果： 4.2 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# coding:utf-8from matplotlib import pyplot as pltfrom sklearn import datasetsimport numpy as npdef kmeans(data, k=2): def _distance(p1, p2): tmp = np.sum((p1 - p2) ** 2) return np.sqrt(tmp) def _rand_center(data, k): &quot;&quot;&quot;Generate k center within the range of data set.&quot;&quot;&quot; n = data.shape[1] # features centroids = np.zeros((k, n)) # init with (0,0).... for i in range(n): dmin, dmax = np.min(data[:, i]), np.max(data[:, i]) centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k) return centroids def _converged(centroids1, centroids2): # if centroids not changed, we say &apos;converged&apos; set1 = set([tuple(c) for c in centroids1]) set2 = set([tuple(c) for c in centroids2]) return (set1 == set2) n = data.shape[0] # number of entries centroids = _rand_center(data, k) label = np.zeros(n, dtype=np.int) # track the nearest centroid assement = np.zeros(n) # for the assement of our model converged = False while not converged: old_centroids = np.copy(centroids) for i in range(n): # determine the nearest centroid and track it with label min_dist, min_index = np.inf, -1 for j in range(k): dist = _distance(data[i], centroids[j]) if dist &lt; min_dist: min_dist, min_index = dist, j label[i] = j assement[i] = _distance(data[i], centroids[label[i]]) ** 2 # update centroid for m in range(k): centroids[m] = np.mean(data[label == m], axis=0) converged = _converged(old_centroids, centroids) return centroids, label, np.sum(assement)iris = datasets.load_iris()X, y = iris.data, iris.targetdata = X[:, [1, 3]] # 为了便于可视化，只取两个维度# plt.scatter(data[:,0],data[:,1]);best_assement = np.infbest_centroids = Nonebest_label = Nonefor i in range(10): centroids, label, assement = kmeans(data, 2) if assement &lt; best_assement: best_assement = assement best_centroids = centroids best_label = labeldata0 = data[best_label == 0]data1 = data[best_label == 1]fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))ax1.scatter(data[:, 0], data[:, 1], c=&apos;c&apos;, s=30, marker=&apos;o&apos;)ax2.scatter(data0[:, 0], data0[:, 1], c=&apos;r&apos;)ax2.scatter(data1[:, 0], data1[:, 1], c=&apos;c&apos;)ax2.scatter(centroids[:, 0], centroids[:, 1], c=&apos;b&apos;, s=120, marker=&apos;o&apos;)plt.show()]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各种评价指标]]></title>
    <url>%2F2018%2F09%2F29%2Fmachinelearningevaluate%2F</url>
    <content type="text"><![CDATA[1. 混淆矩阵及其衍生1.1 Confusion Matrix 实际负例 实际正例 预测负例 TN (将负例预测为负例个数) FN (将正例预测为负例个数) 实际正例 FP (将负例预测为正例个数) TP (将正例预测为正例个数) 1.2 召回率 RecallRecall = \frac{TP}{TP + FN}1.3 精确率 PrecisionPrecision = \frac{TP}{TP + FP}1.4 准确率 AccuracyAccuracy = \frac{TP + FN}{TP + TN + FP + FN}1.5 调和均值 F-scoreF-score = \frac{2 \times Precision \times Recall}{Precision + Recall}1.6 True Positive Rate (TPR)：代表将正例分对的概率TPR = \frac{TP}{TP + FN}1.7 False Positive Rate (FPR)：代表将负例错分为正例的概率FPR = \frac{FP}{FP + TN}2. ROC &amp; AUC &amp; GAUC2.1 受试者曲线 ROC在ROC空间中，每个点的横坐标是 FPR(假正数-负样本被预测为正样本的个数/概率)，纵坐标是 TPR(真正数-正样本被预测为正样本的个数/概率)， 这也就描绘了分类器在 TP(真正的正例) 和 FP(错误的正例) 间的 trade-off。 ROC的主要分析工具是一个画在ROC空间的曲线(ROC curve)，对于二分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正例或负例（比如大于阈值划分为正类）。 因此可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。 ROC curve经过(0,0)、(1,1)，实际上(0,0)和(1,1)连线形成的ROC curve实际上代表的是一个随机分类器；一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方。 2.2 受试者曲线下面积 AUC用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。于是 AUC (Area Under roc Curve)就出现了。AUC的值就是处于ROC curve下方的那部分面积的大小。 计算方法一 example1: index label predictScore 0 0 0.1 1 0 0.4 2 1 0.35 3 1 0.8 负样本和正样本两两组合后的index集合是：(0, 2)、(0, 3)、(1, 2)、(1, 3) (0, 2): 中正样本概率大于负样本概率(0.35 &gt; 0.1), 计 1 (0, 3): 中正样本概率大于负样本概率(0.8 &gt; 0.1), 计 1 (1, 2): 中正样本概率小于负样本概率(0.35 &lt; 0.4), 计 0 (1, 3): 中正样本概率大于负样本概率(0.8 &gt; 0.4), 计 1 所以这个batch的AUC为：$AUC = \frac{3}{4} = 0.75$ example2: 当存在正负样本得到的 predictScore 相同时，计 $0.5$ index label predictScore 0 0 0.1 1 0 0.4 2 1 0.4 3 1 0.8 负样本和正样本两两组合后的index集合是：(0, 2)、(0, 3)、(1, 2)、(1, 3) (0, 2): 中正样本概率大于负样本概率(0.4 &gt; 0.1), 计 1 (0, 3): 中正样本概率大于负样本概率(0.8 &gt; 0.1), 计 1 (1, 2): 中正样本概率小于负样本概率(0.4 = 0.4), 计 0.5 (1, 3): 中正样本概率大于负样本概率(0.8 &gt; 0.4), 计 1 所以这个batch的AUC为：$AUC = \frac{3.5}{4} = 0.875$ 1234567891011121314151617181920def calculate_auc(y_true, y_pred): import numpy as np pos_index = np.where(y_true == 1) neg_index = np.where(y_true == 0) pos_cnt = sum(y_true) neg_cnt = len(y_true) - pos_cnt count = 0 for pindex in pos_index[0]: for nindex in neg_index[0]: if y_pred[pindex] &gt; y_pred[nindex]: count += 1 elif y_pred[pindex] == y_pred[nindex]: count += 0.5 else: count += 0 return 1.0 * count / (pos_cnt * neg_cnt) 计算方法二 AUC = \frac{\sum_{ins_i \in positiveclass} rank_{ins_i} - 0.5 \times M \times (M + 1)}{M \times N} 其中 $rank_{ins_i}$ 代表第 $i$ 条样本的序号。(概率得分从小到大，排在第$rank$个位置); $M$表示正样本的个数; $N$表示负样本的个数; $\sum_{ins_i \in positiveclass}$ 把所有正样本的累加 使用example1中的样本计算，按照概率排序后得到： index label predictScore rank 0 0 0.1 1 1 1 0.35 2 2 0 0.4 3 3 1 0.8 4 按照上面的公式，将正样本的序号加起来，即index=1, index=3的rank值加起来，之后减去一个常数项 $0.5 \times M \times (M + 1)$; 得到的公式结果：$\frac{(2 + 4) - 0.5 \times 2 \times (2 + 1)}{2 \times 2} = \frac{6 - 3}{4} = 0.75$ 当存在相同的 predictScore, 并且其中存在着正负样本 index label predictScore rank 0 0 0.3 1 1 1 0.5 2 2 1 0.5 3 3 0 0.5 4 4 0 0.5 5 5 1 0.7 6 6 1 0.8 7 这里需要注意的是：相等概率得分的样本，无论正负，谁在前，谁在后无所谓。 由于只考虑正样本的rank值： 对于正样本index=1，其rank值为 (5+4+3+2)/4 对于正样本index=2，其rank值为 (5+4+3+2)/4 对于正样本index=5，其rank值为 6 对于正样本index=6，其rank值为 7 最终得到：$\frac{(5+4+3+2)/4 + (5+4+3+2)/4 + 6 + 7 - 0.5 \times 4 \times (4 + 1)}{4 \times 3} = \frac{10}{12}$ 123456789101112131415161718192021def calculate_auc(y_true, y_pred): pos_cnt = sum(y_true) neg_cnt = len(y_true) - pos_cnt sum_rank = 0 ranked_data = sorted(zip(y_true, y_pred), key=lambda x: x[1]) score2rank = &#123;&#125; for index, (label, score) in enumerate(ranked_data): if score not in score2rank: score2rank[score] = [index + 1] else: score2rank[score].append(index + 1) for label, score in ranked_data: if label == 1: sum_rank += sum(score2rank[score]) / 1.0 * len(score2rank[score]) numerator = sum_rank - 0.5 * pos_cnt * (pos_cnt + 1) denominator = pos_cnt * neg_cnt return numerator / denominator 2.3 推荐系统常用 GAUC (group AUC)即按照用户进行聚合后单独计算每个用户的AUC，然后将所有用户的AUC进行按照样本数量进行加权平均 123456789101112131415161718192021222324252627282930313233343536373839404142434445from collections import defaultdictfrom sklearn.metrics import roc_auc_scoreimport numpy as npdef cal_group_auc(labels, preds, user_id_list): """Calculate group auc""" if len(user_id_list) != len(labels): raise ValueError( "impression id num should equal to the sample num," \ "impression id num is &#123;0&#125;".format(len(user_id_list))) group_score = defaultdict(lambda: []) group_truth = defaultdict(lambda: []) for idx, truth in enumerate(labels): user_id = user_id_list[idx] score = preds[idx] truth = labels[idx] group_score[user_id].append(score) group_truth[user_id].append(truth) # 标记一个用户是不是全是正样本或者全是负样本 group_flag = defaultdict(lambda: False) for user_id in set(user_id_list): truths = group_truth[user_id] flag = False for i in range(len(truths) - 1): if truths[i] != truths[i + 1]: flag = True break group_flag[user_id] = flag impression_total = 0 total_auc = 0 for user_id in group_flag: # 全是正样本或负样本的用户不统计 if group_flag[user_id]: auc = roc_auc_score(np.asarray(group_truth[user_id]), np.asarray(group_score[user_id])) # 用户的auc按照样本数量进行加权 total_auc += auc * len(group_truth[user_id]) # 总的统计auc的样本数量 impression_total += len(group_truth[user_id]) group_auc = float(total_auc) / impression_total group_auc = round(group_auc, 4) return group_auc 3. AP &amp; MAP3.1 AP(Average Precision)假使当我们使用google搜索某个关键词，返回了10个结果。当然最好的情况是这10个结果都是我们想要的相关信息。 但是假如只有部分是相关的，比如5个，那么这5个结果如果被显示的比较靠前也是一个相对不错的结果。 但是如果这个5个相关信息从第6个返回结果才开始出现，那么这种情况便是比较差的。 这便是AP所反映的指标，与recall的概念有些类似，不过是“顺序敏感的recall”。 比如对于用户 $u$, 我们给他推荐一些物品，那么 $u$ 的平均准确率定义为： AP_u = \frac{1}{\| I^{te}_u \|} \sum _{i \in I^{te}_u} \frac{\sum _{j \in I^{te}_u} \delta ( p _{uj} \lt p _{ui} ) + 1}{p _{ui}}在这里 $p_{ui}$ 表示推荐列表中物品 $i$ 的排序位置。$p_{uj} \lt p_{ui}$ 表示在对用户 $u$ 的排序列表中物品 $j$ 的排序位置在物品 $i$ 的前面。 1234567891011121314def AP(ranked_list, ground_truth): """ Compute the average precision (AP) of a list of ranked items """ hits = 0 sum_precs = 0 for n in range(len(ranked_list)): if ranked_list[n] in ground_truth: hits += 1 sum_precs += hits / (n + 1.0) if hits &gt; 0: return sum_precs / len(ground_truth) else: return 0 3.2 MAP(Mean Average Precision)即为所有用户 $u$ 的AP再取均值(mean)而已。 MAP = \frac{\sum_{u \in U^{te}} AP_{u}}{ U^{te} }4. CG &amp; DCG &amp; NDCG4.1 CG(Cummulative Gain)直接翻译的话叫做“累计增益”。 在推荐系统中，CG即将每个推荐结果相关性(relevance)的分值累加后作为整个推荐列表(list)的得分。 CG_k = \sum_{i=1}^k rel_i这里， $rel_i$ 表示处于位置 $i$ 的推荐结果的相关性，$k$ 表示所要考察的推荐列表的大小。 4.2 DCG(Discounted Cummulative Gain)CG的一个缺点是没有考虑每个推荐结果处于不同位置对整个推荐效果的影响，例如我们总是希望相关性高的结果应排在前面。显然，如果相关性低的结果排在靠前的位置会严重影响用户体验， 所以在CG的基础上引入位置影响因素，即DCG(Discounted Cummulative Gain), “Discounted”有打折，折扣的意思，这里指的是对于排名靠后推荐结果的推荐效果进行“打折处理”: \begin{equation*} DCG_k = \sum_{i=1}^k \frac{2^{rel_i}-1}{\log_2 \left(i+1\right)} \end{equation*} 分子部分 $2^{rel_i}−1$, $rel_i$越大，即推荐结果 $i$ 的相关性越大，推荐效果越好， DCG越大。 分母部分 $log_2 (i+1)$, $i$ 表示推荐结果的位置，$i$ 越大，则推荐结果在推荐列表中排名越靠后，推荐效果越差，DCG越小。 4.3 NDCG(Normalized Discounted Cummulative Gain)DCG仍然有其局限之处，即不同的推荐列表之间，很难进行横向的评估。而我们评估一个推荐系统，不可能仅使用一个用户的推荐列表及相应结果进行评估， 而是对整个测试集中的用户及其推荐列表结果进行评估。 那么不同用户的推荐列表的评估分数就需要进行归一化，也即NDCG(Normalized Discounted Cummulative Gain)。 在介绍NDCG之前，还需要了解一个概念：IDCG, 即Ideal DCG， 指推荐系统为某一用户返回的最好推荐结果列表， 即假设返回结果按照相关性排序，最相关的结果放在最前面，此序列的DCG为IDCG。因此DCG的值介于 $(0, IDCG]$，故NDCG的值介于$(0,1]$。 对于用户 $u$ 的 $NDCG@k$ 定义为： \begin{equation*} NDCG_u@k = \frac{DCG_u@k}{IDCG_u} \end{equation*}这里的 $k$ 表示推荐列表的大小。那么，则有： \begin{equation*} NDCG@k = \frac{\sum_{u\in \mathcal{U}^{te}}NDCG_u@k}{|\mathcal{U}^{te}|} \end{equation*}在具体操作中， 可以事先确定推荐目标和推荐结果的相关性分级。 例如可以使用 $0，1$ 分别表示相关或不相关，比如此处我们用 $ref_i = \delta( i \in I_u^{te} )$, 在这里如果 $x$ 为true, 则 $\delta (x) = 1$，否则 $\delta(x)=0$。 或是使用 $0 \sim 5$ 分别表示严重不相关到非常相关, 也即相当于确定了 $rel$ 值的范围。之后对于每一个推荐目标的返回结果给定 $rel$ 值，然后使用DCG的计算公式计计算出返回结果的DCG值。使用根据排序后的 rel 值序列计算IDCG值，即可计算NDCG. Ref 推荐系统常见评测标准之MAP与NDCG AUC的计算方法 sklearn.metrics.roc_auc_score 图解AUC和GAUC qiaoguan/deep-ctr-prediction/DeepCross/metric.py]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>评估方法</tag>
        <tag>ROC &amp; AUC &amp; GAUC</tag>
        <tag>AP &amp; MAP</tag>
        <tag>CG &amp; DCG &amp; NDCG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My Resume (2018)]]></title>
    <url>%2F2018%2F08%2F08%2Fresume%2F</url>
    <content type="text"><![CDATA[目录 自我介绍 项目1 - 基于神经网络的安卓恶意代码检测 项目2 - 2017开放学术精准画像大赛 项目3 - 微博用户画像 面试遇到的算法问题 面试遇到的数据结构问题 我的简历2018-08-08模型适用范围及特点在这里自我介绍你好，我是赵卫国，目前在哈尔滨工业大学就读，计算机科学与技术专业研二，专业排名前20%， 2017年7月到10月参加了由微软、清华联合举办的《2017开放学术精准画像大赛》，我们的队伍在400多个队伍中获得了第3名的成绩； 其中我担任队长的职责，主要负责任务一和任务三的解决。 本科就读于青岛大学，网络工程专业，专业排名前10%，参加过一次蓝桥杯软件大赛，获得了山东赛区二等奖； 生活上，有自己的爱好兴趣，喜欢运动，摄影。 项目1 - 基于神经网络的安卓恶意代码检测任务描述： 对提供的安卓应用程序，分析检测识别其中的恶意应用程序。 方法： 静态分析安卓应用程序，利用反编译工具获取函数调用关系图和操作码序列，使用污点分析技术获取API序列; 将函数调用关系图和API序列利用LSTM学习特征向量，将操作码构造成灰度图使用CNN学习特征向量； 使用SVM + Stacking技术进行分类。对安卓应用程序的最终分类准确率为96%。 涉及知识点 LSTM CNN SVM 面试遇到的问题 1.LSTM与RNN的不同 2.为什么出现梯度消失和梯度爆炸 项目2 - 2017开放学术精准画像大赛任务描述： Task 1：基于学者名+组织名的搜索结果，确定学者的个人主页和个人信息。 Task 2：基于学者的论文信息，挖掘学者的研究兴趣。 Task 3：基于学者的论文信息，预测学者截止2017年6月的论文被引用量。 方法： Task 1：使用搜索结果标题、URL等构造特征，样本正负比1:9，尝试过采样和负采样方法，使用XGBoost确定个人主页，CNN识别照片，朴素贝叶斯确定性别，正则表达式确定邮件等信息； Task 2： 基于论文题目，合著作者、论文引用关系等构造特征，使用PCA降维后训练三个分类器并将其输出结果后处理，选出学者研究兴趣； Task 3： 基于业务需要建立特征(h-index、论文数量等)，使用XGBoost二分类后，对正例数据使用XGBoost回归预测学者论文被引用量。 赛后解题报告 涉及知识点 XGBoost Naive Bayes 面试遇到的问题 1.SVM与LR的区别 2.SVM、LR、XGBoost各适用于什么场景？（数据规模、特征数量、特征特点） 3.手写朴素贝叶斯 项目3 - 微博用户画像任务描述： 给定用户微博文本数据以及社交网络图，预测用户的性别(二分类)、年龄(三分类)、地域(八分类)，最终采用加权准确率作为评价指标。 方法： 使用卡方分析提取关键词，构造BOW特征； 利用微博文本，使用Doc2Vec技术训练出用户的Document Vector； 利用社交关系，使用Graph Embedding训练用户的Node Vector作为特征； 综合上述特征，使用PCA降维、SVM基分类器，并采用Stacking技术进行模型融合。 对地域分类，额外构建了地域常识词典；对性别分类，额外建立性别倾向性词典。 最终准确率为性别88.3%，年龄64.8%，地域72.7%。 涉及知识点 卡方检验 Word2Vec Doc2Vec PCA降维 LINE - Node2Vec 面试遇到的问题 1.Word2Vec和Doc2Vec的区别联系 2.LINE的具体思想和目标函数 面试遇到的算法问题 1.生成模型和判别模型辨析 常见判别模型：KNN，SVM，LR 常见生成模型：朴素贝叶斯，隐马尔可夫模型 判别模型会生成一个表示$P(Y | X)$的判别函数(或预测模型)，而生成模型先计算联合概率$P(Y,X)$，然后通过贝叶斯公式转化为条件概率。简单来说，判别模型不会计算联合概率分布，生成模型需要先计算联合概率分布。 或者理解：生成算法尝试去找到底这个数据是怎么生成的，然后再对一个信号进行分类；基于生成假设，那么哪个类别最有可能产生这个信号，这个信号就属于那个类别。 判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。 2.LR与SVM对比 相同点 都是分类算法、监督学习算法、判别模型 不考虑核函数，都是线性分类算法，即它们的分类决策面都是线性的 不同点 损失函数不同。 SVM只考虑局部的边界线附近的点(支持向量)，而LR考虑全局(远离的点对分离平面起作用) 解决非线性问题时，SVM使用核函数机制，LR通常不采用核函数(自组合特征/Sigmoid) 线性SVM依赖数据表达的距离方式，需要先做normalization，LR不受其影响 SVM自带正则，LR需要额外增加正则项 LR可以给出每个点属于每一类的置信度(注非概率)，而SVM只能分类 LR损失函数： min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} (-log h_{\theta} (x^{(i)})) + (1-y^{(i)})( -log (1- h_{\theta}(x^{(i)})) ) \right] + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2SVM损失函数： \sum_{i=1}^n \lbrace 1-y^{(i)} ( w\cdot x^{(i)} + b ) \rbrace _+ + \lambda || w ||^2 \\ \lbrace z \rbrace _+ = \begin{cases} z, \quad z>0 \\ 0, \quad z\leq 0 \end{cases}如何选择LR和SVM 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM + Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 &gt; LR和SVM的异同点 3.计算CNN中的感受野 公式：$ \frac{in - size}{stride} + 1 = out $ 例子：经过下列卷积操作后，3×3 conv -&gt; 3×3 conv -&gt; 2×2 maxpool -&gt; 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？ 反向推导计算： conv 3x3， stride=1， out=1. $\frac{in - 3}{1} + 1 = 1 \quad \Rightarrow \quad in = 3$ maxpool 2x2， stride=1， out=3. $ \frac{in - 2}{1} + 1 = 3 \quad \Rightarrow \quad in = 4 $ conv 3x3， stride=1， out=4. $ \frac{in - 3}{1} + 1 = 4 \quad \Rightarrow \quad in = 6 $ conv 3x3， stride=1， out=6. $ \frac{in - 3}{1} + 1 = 6 \quad \Rightarrow \quad in = 8 $ 所以输出神经元的感受野为 8x8。 4.Bias和Variance的tradeoff A：让Error(train)尽可能小 B：让Error(train)尽可能等于Error(test) 即，因为A小 and A=B，所以B小。 让Error(train)小 –&gt; 模型复杂化，增加参数数量 –&gt; low bias 让Error(train) == Error(test) –&gt; 模型简单化，减少参数数量 –&gt; low variance bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲就是在样本上拟合的好不好。 要想在bias上表现好(low bias)，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。 low bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。 variance描述的是样本上训练出来的模型在测试集上的表现， 要想在variance上表现好(low variance)，就要简化模型，减少模型的参数，但这样容易欠拟合(underfitting)， 欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。 这个靶子上的点(hits)可以理解成一个一个的拟合模型，如果许多个拟合模型都聚集在一堆，位置比较偏，如图中(high bias, low variance)这种情景，意味着无论什么样子的数据灌进来， 拟合的模型都差不多，这个模型过于简陋了，参数太少了，复杂度太低了，这就是欠拟合； 但如果是图中(low bias, high variance)这种情景，所有拟合模型都围绕中间那个 correct target 均匀分布， 但又不够集中、很散，这就意味着，灌进来的数据一有风吹草动，拟合模型就跟着剧烈变化，这说明这个拟合模型过于复杂了、不具有普适性、泛化能力差，就是过拟合(overfitting)。 所以 bias 和 variance 的选择是一个 tradeoff ， 过高的 variance 对应的概念，有点“剑走偏锋”、“矫枉过正”的意思， 如果说一个人variance比较高，可以理解为，这个人性格比较极端偏执，一条道走到黑哪怕是错的。 而过高的 bias 对应的概念，有点像“面面俱到”、“大巧若拙”的意思，如果说一个人bias比较高，可以理解为，这个人是个圆滑世故，可以照顾到每一个人，所以就比较离散。 训练一个模型的最终目的，是为了让这个模型在测试数据上拟合效果好，也就是 Error(test) 比较小，但在实际问题中 testdata 是拿不到的， 也根本不知道 testdata 的内在规律，所以我们通过什么策略来减小Error(test)呢？ 机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？ Understanding the Bias-Variance Tradeoff 5.神经网络中最后两个全连接层的区别 最后一层为logits代表分类的后验概率(对于分类问题)，神经元数目为类别数目，通常连接sigmoid softmax来得到最后的后验概率； 普通的全连接层，神经元数目没有固定的要求，相当于用不同的模式将前层信息进行融合。 6.核函数与VC维之间的关系 参见：结构化风险最小、VC维到SVM的理解 7.归一化和标准化的区别 归一化(normalization)：为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性。 标准化(standardization)：方便数据的下一步处理而进行的数据缩放等变换，并不是为了方便与其他数据异同处理或比较，比如数据经过零均值标准化之后，更利于使用标准正态分布的性质。 Rescaling: $ x’ = \frac{x - min(x)}{max(x) - min(x)} $ Mean Normalization: $ x’ = \frac{x - mean(x)}{max(x) - min(x)} $ Standardization: $ x’ = \frac{x - \bar{x}}{\sigma} $ Scaling to unit length: $ x’ = \frac{x}{|| x ||} $ 8.Word2Vec 介绍Word2Vec Word2Vec的两个模型分别是CBOW和Skip-gram，两个加快训练的方法是Hierarchical Softmax和负采样； 假设一个训练样本是由中心词$w$和上下文$context(w)$组成，那么CBOW就是用$context(w)$去预测$w$；而Skip-gram则相反，用$w$去预测$context(w)$里的所有词。 HS是试图用词频建立一颗哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，和词典中词的数量一致，而非叶节点是模型的参数。要预测的词，转化成预测从根节点到该词所在的叶子节点的路径，是多个二分类问题。 对于负采样，则是把原来的Softmax多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测1，负例预测0，这样更新局部的参数。 对比CBOW和Skip-gram 训练速度上CBOW会更快一点，因为每次会更新$context(w)$的词向量，而Skip-gram只更新中心词的词向量； Skip-gram对低频词效果比CBOW好，因为是尝试用当前词去预测上下文，当前词是低频高频没有区别，但CBOW类似完形填空，会倾向于选择常见或概率大的词来补全，不太会选择低频词。 Skip-gram在大数据集上可以提取更多的信息，总体比CBOW要好一些； 对比HS和负采样 负采样更快，特别是词表比较大的时候； 负采样为什么要用词频来做采样频率？ 这样可以让频率高的词先学习，然后带动其他词学习； 训练完有两套词向量，为什么只用其中一套？ 对于HS来说，哈夫曼树中的参数是不能拿来做词向量的，因为没办法和词典中的词对应。 负采样中的参数其实可以考虑做词向量，因为中间是和前一套词向量做内积，应该是有意义的，但是考虑到负采样是基于词频的，有些词可能采不到，也就学的不好； 9.training set, validation set, test set的区别 training set: 用来训练模型 validation set：用来模型选择 test set：用来评估所选模型的实际性能 10.LSTM的公式 遗忘门：$ ft = \sigma ( W_f x_t + U_f h{t-1} + b_f ) $ 输入门：$ it = \sigma ( W_i x_t + U_i h{t-1} + b_i ) $ 输出门：$ ot = \sigma ( W_o x_t + U_o h{t-1} + b_o ) $ 神经元状态：$ ct = f_t \circ c{t-1} + it \circ \tanh ( W_c x_t + U_c h{t-1} + b_c ) $ 输出值：$ h_t = o_t \circ \sigma ( c_t ) $ LSTM详细内容 11.GBDT、随机森林、XGBoost的区别 随机森林 容易理解和解释，树可以被可视化 不需要进行数据归一化 不容易过拟合 自带out-of-bag (oob)错误评估功能 易于并行化 不适合小样本，只适合大样本 树之间的相关性越大，整体错误率越高 单棵树的错误率越高，整体错误率越高 基树可以是分类树，也可以是回归树 能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性 GBDT 基树只能是回归树，因为要拟合残差 灵活的处理各种类型的数据 基树的关系，只能串行 缺失值敏感 XGBoost 不仅支持回归树，还支持线性分类器 GBDT用到一阶导数，XGB用到二阶导数 代价函数中有正则项（叶结点数量+叶结点输出值的平方） 列抽样 缺失值不敏感，可以自学习分裂方向 部分并行 12.数据之间如果不是独立同分布的会怎样 独立同分布是为了合理用于测试集，可解释 统计机器学习的假设就是数据独立同分布 13.神经网络里面的损失函数 Softmax Cross Entropy Loss $ E(y, \hat{y} ) = - \sum_j y_j log \hat{y}_j $ 其中$y$表示神经网络的目标标签，$\hat{y}$表示神经网络的输出值。 $\hat{y}$表示softmax损失函数 $ \hat{y}_j = softmax(z_j) = e^{z_j} / \sum_j e^{z_j} $ Mean Square Loss $ mse = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2 $ 14.常见的激活函数及导数 指数 $ y = exp(h) $ $ y’ = exp(h) $ sigmoid $ y = \frac{1}{1+e^{-h}} $ $ y’ = y \times (1-y) $ tanh $ y = tanh(h) = \frac{e^h - e^{-h}}{e^h + e^{-h}} $ $ y’ = 1- h^2 $ 零均值的解释 Sigmoid 的输出不是0均值的，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正(e.g. x&gt;0 elementwise in ),那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 15.bagging减小方差,boosting减小偏差 来源 Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均 Boostring是增量学习算法，每次学习前向的偏差(梯度)，将多个模型的结果相加 bagging由于$ E[\sum X_i / n] = E[X_i] $，所以bagging后的bias和单个子模型的bias接近，不能减小bias bagging由于$ Var[\sum X_i / n] = Var(X_i) / n $，所以可以显著降低variance boostring是在迭代最小化损失函数，其bias自然逐步下降。但是各子模型之间是强相关的，故子模型之和不能降低variance 16.深度学习中,L2和dropout的区别 L2通过控制参数的值，而不修改模型 dropout直接减少训练参数的数量，将大模型分割成多个小模型 机器学习算法使用场景 CRF分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被 RNN 以及它的变种们所替代，LSTM+CRF的解决方案取得了state of art的效果 LR做CTR预估，商品推荐转换为点击率预估也可以用该模型。着重了解推导，正则化及并行，但现在越来越多的依赖于深度学习，包括DNN，DRL SVM可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用SMO求解，还有了解下核函数 adaboost 本身是exp loss在boosting方法下的模型 EM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的还是pLSA，k-means背后也有em的思想 决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。 HMM在基础的一阶马尔可夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错 面试遇到的数据结构问题 1.链表判断是否有环及环的入口节点 判断链表有环没环及环的入口结点问题 和 判断两个链表是否相交 先用一快一慢指针，当慢指针追上快指针时-有环；当快指针指向空时-没有环；之后，统计环的长度，快指针和慢指针相等时，慢指针停下，快指针接着走，再次相等时可以得到环的长度K； 重新从起始点开始，快指针比慢指针多走K步，二者相等时所指的节点为环的入口节点； 2.快速排序、二分查找 快速排序 二分查找 3.TopK问题 应用C++ STL以最小堆方法解决Top K 问题 面试遇到的实践题 1.往模型中加入一个特征，如何判定这个特征是否有效 如何判断分类特征值选取是否有效-知乎 2.逻辑回归的所有样本的都是正样本，那么学出来的超平面是怎样的？ 所有数据点分布在超平面的一侧]]></content>
      <categories>
        <category>个人小结</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RNN梯度消失、梯度爆炸&LSTM解决梯度消失的办法]]></title>
    <url>%2F2018%2F07%2F30%2Frnngradientdisappear%2F</url>
    <content type="text"><![CDATA[目录 RNN梯度消失和梯度爆炸的原因 LSTM解决梯度消失问题 RNN梯度消失和梯度爆炸的原因经典的RNN结构如下图所示：假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2} S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}假设在$t=3$时刻，损失函数为 L_{3}=\frac{1}{2}(Y_{3}-O_{3})^{2}则对于一次训练任务的损失函数为 $L=\sum_{t=0}^{T}{L_{t}}$ ，即每一时刻损失值的累加。 使用随机梯度下降法训练RNN其实就是对 $W_{x} 、 W_{s} 、 W_{o}$ 以及 $b_{1}、 b_{2}$ 求偏导，并不断调整它们以使$L$尽可能达到最小的过程。 现在假设我们我们的时间序列只有三段，$t1，t2，t3$。 我们只对t3时刻的 $W_{x}、W_{s}、W_{0}$ 求偏导（其他时刻类似）： \frac{\partial{L_{3}}}{\partial{W_{0}}}=\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{W_{o}}} \frac{\partial{L_{3}}}{\partial{W_{x}}}=\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{W_{x}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{W_{x}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{S_{1}}}\frac{\partial{S_{1}}}{\partial{W_{x}}} \frac{\partial{L_{3}}}{\partial{W_{s}}}=\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{S_{1}}}\frac{\partial{S_{1}}}{\partial{W_{s}}}可以看出对于 $W_{0}$ 求偏导并没有长期依赖，但是对于 $W_{x}、W_{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S_{t}$ 随着时间序列向前传播，而 $S_{t}$ 又是 $W_{x}、W_{s}$ 的函数。 根据上述求偏导的过程，我们可以得出任意时刻对 $W_{x}、W_{s}$ 求偏导的公式： \frac{\partial{L_{t}}}{\partial{W_{x}}}=\sum_{k=0}^{t}{\frac{\partial{L_{t}}}{\partial{O_{t}}}\frac{\partial{O_{t}}}{\partial{S_{t}}}}(\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}})\frac{\partial{S_{k}}}{\partial{W_{x}}}任意时刻对 $W_{s}$ 求偏导的公式同上。 如果加上激活函数， $S_{j}=tanh(W_{x}X_{j}+W_{s}S_{j-1}+b_{1})$ ， 则 \prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}W_{s}激活函数tanh和它的导数图像如下。 由上图可以看出 $tanh’ \leq 1$ ，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现 $W_{x}X_{j}+W_{s}S_{j-1}+b_{1}=0$ ，如果 $W_{s}$ 也是一个大于0小于1的值， 则当t很大时 $\prod_{j=k+1}^{t} {tanh’} W_{s}$ ，就会趋近于$0$，和 $0.01^{50}$ 趋近与$0$是一个道理。同理当 $W_{s}$ 很大时 $\prod_{j=k+1}^{t}{tanh^{‘}}W_{s}$ 就会趋近于无穷， 这就是RNN中梯度消失和爆炸的原因。 至于怎么避免这种现象，看看 $\frac{\partial{L_{t}}}{\partial{W_{x}}}=\sum_{k=0}^{t}{\frac{\partial{L_{t}}}{\partial{O_{t}}}\frac{\partial{O_{t}}}{\partial{S_{t}}}}(\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}})\frac{\partial{S_{k}}}{\partial{W_{x}}}$ 梯度消失和爆炸的根本原因就是 $\prod_{j=k+1}^{t}{\frac{\partial{S_{j}}}{\partial{S_{j-1}}}}$ 这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉。至于怎么去掉，一种办法就是使 ${\frac{\partial{S_{j}}}{\partial{S_{j-1}}}}\approx1$，另一种办法就是使 ${\frac{\partial{S_{j}}}{\partial{S_{j-1}}}}\approx0$ 。其实这就是LSTM做的事情。 摘自： RNN梯度消失和爆炸的原因 LSTM解决梯度消失问题先上一张LSTM的经典图： 传统RNN可以抽象成下面这幅图： 而LSTM可以抽象成这样： 三个 × 分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。 这三个gate是如何控制流入流出的呢，其实就是通过下面 $f{t},i{t},o_{t}$ 三个函数来控制，因为 $\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的， 刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。 f_{t}=\sigma({W_{f}X_{t}}+b_{f})i_{t}=\sigma({W_{i}X_{t}}+b_{i})o_{i}=\sigma({W_{o}X_{t}}+b_{o})当前的状态 $S_{t}=f_{t}S_{t-1}+i_{t}X_{t}$类似与传统RNN $S_{t}=W_{s}S_{t-1}+W_{x}X_{t}+b_{1}$。将LSTM的状态表达式展开后得： S_{t}=\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\sigma(W_{i}X_{t}+b_{i})X_{t}如果加上激活函数， S_{t}=tanh\left[\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\sigma(W_{i}X_{t}+b_{i})X_{t}\right]RNN梯度消失和爆炸这部分中传统RNN求偏导的过程包含 \prod_{j=k+1}^{t}\frac{\partial{S_{j}}}{\partial{S_{j-1}}}=\prod_{j=k+1}^{t}{tanh{'}W_{s}}对于LSTM同样也包含这样的一项，但是在LSTM中 \prod_{j=k+1}^{t}\frac{\partial{S_{j}}}{\partial{S_{j-1}}}=\prod_{j=k+1}^{t}{tanh{’}\sigma({W_{f}X_{t}+b_{f}})}假设 $Z=tanh{‘}(x)\sigma({y})$ ，则 $Z$ 的函数图像如下图所示： 可以看到该函数值基本上不是 0 就是 1 。 再看看RNN梯度消失和爆炸原因这部分中传统RNN的求偏导过程： \frac{\partial{L_{3}}}{\partial{W_{s}}}=\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{S_{2}}}\frac{\partial{S_{2}}}{\partial{S_{1}}}\frac{\partial{S_{1}}}{\partial{W_{s}}}如果在LSTM中上式可能就会变成： \frac{\partial{L_{3}}}{\partial{W_{s}}}=\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{3}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{2}}}{\partial{W_{s}}}+\frac{\partial{L_{3}}}{\partial{O_{3}}}\frac{\partial{O_{3}}}{\partial{S_{3}}}\frac{\partial{S_{1}}}{\partial{W_{s}}}因为 $\prod_{j=k+1}^{t}\frac{\partial{S_{j}}}{\partial{S_{j-1}}}=\prod_{j=k+1}^{t}{tanh{’}\sigma({W_{f}X_{t}+b_{f}})}\approx 0 | 1 $ ，这样就解决了传统RNN中梯度消失的问题。 摘自： LSTM如何解决梯度消失问题]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LINE Tutorial]]></title>
    <url>%2F2018%2F07%2F13%2FLINE%2F</url>
    <content type="text"><![CDATA[目录 写作动机 引子 问题定义 LINE:大规模信息网络嵌入 写作动机当前大多数图形嵌入方法不能对包含数百万个节点的真实信息网络进行扩展，分析大型信息网络在学术界和行业中一直受到越来越多的关注。 而现在的大多数嵌入方法在小型网络中适用性非常不错，但当网络包含数百万乃至数百亿节点时，就看起来并不那么有效，其时间复杂度至少是节点数的二次方。 最重要的是，它们着重于关注节点之间的一阶相似性，及两点之间是否直接相连，而忽略了其二阶相似性(即拥有许多共同的邻节点)。 因此LINE模型就是为了在信息网络嵌入至低维空间时保留其一阶相似以及二阶相似。 引子下图展示了一个说明性例子。因为顶点6和7之间的边的权值很大，即6和7有非常高的一阶近似，它们应该在嵌入领域中彼此紧密相连。 另一方面，虽然顶点5和6之间的顶点没有链接，但是它们有很多共同的邻域，即它们有很高的二阶近似和两者的表示会是十分的接近。 问题定义 1.信息网络的定义 信息网络定义为$G=(V,E)$，其中$V$是顶点集合，顶点表示数据对象，$E$是顶点之间的边缘的集合，每条边表示两个数据对象之间的关系。每条边$e\in E$表示为有序对$e=(u,v)$，并且与权重$W{uv} &gt; 0$相关联，权重表示关系的强度。如果$G$是无向的，我们有$(u,v)\equiv (v,u)$和$W{uv}\equiv W{vu}$;如果G是有向的，我们有$(u,v)\ne (v,u)$和$W{uv}\ne W_{vu}$。关于边权重问题，本次论文中只研究非负权重的情况。 2.一阶相似性的定义 网络中的一阶相似性是两个顶点之间的局部点对的邻近度。对于由边缘$(u,v)$链接的每对顶点，该边缘的权重$W_{uv}$表示$u$和$v$之间的一阶相似性，如果在$u$和$v$之间没有观察到边缘，它们的一阶相似性为$0$。 3.二阶相似度的定义 二阶相似度指的是一对顶点之间的接近程度$(u,v)$在网络中是其邻域网络结构之间的相似性。数学上，让$p{u} =(w{u,1},…,w{u, | V | })$表示一阶附近$u$与所有其他的顶点，那么$u$和$v$之间的二阶相似性由$p{u}$和$p_{v}$之间的相似性来决定。如果没有一个顶点同时和$u$与$v$链接，那么$u$和$v$的二阶相似性是0。 4.大规模信息网络嵌入的定义 给定大网络$G=(V,E)$，大规模信息网络嵌入是将每个顶点$v\in V$表示为低维空间$R^{d}$中的向量，学习一个函数$f_{G} : V \rightarrow R^{d}$，其中$d\ll | V |$. 在空间$R^{d}$中，顶点之间的一阶相似性和二阶相似性都被保留。 LINE:大规模信息网络嵌入前文已经提到了信息网络的两大相似性，LINE模型也是由此展开的。 LINE模型的一阶相似性：对每个无向边$(i,j)$，定义顶点$v_{i}$和$v_{j}$的联合概率分布为： p_1(v_i, v_j) = \frac{1}{1+ exp( - \vec{u}_i^T \cdot \vec{u}_j )}$u_{i}\in {R}^{d}$是顶点$v_{i}$的低维向量表示，为保持其一阶相似性，一种直接方法是最小化以下的目标函数： O_1 = d( \hat{p}_1 (\cdot, \cdot), p_1 (\cdot, \cdot) )$d(\cdot , \cdot)$为两种分布之间的距离，$p(,)$为空间$V\times V$上的一个分布， 其经验分布可以定义为 $ \hat{p}_{1} (i,j) = \frac{ w_{i,j} }{ W } $，其中$ W = \sum_{(i,j) \in E} w_{ij} $。 我们选择尽量减少两个概率分布的KL 散度。将$d(,)$替换为 KL 散度并省略一些常数，我们得到︰ O_1 = - \sum_{(i,j) \in E} w_{i,j} log p_1 (v_i, v_j)需要注意的是，一阶相似度仅适用于无向图,而不适用于有向图。 LINE模型的二阶相似性：二阶相似性假定与其他顶点共享邻居顶点的两个点彼此相似(无向有向均可)，一个向量$u^{} $和$u^{‘}$ 分别表示顶点本身和其他顶点的特定“上下文”，意为二阶相似。对于每个有向边$(i,j)$，我们首先定义由生成“上下文”的概率： p_2(v_j | v_i) = \frac{exp(\vec{u}_j^{'T} \cdot \vec{u}_i)}{\sum_{k=1}^{|V|} exp(\vec{u}_k^{'T} \cdot \vec{u}_i)}可以看到，上式其实是一个条件分布，我们取$i$为研究对象，$p(,v_{i} )$，在降维之后使其接近与经验分布$\hat{p}_{2} $。因此最小化以下目标函数： O_2 = \sum_{i \in V} \lambda_i d ( \hat{p}_2 (\cdot | v_i), p_2(\cdot | v_i) )$d(,)$上文已经说明，$\lambda_{i}$ 来表示网络中顶点$i$的声望,本文中即是顶点$i$的度数，因此二阶相似性的计算公式为： O_2 = - \sum_{(i,j) \in E} w_{i,j} log p_2(v_j | v_i)将上述的$O_{1}$ 以及$O_{2}$ 进行训练，再进行综合。 模型优化：$O_{2}$ 的计算代价十分的昂贵，因此优化时使用了负采样方法，为每条边指定了一个目标函数： log \sigma (\vec{u}_j^{'T} \cdot \vec{u}_i) + \sum_{i=1}^K E_{v_n \sim P_n (v)} \left[ log \sigma (- \vec{u}_n^{'T} \cdot \vec{u}_i) \right]其中 $\sigma (x) = 1 / (1+exp(−x))$ 是 sigmoid 函数。这个模型的第一项是观察的边，模型的第二项是从噪声分布中得出的负样例的边 和 K 是负样例的边的数量。 这里设$ P_n (v) \propto d^{3/4}_v $，其中$ d_v $是顶点$ v $的 out-degree。 对于目标函数$O_1$，存在一个非常简单的解：$u_{ik} = \infty$ ，对 $i=1,…, | V | $和 $k=1,…,d$。为了避免这个简单的解，这里仍然可以利用负采样的方法，只需要将$ \vec{v}_j^{‘T} $改为$\vec{v}_j^T$。 这里采样异步随机梯度算法（ASGD）来优化上式。在每一步，ASGD 算法采样一个 mini-batch 的边并更新参数。如果采样了一个边 $(i,j)$，顶点 $i$ 的嵌入向量 $\vec{u}_i$ 梯度会被计算为： \frac{\partial O_2}{\partial \vec{u}_i} = w_{i,j} \cdot \frac{\partial log p_2 (v_j | v_i)}{\partial \vec{u}_i}注意这个梯度是与边的权值相乘，当边的权值有很大的方差的时候会出现问题。例如，在此共现网络中，有些单词共现了很多次但有些词却共现很少次。 在这样的网络中，梯度的比例是发散的，很难找到一个好的学习速率。如果根据小权重的边选择一个大的学习速率，那么在有较大权重的边上的梯度将会爆炸， 如果根据具有较大权重的边来选择学习速率，那么梯度将会变得太小。因此边缘采样同样要优化。从起始边缘采样并将采样的边缘作为二进制边缘，其中采样概率与原始边缘的权重成比例。 LINE：Large-scale Information Network Embedding LINE:Large-scale Information Network Embedding阅读笔记 论文 LINE：Large-scale Information Network Embedding 阅读笔记]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Doc2Vec Tutorial]]></title>
    <url>%2F2018%2F07%2F08%2Fdoc2vectutorial%2F</url>
    <content type="text"><![CDATA[目录 gensim简单使用 Doc2Vec(PV-DM) Doc2Vec(PV-DBOW) gensim简单使用模型参数说明： dm=1 PV-DM dm=0 PV-DBOW。 size 所得向量的维度。 window 上下文词语离当前词语的最大距离。 alpha 初始学习率，在训练中会下降到min_alpha。 min_count 词频小于min_count的词会被忽略。 max_vocab_size 最大词汇表size，每一百万词会需要1GB的内存，默认没有限制。 sample 下采样比例。 iter 在整个语料上的迭代次数(epochs)，推荐10到20。 hs=1 hierarchical softmax ，hs=0(default) negative sampling。 dm_mean=0(default) 上下文向量取综合，dm_mean=1 上下文向量取均值。 dbow_words:1训练词向量，0只训练doc向量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding: utf-8from gensim.models.doc2vec import Doc2Vec, TaggedDocumentfrom nltk.tokenize import word_tokenizedata = ["I love machine learning. Its awesome.", "I love coding in python", "I love building chatbots", "they chat amagingly well"]tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[i]) for i, _d in enumerate(data)]# for item in tagged_data:# print(item)# TaggedDocument(['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], ['0'])# TaggedDocument(['i', 'love', 'coding', 'in', 'python'], ['1'])# TaggedDocument(['i', 'love', 'building', 'chatbots'], ['2'])# TaggedDocument(['they', 'chat', 'amagingly', 'well'], ['3'])max_epochs = 100vec_size = 10alpha = 0.025model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)model.build_vocab(tagged_data)for epoch in range(max_epochs): print("iteration &#123;0&#125;".format(epoch)) model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter) model.alpha -= 0.0002 model.min_alpha = model.alphamodel.save("d2v.model")print("Model Saved!")model = Doc2Vec.load("d2v.model")test_data = word_tokenize("I love chatbots".lower())v1 = model.infer_vector(test_data)print("V1_infer:", v1, end="\n\n")similar_doc = model.docvecs.most_similar(1)print(similar_doc, end="\n\n")for i in range(len(data)): print(i, model.docvecs[i], end="\n\n")"""V1_infer: [ 0.07642581 0.11099993 -0.02696064 -0.06895891 0.01907274 -0.08622721 -0.00581482 -0.08242869 -0.02741096 -0.05718143][(0, 0.9964085817337036), (2, 0.9903678894042969), (3, 0.985236406326294)]0 [ 1.22332275 0.93302435 0.0843188 -0.40848678 -0.26203951 -0.54852372 -0.07185869 -0.72306669 -0.635378 -0.05744991]1 [ 0.83533287 0.64208162 0.04288336 -0.28720176 -0.17140444 -0.43564293 -0.0435797 -0.45327306 -0.46411869 -0.12211297]2 [ 0.71188128 0.49025729 -0.03114104 -0.2172543 -0.18351653 -0.35265383 -0.09494802 -0.47745392 -0.33393192 -0.1065111 ]3 [ 0.86559838 0.77232999 -0.0108105 -0.179581 -0.10455605 -0.41468951 -0.11108498 -0.59402496 -0.59637135 -0.22117028]""" DOC2VEC gensim tutorial 使用gensim的doc2vec生成文档向量 需要说明的一点是这里的Paragraph Vector不是真的段落向量的意思，它可以根据需要的不同进行变化，可以是短语、句子甚至是文档。 Doc2Vec(PV-DM)PV-DM在模型的输入层新增了一个Paragraph id，用于表征输入上下文所在的Paragraph。 例如如果需要训练得到句子向量，那么Paragraph id即为语料库中的每个句子的表示。 Paragraph id其实也是一个向量，具有和词向量一样的维度，但是它们来自不同的向量空间，D和W，也就是来自于两个不同的矩阵。 剩下的思路和CBOW模型基本一样。在模型中值得注意的一点是，在同一个Paragraph中，进行窗口滑动时，Paragraph id是不变的。 Paragraph id本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。 模型的具体步骤如下： 每个段落都映射到一个唯一的向量，由矩阵$D$中的一列表示，每个词也被映射到一个唯一的向量，表示为$W$ ; 对当前段落向量和当前上下文所有词向量一起进行取平均值或连接操作，生成的向量用于输入到softmax层，以预测上下文中的下一个词: y=b+Uh(w_{t-k}, \dots, w_{t+k}; W; D) 这个段落向量可以被认为是另一个词。可以将它理解为一种记忆单元，记住当前上下文所缺失的内容或段落的主题 ； 矩阵$D$ 和$W$ 的区别: 通过当前段落的index，对$D$ 进行Lookup得到的段落向量，对于当前段落的所有上下文是共享的，但是其他段落的上下文并不会影响它的值，也就是说它不会跨段落(not across paragraphs) ； 当时词向量矩阵$W$对于所有段落、所有上下文都是共享的。 Doc2Vec(PV-DBOW)模型希望通过输入一个Paragraph id来预测该Paragraph中的单词的概率，和Skip-gram模型非常的类似。 PV-DBOW模型的输入忽略了的上下文单词，但是关注模型从输出的段落中预测从段落中随机抽取的单词； PV-DBOW模型和训练词向量的Skip-gram模型非常相似。 Doc2Vec的特点 可以从未标记的数据中学习，在没有足够多带标记的数据上仍工作良好； 继承了词向量的词的语义（semantics）的特点； 会考虑词的顺序（至少在某个小上下文中会考虑） Distributed representations of sentences and documents Distributed representations of sentences and documents总结 - paperweekly 用 Doc2Vec 得到文档／段落／句子的向量表达 关于Gensim的初次见面 和 Doc2vec 的模型训练 论文笔记：Distributed Representations of Sentences and Documents paragraph2vec介绍]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络CNN初识]]></title>
    <url>%2F2018%2F06%2F28%2Fcnn1%2F</url>
    <content type="text"><![CDATA[目录 1.引入 2.卷积神经网络的结构 3.卷积神经网络的正式表达 4.理解卷积 译自：Conv Nets: A Modular Perspective， Understanding Convolutions1.引入在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。 简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 但是因为其具有的重复性，我们需要学习的参数相对来说非常少。 这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。 这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数， 并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。 2.卷积神经网络的结构假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。 假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。 引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出） 更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？ 我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性， 所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。 因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$ 在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些 （所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数） 在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。 卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。 如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。 卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。 通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。 比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。 最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。 最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。 最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。 由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。 至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。 在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。 例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。 上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。 同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。 这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。 有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。 上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。 在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。 例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。 在论文Network in Network中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点， 由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。 这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。 3.卷积神经网络的正式表达考虑一维的卷积层，其中输入为$\lbrace x_n \rbrace$，输出为$ \lbrace y_n \rbrace $ 很容易可以用输入表示输出： y_n = A( x_n, x_{n+1}, \cdots )如上图： y_0 = A(x_0, x_1) \\ y_1 = A(x_1, x_2)相似地，如果我们考虑二维的卷积层，其中输入为$\lbrace x{n,m} \rbrace$，输出为$ \lbrace y{n,m} \rbrace $ 同样地，我们可以用输入表示输出： y_{n,m} = A \begin{pmatrix} x_{n,m}, & x_{n+1, m}, & \cdots , \\ x_{n,m+1}, & x_{n+1, m+1}, & \cdots , \\ & \cdots & \end{pmatrix}例如： y_{0,0}=A \begin{pmatrix} x_{0,0}, & x_{1,0}, \\ x_{1,0}, & x_{1,1}, \\ \end{pmatrix} y_{1,0}=A \begin{pmatrix} x_{1,0}, & x_{2,0}, \\ x_{1,1}, & x_{2,1}, \\ \end{pmatrix}把这些和矩阵方程联合起来，得$A(x)$。 A(x) = \sigma (W x + b)实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式—关于卷积的数学运算，这通常更有帮助。 卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。 它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。 其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。 最后，卷积将为我们提供一个非常不同的角度来解释卷积层。 4.理解卷积上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？ 球体下落的启示 试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。 球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。 那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？ 我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。 球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同， 那么$g$和$f$是两个不同的概率分布。 假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \cdot f(b)$。 从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\cdot g(1)$。 然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。 上述两种可能各种的概率表示依次是$f(1) \cdot g(2)$和$ f(0) \cdot g(3)$。 为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。 \cdots f(0) \cdot g(3) + f(1) \cdot g(2) + f(2) \cdot g(1) \cdots我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为： \sum_{a+b=c} f(a) \cdot g(b)事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下: (f ∗ g) (c) = \sum_{a+b=c} f(a) \cdot g(b)如果用$b=c-a$替换，则有 (f ∗ g) (c) = \sum_a f(a) \cdot g(c−a)这正是标准的卷积定义。 更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$ 为了完成卷积，我们考虑所有的中转位置。 卷积的可视化 有一个非常好的例子可以帮助我们更好地理解卷积的过程。 首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。 设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？ 第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $ 我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。 对所有的$a$求和，我们就得到了卷积的结果。 这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。 如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。 具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值) 随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。 当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。 下图，我们用两个箱型图变现了卷积的过程： 从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。 卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。 采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。 高维卷积 卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。 卷积过程还是和之前的一样 (f ∗ g) ( c ) = \sum_{a+b=c} f(a) \cdot g(b)只是这里的$a，b，c$是向量，更明确地表示是 (f ∗ g)(c_1, c_2) = \sum_{a_1 + b_1 = c_1 \\ a_2 + b_2 = c_2} f ( a_1, a_2) \cdot g(b_1, b_2 )或者使用标准定义： (f ∗ g ) ( c_1, c_2) = \sum{a_1,a_2} f(a_1, a_2) \cdot g(c_1−a_1, c_2−a_2)和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。 二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。 核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。 比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9 我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。 也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。 如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。 gimp documentation 中有很多其他的例子。 卷积神经网络 那么，卷积操作和卷积神经网络是如何联系起来的呢？ 首先我们考虑一维卷积神经网络，其中输入为$ \lbrace x_n \rbrace$，输出为$ \lbrace y_n \rbrace $， 由图可得，输入层与输出层之间的函数关系为： y_n = A ( x_n, x_{n+1}, \cdots)通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成： \sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \cdots + b)在这个公式中，$x_0, x_1, \cdots $代表输入；$w_0, w_1, \cdots $代表神经节点与输入之间连接的权重。 这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。 正是这些权重，决定了卷积神经网络可以为我们处理的任务。 具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$： y = \sigma ( W x + b )由此我们可以得到： y_0 = \sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \cdots) \\ y_1 = \sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \cdots)参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。 上图描述的这种神经网络可以表达成如下等式： y_0 = \sigma ( W_{0} x_0 + W_{1} x_1 - b) \\ y_1 = \sigma ( W_{0} x_1 + W_{1} x_2 - b)然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）： W = \left[ \begin{array}{ccccc} W_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\ W_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\ W_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\ W_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\ ... & ... & ... & ... & ...\\ \end{array} \right]上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。 W = \left[ \begin{array}{ccccc} w_0 & w_1 & 0 & 0 & ...\\ 0 & w_0 & w_1 & 0 & ...\\ 0 & 0 & w_0 & w_1 & ...\\ 0 & 0 & 0 & w_0 & ...\\ ... & ... & ... & ... & ...\\ \end{array} \right]与上述矩阵的点乘，其实和卷积一个向量$ \left[ \cdots 0, w_1, w_0, 0 \cdots \right] $是一样的。 当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。 那么二维的卷积层又是什么样呢？ 上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。 回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。 结语 在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？ 非常强大的语言来描述网络的连接。 到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。 卷积运算具有显著的实现优势。 许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。 实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正负样本不平衡的处理方法]]></title>
    <url>%2F2018%2F06%2F21%2F2018-6-21-unbalancedata%2F</url>
    <content type="text"><![CDATA[1 通过过抽样和欠抽样解决样本不均衡抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。过抽样过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见原始论文。 这里有SMOTE算法的多个不同语言的实现版本： Python: UnbalancedDataset模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。 R: DMwR package。 Weka: SMOTE supervised filter 欠抽样 欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。 总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。 2 通过正负样本的惩罚权重解决样本不均衡通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。 使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight : {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。 如果算法本身支持，这种思路是更加简单且高效的方法。 3 通过组合/集成方法解决样本不均衡组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。 例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。 这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。 如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。 4 通过特征选择解决样本不均衡上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。 一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。 5 尝试其它评价指标 混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。 精确度(Precision) 召回率(Recall) F1值(F1 Score)：精确度与找召回率的加权平均。 Kappa (Cohen kappa) ROC曲线(ROC Curves)：见Assessing and Comparing Classifier Performance with ROC Curves 6 尝试不同的分类算法应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据，具体请见Why you should be Spot-Checking Algorithms on your Machine Learning Problems 7 尝试一个新的角度理解问题我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。 异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。 变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。 推荐看论文Learning from Imbalanced Data 主要包括四大类方法: Sampling Cost Sensitive Methods Kernal-Based Methods and Active Learning Methods One-Class Learning or Novelty Detection Methods 极端情况下，只有正例(负例)如何做分类？ 当作异常点检测问题或变化趋势检测问题 识别异常点可以用四分位数(Tukey’s text)方法，详见这里 One-Class SVM介绍 CTR 预估正负样本 不平衡，请问怎么解决? 在分类中如何处理训练集中不平衡问题]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归神经网络RNN初识 - LSTM]]></title>
    <url>%2F2018%2F06%2F20%2Frnn1%2F</url>
    <content type="text"><![CDATA[目录 1.循环神经网络 2.长期依赖的问题 3.LSTM 网络 4.LSTM的变种 5.结论 译自：Understanding LSTM Networks1.循环神经网络人类不会每秒钟都从头开始思考。在阅读本文时，你可以根据以前单词的理解来理解每个单词。你不要把所有东西都扔掉，再从头开始思考。 你的想法有持久性。 在这方面，传统神经网络不能做到这一点，这是一个很大的缺点。 例如，想象一下，当你想要分析电影中剧情的发展。 传统神经网络是不能够利用电影中以前的事件来推理以后的事情的。 而循环神经网络解决了这个问题。 它们是具有循环的网络，允许信息影响持续存在。 在上图中，一组神经网络$A$输入$X_t$并输出一个值$h_t$。 循环允许信息从网络的一个步骤传递到下一个。 这些循环使得循环神经网络显得很神秘。 然而，如果你仔细想想，它们并不与一般的神经网络完全不同。 一个循环神经网络可以被认为是同一个网络的多个副本，每一个都传递一个信息给后继者。 考虑如果我们展开循环会发生什么： 这种链状特征揭示了循环神经网络与序列和列表密切相关，它们是用于此类数据的神经网络的自然结构。 而且他们确实得到应用！ 在过去几年里，将RNN应用于语音识别，语言建模，翻译，图像字幕等各种问题已经取得了令人难以置信的成功。 我将讨论可以通过RNN实现的令人惊叹的专长，使Andrej Karpathy的优秀博客 “The Unreasonable Effectiveness of Recurrent Neural Networks”成为可能。 但他们真的很神奇。这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，对于许多任务来说，它们比标准版本好得多。 几乎所有令人兴奋的结果基于循环神经网络是通过他们实现的。这篇文章将探讨这些LSTM。 2.长期依赖的问题RNNs吸引人的一点在于它能够将过去的信息连接到当前任务，例如使用先前的视频帧可以为当前帧的理解提供信息。如果RNNs可以做到这一点，他们将非常有用。 但他们可以吗？这取决于不同的情况。有时候，我们只需要最近的信息来处理当前的任务。 举个例子，输入法能够通过你输入的上一个词来预测你想要输入的下一个词。比如，我们想要预测“云彩在天上”，我们不需要更多的信息就能够预测最后两个字。 在这种情况下，如果相关信息与所需预测的信息间隔很小，则RNN可以学习使用过去的信息。 但有些情况下我们就需要更多的文本。比如预测“我来自法国…我法语很流利…”，最近的信息预测这是一个语言名词，但是当我们想确定具体是哪一种语言的时候， 我们需要找到包含法国的文本，但这个文本很可能间隔非常远。很不幸，当间隔增加，RNNs就会更难以连接相关的信息。 理论上，RNNs完全有能力有能力处理这样的“长期依赖”。一个人可以仔细挑选参数来解决这种形式的问题。但实践却表明RNNs似乎不能学习。 Hochreiter(1991) German深层次的揭露了这个问题， 而Bengio,et al.(1994)解释了一些基本的原因。 但是，LSTMs没有这个问题！ 3.LSTM 网络长短记忆网络（Long Short Term Memory networks）通常称为“LSTMs”-是一种特殊的RNN，有能力学习长期的依赖。 LSTMs最初由Hochreiter &amp; Schmidhuber (1997)提出， 并在之后的工作中由许多人优化推广。LSTM在各种各样的问题上工作得很好，现在被广泛使用。 LSTM专为解决长期依赖问题而设计。记忆长周期的信息是默认的行为而不是努力需要学习的东西。 现有的循环神经网络都会有一个链来重复神经网络的模块。在标准的RNN中一般是一个非常简单的结构，如一个$tanh$层。 LSTMs也有一个类似链结构，但是重复模块有一个复杂的结构。相对于单神经网络层，LSTM有4层，以一个非常特殊的方式交互。 不用担心其中的细节。我们将会一步步图解LSTM。现在让我们熟悉一下将用到的符号。 在上图中，每个线代表一个完整的向量，从一个节点输出到其他节点的输入，粉色的圈代表节点操作， 比如向量加，而黄色框是学习到的神经网络层。线合并代表连接，线分叉代表复制。 LSTMs背后的核心概念 LSTM的关键是Cell的状态，水平线穿过计算图的顶部。Cell状态有点像输送带。直接在整个链上运行，只有一些小的线性相互作用。 信息流畅地保持不变。 LSTMs没有能力去添加或者删除cell状态的信息，而是由称作门（gates)的结构进行调节。 门是一个可以让信息选择性通过的结构。由一个$sigmoid$神经网络层和一个节点乘算法组成。 一步步了解LSTM LSTM中，第一步是决定我们将丢弃什么信息，并把剩下的送入cell状态。这个决定由叫做遗忘门层（forget gate layer）的$sigmoid$层做出。 它通过观察$h{t-1}$和$x_t$，为cell状态$C{t-1}$中的每个数输出一个 0 到 1 之间的数，1表示完全保留，0表示完全抛弃。 让我们回到语言模型的例子。在这个预测下一个单词的问题中，cell状态也许会包含前面对象的属性，所以可以使用正确的代名词。当我们看到一个新的对象，我们希望忘记旧对象的属性。 下一步是来决定我们将保存什么新信息到cell状态中。这有两个部分: 首先，一个$sigmoid$层（input gate layer, 输入门层）决定哪个值将被更新; 然后，一个$tanh$层创建一个新候选值的向量$ \tilde{C}_t $，可以被加入到cell状态中。在下一步，我们将要合并这两步来创建一个状态的更新。 在语言模型的例子里，我们想要添加新对象的属性到cell状态中来替换我们希望遗忘的旧对象的属性。 现在是时候来更新旧的cell状态$C_{t-1}$为新状态$C_t$。前面的步骤已经决定了应该做什么，我们只需要实施。 我们将旧状态乘$f_t$，忘记我们之前决定忘记的东西。然后我们加上$i_t * \tilde{C}_t$。这是新的候选值，由我们决定更新每一个状态值的多少来缩放。 在语言模型的例子里，这是我们丢弃旧对象信息并添加新信息的地方。就如同我们在前一步决定的那样。 最终，我们需要决定输出什么(output gate layer，输出门层)。这输出将基于我们的cell状态，但会是一个过滤(filtered)后的版本。 首先，我们运行一个$sigmoid$层来决定cell状态的哪些部分将被输出。然后，我们将cell状态通过$tanh$(将值转化为-1到1之间)并与$sigmoid$门的输出相乘，就可以只输出我们想要的部分。 对于语言模型例子，因为只看到一个对象，在预测下一个词的时候，它也许想要输出一个关于动词的信息。 比如，它也许输出这个对象是单数还是复数，因此我们知道接下来的是一个动词形式的。 4.LSTM的变种上文讲述了一个普通的LSTM。但不是所有的LSTMs都和上面一样。事实上，看起来几乎所有论文都用了一个有一点不一样的LSTM。区别很小，但是值得注意。 一个著名的LSTM变种，由Gers &amp; Schmidhuber (2000)提出， 添加了窥视孔连接（peephole connections）。这意味着可以让门层查看cell状态(This means that we let the gate layers look at the cell state)。 上面的图添加了链接到所有门的窥孔，但很多论文只会添加其中的一些。 另一个变种是使用耦合的遗忘和输出门。相对于普通的LSTM分别决定添加和遗忘什么信息，这个变种同时作出决定，只在输入相同位置信息的时候遗忘当前的信息。 只有在忘记一些值的时候才输入新值到状态中。 一个有点戏剧化的LSTM变种是GRU（Gated Recurrent Unit），由 Cho, et al. (2014)介绍。 它把遗忘和输入门结合成一个“更新门”，同时还合并了cell状态和隐藏状态，并有其他一些改变。这使得模型比标准LSTM要简洁，并越来越流行。 这只是众多值得注意的LSTM变种中的几个。还有很多，如Depth Gated RNNs,Yao, et al. (2015)。 同时还有一些完全不同的追踪长依赖的方式，比如Clockwork RNNs，Koutnik, et al. (2014). 哪个变种是最好的？差别有必要吗？Greff, et al. (2015)对流行的变种做了一个总结，发现他们几乎一样。 Jozefowicz, et al. (2015)测试了超过一万种RNN结构，发现有一些在特定问题上比LSTM更好。 5.结论早前提到了人们在RNN获得的显著成就。基本都是由LSTM实现的，他们对于大多数任务都工作的很好！ LSTM的方程看起来很吓人，但是通过本文一步一步讲解，希望它们看起来更易懂。 LSTM是我们可以用RNN完成的一大进步。很自然的，我们会想，还有另一大步吗？研究人员的共同观点是：是的！ 还有下一步这就是注意力（attention）！这个想法是让RNN的每一步从一些较大的信息合集中挑选信息。 比如，您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个字。 事实上Xu, et al. (2015)做了这个，如果你想要探索更多关于注意力，这可能是一个有趣的起点！ 有一些真正令人兴奋的结果使用了注意力… 注意力并不是RNN研究中唯一的突破口。例如Grid LSTMs，Kalchbrenner, et al. (2015)看起来非常有意思。 在生成模型中使用 RNN 进行工作，如Gregor, et al. (2015), Chung, et al. (2015), Bayer &amp; Osendorfer (2015)。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SMOTE过采样技术]]></title>
    <url>%2F2018%2F06%2F18%2Fsmote%2F</url>
    <content type="text"><![CDATA[目录 类别不平衡问题 类别不平衡引发的问题 解决不平衡问题的方法 SMOTE算法 论文：SMOTE Synthetic Minority Over-sampling Technique1. 类别不平衡问题类不平衡 (class-imbalance) 是指在训练分类器中所使用的训练集的类别分布不均。 比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类别不平衡。 在后文中，把样本数量过少的类别称为“少数类”。 但实际上，数据集上的类不平衡到底有没有达到需要特殊处理的程度，还要看不处理时训练出来的模型在验证集上的效果，有些时候是没必要处理的。 2. 类别不平衡引发的问题2.1 模型训练过程角度从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。 使用经验风险 (模型在训练集上的平均损失) 最小化作为模型的学习准则。设损失函数为 0-1 loss(这是一种典型的均等代价的损失函数)，那么优化目标就等价于错误率最小化(也就是accuracy最大化)。 考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。 训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%。 于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，训练结束！于是这个模型…… 模型没有学习到如何去判别出少数类，这时候模型的召回率会非常低。 2.2 模型预测过程角度考虑二项Logistic回归模型。输入一个样本 $x$ ，模型输出的是其属于正类的概率 $\hat{y}$ 。当 $\hat{y} &gt; 0.5$ 时，模型判定该样本属于正类，否则就是属于负类。 为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？ 从几率 (odds) 的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为：$ \frac{\hat{y}}{1-\hat{y}} $。 模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 $\theta$ 等于正类样本数除以全部样本数，那么样本的真实几率为： $ \frac{\theta}{1-\theta} $。 当观测几率大于真实几率时，也就是 $ \hat{y} &gt; \theta $ 时，那么就判定这个样本属于正类。 虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。 正是因为这个假设，所以认为训练集的观测几率 $ \frac{\hat{\theta}}{1-\hat{\theta}} $ 就代表了真实几率 $ \frac{\theta}{1-\theta} $ 。 所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。 3. 解决不平衡问题的方法3.1 调整$\theta$值根据训练集的正负样本比例，调整 $ \theta $ 值。 这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。 3.2 欠采样对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。 3.3 过采样对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。下面将介绍一种经典的过采样算法：SMOTE。 4. SMOTE算法SMOTE，合成少数类过采样技术。它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题， 即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。 4.1 SMOTE算法流程设训练集的一个少数类的样本数为 $T$ ，那么SMOTE算法将为这个少数类合成 $NT$ 个新样本。这里要求 $N$ 必须是正整数，如果给定的 $N&lt;1$， 那么算法将“认为”少数类的样本数 $T=NT$，并将强制 $N=1$。 考虑该少数类的一个样本 $i$ ，其特征向量为 $x_i, i \in \lbrace 1,…,T \rbrace$： (1) 首先从该少数类的全部 $T$ 个样本中找到样本 $x_i$ 的 $k$ 近邻（例如用欧氏距离），记为 $ x_{i,near}, near \in \lbrace 1,…,k \rbrace $； (2) 然后从这 $k$ 近邻中随机选择一个样本 $x_{i,nn}$，再生成一个 $0$ 到 $1$ 之间的随机数 $\eta_1$，从而合成一个新样本 $x_{i,1} $： x_{i,1} = x_i + \eta_1 \cdot (x_{i,nn} − x_i ) (3) 将步骤(2)重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$x_{i,new}, new \in \lbrace 1,…,N \rbrace $。 那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。 如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。 SMOTE算法所合成出的一个新样本 $x{i,1}$ 相当于是表示样本 $x_i$ 的点和表示样本 $x{i,nn}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。 进一步阅读 解决真实世界问题：如何在不平衡类上使用机器学习？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛顿法&拟牛顿法]]></title>
    <url>%2F2018%2F06%2F03%2Fnewstonmethod%2F</url>
    <content type="text"><![CDATA[目录 1.原始牛顿法 2.阻尼牛顿法 3.拟牛顿条件 4.DFP算法 5.BFGS算法 6.L-BFGS算法 考虑如下无约束的极小化问题min_x f(x)其中 $ x=(x_1, x_2, …, x_n)^T \in R^n $，由于本文不对收敛性进行讨论，因此对目标函数 $ f: R^n \rightarrow R $做一个较苛刻的假设。 这里假定$ f $为凸函数，且二阶连续可微，此外记极小化问题的解为 $ x^{\ast} $ . 1.原始牛顿法牛顿法的基本思想是：在现有极小点估计值的附近对 $ f(x) $ 做二阶泰勒展开，进而找到极小点的下一个估计值。 为简单起见，首先考虑$ n=1 $的情况，设$ x_k $ 为当前的极小点估计值，则 \phi (x) = f(x_k) + f'(x_k)(x-x_k) + \frac{1}{2}f''(x_k) (x-x_k)^2表示$ f(x) $ 在 $ x_k $附近的二阶泰勒展开式（略去了关于$ x-x_k $ 的高阶项）。由于求的是最值，由极值必要条件可知，$ \phi(x) $应该满足 \phi ' (x) = 0 \quad \Rightarrow \quad f'(x_k) + f^{''}(x_k) (x - x_k) = 0从而求得： \[ x = x_k - \frac{f’(x_k)}{f^{‘’}(x_k)} \] 于是，若给定初始值$ x_0 $，则可以构造如下的迭代公式： x_{k+1} = x_k - \frac{f'(x_k)}{f^{''}(x_k)}, \quad k=0,1,...产生序列$ \lbrace x_k \rbrace $来逼近 $ f(x) $的极小点，在一定条件下，$ \lbrace x_k \rbrace $可以收敛到$ f(x) $的极小点。 对于 $ n&gt;1 $的情形，二阶泰勒展开式可以做推广，此时： \phi(X) = f(X_k) + \nabla f(X_k) \cdot (X - X_k) + \frac{1}{2} \cdot (X - X_k)^T \nabla^2 f(X_k) \cdot (X - X_k)其中$ \nabla f $ 为 $ f $的梯度向量，$ \nabla^2 f $为 $ f $的海森矩阵（Hessian matrix），其定义分别为： \nabla f = \left[ \begin{matrix} \frac{\partial f}{\partial x_1} \\\ \frac{\partial f}{\partial x_2} \\\ \cdots \\\ \frac{\partial f}{\partial x_n} \end{matrix} \right] \nabla^2 f = \left[ \begin{matrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x^2_2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\\ & & \ddots & \\\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \\\ \end{matrix} \right]注意，$ \nabla f $ 和 $ \nabla^2 f $ 中的元素均为关于$ x $ 的函数，以下分别将其简记为$g$和$H$，特别地，若$f$的混合偏导数可交换次序，则海森矩阵$H$为对称矩阵。 而$ \nabla f(X_k) $ 和 $ \nabla^2 f(X_k) $则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$ g_k $和$H_k$（这里字母$g$表示$gradient$，$H$表示$Hessian$）。 同样地，由于是求极小点，极值必要条件要求它为$ \phi (X)$的驻点，即 \nabla \phi (X) = 0 \quad \Rightarrow \quad g_k + H_k \cdot (x - x_k) = 0进一步，若矩阵$ H_k $非奇异，则可解得 X = X_k - H_k^{-1} \cdot g_k于是，给定初始值$ X_0 $，则同样可以构造出迭代格式 X_{k+1} = X_k - H_k^{-1} \cdot g_k, \quad k=0,1,2...这就是原始的牛顿迭代法，其迭代格式中的搜索方向$ d_k = - H_k^{-1} \cdot g_k $称为牛顿方向，下面给出牛顿法的完整算法描述。 给定初值$ x_0 $和精度阈值$ \epsilon $，并令$ k=0$； 计算$ g_k $和$ H_k $； 若$ || gk || &lt; \epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \cdot g_k $； 计算新的迭代点 $ x_{k+1} = x_k + d_k $； 令$ k = k+1 $，转至第2步。 原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$ f(x_{k+1} &gt; f(x_k) $的情况， 这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$ \lbrace x_k \rbrace $的发散而导致计算失败。 2.阻尼牛顿法为消除原始牛顿法中的弊端，提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍采用 $ d_k $，但每次迭代需沿此方向做一维搜索(line search)，寻求最优的步长因子$ \lambda_k $，即 \lambda_k = \arg\min_{\lambda \in R} f(x_k + \lambda d_k) \tag{*}下面给出一个阻尼牛顿法的完整算法描述： &gt; 给定初值$ x_0 $和精度阈值$ \epsilon $，并令$ k=0$； 计算$ g_k $和$ H_k $； 若$ || gk || &lt; \epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \cdot g_k $； 利用$(*)$式得到步长$ \lambda_k $，并令 $ x_{k+1} = x_k + \lambda_k d_k $； 令$ k = k+1 $，转至第2步。 小结 牛顿法是梯度(下降)发的进一步发展，梯度法利用目标函数的一阶偏导数信息，以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质； 而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛。 但是牛顿法主要存在以下两个缺点： 对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定； 计算相当复杂，除需计算梯度以外，还需计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$ n $的平方比增加，当$n$很大时这个问题尤为突出。 3.拟牛顿条件上面说到了牛顿法的两个缺点，为了客服这两个问题，人们提出了拟牛顿法，这个方法的基本思想是：不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数。 不同的构造方法就产生了不同的拟牛顿法，都是对牛顿法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。 下面先推到拟牛顿条件，或者叫拟牛顿方程、割线条件，它是用来提供理论指导，指出了用来近似的矩阵应该满足的条件。 为了明确起见，下文中用$ B $ 表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$ H^{-1} $的近似，即 $ B \approx H, D \approx H^{-1} $。 假设经过 $ k+1 $ 次迭代后得到 $ x{k+1} $，此时将目标函数 $ f(x) $ 在 $ x{k+1} $附近作泰勒展开，取二阶近似，得到： f(x) \approx f(x_{k+1} + \nabla f(x_{k+1}) \cdot (x - x_{k+1}) + \frac{1}{2} \cdot (x - x_{k+1}^T \cdot \nabla^2 f(x_{k+1}) \cdot (x - x_{k+1}) \tag{**}在$ (**) $式两边同时作用一个梯度算子 $ \nabla $， 可得 \nabla f(x) \approx \nabla f(x_{k+1}) + H_{k+1} \cdot (x - x_{k+1}) \tag{***}在$ (*) $式中取$ x = x_k$，并整理可得： g_{k+1} - g_k = H_{k+1} \cdot ( x_{k+1} - x_k )引入新的记号： s_k = x_{k+1} - x_k, \quad y_k = g_{k+1} - g_k则上式可以重新表示为： y_k \approx H_{k+1} \cdot s_k \qquad \Longleftrightarrow \qquad s_k \approx H_{k+1}^{-1} \cdot y_k这就是所谓的拟牛顿条件，它对迭代过程中的海森矩阵$ H{k+1}$作约束，因此，对$ H{k+1} $做近似的$B{k+1}$，以及对$ H{k+1}^{-1} $做近似的$ D_{k+1}$ 可以将 y_k = B_{k+1} \cdot s_k \qquad \Longleftrightarrow \qquad s_k = D_{k+1} \cdot y_k作为指导。 4.DFP算法该算法的核心是：通过迭代的方法，对$ H_{k+1}^{-1}$做近似，迭代格式为： D_{k+1} = D_k + \Delta D_k, \quad k=0,1,2...其中$ D_0$通常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$ \Delta D_k$如何构造。 这里采用“待定法”，即首先将$\Delta D_k$待定成某种形式，然后结合拟牛顿条件来推到。这里我们将$ \Delta D_k $ 待定为： \Delta D_k = \alpha u u^T + \beta v v^T其中$ \alpha, \beta$为待定系数，$ u, v \in R^n $为待定向量，从形式上来看，这种待定公式至少保证了矩阵$ \Delta D_k $的对称性（因为$uu^T$和$vv^T$都是对称矩阵） \begin{cases} D_{k+1} = D_k + \Delta D_k \\ \Delta D_k = \alpha u u^T + \beta v v^T \\ s_k = D_{k+1} \cdot y_k \\ \end{cases} \Longleftrightarrow s_k = D_k y_k + \alpha u u^T y_k + \beta v v^T y_k调整上面的结论： \begin{equation} \begin{aligned} s_k & = D_k y_k + u (\alpha u^T y_k) + v(\beta v^T y_k) \\ & = D_k y_k + (\alpha u^T y_k) u + (\beta v^T y_k) v \end{aligned} \end{equation} \tag{+}括号中的$\alpha u^T y_k$和$\beta v^T y_k$是两个数，既然是数，我们不妨作如下简单赋值： \alpha u^T y_k = 1, \qquad \beta v^T y_k = -1 \tag{++}即 \alpha = \frac{1}{u^T y_k}, \qquad \beta = - \frac{1}{v^T y_k}将$ (++) $式代入 $ (+) $，得到： u - v = s_k - D_k y_k要使上式成立，不妨就直接取 u = s_k, \qquad v = D_k y_k再将$(21)$代入$(19)$，便得 \alpha = \frac{1}{s_k^T y_k}, \qquad \beta = - \frac{1}{(D_k y_k)^T y_k} = - \frac{1}{y_k^T D_k y_k}其中第二个等式用到了$D_k$的对称性。 至此，我们已经可以将校正矩阵$ \Delta D_k $构造出来了， \Delta D_k = \frac{s_k s_k^T}{s_k^T y_k} - \frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k}综上，我们给出DFP算法的一个完整算法描述： 给定初值$x_0$和精度阈值$\epsilon$，并令$ D_0=I, \quad k=0$， 确定搜索方向$ d_k = - D_k \cdot g_k $， 利用$(*)$式得到步长$\lambda_k$，令$ s_k = \lambda_k d_k, \quad x_{k+1}=x_k + s_k $， 若$ || g_{k+1} || &lt; \epsilon$，则算法结束， 计算 $ y_k = g_{k+1} - g_k $， 计算$ D_{k+1} = D_k + \Delta D_k = D_k + \frac{s_k s_k^T}{s_k^T y_k} - \frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $， 令$ k = k+1$，转至第2步。 5.BFGS算法与DFP算法相比，BFGS算法性能更佳，目前它已成为求解无约束非线性优化问题最常用的方法之一。 BFGS算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果。 BFGS算法中核心公式的推到和DFP算法完全类似，只是互换了其中$s_k$和$y_k$的位置。 需要注意的是，BFGS算法是直接逼近海森矩阵，即$ B_k \approx H_k$，仍采用迭代方法，设迭代格式为： B_{k+1} = B_k + \Delta B_k, \qquad k=0,1,2...其中的 $ B_0 $也常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$\Delta B_k$如何构造。同样，将其待定为： \Delta B_k = \alpha u u^T + \beta v v^T类比DFP算法，可得 y_k = B_k s_k + (\alpha u^T s_k)u + (\beta v^T s_k) v通过令 $ \alpha u^T s_k = 1,\qquad \beta v^T s_k = -1 $，以及 $ u = y_k, \qquad v = B_k s_k $，可以算得： \alpha = \frac{1}{y_k^T s_k}, \qquad \beta = - \frac{1}{s_k^T B_k s_k}综上，便得到了如下校正矩阵$ \Delta B_k $的公式： \Delta B_k = \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}先讲把矩阵$ \Delta B_k $ 和 $ \Delta D_k $拿出来对比一下，是不是除了将$D$换成$B$外，其他只是将$s_k$和$y_k$互调了一下位置呢？ 最后，我们给出BFGS算法的一个完整算法描述： 给定初值$x_0$和精度阈值$\epsilon$，并令$ B_0=I, \quad k=0$， 确定搜索方向$ d_k = - B_k^{-1} \cdot g_k $， 利用$(*)$式得到步长$\lambda_k$，令$ s_k = \lambda_k d_k, \quad x_{k+1}=x_k + s_k $， 若$ || g_{k+1} || &lt; \epsilon$，则算法结束， 计算 $ y_k = g_{k+1} - g_k $， 计算$ B_{k+1} = B_k + \Delta B_k = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $， 令$ k = k+1$，转至第2步。 上面算法中的第2步通常是通过求解线性代数方程组$ B_k d_k = -g_k $来进行，然而更一般的做法是通过对第6步中的递推关系， 应用$Sherman-Morrison$公式，直接给出$B_{k+1}^{-1}$和$B_k^{-1}$之间的关系： B_{k+1}^{-1} = \left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) B_k^{-1} \left( I - \frac{y_k s_k^T}{y_k^T s_k} \right) + \frac{s_k s_k^T}{y_k^T s_k}利用上式，我们很容易将上述BFGS算法进行改进，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$（这样做仅仅只是符号上看起来舒服起见）。 这样，整个算法中不再需要求解线性代数方程组，由矩阵-向量运算就可以完成了。 改进后的BFGS算法如下： 给定初值$x_0$和精度阈值$\epsilon$，并令$ D_0=I, \quad k=0$， 确定搜索方向$ d_k = - D_k \cdot g_k $， 利用$(*)$式得到步长$\lambda_k$，令$ s_k = \lambda_k d_k, \quad x_{k+1}=x_k + s_k $， 若$ || g_{k+1} || &lt; \epsilon$，则算法结束， 计算 $ y_k = g_{k+1} - g_k $， 计算$ D_{k+1} = \left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) D_k \left( I - \frac{y_k s_k^T}{y_k^T s_k} \right) + \frac{s_k s_k^T}{y_k^T s_k} $， 令$ k = k+1$，转至第2步。 至此，关于DFP算法和BFGS算法的介绍就完成了。 最后再补充一下一维搜索(line search)的问题，在之前几个算法描述中，为简单起见，均采用了$(*)$时来计算步长$ \lambda_k $，其实这是一种精确搜索； 实际应用中，还有像Wolfe型搜索、Armijo搜索以及满足Goldstein条件的非精确搜索。这里我们以Wolfe搜索为例，简单介绍。 设$ \tilde{\beta} \in (0, \frac{1}{2}), \beta \in (\tilde{\beta}, 1)$，所谓的Wolfe搜索是指$ \lambda_k $满足如下Wolfe条件 \begin{cases} f(x_k + \lambda_k d_k) \qquad \leq f(x_k) + \tilde{\beta} \lambda_k d_k^T g_k; \\ d_k^T g(x_k + \lambda_k d_k) \geq \beta d_k^T g_k \end{cases}带非精确搜索的拟牛顿法的研究是从1976年Powell的工作开始的，他证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。 6.L-BFGS算法在BFGS算法中，需要用到一个$NxN$的矩阵$D_k$，当 N 很大时，存储这个矩阵将变得很耗计算机资源，考虑 N 为 10万 的情形，且 用double型（8字节）来存储$D_k$，需要多大的内存呢？来计算一下： \frac{N阶矩阵的字节数}{1GB的字节数} \qquad = \qquad \frac{10^5 \times 10^5 \times 8}{2^{10} \times 2^{10} \times 2^{10}} \qquad = \qquad 74.5(GB)即使考虑到矩阵$D_k$的对称性，内存还可以降一半，但是这对于一般的服务器仍然是难以承受的。况且我们还只是假定了 N=10W 的情况，在实际机器学习问题中，这只能算是中小规模。 L-BFGS(Limited-memory BFGS)算法就是通过对BFGS算法进行改造，从而减少其迭代过程中所需内存开销的算法。 它对BFGS算法进行了近似，其基本思想是：不再存储完整的矩阵$D_k$，而是存储计算过程中的向量序列${s_i}，{y_i}$，需要矩阵$D_k$时，利用向量序列${s_i}，{y_i}$的计算来代替。 而且，向量序列${s_i}，{y_i}$也不是所有的都存，而是固定存最新的$m$个（参数$m$可由用户根据机器的内存自行指定），每次计算$D_k$时，只利用最新的$m$个${s_i}$和$m$个${y_i}$， 显然，这样我们将存储由原来的$O(n^2)$降到了$O(mN)$。 接下来，讨论L-BFGS算法的具体实现过程，我们的出发点是改进的BFGS算法中的第6步中的迭代式： D_{k+1} = \left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) D_k \left( I - \frac{y_k s_k^T}{y_k^T s_k} \right) + \frac{s_k s_k^T}{y_k^T s_k}若记$ \rho_k = \frac{1}{y_k^T s_k}，V_k = I - \rho_k y_k s_k^T $，则上式可写成 D_{k+1} = V_k^T D_k V_k + \rho_k s_k s_k^T \tag{6.*}如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如$D_0 = I$），则可利用$(6.*)$式，依次可得： \begin{equation} \begin{aligned} D_1 &= V_0^T D_0 V_0 + \rho_0 s_0 s_0^T; \\ D_2 &= V_1^T D_1 V_1 + \rho_1 s_1 s_1^T \\ &= v_1^T (V_0^T D_0 V_0 + \rho_0 s_0 s_0^T) V_1 + \rho_1 s_1 s_1^T \\ &= V_1^T V_0^T D_0 V_0 V_1 + V_1^T \rho_0 s_0 s_0^T V_1 + \rho_1 s_1 s_1^T \\ D_3 &= V_2^T D_2 V_2 + \rho_2 s_2 s_2^T \\ &= V_2^T (V_1^T V_0^T D_0 V_0 V_1 + V_1^T \rho_0 s_0 s_0^T V_1 + \rho_1 s_1 s_1^T) V_2 + \rho_2 s_2 s_2^T \\ &= v_2^T V_1^T V_0^T D_0 V_0 V_1 V_2 + V_2^T V_1^T \rho_0 s_0 s_0^T V_1 V_2 + V_2^T \rho_1 s_1 s_1^T V_2 + \rho_2 s_2 s_2^T \\ \cdots \end{aligned} \end{equation}一般地，我们有： \begin{equation} \begin{aligned} D_{k+1} = &\ (V_k^T V_{k-1}^T \cdots V_1^T V_0^T) D_0 (V_0 V_1 \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_2^T V_1^T) (\rho_0 s_0 s_0^T) (V_1 V_2 \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_3^T V_2^T) (\rho_1 s_1 s_1^T) (V_2 V_3 \cdots V_{k-1} V_k) \\ &+ \cdots \\ &+ (V_k^T V_{k-1}^T) (\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\ &+ V_k^T (\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\ &+ \rho_k s_k s_k^T \end{aligned} \end{equation} \tag{6.**}由上式可以看到，计算$D_{k+1}$需要用到$ \lbrace s_i, y_i \rbrace_{i=0}^k $，因此，若从$ s_0, y_0 $开始连续存储 $ m $ 组的话，只能存储到$ s_{m-1}, y_{m-1} $， 亦即只能依次计算 $ D_1, D_2, \cdots $，直到$ D_m $，那$ D_{m+1}, D_{m+2} $该如何计算呢？ 自然地，如果一定要丢弃一些向量，那么肯定优先考虑那些最早生成的向量，具体来说，计算$D_{m+1}$时，我们保存$ \lbrace s_i, y_i \rbrace_{i=1}^m $，丢掉$ \lbrace s_0, y_0 \rbrace;$ 计算$ D_{m+2} $时，我们保存$ \lbrace s_i, y_i\rbrace_{i=2}^{m+1} $，丢掉了$ \lbrace s_i, y_i \rbrace_{i=0}^1; \cdots $ 但是舍弃掉一些向量后，就只能近似计算了，当$k+1 &gt; m$时，仿照$(6.**)$式，可以构造近似计算公式： \begin{equation} \begin{aligned} D_{k+1} = &\ (V_k^T V_{k-1}^T \cdots V_{k-m+2}^T V_{k-m+1}^T) D_0 (V_{k-m+1} V_{k-m+2} \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_{k-m+3}^T V_{k-m+2}^T) (\rho_0 s_0 s_0^T) (V_{k-m+2} V_{k-m+3} \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_{k-m+4}^T V_{k-m+3}^T) (\rho_1 s_1 s_1^T) (V_{k-m+3} V_{k-m+4} \cdots V_{k-1} V_k) \\ &+ \cdots \\ &+ (V_k^T V_{k-1}^T) (\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\ &+ V_k^T (\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\ &+ \rho_k s_k s_k^T \end{aligned} \end{equation} \tag{6.***}$ (6.**) $ 和 $ (6.***) $ 被称为$ special BFGS matrices $，若引入 $ \hat{m} = min\lbrace k, m-1 \rbrace $，则还可以将两式合并成 \begin{equation} \begin{aligned} D_{k+1} = &\ (V_k^T V_{k-1}^T \cdots V_{k-\hat{m}+1}^T V_{k-\hat{m}}^T) D_0 (V_{k-\hat{m}} V_{k-\hat{m}+1} \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_{k-\hat{m}+2}^T V_{k-\hat{m+1}}^T) (\rho_0 s_0 s_0^T) (V_{k-\hat{m}+1} V_{k-\hat{m}+2} \cdots V_{k-1} V_k) \\ &+ (V_k^T V_{k-1}^T \cdots V_{k-\hat{m}+3}^T V_{k-\hat{m+2}}^T) (\rho_1 s_1 s_1^T) (V_{k-\hat{m}+2} V_{k-\hat{m}+3} \cdots V_{k-1} V_k) \\ &+ \cdots \\ &+ (V_k^T V_{k-1}^T) (\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\ &+ V_k^T (\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\ &+ \rho_{k} s_{k} s_{k}^T \end{aligned} \end{equation}事实上，由BFGS算法流程易知，$D_k$的作用仅用来计算$ D_k g_k $ 获取搜索方向，因此，若能利用上式设计出一种计算$ D_k g_k $的快速算法，则大功告成。 ($D_k \cdot g_k$的快速算法) Step 1 初始化 \delta = \begin{cases} 0, \qquad k \leq m \\ k-m, \qquad k > m \end{cases}; \qquad L = \begin{cases} k, \qquad k \leq m \\ m, \qquad k > m \end{cases}; \qquad q_L = g_kStep 2 后向循环 FOR \quad i=L-1, L-2, \cdots, 1, 0 \quad DO \\ \{ \\ \qquad j=i+\delta; \\ \qquad \alpha_i = \rho_j s_j^T q_{i+1}; // \alpha_i 需要保存下来，前向循环要用！！ \\ \qquad q_i = q_{i+1} - \alpha_i y_j \\ \}Step 3 前向循环 r_0 = D_0 \cdot q_0; \\ FOR \quad i=0,1,\cdots,L-2,L-1 \quad DO \\ \{ \\ \qquad j = i+\delta; \\ \qquad \beta_j = \rho_j y_j^T r_i; \\ \qquad r_{i+1} = r_i + (\alpha_i - \beta_i) s_j \\ \} 最后算出的$r_L$即为$H_k \cdot g_k $的值。 &gt; 牛顿法与拟牛顿法学习笔记]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播 BP & BPTT]]></title>
    <url>%2F2018%2F05%2F23%2Fbackpropagation%2F</url>
    <content type="text"><![CDATA[目录 反向传播BP 随时间反向传播BPTT 1. 反向传播BP[Calculus on Computational Graphs: Backpropagation，英文原版]、 [详解反向传播算法，中文翻译理解]解释了为什么从上而下计算梯度。一言以蔽之：从下而上会有重复计算，当参数数量太多，时间耗费太严重；从上而下每个节点只计算一次。 如何直观地解释 back propagation 算法？中的例子比较清晰的刻画了反向传播的优势。 A Neural Network in 11 lines of Python每行代码都有解释。 CHAPTER 2How the backpropagation algorithm works讲的特别详细， 中文版在这里 - 神经⽹络与深度学习。 以下部分参考Neural Networks and Deep Learning(神经⽹络与深度学习P37 - P42) 反向传播的四个方程式： \[ \delta^L = \nabla _a C \odot \sigma’ (z^L) \tag{BP1} \] \[ \delta^l = ( (w^{l+1})^T \delta^{l+1} ) \odot \sigma’ (z^l) \tag{BP2} \] \[ \frac{\partial C}{\partial b_j^l} = \delta_j^l \tag{BP3} \] \[ \frac{\partial C}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l \tag{BP4} \] 证明上面的四个方程式： 证明BP1 \[ \begin{equation} \begin{aligned} \delta_j^L &amp; = \frac{\partial C}{\partial z_j^L} \\ &amp; = \frac{\partial C}{\partial a_j^L} \cdot \frac{\partial a_j^L}{\partial z_j^L} \\ &amp; = \frac{\partial C}{\partial a_j^L} \cdot \sigma’ (z_j^L) \quad (\because a_j^L = \sigma (z_j^L)) \end{aligned} \end{equation} \] 证明BP2 \begin{equation*} \begin{aligned} \because z_k^{l+1} &= \sum_j w_{kj}^{l+1} a_j^l + b_k^{l+1} = \sum_j w_{kj}^{l+1} \cdot \sigma(z_j^l) + b_k^{l+1} \\ \therefore \frac{\partial z_k^{l+1}}{\partial z_j^l} & = w_{kj}^{l+1} \cdot \sigma' (z_j^l) \end{aligned} \end{equation*}\[ \begin{equation} \begin{aligned} \delta_j^l &amp; = \frac{\partial C}{\partial z_j^l} \\ &amp; = \sum_k \frac{\partial C}{\partial z_k^{l+1}} \cdot \frac{\partial z_k^{l+1}}{\partial z_j^l} \\ &amp; = \sum_k \frac{\partial z_k^{l+1}}{\partial z_k^l} \cdot \delta_k^{l+1} \\ &amp; = \sum_k w_{kj}^{l+1} \cdot \sigma’ (z_j^l) \cdot \delta_k^{l+1} \\ &amp; = \sum_k w_{kj}^{l+1} \cdot \delta_k^{l+1} \cdot \sigma’ (z_j^l) \end{aligned} \end{equation} \] 证明BP3 \[ \begin{equation} \begin{aligned} \frac{\partial C}{\partial b_j^l} &amp; = \frac{\partial C}{\partial z_j^l} \cdot \frac{\partial z_j^l}{\partial b_j^l} \\ &amp; = \frac{\partial C}{\partial z_j^l} \quad (\because z_j^l = \sum_k w_{jk}^l \cdot a_k^{l-1} + b_j^l \therefore \frac{\partial z_j^l}{\partial b_j^l} = 1) \\ &amp; = \delta_j^l \end{aligned} \end{equation} \] 证明BP4 \[ \begin{equation} \begin{aligned} \frac{\partial C}{\partial w_{jk}^l} &amp; = \frac{\partial C}{\partial z_j^l} \cdot \frac{\partial z_j^l}{\partial w_{jk}^l} \\ &amp; = \delta_j^l \cdot a_k^{l-1} \quad (\because z_j^l = \sum_k w_{jk}^l \cdot a_k^{l-1} + b_j^l) \end{aligned} \end{equation} \] 以下摘抄自：反向传导算法 反向传播算法的思路如下：给定一个样例 \( (x,y) \)，首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \( h_{W,b}(x) \) 的输出值。 之后，针对第 \( l \) 层的每一个节点 \( i \)，计算出其 “残差” \( \delta^{(l)}_i \)，该残差表明了该节点对最终输出值的残差产生了多少影响。 对于最终的输出节点，可以直接算出网络产生的激活值与实际值之间的差距，将这个差距定义为 \( \delta^{(n_l)}_i \)（第 \( n_l \) 层表示输出层）。 对于隐藏单元如何处理呢？基于节点（第 \( l+1 \) 层节点）残差的加权平均值计算 \( \delta^{(l)}_i \)，这些节点以 \( a^{(l)}_i \) 作为输入。 下面将给出反向传导算法的细节: (1) 进行前馈传导计算，利用前向传导公式，得到 \( L_2, L_3, \ldots \) 直到输出层 \( L_{n_l} \) 的激活值。 (2) 对于第 \( n_l \) 层（输出层）的每个输出单元 \( i \) ，根据以下公式计算残差： \[ \delta^{(n_l)}_i = \frac{\partial}{\partial z^{(n_l)}_i} \frac{1}{2} || y - h_{W,b}(x) || ^2 = - (y_i - a^{(n_l)}_i) \cdot f’(z^{(n_l)}_i) \] 推导过程 \[ \begin{equation} \begin{aligned} \delta^{(n_l)}_i &amp; = \frac{\partial}{\partial z^{(n_l)}_i} J(W,b; x,y) \\ &amp; = \frac{\partial}{\partial z^{(n_l)}_i} \frac{1}{2} ||y - h_{W,b}(x) ||^2 \\ &amp; = \frac{\partial}{\partial z^{(n_l)}_i} \frac{1}{2} \sum_{j=1}^{S_{n_l}} (y_j - a_j^{(n_l)})^2 \\ &amp; = \frac{\partial}{\partial z^{(n_l)}_i} \frac{1}{2} \sum_{j=1}^{S_{n_l}} (y_j - f(z_j^{(n_l)}))^2 \\ &amp; = - (y_i - f(z_i^{(n_l)})) \cdot f’(z^{(n_l)}_i) \\ &amp; = - (y_i - a^{(n_l)}_i) \cdot f’(z^{(n_l)}_i) \end{aligned} \end{equation} \] (3) 对 $ l = n_l - 1, n_l-2, n_l-3, \ldots, 2 $ 的各个层，第 $ l $ 层的第 $i$ 个节点的残差计算方法如下： \[ \delta^{(l)}_i = \left( \sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \delta^{(l+1)}_j \right) f’(z^{(l)}_i) \] 推导过程 \begin{equation*} \begin{aligned} \delta^{(n_l-1)}_i & = \frac{\partial}{\partial z^{(n_l-1)}_i} J(W,b; x,y) \\ & = \frac{\partial}{\partial z^{(n_l-1)}_i} \frac{1}{2} ||y - h_{W,b}(x) ||^2 \\ & = \frac{\partial}{\partial z^{(n_l-1)}_i} \frac{1}{2} \sum_{j=1}^{S_{n_l}}(y_j - a_j^{(n_l)})^2 \\ & = \frac{1}{2} \sum_{j=1}^{S_{(n_l)}}\frac{\partial}{\partial z^{n_l-1}_i}(y_j - a_j^{(n_l)})^2 \\ & = \frac{1}{2} \sum_{j=1}^{S_{(n_l)}}\frac{\partial}{\partial z^{n_l-1}_i}(y_j - f(z_j^{(n_l)}))^2 \\ & = \sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \cdot \frac{\partial}{\partial z_i^{(n_l-1)}}f(z_j^{(n_l)}) \\ & = \sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \cdot f'(z_j^{(n_l)}) \cdot \frac{\partial z_j^{(n_l)}}{\partial z_i^{(n_l-1)}} \\ & = \sum_{j=1}^{S_{n_l}} \delta_j^{(n_l)} \cdot \frac{\partial z_j^{(n_l)}}{\partial z_i^{(n_l-1)}} \\ & = \sum_{j=1}^{S_{n_l}} \left(\delta_j^{(n_l)} \cdot \frac{\partial}{\partial z_i^{(n_l-1)}}\sum_{k=1}^{S_{n_l-1}}f(z_k^{(n_l-1)}) \cdot W_{jk}^{n_l-1} \right) \\ & = \sum_{j=1}^{S_{n_l}} \delta_j^{(n_l)} \cdot W_{ji}^{n_l-1} \cdot f'(z_i^{(n_l-1)}) \\ & = \left( \sum_{j=1}^{S_{n_l}} W_{ji}^{n_l-1} \delta_j^{(n_l)} \right) f'(z_i^{(n_l-1)}) \\ \end{aligned} \end{equation*}将上式中的 \( n_l-1 \) 与 \( n_l \) 的关系替换为 \( l \) 与 \( l+1 \) 的关系，就可以得到： \delta^{(l)}_i = \left( \sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \delta^{(l+1)}_j \right) f'(z^{(l)}_i)以上逐次从后向前求导的过程即为“反向传导”的本意所在。 (4) 计算需要的偏导数，计算方法如下： \[ \begin{aligned} \frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y) &amp;= a^{(l)}_j \delta_i^{(l+1)} \\ \frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x, y) &amp;= \delta_i^{(l+1)}. \end{aligned} \] 最后，用矩阵-向量表示法重写以上算法。我们使用 “ \( \bullet \)” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。 若 \( a = b \bullet c \)，则 \( a_i = b_i c_i \)。扩展 \( f(\cdot) \) 的定义，使其包含向量运算，于是有 \( f’([z_1, z_2, z_3]) = [f’(z_1), f’(z_2), f’(z_3)] \)。 那么，反向传播算法可表示为以下几个步骤： 进行前馈传导计算，利用前向传导公式，得到 \( L_2, L_3, \ldots \) 直到输出层 \( L_{n_l} \) 的激活值。 对输出层（第 \( n_l \) 层），计算:\[\begin{align} \delta^{(n_l)} = - (y - a^{(n_l)}) \bullet f’(z^{(n_l)}) \end{align}\] 对于 \( l = n_l-1, n_l-2, n_l-3, \ldots, 2 \) 的各层，计算：\[\begin{align} \delta^{(l)} = \left((W^{(l)})^T \delta^{(l+1)}\right) \bullet f’(z^{(l)}) \end{align} \] 计算最终需要的偏导数值： \begin{equation*} \begin{aligned} \nabla_{W^{(l)}} J(W,b;x,y) &= \delta^{(l+1)} (a^{(l)})^T, \\ \nabla_{b^{(l)}} J(W,b;x,y) &= \delta^{(l+1)}. \end{aligned} \end{equation*} 实现中应注意： 在以上的第2步和第3步中，我们需要为每一个 \( i \) 值计算其 \( f’(z^{(l)}_i) \) 。 假设 \( f(z) \) 是sigmoid函数，并且我们已经在前向传导运算中得到了 \( a^{(l)}_i \) 。 那么，使用我们早先推导出的 \( f’(z) \) 表达式，就可以计算得到 \( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i) \)。 最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，\( \Delta W^{(l)} \) 是一个与矩阵 \( W^{(l)} \) 维度相同的矩阵， \( \Delta b^{(l)} \) 是一个与 \( b^{(l)} \) 维度相同的向量。 注意这里 “\( \Delta W^{(l)} \) ”是一个矩阵，而不是“\( \Delta \) 与 \( W^{(l)} \) 相乘”。 下面，我们实现批量梯度下降法中的一次迭代： (1) 对于所有 \( l \)，令 \( \Delta W^{(l)} := 0, \Delta b^{(l)} := 0 \) (设置为全零矩阵或全零向量) (2) 对于 \( i = 1 \) 到 \( m \)， (2.1) 使用反向传播算法计算 \( \nabla_{W^{(l)}} J(W,b;x,y) \) 和 \( \nabla_{b^{(l)}} J(W,b;x,y) \); (2.2) 计算 \( \Delta W^{(l)} := \Delta W^{(l)} + \nabla_{W^{(l)}} J(W,b;x,y) \); (2.3) 计算 \( \Delta b^{(l)} := \Delta b^{(l)} + \nabla_{b^{(l)}} J(W,b;x,y) \); (3) 更新权重参数： \[ \begin{align} W^{(l)} &amp;= W^{(l)} - \alpha \left[ \left(\frac{1}{m} \Delta W^{(l)} \right) + \lambda W^{(l)}\right] \\ b^{(l)} &amp;= b^{(l)} - \alpha \left[\frac{1}{m} \Delta b^{(l)}\right] \end{align} \] 现在，可以重复梯度下降法的迭代步骤来减小代价函数 \( J(W,b) \) 的值，进而求解我们的神经网络。 2. [转]随时间反向传播BPTT转自：hschen0712/machine-learning-notes RNN（递归神经网络，Recurrent Neural Network）是一种具有长时记忆能力的神经网络模型，被广泛应用于序列标注（Sequence Labeling）问题。 在序列标注问题中，模型的输入是一段时间序列，记为$ x = \lbrace x_1, x_2, …, x_T \rbrace $， 我们的目标是为输入序列的每个元素打上标签集合中的对应标签，记为$ y = \lbrace y_1, y_2, …, y_T \rbrace $。 NLP中的大部分任务（比如分词、实体识别、词性标注）都可以最终归结为序列标注问题。 这类问题中，输入是语料库中一段由 $T$ 个词（或字）构成的文本 $ x = \lbrace x_1, x_2, …, x_T \rbrace $（其中$x_t$表示文本中的第$t$个词）； 输出是每个词对应的标签，根据任务的不同标签的形式也各不相同，但本质上都是针对每个词根据它的上下文进行标签的分类。 一个典型的RNN的结构如下图所示： 从图中可以看到，一个RNN通常由三层组成，分别是输入层、隐藏层和输出层。 与一般的神经网络不同的地方是，RNN的隐藏层存在一条有向反馈边，正是这种反馈机制赋予了RNN记忆能力。 要理解左边的图可能有点难度，我们可以将其展开为右边这种更直观的形式，其中RNN的每个神经元接受当前时刻的输入$x_t$ 以及上一时刻隐单元的输出$h_{t-1}$， 计算出当前神经元的输入 $s_t$，经过激活函数变换得到输出 $h_t$，并传递给下一时刻的隐单元。 此外，我们还需要注意到RNN中每个时刻上的神经元的参数都是相同的（类似CNN的权值共享），这么做一方面是减小参数空间，保证泛化能力； 另一方面是为了赋予RNN记忆能力，将有用信息存储在$W_{in},W_{rec},W_{out}$三个矩阵中。 由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化，这要求我们提出新的优化算法。 RNN中最常用的优化算法是随时间反向传播（BackPropagation Through Time，BPTT），下文将叙述BPTT算法的数学推导。 2.1 符号注解 符号 注解 $K$ 词汇表的大小 $T$ 句子的长度 $H$ 隐藏层单元数 $x=\lbrace x_1, x_2, ..., x_T \rbrace $ 句子的单词序列 $x_t \in R^{K \times 1}$ 第$t$时刻RNN的输入，one-hot vector $\hat{y}_t \in R^{K \times 1} $ 第$t$时刻softmax层的输出，估计每个词出现的概率 $y_t \in R^{K \times 1} $ 第$t$时刻的label，为每个词出现的概率，one-hot vector $E_t$ 第$t$时刻（第$t$个word）的损失函数，定义为交叉熵误差$E_t = - y_t^T log (\hat{y}_t) $ $E$ 一个句子的损失函数，由各个时刻（即每个word）的损失函数组成,$E=\sum_t^T E_t$（注:由于要推导的是SGD算法，更新梯度是相对于一个训练样例而言的，因此一次只考虑一个句子的误差，而不是整个训练集的误差（对应BGD算法）） $s_t \in R^{H \times 1}$ 第$t$个时刻RNN隐藏层的输入 $h_t \in R^{H \times 1}$ 第$t$个时刻RNN隐藏层的输出 $z_t \in R^{K \times 1}$ 输出层的汇集输入 $\delta^{(t)}_k=\frac{\partial E_t}{\partial s_k}$ 第$t$个时刻损失函数$E_t$对第$k$时刻带权输入$s_k$的导数 $r_t=\hat{y}_t-y_t$ 残差向量 $W_{in}\in\mathbb{R}^{H\times K}$ 从输入层到隐藏层的权值 $W_{rec}\in\mathbb{R}^{H\times H}$ 隐藏层上一个时刻到当前时刻的权值 $W_{out}\in\mathbb{R}^{K\times H}$ 隐藏层到输出层的权值 上述符号之间的关系: \begin{cases} s_t &= W_{rec} h_{t-1} + W_{in} x_t \\ h_t &= tanh(s_t) \\ z_t &= W_{out} h_t \\ \hat{y}_t &= softmax( z_t ) \\ E_t &= - y_t^T log( \hat{y}_t) \\ E &= \sum_t^T E_t \end{cases}这里有必要对上面的一些符号进行进一步解释。 本文只讨论输入为one-hot vector的情况，这种向量的特点是茫茫0海中的一个1，即只用一个1表示某个单词的出现；其余的0均表示单词不出现。 RNN要预测的输出是一个one-hot vector，表示下一个时刻各个单词出现的概率。 由于$y_t$是one-hot vector，不妨假设$y_{t,j} = 1( y_{t,i} =0 ,i \neq j)$，那么当前时刻的交叉熵为 E_t = - y_t^T log(\hat{y}_t) = -log(\hat{y}_{t,j})。也就是说如果 $t$ 出现的是第 $j$ 个词，那么计算交叉熵时候只要看 $\hat{y}_t$ 的第$j$个分量即可。 由于$x_t$是one-hot向量，假设第$j$个词出现，则$W_{in}x_t$相当于把$W_{in}$的第$j$列选出来，因此这一步是不用进行任何矩阵运算的，直接做下标操作即可。 BPTT与BP类似，是在时间上反传的梯度下降算法。 RNN中，我们的目的是求得 $\frac{\partial E}{\partial W_{in}}, \frac{\partial E}{\partial W_{rec}}, \frac{\partial E}{\partial W_{out}}$， 根据这三个变化率来优化三个参数 $W_{in},W_{rec},W_{out}$。注意到 $\frac{\partial E}{\partial W_{in}} = \sum_t \frac{\partial E_t}{\partial W_{in}}$， 因此我们只要对每个时刻的损失函数求偏导数再加起来即可。矩阵求导有两种布局方式：分母布局（Denominator Layout）和分子布局（Numerator Layout），关于分子布局和分母布局的区别，请参考文献3。 如果这里采用分子布局，那么更新梯度时还需要将梯度矩阵进行一次转置，因此出于数学上的方便，后续的矩阵求导都将采用分母布局。 2.2 计算 $\frac{\partial E_t}{\partial W_{out}}$注意到$E_t$是$W_{out}$的复合函数，参考文献3中Scalar-by-matrix identities一节中关于复合矩阵函数求导法则（右边的是分母布局）： 我们有： \begin{aligned} \frac{\partial E_t}{\partial W_{out}(i,j)} &= tr \left( \left( \frac{\partial E_t}{\partial z_t} \right)^T \cdot \frac{\partial z_t}{\partial W_{out}(i,j)} \right) \\ &= tr \left( (\hat{y}_t - y_t)^T \cdot \begin{bmatrix} 0\\ \vdots \\ \frac{\partial z_{t}^{(i)}}{\partial W_{out}(i,j)} \\ \vdots \\ 0 \end{bmatrix} \right) \\ &= r_t^{(i)} h_t^{(j)} \end{aligned}其中$r_t^{(i)}=(\hat{y}_t-y_t)^{(i)}$表示残差向量第$i$个分量，$h_t^{(j)}$表示$h_t$的第j个分量。 上述结果可以改写为： \frac{\partial E_t}{\partial W_{out}} = r_t \otimes h_t\frac{\partial E}{\partial W_{out}} = \sum_{t=0}^T r_t\otimes h_t其中$\otimes$表示向量外积。 2.3 计算$\frac{\partial E_t}{\partial W_{rec}}$由于$W_{rec}$是各个时刻共享的，所以$t$时刻之前的每个时刻$W_{rec}$的变化都对$E_t$有贡献，反过来求偏导时，也要考虑之前每个时刻$W_{rec}$对$E$的影响。 我们以$s_k$为中间变量，应用链式法则： \frac{\partial E_t}{\partial W_{rec}} = \sum_{k=0}^t \frac{\partial s_k}{\partial W_{rec}} \frac{\partial E_t}{\partial s_k}但由于$\frac{\partial s_k}{\partial W_{rec}}$（分子向量，分母矩阵）以目前的数学发展水平是没办法求的， 因此我们要求这个偏导，可以拆解为$E_t$对$W_{rec}(i,j)$的偏导数： \frac{\partial E_t}{\partial W_{rec}(i,j)} = \sum_{k=0}^t tr[(\frac{\partial E_t}{\partial s_k})^T \frac{\partial s_k}{\partial W_{rec}(i,j)}] = \sum_{k=0}^t tr[(\delta_k^{(t)})^T\frac{\partial s_k}{\partial W_{rec}(i,j)}]其中，$\delta^{(t)}_k=\frac{\partial E_t}{\partial s_k}$，遵循 s_k\to h_k\to s_{k+1}\to ...\to E_t的传递关系。 应用链式法则有： \delta^{(t)}_k = \frac{\partial h_k}{\partial s_k}\frac{\partial s_{k+1}}{\partial h_k} \frac{\partial E_t}{\partial s_{k+1}} = diag(1-h_k\odot h_k)W_{rec}^T\delta^{(t)}_{k+1}=(W_{rec}^T\delta^{(t)}_{k+1})\odot (1-h_k\odot h_k)其中，$\odot$表示向量点乘（element-wise product）。注意$E_t$求导时链式法则的顺序，$E_t$是关于$s_k$的符合函数，且求导链上的每个变量都是向量， 根据参考文献3，这种情况下应用分母布局的链式法则，方向应该是相反的。 接下来计算$\delta^{(t)}_t$： \delta^{(t)}_t = \frac{\partial E_t}{\partial s_t} = \frac{\partial h_t}{\partial s_t}\frac{\partial z_t}{\partial h_t}\frac{\partial E_t}{\partial z_t} = diag(1-h_t\odot h_t)\cdot W_{out}^T\cdot(\hat{y}_t-y_t)=(W_{out}^T(\hat{y}_t-y_t))\odot (1-h_t\odot h_t)于是，我们得到了关于$\delta$ 的递推关系式: \begin{cases} \delta^{(t)}_t &= (W_{out}^T r_t)\odot (1-h_t\odot h_t) \\ \delta^{(t)}_k &= (W_{rec}^T\delta^{(t)}_{k+1})\odot (1-h_k\odot h_k) \end{cases}由 $ \delta^{(t)}_t $ 出发， 我们可以往前推出每一个 $ \delta $ ， 将 \delta^{(t)}_0,...,\delta^{(t)}_t 代入 \frac{\partial E_t}{\partial W_{rec}(i,j)} 有： \frac{\partial E_t}{\partial W_{rec}(i,j)} = \sum_{k=0}^t \delta_k^{(t)} h_{k-1}^{(j)}将上式写成矩阵形式： \frac{\partial E_t}{\partial W_{rec}} = \sum_{k=0}^t \delta^{(t)}_k \otimes h_{k-1} \\ \frac{\partial E}{\partial W_{rec}} =\sum_{t=0}^T \sum_{k=0}^t \delta^{(t)}_k \otimes h_{k-1}不失严谨性，定义$h_{-1}$为全0的向量。 2.4 计算$\frac{\partial E_t}{\partial W_{in}}$按照上述思路，我们可以得到 \frac{\partial E_t}{\partial W_{in}} = \sum_{k=0}^t \delta_k \otimes x_{k}由于$x_k$是个one-hot vector，假设$x_k(m)=1$，那么我们在更新$W$时只需要更新$W$的第$m$列即可。 2.5 参数更新我们有了$E_t$关于各个参数的偏导数，就可以用梯度下降来更新各参数了: \begin{cases} W_{in} &= W_{in} - \lambda \sum_{t=0}^T \sum_{k=0}^t \delta_k \otimes x_{k} \\ W_{rec}&=W_{rec}-\lambda \sum_{t=0}^T \sum_{k=0}^t \delta_k \otimes h_{k-1} \\ W_{out}&=W_{out}-\lambda \sum_{t=0}^T r_t \otimes h_t \end{cases}其中 r_t= \hat{y}_t - y_t， \delta_t = \frac{\partial E_t}{\partial s_t} = (W_{out}^T r_t) \odot (1 - h_t\odot h_t), \lambda > 0 表示学习率。 2.6 部分思考 为什么RNN中要对隐藏层的输出进行一次运算$z_t=W_{out}h_t$，然后再对$z_t$进行一次softmax，而不是直接对$h_t$进行softmax求得概率？为什么要有$W_{out}$这个参数？ 答：$x_t$是一个$K\times 1$的向量，我们要将它映射到一个$H\times 1$的$h_t$（其中$H$是隐神经元的个数），从$x_t$到$h_t$相当于对词向量做了一次编码；最终我们要得到的是一个$K\times 1$的向量（这里$K$是词汇表大小），表示每个词接下来出现的概率，所以我们需要一个矩阵$K\times H$的$W_{out}$来将$h_t$映射回原来的空间去，这个过程相当于解码。因此，RNN可以理解为一种编解码网络。 $W_{in},W_{rec},W_{out}$三个参数分别有什么意义？ 答： $W_{in}$将$K\times 1$的one-hot词向量映射到$H\times 1$隐藏层空间，将输入转化为计算机内部可理解可处理的形式，这个过程可以理解为一次编码过程；$W_{rec}$则是隐含层到自身的一个映射，它定义了模型如何结合上文信息，在编码中融入了之前的“记忆”；$W_{in},W_{rec}$结合了当前输入单词和之前的记忆，形成了当前时刻的知识状态。$W_{out}$是隐含层到输出的映射，$z=W_{out}h$是映射后的分数，这个过程相当于一次解码。这个解码后的分数再经过一层softmax转化为概率输出来，我们挑选概率最高的那个作为我们的预测。作为总结， RNN的记忆由两部分构成，一部分是当前的输入，另一部分是之前的记忆。 BPTT和BP的区别在哪？为什么不能用BP算法训练RNN？ 答：BP算法只考虑了误差的导数在上下层级之间梯度的反向传播；而BPTT则同时包含了梯度在纵向层级间的反向传播和在时间维度上的反向传播，同时在两个方向上进行参数优化。 文中词$x_t$的特征是一个one-hot vector，这里能不能替换为word2vec训练出的词向量？效果和性能如何？ 答：RNNLM本身自带了训练词向量的过程。由于$x_t$是one-hot向量，假设出现的词的索引为$j$，那么$W_{in}x_t$就是把$W_{in}$的第$j$列$W[:,j]$取出，这个列向量就相当于该词的词向量。实际上用语言模型训练词向量的思想最早可以追溯到03年Bengio的一篇论文《A neural probabilistic language model 》，这篇论文中作者使用一个神经网络模型来训练n-gram模型，顺便学到了词向量。本文出于数学推导以及代码实现上的方便采用了one-hot向量作为输入。实际工程中，词汇表通常都是几百万，内存没办法装下几百万维的稠密矩阵，所以工程上基本上没有用one-hot的，基本都是用词向量。 &gt; 使用RNN解决NLP中序列标注问题的通用优化思路 wildml的rnn tutorial part3 Matrix Calculus Wiki 《神经网络与深度学习讲义》 邱锡鹏]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降]]></title>
    <url>%2F2018%2F05%2F20%2Fgradientdescent%2F</url>
    <content type="text"><![CDATA[目录 1 批量梯度下降BGD 2 随机梯度下降SGD 3 小批量梯度下降MBGD 在应用机器学习算法时，通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。下面以线性回归算法来对三种梯度下降法进行比较。 一般线性回归函数的假设函数为： \[ h_{\theta} = \sum_{j=1}^n \theta_j x_j \] 对应的损失函数形式为： \[ J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)})^2 \] 其中的\( \frac{1}{2} \)是为了计算方便，上标表示第 i 个样本，下标 j 表示第 j 维度。 下图为一个二维参数\( \theta_0 \) 和 \( \theta_1 \) 组对应损失函数的可视化图： 1. 批量梯度下降BGD批量梯度下降法(Batch Gradient Descent，BGD)是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。 具体的操作流程如下： 1 随机初始化\( \theta \)； 2 更新 \( \theta \)使得损失函数减小，直到满足要求时停止； \[ \theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) \] 这里\( \alpha \)表示学习率。 \[ \begin{equation} \begin{aligned} \frac{\partial}{\partial \theta_j} J(\theta) &amp; = \frac{\partial}{\partial \theta_j} \frac{1}{2} (h_{\theta}(x) - y )^2 \\ &amp; = 2 \cdot \frac{1}{2} ( h_{\theta}(x) - y ) \cdot \frac{\partial}{\partial \theta_j} ( h_{\theta}(x) - y ) \\ &amp; = (h_{\theta}(x) - y ) \cdot \frac{\partial}{\partial \theta_j} \left( \sum_{i=0}^n \theta_i x_i - y \right) \\ &amp; = (h_{\theta}(x) - y) x_j \end{aligned} \end{equation} \] 则对所有数据点，上述损失函数的偏导(累加和)为： \[ \frac{\partial J(\theta)}{\partial \theta_j} = - \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \] 在最小化损失函数的过程中，需要不断反复的更新\( \theta \)使得误差函数减小，更新方式如下： \[ \theta_j^{‘} = \theta_j + \frac{1}{m} \sum_{i=1}^m ( y^{(i)} - h_{\theta} (x^{(i)}) x_j^{(i)} \] BGD得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目 m 很大，那么这种方法的迭代速度很慢！ 所以，这就引入了另外一种方法，随机梯度下降。 优点：全局最优解；易于并行实现 缺点：样本数目很多时，训练过程很慢 从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下： 2. 随机梯度下降SGD由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。 随机梯度下降法(Stochastic Gradient Descent，SGD)正是为了解决批量梯度下降法这一弊端而提出的。 将上面的损失函数写为如下形式： \[ \theta_j^{‘} = \theta_j + ( y^{(i)} - h_{\theta} (x^{(i)}) ) x_j^{(i)} \] 利用每个样本的损失函数对 \( \theta \) 求偏导得到对应的梯度，来更新 \( \theta \)： 随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将\( \theta\)迭代到最优解了， 对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。 但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 优点：训练速度快 缺点：准确度下降，不是全局最优；不易于并行实现 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下： 3. 小批量梯度下降MBGD有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法(Mini-batch Gradient Descent，MBGD)的初衷。 MBGD在每次更新参数时使用 b 个样本( b一般为10 )，其具体的伪代码形式为： 4. 总结 BGD：每次迭代使用所有的样本； SGD：每次迭代使用一个样本； MBGD：每次迭代使用 b 个样本； 参考&gt; [Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD 详解梯度下降法的三种形式BGD、SGD以及MBGD]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD&PCA&LDA降维]]></title>
    <url>%2F2018%2F05%2F18%2Fdimreduction%2F</url>
    <content type="text"><![CDATA[目录 SVD 奇异值分解 [无监督] PCA 主成份分析 [无监督] LDA 线性判别分析 [有监督] PCA与LDA的异同 1. SVD 奇异值分解(Singular Value Decomposition)不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。 1.1 特征值与特征向量特征值和特征向量的定义如下： \[ Ax = \lambda x \] 其中A是一个n x n的矩阵，\(x\)是一个n维向量，则我们\(\lambda\)是矩阵A的一个特征值，而x是矩阵A的特征值\( \lambda \)所对应的特征向量。 根据特征值和特征向量，可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值\( \lambda_1 \leq \lambda_2 \leq … \leq \lambda_n \)以及这n个特征值所对应的特征向量\( \lbrace w_1, w_2, …, w_n \rbrace \)，那么矩阵A就可以用下式的特征分解表示： \[ A = W \Sigma W^{-1} \] 其中W是这n个特征向量所组成的n x n维矩阵，而\( \Sigma \)为这n个特征值为主对角线的n x n维矩阵。 一般我们会把W的这n个特征向量标准化，即满足 \( || w_i ||_2=1 \), 或者说 \( w^T_i w_i=1 \)，此时W的n个特征向量为标准正交基，满足\( W^T W=I \)，即\( W^T=W^{−1}\), 也就是说W为酉矩阵。 这样我们的特征分解表达式可以写成 \[ A=WΣW^T \] 注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。 1.2 SVD的定义SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\(A\)是一个m × n的矩阵，那么我们定义矩阵\(A\)的SVD为： \[ A=UΣV^T \] 其中\(U\)是一个m × m的矩阵，\(\Sigma\)是一个m × n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\(V\)是一个n × n的矩阵。\(U\)和\(V\)都是酉矩阵，即满足\( U^TU=I,V^TV=I\)。下图可以很形象的看出上面SVD的定义： 如何求出SVD分解后的\( U, \Sigma, V \) 这三个矩阵呢？ 如果将\(A^T\)和\(A\)做矩阵乘法，那么会得到n × n的一个方阵\(A^TA\)。既然\(A^TA\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式： \[ (A^TA)v_i=\lambda_i v_i \] 这样就可以得到矩阵\(A^TA\)的n个特征值和对应的n个特征向量\(v\)了。将\(A^TA\)的所有特征向量张成一个n × n的矩阵\(V\)，就是SVD公式里面的\(V\)矩阵了。一般将\(V\)中的每个特征向量叫做\(A\)的右奇异向量。 如果将\(A\)和\(A^T\)做矩阵乘法，那么会得到m × m的一个方阵\(AA^T\)。既然\(AA^T\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式： \[ (AA^T)u_i=\lambda_iu_i \] 这样就可以得到矩阵\(AA^T\)的m个特征值和对应的m个特征向量\(u\)了。将\(AA^T\)的所有特征向量张成一个m × m的矩阵\(U\)，就是SVD公式里面的\(U\)矩阵了。一般我们将\(U\)中的每个特征向量叫做\(A\)的左奇异向量。 \(U\)和\(V\)都求出来了，现在就剩下奇异值矩阵\( \Sigma \)没有求出了。由于\( \Sigma \)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\( \sigma \)就可以了。 注意到: \[ A=UΣV^T \Rightarrow AV=UΣV^TV \Rightarrow AV=UΣ \Rightarrow Av_i=\sigma_iu_i \Rightarrow \sigma_i= \frac{Av_i}{u_i} \] 这样可以求出每个奇异值，进而求出奇异值矩阵\( \Sigma \)。 上面还有一个问题没有讲，就是说\(A^TA\)的特征向量组成的就是我们SVD中的\(V\)矩阵，而\(AA^T\)的特征向量组成的就是我们SVD中的\(U\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\(V\)矩阵的证明为例。 \[ A=UΣV^T \Rightarrow A^T=VΣ^TU^T \Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T \] 上式证明使用了:\( U^TU=I,Σ^TΣ=Σ^2\)，可以看出\( A^TA \)的特征向量组成的的确就是我们SVD中的\(V\)矩阵。类似的方法可以得到\(AA^T\)的特征向量组成的就是我们SVD中的\(U\)矩阵。 进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系： \[ \sigma_i=\sqrt{\lambda_i} \] 这样也就是说，可以不用\( \sigma_i= Av_i / u_i \)来计算奇异值，也可以通过求出\( A^TA \)的特征值取平方根来求奇异值。 1.3 SVD的性质对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说： \[ A_{m×n}=U_{m×m} \Sigma_{m×n} V^T_{n×n} \sim U_{m×k} \Sigma_{k×k} V^T_{k×n} \] 其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\( U_{m×k}, \Sigma_{k×k}, V^T_{k×n} \)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。 由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。 &gt; 奇异值分解(SVD)原理与在降维中的应用 2. PCA 主成份分析事先声明 当 X = 样本数量 * 每个样本的维度 时， 协方差矩阵为$ X^T X $；当 X = 每个样本的维度 * 样本数量 时， 协方差矩阵为$ X X^T $； 2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)假设 m 个 n 维数据\( (x^{(1)}, x^{(2)},…,x^{(m)}) \)都已经进行了中心化，即 \( \sum_{i=1}^m x^{(i)}=0 \)。经过投影变换后得到的新坐标系为\( \lbrace w_1,w_2,…,w_n \rbrace \)，其中\(w\)是标准正交基，即\( || w ||^2 = 1, w^T_iw_j=0 \)。 如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\( \lbrace w_1, w_2, …, w_{n′} \rbrace \)，样本点\( x^{(i)} \)在 n’ 维坐标系中的投影为：\( z^{(i)} = ( z^{(i)}_1,z^{(i)}_2, …, z^{(i)}_{n′}) \)。其中，\( z^{(i)}_j = w^T_j x^{(i)} \)是\( x^{(i)} \)在低维坐标系里第 j 维的坐标。 如果用\( z^{(i)} \)来恢复原始数据\( x^{(i)} \)，则得到的恢复数据 \( \bar{x}^{(i)} = \sum_{j=1}^{n′} z^{(i)}_j w_j = Wz^{(i)} \)，其中，\(W\)为标准正交基组成的矩阵。 现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式： \[ \sum_{i=1}^m || \bar{x}^{(i)} − x^{(i)} ||^2_2 \] 将这个式子进行整理，可以得到: \[ \begin{equation} \begin{aligned} \sum_{i=1}^m || \bar{x}^{(i)} − x^{(i)} ||^2_2 &amp; = \sum_{i=1}^m || Wz^{(i)} − x^{(i)} ||^2_2 \\ &amp; = \sum_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \sum_{i=1}^m (Wz^{(i)})^T x^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \\ &amp; = \sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \sum_{i=1}^m z^{(i)T}W^T x^{(i)} + \sum_{i=1}^m x^{(i)T} x^{(i)} \\ &amp; = \sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \sum_{i=1}^m z^{(i)T}z^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \\ &amp; = − \sum_{i=1}^m z^{(i)T}z^{(i)} + \sum_{i=1}^m x^{(i)T}x^{(i)} \\ &amp; = − tr( W^T ( \sum_{i=1}^m x^{(i)} x^{(i)T} ) W) + \sum_{i=1}^m x^{(i)T} x^{(i)} \\ &amp; = − tr( W^T X X^T W) + \sum_{i=1}^m x^{(i)T} x^{(i)} \end{aligned} \end{equation} \] 第1个等式用到了\( \bar{x}^{(i)} = W z^{(i)} \) 第2个等式用到了平方和展开 第3个等式用到了矩阵转置公式\( (AB)^T= B^T A^T \) 和 \( W^T W =I \) 第4个等式用到了\( z^{(i)} = W^T x^{(i)} \) 第5个等式合并同类项 第6个等式用到了\( z^{(i)} = W^T x^{(i)} \)和矩阵的迹 第7个等式将代数和表达为矩阵形式 注意到\( \sum_{i=1}^m x^{(i)} x^{(i)T} \)是数据集的协方差矩阵，\(W\)的每一个向量\(w_j\)是标准正交基。而\( \sum_{i=1}^m x^{(i)T} x^{(i)} \) 是一个常量。最小化上式等价于： \[ \arg\min_W - tr( W^T X X^T W ) \quad \quad s.t. W^TW = I \] 这个最小化不难，直接观察也可以发现最小值对应的\(W\)由协方差矩阵\(XX^T\)最大的 n’ 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到 \[ J(W) = −tr( W^T X X^T W) + \lambda ( W^T W − I ) \] 对\( W \)求导有\( − X X^T W + \lambda W = 0 \)，整理下即为： \[ XX^T W = \lambda W \] 这样可以更清楚的看出，\(W\)为\(X X^T \)的 n’ 个特征向量组成的矩阵，而\( \lambda \) 为 \( XX^T \) 的特征值。当我们将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\( W\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\( z^{(i)} = W^T x^{(i)} \)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。 2.2 PCA的推导:基于最大投影方差假设 m 个 n 维数据\( (x^{(1)}, x^{(2)}, …, x^{(m)} ) \)都已经进行了中心化，即\( \sum_{i=1}^m x^{(i)} = 0 \)。经过投影变换后得到的新坐标系为\( \lbrace w_1, w_2, …, w_n \rbrace \)，其中\( w\) 是标准正交基，即\( || w ||^2 = 1, w^T_i w_j = 0\)。 如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\( \lbrace w_1, w_2, …, w_{n′} \rbrace \)，样本点\( x^{(i)} \) 在 n’ 维坐标系中的投影为：\( z^{(i)} = ( z^{(i)}_1, z^{(i)}_2, …, z^{(i)}_{n′}) \) 。其中，\( z^{(i)}_j = w^T_j x^{(i)} \)是 \( x^{(i)} \) 在低维坐标系里第 j 维的坐标。 对于任意一个样本\( x^{(i)} \)，在新的坐标系中的投影为\( W^T x^{(i)} \)，在新坐标系中的投影方差为\( W^T x^{(i)} x^{(i)T}W \)，要使所有的样本的投影方差和最大，也就是最大化\( \sum_{i=1}^m W^T x^{(i)} x^{(i)T} W \)，即： \[ \arg\max_W tr(W^T X X^T W) \quad \quad s.t. W^TW=I \] 观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。 利用拉格朗日函数可以得到 \[ J(W) = tr( W^T X X^T W) + \lambda (W^T W − I) \] 对\( W \) 求导有 \( XX^T W + \lambda W = 0 \)，整理下即为： \[ XX^TW = (−\lambda)W \] 和上面一样可以看出，\(W\)为\(XX^T\)的 n’ 个特征向量组成的矩阵，而 \( −\lambda \) 为\( XX^T\) 的特征值。将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\( W \)即为需要的矩阵。对于原始数据集，只需要用\( z^{(i)} = W^T x^{(i)} \)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。 2.3 PCA算法流程输入: n 维样本集 \( D = ( x^{(1)}, x^{(2)}, …, x^{(m)} ) \)，要降维到的维数 n’。 输出: 降维后的样本集 \( D′ \) 对所有的样本进行中心化： \( x^{(i)} = x^{(i)} − \frac{1}{m} \sum_{j=1}^m x^{(j)} \) 计算样本的协方差矩阵\( XX^T \) 对矩阵\( XX^T \)进行特征值分解 取出最大的 n’ 个特征值对应的特征向量\( (w_1, w_2, …, w_{n′}) \)，将所有的特征向量标准化后，组成特征向量矩阵\(W\) 对样本集中的每一个样本\( x^{(i)}\)，转化为新的样本\( z^{(i)} = W^T x^{(i)} \) 得到输出样本集 \( D′=(z^{(1)}, z^{(2)}, …, z^{(m)} ) \) 有时候，我们不指定降维后的 n’ 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \( \lambda_1 \geq \lambda_2 \geq … \geq \lambda_n \)，则 n’ 可以通过下式得到: \[ \sum_{i=1}^{n′} \lambda_i / \sum_{i=1}^n \lambda_i \geq t \] 2.4 PCA实例假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下： 12(2.5, 2.4), (0.5, 0.7), (2.2, 2.9), (1.9, 2.2), (3.1, 3.0), (2.3, 2.7), (2 , 1.6), (1 , 1.1), (1.5, 1.6), (1.1, 0.9) 首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为 12(0.69, 0.49), (-1.31, -1.21), ( 0.39, 0.99), ( 0.09, 0.29), ( 1.29, 1.09), (0.49, 0.79), ( 0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01) 现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为： \[ XX^T = \left( \begin{matrix} cov(x1,x1) &amp; cov(x2,x1) \\ cov(x1,x2) &amp; cov(x2,x2) \end{matrix} \right) \] 对于上述数据，求出协方差矩阵为： \[ XX^T = \left( \begin{matrix} 0.616555556 &amp; 0.615444444 \\ 0.615444444 &amp; 0.716555556 \end{matrix} \right) \] 求出特征值为 \(（0.0490834, 1.28402771）\)，对应的特征向量分别为：\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \)， 由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \( (0.6778734, -0.73517866)^T \)。 则 \( W=(0.6778734, -0.73517866)^T \)。 对所有的数据集进行投影\( z^{(i)} = W^T x^{(i)} \)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为： 12 0.1074951 0.00155202 -0.46345624 -0.1521932 0.07311195 -0.24863317 0.35670133 0.04641726 0.01776463 0.26124033 下面是例子的Python代码 12345678910111213141516171819202122232425262728293031323334353637import numpy as nporiginalData = np.array([ [ 2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2. , 1. , 1.5, 1.1,], [ 2.4, 0.7, 2.9, 2.2, 3. , 2.7, 1.6, 1.1, 1.6, 0.9]])mean = np.mean(originalData, axis=1)print("均值：")print(mean) # [ 1.81 1.91]preData = (originalData.T - mean).Tprint("中心化后数据：")print(preData)# [[ 0.69 -1.31 0.39 0.09 1.29 0.49 0.19 -0.81 -0.31 -0.71]# [ 0.49 -1.21 0.99 0.29 1.09 0.79 -0.31 -0.81 -0.31 -1.01]]cov = np.cov(preData)print("协方差矩阵：")print(cov)# [[ 0.61655556 0.61544444]# [ 0.61544444 0.71655556]]feature_value, feature_vector = np.linalg.eig(cov)print("特征值：")print(feature_value) # [ 0.0490834 1.28402771]print("特征向量:")print(feature_vector)# [[-0.73517866 -0.6778734 ]# [ 0.6778734 -0.73517866]]maxIndex = 1W = feature_vector[maxIndex, :]redData = W.T.dot(preData)print("降维后数据：")print(redData)# [ 0.1074951 0.00155202 -0.46345624 -0.1521932 0.07311195 -0.24863317# 0.35670133 0.04641726 0.01776463 0.26124033] 2.5 PCA总结作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。 PCA算法的主要优点有： 仅仅需要以方差衡量信息量，不受数据集以外的因素影响 各主成分之间正交，可消除原始数据成分间的相互影响的因素 计算方法简单，主要运算是特征值分解，易于实现 PCA算法的主要缺点有： 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响 2.6 PCA实现方面PCA降维需要找到样本协方差矩阵\( X^TX \)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。 可以看出，在这个过程中需要先求出协方差矩阵\( X^TX\)，当样本数多样本特征数也多的时候，这个计算量是很大的。 注意到SVD也可以得到协方差矩阵\( X^TX \)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\(X^TX\)，也能求出右奇异矩阵\( V \)。 也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。 实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。 另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？ 假设样本是 m×n 的矩阵 \(X\)，如果通过SVD找到了矩阵\( XX^T \)最大的 d 个特征向量张成的 m×d 维矩阵\(U\)，则如果进行如下处理： \[ X′_{d×n} = U^T_{d×m} X_{m×n} \] 可以得到一个 d×n 的矩阵\( X’ \)，这个矩阵和原来的 m×n 维样本矩阵\( X \)相比，行数从 m 减到了 k ，可见对行数进行了压缩。 也就是说，左奇异矩阵可以用于行数的压缩；相对的，右奇异矩阵可以用于列数即特征维度的压缩，也就是PCA降维。 &gt; 主成分分析（PCA）原理总结 3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)LDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。 LDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。 我们的目标就是找到一条直线，尽可能满足上面的要求。 来看一个例子：两类会这样被降维 设数据集\( D=\lbrace (x_i, y_i) \rbrace^m_{i=1} \)，投影向量为\(w\)，则点\(x_i\)经过投影后为\( y=w^Tx_i\)，投影前的样本中心点为\( u \)，投影后的中心点为\( \bar{u} = w^T u\)。 希望投影后不同类别的样本尽量离得较远：使用度量值 \[ || \hat{u}_0 - \hat{u}_1 ||^2_2 \] 同时希望投影后相同类别的样本之间尽量离得较近：使用度量值 \[ \sum_{y \in Y_i} (y - \hat{u}_i)^2 \] 这个值其实就是投影后样本的方差乘以此类样本集合中样本的数量。 所以总的优化目标函数为： \[ J(W) = \frac{|| \hat{u}_0 - \hat{u}_1 ||^2_2}{\sum_{y\in Y_i} ( y - \hat{u}_i )^2} = \frac{|| \hat{u}_0 - \hat{u}_1 ||^2_2}{\sum_0 ( y - \hat{u}_0 )^2 + \sum_1 (y - \hat{u}_1)^2} \] 目标\(J(W)\)当然是越大越好； 定义类内散度矩阵为: \( S_w = \sum_0 + \sum_1 = \sum_{X_0} (x-u_0)(x-u_0)^T + \sum_{X_1} (x-u_1)(x-u_1)^T \) 定义类间散度矩阵：\( S_b = (u_0 - u_1)(u_0 - u_1)^T \) 分子：\( || \hat{u}_0 - \hat{u}_1 ||^2_2 = w^T (u_0 - u_1)(u_0 - u_1)^T w = w^TS_bw \) 分母：\( \sum_0 (y-\hat{u}_0)^2 + \sum_1 (y-\hat{u}_1)^2 = w^T S_w w \) 所以\( J(w)= w^TS_bw / w^T S_w w \)，因为向量\( w \)的长度成比例改变不影响\( J(W) \)的取值，所以我们令\( w^TS_ww=1 \)，那么原优化目标就变为 \[ \min_{w} J(W) = - w^T S_b w, \quad \quad s.t. \quad w^TS_ww = 1 \] 这里直接使用拉格朗日乘子法就可以了，解得 \[ S_b w = \lambda w S_w \] 因为\( S_bw = (u_0-u_1)(u_0-u_1)^Tw = (u_0 - u_1) \lambda_t \)， 所以\( (u_0 - u_1)\lambda_t = \lambda w S_w \)，可以得到 \[ w = S_w^{-1} (u_0 - u_1) \] LDA局限性： 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出 LDA不适合对非高斯分布的样本进行降维 LDA在样本分类信息依赖方差而不是均值时，效果不好 LDA可能过度拟合数据 4. PCA与LDA的异同 出发思想不同。PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。 学习模式不同。PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。 降维后可用维度数量不同。LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。 投影的坐标系不一定相同。PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。 上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。 &gt; LDA与PCA都是常用的降维方法，二者的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最长回文子串]]></title>
    <url>%2F2018%2F05%2F10%2FLongestPalindromicSubstring%2F</url>
    <content type="text"><![CDATA[题目给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。示例1：123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot;也是一个有效答案。示例2：12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 方法一 - 中心扩展算法事实上，只需使用恒定的空间，我们就可以在 \( O(n^2) \) 的时间内解决这个问题。 我们观察到回文中心的两侧互为镜像。因此，回文可以从它的中心展开，并且只有 \( 2n - 1 \)个这样的中心 [其中自身\( n \)个，间隙\( n-1 \)个，故可能的中心有\( 2n-1\)个]。 1234567891011121314151617181920212223int expandAroundCenter(string s, int left, int right)&#123; while (left &gt;= 0 &amp;&amp; right &lt; s.length() &amp;&amp; s[left] == s[right]) &#123; left--; right++; &#125; return right - left - 1;&#125;string longestPalindrome(string s) &#123; // O(n^2) + O(1) int start = 0, length = 0; for (int i = 0; i &lt; s.length(); i++) &#123; int len1 = expandAroundCenter(s, i, i); // i 为中心位置，回文字符奇数个 int len2 = expandAroundCenter(s, i, i + 1); // i和i+1之间为中心位置，回文字符偶数个 int len = len1 &gt; len2 ? len1 : len2; // 当前最长的回文串 if (len &gt; length) // 更新最长的回文串 &#123; start = i - (len - 1) / 2; length = len; &#125; &#125; return s.substr(start, length);&#125; 时间复杂度：\( O(n^2)\)，由于围绕中心来扩展回文会耗去 \( O(n)\) 的时间，所以总的复杂度为 \( O(n^2)\)。 空间复杂度：\( O(1) \)。 方法二 - Manacher算法首先通过在每个字符的两边都插入一个特殊的符号，将所有可能的奇数或偶数长度的回文子串都转换成了奇数长度。 比如 abba 变成 #a#b#b#a#， aba变成 #a#b#a#。 此外，为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如$#a#b#a#。 以字符串12212321为例，插入#和这两个特殊符号，变成了T[]=#1#2#2#1#2#3#2#1#，然后用一个数组 P[i] 来记录以字符 T[i] 为中心的最长回文子串向左或向右扩张的长度（包括T[i]，半径长度）。 比如T和P的对应关系： T # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 # P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1 可以看出，P[i]-1正好是原字符串中最长回文串的总长度，为5。 接下来怎么计算 P[i] 呢？Manacher算法增加两个辅助变量C和R，其中C表示最大回文子串中心的位置，R则为C+P[C]，也就是最大回文子串的边界。得到一个很重要的结论： 如果R &gt; i，那么P[i] &gt;= Min(P[2 * C - i], R - i) 下面详细说明这个结论怎么来的 当 R - i &gt; P[j] 的时候，以T[j]为中心的回文子串包含在以T[C]为中心的回文子串中，由于 i 和 j 对称，以T[i]为中心的回文子串必然包含在以T[C]为中心的回文子串中，所以必有 P[i] = P[j]，见下图。 当 R - i &lt;= P[j] 的时候，以T[j]为中心的回文子串不一定完全包含于以T[id]为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以T[i]为中心的回文子串，其向右至少会扩张到R的位置，也就是说 R - i &lt;= P[i]。至于R之后的部分是否对称，就只能老老实实去匹配了。 对于 R &lt;= i 的情况，无法对 P[i] 做更多的假设，只能P[i] = 1，然后再去匹配了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950string preProcess(string s) &#123; int n = s.length(); if (n == 0) return "^$"; string ret = "^"; for (int i = 0; i &lt; n; i++) ret += "#" + s.substr(i, 1); ret += "#$"; return ret;&#125;string longestPalindrome(string s) &#123; string T = preProcess(s); int n = T.length(); int* P = new int[n]; int C = 0, R = 0; for (int i = 1; i &lt; n - 1; i++) &#123; int j = 2 * C - i; // 等价于 j = C - (i-C) = 2 * C - i P[i] = (R &gt; i) ? min(R - i, P[j]) : 0; // 尝试拓展中心为i的回文串 while (T[i + 1 + P[i]] == T[i - 1 - P[i]]) P[i]++; // If palindrome centered at i expand past R, // adjust center based on expanded palindrome. if (i + P[i] &gt; R) &#123; C = i; R = i + P[i]; &#125; &#125; // Find the maximum element in P. int maxLen = 0; int centerIndex = 0; for (int i = 1; i &lt; n - 1; i++) &#123; if (P[i] &gt; maxLen) &#123; maxLen = P[i]; centerIndex = i; &#125; &#125; delete[] P; return s.substr((centerIndex - 1 - maxLen) / 2, maxLen);&#125; 时间复杂度：\( O(n) \)。 空间复杂度：\( O(n) \)。 参考&gt; Longest Palindromic Substring Part II Manacher’s ALGORITHM: O(n)时间求字符串的最长回文子串 最长回文子串-解决方案]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec Tutorial]]></title>
    <url>%2F2018%2F05%2F03%2Fword2vec%2F</url>
    <content type="text"><![CDATA[目录 Word2Vec - CBOW Word2Vec - Skip-Gram Word2Vec的Tricks 自己根据网上资料及自己的理解对word2vec源码阅读并加上注释， 放在我的github weizhaozhao/annotated_word2vec上了, 有兴趣的同学可以一起学习一下，其中的很多tricks都可以尝试应用在实际的工业生产环境中。 1. Word2Vec - CBOWCBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型， 不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。 先来模型结构图，如下： 可以看出，CBOW模型是预测\( P(w_t | w_{t-k}, w_{t-(k-1)}, …, w_{t-1}, w_{t+1}, w_{t+2}, …, w_{t+k}) \)。 从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下： 123456789101112131415161718// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)next_random = next_random * (unsigned long long) 25214903917 + 11;b = next_random % window;// in -&gt; hiddencw = 0;for (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; c = sentence_position - window + a; if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; last_word = sen[c]; if (last_word == -1) continue; for (c = 0; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size]; cw++; &#125; 其中sentence_position为当前word在句子中的下标，以一个具体的句子A B C D为例， 第一次进入到下面代码时，当前word为A，sentence_position为0，b是一个随机生成的0到window-1的词， 整个窗口的大小为(2 window + 1 - 2 b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动， 其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值b的大小决定了当前窗口的大小。 代码中的neu1即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。 CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。 Hierarchical Softmax该算法结合了Huffman编码，每个词w都可以从树的根节点沿着唯一一条路径被访问到。 假设\( n(w,j) \)为这条路径上的第\(j\)个节点，且\( L(w) \)为这条路径的长度，注意\(j\)从1开始编码，即\( n(w,1)=root, n(w,L(w))=w \)。 对于第\(j\)个节点，Hierarchical Softmax算法定义的Lable为 1-code[j]，而输出为 \[ f = \sigma ( neu1^T \cdot syn1 ) \] Loss为负的log似然，即： \[ Loss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f) \] 那么梯度为： \[ \begin{equation} \begin{aligned} Gradient_{neu1} &amp; = \frac{\partial Loss}{\partial neu1} \\ &amp; = - (1-code[j]) \cdot (1-f) \cdot syn1 + code[j] \cdot f \cdot syn1 \\ &amp; = - (1-code[j] - f) \cdot syn1 \end{aligned} \end{equation} \] \[ \begin{equation} \begin{aligned} Gradient_{syn1} &amp; = \frac{\partial Loss}{\partial syn1} \\ &amp; = - (1-code[j]) \cdot (1-f) \cdot neu1 + code[j] \cdot f \cdot neu1 \\ &amp; = - (1-code[j] - f) \cdot neu1 \end{aligned} \end{equation} \] 需要注意的是，word2vec源码中的g实际为负梯度中公共部分与Learningrate alpha的乘积。 123456789101112131415161718192021222324// Hierarchical Softmaxif (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123; f = 0; l2 = vocab[word].point[d] * layer1_size; // Propagate hidden -&gt; output for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; // syn1 is weight between hidden and output if (f &lt;= -MAX_EXP) continue; else if (f &gt;= MAX_EXP) continue; else // expTable to speed running f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; // 'g' is the common part of gradient multiplied by the learning rate g = (1 - vocab[word].code[d] - f) * alpha; // Propagate errors output -&gt; hidden for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2]; // Learn weights hidden -&gt; output for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c]; &#125; Negative Sampling在源码中就是随机生成negative个负例(也有可能少于这个数，当随机撞上原来的word跳过)。 原来的word为正例，Label为1，其他随机生成的Lable为0，输出f仍为： \[ f = \sigma ( neu1^T \cdot syn1 ) \] Loss为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即： \[ Loss = -Likelihood = - label \cdot log f - (1-label) \cdot log (1-f) \] 那么梯度为： \[ \begin{equation} \begin{aligned} Gradient_{neu1} &amp; = \frac{\partial Loss}{\partial neu1} \\ &amp; = -label \cdot (1-f) \cdot syn1 + (1-label) \cdot f \cdot syn1 \\ &amp; = -(label - f) \cdot syn1 \end{aligned} \end{equation} \] \[ \begin{equation} \begin{aligned} Gradient_{syn1} &amp; = \frac{\partial Loss}{\partial syn1} \\ &amp; = -label \cdot (1-f) \cdot neu1 + (1-label) \cdot f \cdot neu1 \\ &amp; = -(label - f) \cdot neu1 \end{aligned} \end{equation} \] 同样注意代码中g并非梯度，可以看作是乘了Learningrate alpha的error(label与输出f的差)。 1234567891011121314151617181920212223242526// NEGATIVE SAMPLINGif (negative &gt; 0) for (d = 0; d &lt; negative + 1; d++) &#123; if (d == 0) &#123; target = word; label = 1; &#125; else &#123; next_random = next_random * (unsigned long long) 25214903917 + 11; target = table[(next_random &gt;&gt; 16) % table_size]; if (target == 0) target = next_random % (vocab_size - 1) + 1; if (target == word) continue; label = 0; &#125; l2 = target * layer1_size; f = 0; for (c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2]; if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2]; for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * neu1[c]; &#125; 隐含层到输入层的梯度传播因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。 1234567891011// hidden -&gt; infor (a = b; a &lt; window * 2 + 1 - b; a++) if (a != window) &#123; c = sentence_position - window + a; if (c &lt; 0) continue; if (c &gt;= sentence_length) continue; last_word = sen[c]; if (last_word == -1) continue; for (c = 0; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c]; &#125; 2. Word2Vec - Skip-GramSkip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\( P(w_i | w_t) \)，其中\( t-c \leq i \leq t+c, 且 i \not = t\)， c是决定上下文窗口大小的常数，c越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。 假设存在一个\( w_1, w_2, w_3, …, w_T \)的词组序列，Skip-Gram的目标最大化： \[ \frac{1}{T} \mathop{\sum_{t=1}^T} \mathop{\sum_{-c \leq j \leq c, j \not= 0}} log p(w_{t+j} | w_t) \] 基本的Skip-Gram模型定义\( P(w_o | w_i) \)为： \[ P(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \sum_{w=1}^W e^{v_w^T v_{w_i}} \] 从公式中不能看出，Skip-Gram是一个对称的模型，如果\(w_t\)为中心词时，\( w_k \)在其窗口内， 则\( w_t \)也必然在以\( w_k \)为中心词的同样大小的窗口内，也就是： \[ \frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \not=0} log P(w_{t+j} | w_t) = \frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \not=0} log P(w_t | w_{t+j}) \] 同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指在一定窗口内的词两两会计算概率，即使它们之间隔着一些词， 这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。 与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。 Hierarchical Softmax算法也结合了Huffman编码，每个词w都可以从树的根节点沿着唯一一条路径被访问到。 假设\( n(w,j) \)为这条路径上的第j个节点，且L(w)为这条路径的长度，注意j从1开始编码，即\( n(w,1)=root, n(w, L(w))=w\)。 层级softmax定义的概率\( P(w | w_I) \)为： \[ P(w | w_I) = \prod_{j=1}^{L(w)-1} \sigma \lbrace \vartheta [ n(w, j+1) = ch(n(w, j)) ] \cdot v_{n(w,j)}^{‘T} v_I \rbrace \] 其中: \[ \vartheta(x) = \begin{cases} 1, \quad if \quad x \quad is \quad true \\ -1, \quad otherwise \end{cases} \] \( ch(n(w, j)) \)既可以是\( n(w,j)\)的左子节点也可以是\( n(w,j) \)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。 Loss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即 \[ \begin{equation} \begin{aligned} Loss_pair &amp; = -Log Likelihood_pair \\ &amp; = - log P(w| w_I) \\ &amp; = - \sum_{j=1}^{L(w)-1} log \lbrace \sigma [ \vartheta ( n(w,j+1)=ch(n(w,j)) ) \cdot v_{n(w,j)}^{‘T} v_I ] \rbrace \end{aligned} \end{equation} \] 如果当前节点是左子节点，即\( \vartheta( n(w,j+1)=ch(n(w,j)) ) \)为true 那么\( Loss = - log \lbrace \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \)，则梯度为： \[ Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}} = - (1-\sigma (v_{n(w,j)}^{‘T} v_I)) \cdot v_I \\ Gradient_{v_I} = \frac{\partial Loss}{\partial v_I} = - (1-\sigma (v_{n(w,j)}^{‘T} v_I)) \cdot v_{n(w,j)}^{‘} \] 如果当前节点是右子节点，即\( \vartheta( n(w,j+1)=ch(n(w,j)) ) \)为false 那么\( Loss = - log \lbrace \sigma( - v_{n(w,j)}^{‘T} v_I) \rbrace = - log(1- \sigma(v_{n(w,j)}^{‘T} v_I)) \)，则梯度为： \[ Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}} = \sigma (v_{n(w,j)}^{‘T} v_I) \cdot v_I \\ Gradient_{v_I} = \frac{\partial Loss}{\partial v_I} = \sigma (v_{n(w,j)}^{‘T} v_I) \cdot v_{n(w,j)}^{‘} \] 合并式(15)和式(16)，得： \[ Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}} = - \lbrace 1 - code[j] - \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \cdot v_I \\ Gradient_{v_I} = \frac{\partial Loss}{\partial v_I} = - \lbrace 1 - code[j] - \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \cdot v_{n(w,j)}^{‘} \] 此处相关的代码如下，其中g是梯度中的公共部分\( (1-code[j]-\sigma( v_{n(w,j)}^{‘T} v_I)) \)与Learningrate alpha的乘积。 1234567891011121314151617181920212223// HIERARCHICAL SOFTMAXif (hs) for (d = 0; d &lt; vocab[word].codelen; d++) &#123; f = 0; l2 = vocab[word].point[d] * layer1_size; // Propagate hidden -&gt; output for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2]; if (f &lt;= -MAX_EXP) continue; else if (f &gt;= MAX_EXP) continue; else f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; // 'g' is the gradient multiplied by the learning rate g = (1 - vocab[word].code[d] - f) * alpha; // Propagate errors output -&gt; hidden for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2]; // Learn weights hidden -&gt; output for (c = 0; c &lt; layer1_size; c++) syn1[c + l2] += g * syn0[c + l1]; &#125; Negative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下： 1234567891011121314151617181920212223242526// NEGATIVE SAMPLINGif (negative &gt; 0) for (d = 0; d &lt; negative + 1; d++) &#123; if (d == 0) &#123; target = word; label = 1; &#125; else &#123; next_random = next_random * (unsigned long long) 25214903917 + 11; target = table[(next_random &gt;&gt; 16) % table_size]; if (target == 0) target = next_random % (vocab_size - 1) + 1; if (target == word) continue; label = 0; &#125; l2 = target * layer1_size; f = 0; for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2]; if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2]; for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1]; &#125; 3. Word2Vec的Tricks3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？前面提到到 Skip-Gram 中的条件概率为： \[ P(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \sum_{w=1}^W e^{v_w^T v_{w_i}} \] 这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\( p(w_o | w_i) \)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。 另外一种方法是只更新当前 \( w_o、 w_i \)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。 Hierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\( w_i \)，先让模型判断\( w_o \)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。 Negative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。 3.2 指数运算由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为： 12expTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6expTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。 相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。 123456expTable = (real *) malloc((EXP_TABLE_SIZE + 1) * sizeof(real));for (i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123; expTable[i] = exp((i / (real) EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table expTable[i] = expTable[i] / (expTable[i] + 1); // Precompute f(x) = x / (x + 1)&#125; 相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的 下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。 1234for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];if (f &gt; MAX_EXP) g = (label - 1) * alpha;else if (f &lt; -MAX_EXP) g = (label - 0) * alpha;else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; 3.3 按word分布随机抽样word2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料： http://en.wikipedia.org/wiki/Categorical_distribution， http://en.wikipedia.org/wiki/Categorical_distribution#Sampling Categorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。 维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下： 方法一：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。 事先准备： 12341. 计算每个类别或词的未归一化分布值（对于词来即词频）；2. 把上面计算的值加和，利用这个和进行概率归一化；3. 引入类别或词的一定顺序，比如词的下标；4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。 使用时： 1231. 随机抽取一个 0 到 1 之间的随机数；2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；3. 返回 CDF 对应的类别或词。 使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。 方法二：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。 123456789101112131. r = 1;2. s = 0;3. for (i = 1; i &lt;= k; i++) 4. &#123;5. v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r6. //产生 v 个下标为 i 的样本7. for (j = 1; j &lt;= v; j++)8. z[s++] = i;9. n = n – v; //下次需要继续生成 n 个样本10. r = r – p[i]; //剩余样本的概率和11. &#125;12. 随机重新排列(shuffle)z中的左右样本；13. 返回 z。 这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。 方法三：即word2vec实现方法 12345678910111213141516171819202122232425262728(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)|_____________a___________|________b________|____c____|(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|word2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。void InitUnigramTable() &#123; int a, i; double train_words_pow = 0; double d1, power = 0.75; table = (int *) malloc(table_size * sizeof(int)); for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power); i = 0; d1 = pow(vocab[i].cn, power) / train_words_pow; for (a = 0; a &lt; table_size; a++) &#123; table[a] = i; if (a / (double) table_size &gt; d1) &#123; i++; d1 += pow(vocab[i].cn, power) / train_words_pow; &#125; if (i &gt;= vocab_size) i = vocab_size - 1; &#125;&#125; 该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。 3.4 哈希编码个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下： 123456789101112131415161718// Returns hash value of a wordint GetWordHash(char *word) &#123; unsigned long long a, hash = 0; for (a = 0; a &lt; strlen(word); a++) hash = hash * 257 + word[a]; hash = hash % vocab_hash_size; return hash;&#125;// Returns position of a word in the vocabulary; if the word is not found, returns -1int SearchVocab(char *word) &#123; unsigned int hash = GetWordHash(word); while (1) &#123; if (vocab_hash[hash] == -1) return -1; if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash]; hash = (hash + 1) % vocab_hash_size; &#125; return -1;&#125; 3.5 随机数作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。 123real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;next_random = next_random * (unsigned long long) 25214903917 + 11;if (ran &lt; (next_random &amp; 0xFFFF) / (real) 65536) continue; 3.6 高频词亚采样这里的亚采样是指 Sub-Sampling，每个词\( w_i \)被丢弃的概率为： \[ P(w_i) = 1 - \sqrt{\frac{sample}{freq(w_i)}} \] sample是一个可以设置的参数， demo-word.sh 中是 10-3，\( freq(w_i) \)则为\( w_i \)的词频。具体的实现代码如下: 12real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;next_random = next_random * (unsigned long long) 25214903917 + 11; 从具体代码可以看出，\(w_i\)被丢弃的概率为： \[ P(w_i) = 1- ( \sqrt{\frac{sample}{freq(w_i)}} + \frac{sample}{freq(w_i)} ) \] 参考&gt; Getting started with Word2Vec word2vec源码 word2vec源码 - 下载 Word2Vec详解.pdf Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙 机器学习算法实现解析——word2vec源码解析 Word2Vec-知其然知其所以然 word2vec 中的数学原理详解]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F2018%2F04%2F28%2Femalgorithm%2F</url>
    <content type="text"><![CDATA[目录 最大似然 Jensen不等式 算法推导 算法收敛性 算法举例 代码 最大似然(极大似然)最大似然估计是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。 最大似然估计是建立在这样的思想上:已知某个参数能使这个样本出现的概率最大，当然不会再去选择其他小概率的样本，所以就把这个参数作为估计的真实值。 假设需要调查学校的男生和女生的身高分布。肯定是抽样解决，假设你在校园里随便地询问100个男生和100个女生。他们共200个人(也就是200个身高的样本数据，为了方便表示，下面说“人”的意思即对应其身高)都在教室里面了。那之后怎么办？通过某种办法将男女生分开，然后你就先统计抽样得到的100个男生的身高。 假设他们的身高是服从高斯分布的，但是这个分布的均值 \( \mu \) 和方差 \( \sigma^2 \) 不知道，这两个参数是需要估计的。 记作\( \theta=[\mu, \sigma]^T \)。 用数学的语言来说：在学校那么多男生（身高）中，我们独立地按照概率密度 \( p(x|\theta) \) 抽取100个（身高）组成样本集\( X \). 我们想通过样本集\( X \) 来估计出未知参数\( \theta \)。已知概率密度 \( p(x|\theta) \) 是高斯分布\( N(\mu, \sigma) \)的形式，其中的未知参数是\( \theta=[\mu, \sigma]^T \)。抽到的样本集是\( X = \lbrace x_1, x_2, … , x_N \rbrace \)，其中\( x_i \)表示抽到的第\(i\)个人的身高，这里\(N\)就是100，表示抽到的样本个数。 由于每个样本都是独立地从\( p(x| \theta) \)中抽取的，且可以认为这些男生之间是没有关系的。 那么从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？ 因为这些男生（的身高）是服从同一个高斯分布\( p(x|\theta) \)的。那么抽到男生A（的身高）的概率是\( p(x_A | \theta) \)，抽到男生B的概率是\( p(x_B | \theta) \)。又因为它们是独立的，所以同时抽到男生A和男生B的概率是\( p(x_A | \theta) \cdot p(x_B | \theta) \)； 同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。换句话就是从分布是\( p(x | \theta) \)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示： \[ L(\theta) = L(x_1,…,x_N; \theta) = \mathop{\prod_{i=1}^N} p(x_i;\theta), \quad \theta \in \Theta \] 这个概率反映了，在概率密度函数的参数是\(\theta\)时，得到\( X\)这组样本的概率。因为这里\(X\)是已知的，也就是说抽取到的这100个人的身高可以测出来，也就是已知的了。而\(\theta\)是未知的，则上面这个公式只有\(\theta\)是未知数，所以它是\(\theta\)的函数。这个函数放映的是在不同的参数\(\theta\)取值下，取得当前这个样本集的可能性，因此称为参数\( \theta\)相对于样本集\(X\)的似然函数（likehood function）。 记为\( L(\theta)\)。 在学校那么男生中，抽到这100个男生（表示身高）而不是其他人，则表示在整个学校中，这100个人（的身高）出现的概率最大。那么这个概率就可以用上面那个似然函数\( L(\theta) \)来表示。只需要找到一个参数\( \theta \)，其对应的似然函数\( L(\theta) \)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做\( \theta \)的最大似然估计量，记为： \[ \hat{\theta} = \mathop{\arg\max}_{\theta} L(\theta) \] 因此，当知道每个样例属于的分布时，求解分布的参数直接利用最大似然估计或贝叶斯估计法即可。但是对于含有隐变量时，就不能直接使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。 Jensen不等式设\(f\)是定义域为实数的函数，如果对于所有的实数\(x\)，\( f^{\prime \prime}(x) \geq 0 \)，那么\(f\)是凸函数； 当\(x\)是向量时，如果其hessian矩阵H是半正定的(\( H \geq 0 \))，那么\( f\)是凸函数。 如果\( f^{\prime \prime}(x) &gt; 0 \)或者\( H &gt; 0 \)，那么称\(f\)是严格凸函数。 Jensen不等式表述如下： 如果\(f\)是凸函数，\(X\)是随机变量，那么\( E[f(X)] \geq f(EX) \)。 特别地，如果\(f\)是严格凸函数，那么\( E[f(X)]=f(EX) \)当且仅当\( p(X=EX)=1 \)，也就是说\( X \)是常量。 下图表示会更清晰： 图中，实线\( f \)是凸函数，\(X\)是随机变量，有0.5的概率是a，有0.5的概率是b。\(X\)的期望值就是a和b的中值了。 图中可以看到\( E[f(x)] \geq f(EX) \)成立。 当f是（严格）凹函数当且仅当-f是（严格）凸函数。Jensen不等式应用于凹函数时，不等号方向反向，即\( E[f(X)] \leq f(EX) \)。 算法推导给定的训练样本是\( \lbrace x^{(1)},…,x^{(m)} \rbrace \)，样例间独立，想找到每个样例隐含的类别\( z \)，能使得\( p(x,z) \)最大。 \( p(x,z) \)的最大似然估计如下： \[ L(\theta) = \mathop{\sum_{i=1}^m} log p(x^{(i)};\theta) = \mathop{\sum_{i=1}^m} log \sum_z p(x^{(i)},z;\theta) \] 前一步是对极大似然取对数，后一步是对每个样例的每个可能类别\(z\)求联合分布概率和。 对于每一个样例\( x^{(i)}\)，让\( Q_i \)表示该样例隐含变量\( z \)的某种分布，\( Q_i\)满足的条件是\( \sum_z Q_i(z) = 1, Q_i(z) \geq 0 \)。 （如果\(z\)是连续性的，那么\(Q_i\)是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量\( z \)是身高，那么就是连续的高斯分布。 如果按照隐藏变量是男女，那么就是伯努利分布了。 可以由前面阐述的内容得到下面的公式： \[ \begin{equation} \begin{aligned} \sum_i log p(x^{(i)}; \theta) &amp; = \sum_i log \sum_{z} p(x^{(i)}, z; \theta) \\ &amp; = \sum_i log \sum_{z} Q_i(z) \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} \\ &amp; \geq \sum_i \sum_{z} Q_i(z) log \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} \end{aligned} \end{equation} \] 第二个等号比较直接，就是分子分母同乘以一个相等的函数；最后不等号利用了Jensen不等式，考虑到 log(x) 是凹函数(二阶导数小于0)。而且 \( \sum_{z} Q_i(z) \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} \) 就是\( \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} \)的期望 这个过程可以看作是对\( L(\theta) \)求了下界。对于\( Q_i\)的选择，有多种可能，哪种更好的？ 假设\( \theta \)已经给定，那么\(L(\theta)\)的值就决定于\( Q_i(z) \)和\( p(x^{(i)}, z) \)了。 可以通过调整这两个概率使下界不断上升，以逼近\( L(\theta) \)的真实值，什么时候算是调整好了呢？ 当不等式变成等式时，说明调整后的概率能够等价于\( L(\theta) \)了。按照这个思路，根据Jensen不等式，等式成立当且仅当随机变量为常数值，即： \[ \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} = c \] c为常数，不依赖于\(z\)； 对此式子做进一步推导，知道\( \sum_z Q_i(z)=1\)，那么也就有\( \sum_z p(x^{(i)}, z; \theta) = c \)，那么有下式： \[ \begin{equation} \begin{aligned} Q_i(z) &amp; = \frac{p(x^{(i)}, z; \theta)}{\sum_z p(x^{(i)}, z; \theta)} \\ &amp; = \frac{p(x^{(i)}, z; \theta)}{p(x^{(i)}; \theta)} \\ &amp; = p(z | x^{(i)}; \theta) \end{aligned} \end{equation} \] 至此，推出了在固定其他参数\(\theta\)后，\( Q_i(z) \)的计算公式就是后验概率，解决了\( Q_i(z) \)如何选择的问题。 这一步就是E步，建立\( L(\theta) \)的下界。接下来的M步，就是在给定\( Q_i(z) \)后，调整\( \theta\)，极大化\(L(\theta) \)的下界（在固定\( Q_i(z) \)后，下界还可以调整的更大）。 那么一般的EM算法的步骤如下： 选择参数的初始值\(\theta^{(0)}\)，开始迭代； (E步)对于每一个 \( i\)，计算 \( Q_i(z) = p(z | x^{(i)}; \theta^{(t)}) \)； (M步)计算 \( \theta^{(t+1)} = \arg\max_{\theta^{(t)}} \sum_i \sum_{z} Q_i(z) log \frac{p(x^{(i)}, z; \theta^{(t)})}{Q_i(z)} \)； 重复2-3步，直到收敛。 算法收敛性假定\( \theta^{(t)} \)和\( \theta^{(t+1)} \)是EM第 t 次和 t+1 次迭代后的结果。如果我们证明了\( L(\theta^{(t)}) \leq L(\theta^{(t+1)})\)，也就是说最大似然估计单调增加，那么最终会到达最大似然估计的最大值。下面来证明，选定\( \theta^{(t)} \)后，我们得到E步 \[ Q_i(z) = p(z | x^{(i)}; \theta^{(t)}) \] 这一步保证了在给定\( \theta^{(t)} \)时，Jensen不等式中的等式成立，也就是 \[ L(\theta) = \sum_i \sum_{z} Q_i^{(t)}(z) log \frac{p(x^{(i)}, z; \theta^{(t)})}{Q_i(z)} \] 然后进行M步，固定\( Q_i^{(t)}(z) \)，并将\( \theta^{(t)} \)视作变量，对上面的\( L(\theta^{(t)}) \)求导后，得到\( L(\theta^{(t+1)}) \)，这样经过一些推导会有以下式子成立： \[ \begin{equation} \begin{aligned} L(\theta^{(t+1)}) &amp; \geq \sum_i \sum_{z} Q_i^{(t)}(z) log \frac{p(x^{(i)}, z; \theta^{(t+1)})}{Q_i(z)} \\ &amp; \geq \sum_i \sum_{z} Q_i^{(t)}(z) log \frac{p(x^{(i)}, z; \theta^{(t)})}{Q_i(z)} \\ &amp; = L(\theta^{(t)}) \end{aligned} \end{equation} \] 第一个不等号，得到\( \theta^{(t+1)} \)时，只是最大化\( L(\theta^{(t)}) \)，也就是\( L(\theta^{(t+1)}) \)的下界，而没有使等式成立， 等式成立只有是在固定\(\theta\)，并按E步得到\( Q_i \)时才能成立。 况且根据前面得到的下式，对于所有的\(\theta\)和\( Q_i \)都成立 \[ L(\theta) \geq \sum_i \sum_{z} Q_i(z) log \frac{p(x^{(i)}, z; \theta)}{Q_i(z)} \] 第二个不等号利用了M步的定义，M步就是将\( \theta^{(t)} \)调整到\( \theta^{(t+1)} \)，使得下界最大化。 这样就证明了\( L(\theta) \)会单调增加。一种收敛方法是\( L(\theta) \)不再变化，还有一种就是变化幅度很小。 算法举例假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H；(H代表证明朝上)。 a是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？ 针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的\( \hat{\theta}_A \)，这其实也是最大似然求导求出来的。 \[ \begin{equation} \begin{aligned} \mathop{\arg\max}_{\theta} log P(Y | \theta) &amp; = log((\theta_B^5(1-\theta_B)^5) (\theta_A^9(1-\theta_A))(\theta_A^8(1-\theta_A)^2) (\theta_B^4(1-\theta_B)^6) (\theta_A^7(1-\theta_A)^3) ) \\ &amp; = log((\theta_A^{24}(1-\theta_A)^6) (\theta_B^9(1-\theta_B)^{11})) \end{aligned} \end{equation} \] 上面这个式子求导之后就能得出\( \hat{\theta}_A = \frac{24}{24+6} = 0.80 以及 \hat{\theta}_B = \frac{9}{9+11} = 0.45 \)。 针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。 E步:计算在给定的\( \hat{\theta}_A^{(0)} \)和\( \hat{\theta}_B^{(0)} \)下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了) \[ P(z=A | y_1, \theta) = \frac{P(z=A, y_1 | \theta)}{P(z=A,y_1 | \theta) + P(z=B, y_1 | \theta)} = \frac{0.6^5 \cdot 0.4^5}{0.6^5 \cdot 0.4^5 + 0.5^{10}} = 0.45 \] M步:针对Q函数求导，在本题中Q函数形式如下，这里的\( y_j\)代表的是每次正面朝上的个数。 \[ \begin{equation} \begin{aligned} Q(\theta, \theta^{(i)}) &amp; = \sum_{j=1}^N \sum_z P(z| y_j, \theta^{(i)}) log P(y_j, z | \theta) \\ &amp; = \sum_{j=1}^N \mu_j log(\theta_A^{y_j} (1-\theta_A)^{10-y_j}) + (1-\mu_j) log(\theta_B^{y_j} (1-\theta_B)^{10-y_j}) \end{aligned} \end{equation} \] 从而针对这个式子来对参数求导，例如对\(\theta_A\)求导 \[ \begin{equation} \begin{aligned} \frac{\partial Q}{\partial \theta_A} &amp; = \mu_1 (\frac{y_1}{\theta_A} - \frac{10-y_1}{1-\theta_A}) + … + \mu_5 ( \frac{y_5}{\theta_A} - \frac{10-y_5}{1-\theta_5} ) \\ &amp; = \mu_1 ( \frac{y_1 - 10 \theta_A}{\theta_A (1-\theta_A)} ) + … + \mu_5 ( \frac{y_5 - 10 \theta_A}{\theta_A (1-\theta_A)} ) \\ &amp; = \frac{\sum_{j=1}^5 \mu_j y_j - \sum_{j=1}^5 10 \mu_j \theta_A}{\theta_A (1-\theta_A)} \end{aligned} \end{equation} \] 求导等于0之后就可得到图中的第一次迭代之后的参数值\( \hat{\theta}_A^{(1)} = 0.71 和 \hat{\theta}_B^{(1)} = 0.58 \)。 这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。 代码https://blog.csdn.net/u010866505/article/details/77877345 参考&gt; EM算法 The EM Algorithm 机器学习算法系列之一 EM算法实例分析 从最大似然到EM算法浅解 EM算法原理和python简单实现 What is the expectation maximization algorithm?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn-GridSearchCV & hyperopt & hyperopt-sklearn 调参]]></title>
    <url>%2F2018%2F04%2F23%2Ftuneparameters%2F</url>
    <content type="text"><![CDATA[目录 sklearn-GridSearchCV hyperopt hyperopt-sklearn sklearn-GridSearchCV常用参数sklearn.model_selection.GridSearchCV 参数 含义 其他 estimator 所使用的模型 假定这是scikit-learn中模型接口。该模型可以提供score方法或scoring参数 param_grid dict或list 带有参数名称作为键的字典,例如param_grid=param_test, param_test={'n_estimators': range(1, 6)} scoring 评价标准，默认为None 字符串，或是可调用对象，需要其函数形式如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数 cv 交叉验证参数，默认为None，使用三折交叉验证 整数指定交叉验证折数，也可以是交叉验证生成器 refit 默认为True 在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集 iid 默认为True 默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均 verbose 默认为0 日志冗长度。0：不输出训练过程；1：偶尔输出；>1：对每个子模型都输出 n_jobs 并行数，int类型 -1：跟CPU核数一致； 1:默认值 pre_dispatch 指定总共分发的并行任务数 当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次 scikit-learn内置可用评价标准如下：scikit-learn model_evalution 常用方法 方法 含义 grid.fit() 运行网格搜索 grid.grid_scores_ 给出不同参数情况下的评价结果 grid.best_params_ 已取得最佳结果的参数的组合 grid.best_score_ 优化过程期间观察到的最好的评分 代码123456789101112131415161718192021222324252627282930313233# coding:utf-8from sklearn import svm, datasetsfrom sklearn.model_selection import GridSearchCVdef grid_search(params, X, y): svc = svm.SVC() grid = GridSearchCV(estimator=svc, param_grid=param_test, scoring=&quot;accuracy&quot;, cv=3, refit=True, iid=True, verbose=True) grid.fit(X, y) return gridif __name__ == &apos;__main__&apos;: iris = datasets.load_iris() param_test = &#123;&apos;kernel&apos;:[&apos;linear&apos;, &apos;rbf&apos;], &apos;C&apos;:[1, 10]&#125; estimator = grid_search(param_test, iris.data, iris.target) print(&quot;grid_scores_:&quot;, estimator.grid_scores_) print(&quot;best_params_&quot;, estimator.best_params_) print(&quot;best_score_&quot;, estimator.best_score_) # Fitting 3 folds for each of 4 candidates, totalling 12 fits # [Parallel(n_jobs=1)]: Done 12 out of 12 | elapsed: 0.0s finished # grid_scores_: [mean: 0.98000, std: 0.01602, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125;, mean: 0.97333, std: 0.00897, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 1&#125;, mean: 0.97333, std: 0.03697, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 10&#125;, mean: 0.98000, std: 0.01601, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 10&#125;] # best_params_ &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125; # best_score_ 0.98 hyperoptHyperopt-Github Hyheropt四个重要的因素： 指定需要最小化的函数(the objective function to minimize)； 搜索的空间(the space over which to search)； 采样的数据集(trails database)(可选)； 搜索的算法(可选) 目标函数，指定最小化的函数，比如要最小化函数\( q(x,y) = x^2 + y^2 \) 搜索的算法，即hyperopt的fmin函数的algo参数的取值。 当前支持的算法有随机搜索(hyperopt.rand.suggest)，模拟退火(hyperopt.anneal.suggest)，TPE算法。 搜索(参数)空间设置，例如优化函数q，输入 fmin(q，space=hp.uniform(‘a’，0，1))。 hp.uniform函数的第一个参数是标签，每个超参数在参数空间内必须具有独一无二的标签。 hp.uniform指定了参数的分布。 其他的参数分布： hp.choice返回一个选项，选项可以是list或者tuple.options可以是嵌套的表达式，用于组成条件参数。 hp.pchoice(label,p_options)以一定的概率返回一个p_options的一个选项。这个选项使得函数在搜索过程中对每个选项的可能性不均匀。 hp.uniform(label,low,high)参数在low和high之间均匀分布。 hp.quniform(label,low,high,q),参数的取值是round(uniform(low,high)/q)*q，适用于那些离散的取值。 hp.loguniform(label,low,high)绘制exp(uniform(low,high)),变量的取值范围是[exp(low),exp(high)] hp.randint(label,upper) 返回一个在[0,upper)前闭后开的区间内的随机整数。 搜索空间可以含有list和dictionary。 12345from hyperopt import hplist_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1)]tuple_space = (hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1))dict_space = &#123;&apos;a&apos;: hp.uniform(&apos;a&apos;, 0, 1), &apos;b&apos;: hp.loguniform(&apos;b&apos;, 0, 1)&#125; sample函数在参数空间内采样 12345from hyperopt import hpfrom hyperopt.pyll.stochastic import samplelist_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1), hp.randint(&apos;c&apos;, 8)]print(sample(list_space))# (0.7802721043817558, 1.6616883461586371, array(4)) 简单的例子12345678from hyperopt import fmin, hp, tpedef objective(args): x, y = args return x**2 + y**2 + 1space = [hp.randint(&apos;x&apos;, 5), hp.randint(&apos;y&apos;, 5)]best = fmin(objective, space=space, algo=tpe.suggest, max_evals=100)print(best)# &#123;&apos;y&apos;: 0, &apos;x&apos;: 0&#125; Perceptron鸢尾花例子使用感知器判别鸢尾花，使用的学习率是0.1，迭代40次得到了一个测试集上正确率为82%的结果；使用hyperopt优化参数，将正确率提升到了91%。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# coding:utf-8from sklearn import datasetsimport numpy as npfrom sklearn.cross_validation import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import Perceptronfrom hyperopt import fmin, tpe, hp, partial# load data setiris = datasets.load_iris()X = iris.datay = iris.targetX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)# data standardsc = StandardScaler()sc.fit(X_train)X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test)# create perceptron modelppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)ppn.fit(X_train_std, y_train)# show result before tune parametersy_pred = ppn.predict(X_test_std)print(&quot;=&quot;*15, &quot;before tune&quot;, &quot;=&quot;*15)print(accuracy_score(y_test, y_pred))# =============================================================================================# define object which hyperopt usedef percept(args): global X_train_std, y_train, y_test ppn = Perceptron(n_iter=int(args[&quot;n_iter&quot;]), eta0=args[&quot;eta&quot;] * 0.01, random_state=0) ppn.fit(X_train_std, y_train) y_pred = ppn.predict(X_test_std) return -accuracy_score(y_test, y_pred)# define search spacespace = &#123;&quot;n_iter&quot;: hp.choice(&quot;n_iter&quot;, range(30, 50)), &quot;eta&quot;: hp.uniform(&quot;eta&quot;, 0.05, 0.5)&#125;# define search algorithmalgo = partial(tpe.suggest, n_startup_jobs=10)best = fmin(percept, space, algo=algo, max_evals=100)print(&quot;=&quot;*15, &quot;after tune&quot;, &quot;=&quot;*15)print(best)print(percept(best)) 结果 由于使用tpe搜索算法，每次搜索的结果都不一样，不稳定。 12345=============== before tune ===============0.822222222222=============== after tune ===============&#123;&apos;n_iter&apos;: 14, &apos;eta&apos;: 0.12949436553904228&#125;-0.911111111111 xgboost癌症例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# coding:utf-8import numpy as npfrom sklearn.preprocessing import MinMaxScalerimport xgboost as xgbfrom sklearn.cross_validation import cross_val_scorefrom sklearn.datasets import load_breast_cancerfrom hyperopt import fmin, tpe, hp, partialdef loadData(): d = load_breast_cancer() data, target = d.data, d.target minmaxscaler = MinMaxScaler() data_scal = minmaxscaler.fit_transform(data) indices = np.random.permutation(target.shape[0]) train_indices = indices[: int(target.shape[0] * 0.8)] test_indices = indices[int(target.shape[0] * 0.8):] train_x = data[train_indices] test_x = data[test_indices] train_y = target[train_indices] test_y = target[test_indices] return train_x, test_x, train_y, test_ytrain_x, test_x, train_y, test_y = loadData()print(&quot;train_x:&quot;, train_x.shape, &quot; test_x:&quot;, test_x.shape, &quot; train_y:&quot;, train_y.shape, &quot; test_y:&quot;, test_y.shape)# define object function which need to minimizedef GBM(argsDict): max_depth = argsDict[&quot;max_depth&quot;] + 5 n_estimators = argsDict[&apos;n_estimators&apos;] * 5 + 50 learning_rate = argsDict[&quot;learning_rate&quot;] * 0.02 + 0.05 subsample = argsDict[&quot;subsample&quot;] * 0.1 + 0.7 min_child_weight = argsDict[&quot;min_child_weight&quot;] + 1 print(&quot;=&quot;*20) print(&quot;max_depth:&quot; + str(max_depth)) print(&quot;n_estimator:&quot; + str(n_estimators)) print(&quot;learning_rate:&quot; + str(learning_rate)) print(&quot;subsample:&quot; + str(subsample)) print(&quot;min_child_weight:&quot; + str(min_child_weight)) global train_x, train_y gbm = xgb.XGBClassifier(nthread=4, # 进程数 max_depth=max_depth, # 最大深度 n_estimators=n_estimators, # 树的数量 learning_rate=learning_rate, # 学习率 subsample=subsample, # 采样数 min_child_weight=min_child_weight, # 孩子数 max_delta_step=10, # 10步不降则停止 objective=&quot;binary:logistic&quot;) metric = cross_val_score(gbm, train_x, train_y, cv=5, scoring=&quot;roc_auc&quot;).mean() print(&quot;cross_val_score:&quot;, metric, &quot;\n&quot;) return -metric# define search spacespace = &#123;&quot;max_depth&quot;: hp.randint(&quot;max_depth&quot;, 15), &quot;n_estimators&quot;: hp.randint(&quot;n_estimators&quot;, 10), # [0,1,2,3,4,5] -&gt; [50,] &quot;learning_rate&quot;: hp.randint(&quot;learning_rate&quot;, 6), # [0,1,2,3,4,5] -&gt; 0.05,0.06 &quot;subsample&quot;: hp.randint(&quot;subsample&quot;, 4), # [0,1,2,3] -&gt; [0.7,0.8,0.9,1.0] &quot;min_child_weight&quot;: hp.randint(&quot;min_child_weight&quot;, 5), # [0,1,2,3,4] -&gt; [1,2,3,4,5] &#125;# define search algorithmalgo = partial(tpe.suggest, n_startup_jobs=1)# search best parametersbest = fmin(GBM, space, algo=algo, max_evals=4)print(&quot;best param:&quot;, best) # output best parametersprint(&quot;best result:&quot;, GBM(best)) # output model result in best parameters 结果 12345678910111213141516171819202122232425262728293031323334353637383940414243train_x: (455, 30) test_x: (114, 30) train_y: (455,) test_y: (114,)====================max_depth:16n_estimator:60learning_rate:0.07subsample:0.7999999999999999min_child_weight:5cross_val_score: 0.988504345257 ====================max_depth:6n_estimator:60learning_rate:0.15000000000000002subsample:0.7min_child_weight:5cross_val_score: 0.990053680824 ====================max_depth:8n_estimator:95learning_rate:0.13subsample:0.8999999999999999min_child_weight:5cross_val_score: 0.988304093567 ====================max_depth:12n_estimator:60learning_rate:0.09subsample:0.7min_child_weight:3cross_val_score: 0.990361013789 best param: &#123;&apos;n_estimators&apos;: 2, &apos;max_depth&apos;: 7, &apos;subsample&apos;: 0, &apos;learning_rate&apos;: 2, &apos;min_child_weight&apos;: 2&#125;====================max_depth:12n_estimator:60learning_rate:0.09subsample:0.7min_child_weight:3cross_val_score: 0.990361013789 best result: -0.990361013789 hyperopt-sklearnhyperopt-sklearn-Github 目前hyperopt-sklearn只支持部分Classifiers\Regressors\Preprocessing，具体请见上面hyperopt-sklearn的Github主页。 安装12git clone git@github.com:hyperopt/hyperopt-sklearn.git(cd hyperopt-sklearn &amp;&amp; pip install -e .) 使用模版1234567891011121314from hpsklearn import HyperoptEstimator, svcfrom sklearn import svm# Load Data# ...if use_hpsklearn: estim = HyperoptEstimator(classifier=svc(&apos;mySVC&apos;))else: estim = svm.SVC()estim.fit(X_train, y_train)print(estim.score(X_test, y_test)) 鸢尾花例子12345678910111213141516171819202122232425262728293031323334353637383940# coding:utf-8from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessingfrom sklearn.datasets import load_irisfrom hyperopt import tpeimport numpy as np# Download the data and split into training and test setsiris = load_iris()X = iris.datay = iris.targettest_size = int(0.2 * len(y))np.random.seed(13)indices = np.random.permutation(len(X))train_x = X[indices[:-test_size]]train_y = y[indices[:-test_size]]test_x = X[indices[-test_size:]]test_y = y[indices[-test_size:]]# Instantiate a HyperoptEstimator with the search space and number of evaluationsestim = HyperoptEstimator(classifier=any_classifier(&apos;my_clf&apos;), preprocessing=any_preprocessing(&apos;my_pre&apos;), algo=tpe.suggest, max_evals=100, trial_timeout=120)# Search the hyperparameter space based on the dataestim.fit(train_x, train_x)# Show the resultsprint(estim.score(test_x, test_y))# 1.0print(estim.best_model())# &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&apos;gini&apos;,# max_depth=3, max_features=&apos;log2&apos;, max_leaf_nodes=None,# min_impurity_decrease=0.0, min_impurity_split=None,# min_samples_leaf=1, min_samples_split=2,# min_weight_fraction_leaf=0.0, n_estimators=13, n_jobs=1,# oob_score=False, random_state=1, verbose=False,# warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125; MNIST例子1234567891011121314151617181920212223242526272829303132333435363738394041# coding:utf-8from hpsklearn import HyperoptEstimator, extra_treesfrom sklearn.datasets import fetch_mldatafrom hyperopt import tpeimport numpy as np# Download the data and split into training and test setsdigits = fetch_mldata(&apos;MNIST original&apos;)X = digits.datay = digits.targettest_size = int(0.2 * len(y))np.random.seed(13)indices = np.random.permutation(len(X))X_train = X[indices[:-test_size]]y_train = y[indices[:-test_size]]X_test = X[indices[-test_size:]]y_test = y[indices[-test_size:]]# Instantiate a HyperoptEstimator with the search space and number of evaluationsestim = HyperoptEstimator(classifier=extra_trees(&apos;my_clf&apos;), preprocessing=[], algo=tpe.suggest, max_evals=10, trial_timeout=300)# Search the hyperparameter space based on the dataestim.fit(X_train, y_train)# Show the resultsprint(estim.score(X_test, y_test))# 0.962785714286print(estim.best_model())# &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion=&apos;entropy&apos;,# max_depth=None, max_features=0.959202875857,# max_leaf_nodes=None, min_impurity_decrease=0.0,# min_impurity_split=None, min_samples_leaf=1,# min_samples_split=2, min_weight_fraction_leaf=0.0,# n_estimators=20, n_jobs=1, oob_score=False, random_state=3,# verbose=False, warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125; 参考&gt; sklearn-GridSearchCV,CV调节超参使用方法 python调参神器hyperopt-简略 python调参神器hyperopt-详细]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机森林 Random Forest]]></title>
    <url>%2F2018%2F04%2F21%2Frandomforest%2F</url>
    <content type="text"><![CDATA[随即森林 - Random Forest 目录 基本概念 袋外错误率(out of bag error, oob error) 例子 特点及细节 TODO 分布式实现原理 https://www.jianshu.com/p/d90189008864 https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/ 基本概念随机森林是由多颗CART树组成的，具体决策树部分知识在这里 前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。 打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。 该动物到底是老鼠还是松鼠，要依据投票情况来确定，获得票数最多的类别就是森林的分类结果。 森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。 少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。 将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想 （bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）。 有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？ 每棵树的按照如下规则生成： （1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。 （2）如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树(节点)进行分裂时，从这m个特征中选择最优的； 具体意思是：每次进行分支的时候，都重新随机选择m个特征，计算这m个特征里，分裂效果最好的那个特征进行分类 （3）每棵树都尽最大程度的生长，并且没有剪枝过程。 一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性（采样数据、采样特征）。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。 随机森林分类效果（错误率）与两个因素有关： 森林中任意两棵树的相关性：相关性越大，错误率越大； 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。 袋外错误率(out of bag error, oob error)上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error。 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。 它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。 所以对于每棵树而言（假设对于第k棵树），大约有1/3(\( 1/e \approx 0.368 \))的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。 而这样的采样特点就允许我们进行oob估计，它的计算方式如下：(note：以样本为单位) （1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）； （2）然后以简单多数投票作为该样本的分类结果； （3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。 oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。 例子描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。 收入层次 : Band 1 : &lt; 40,000 Band 2: 40,000 – 150,000 Band 3: &gt; 150,000 随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。 CART 1 : Variable Age CART 2 : Variable Gender CART 3 : Variable Education CART 4 : Variable Residence CART 5 : Variable Industry 我们要预测的某个人的信息如下： Age : 35 years Gender : Male Highest Educational Qualification : Diploma holder Industry : Manufacturing Residence : Metro 根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况： 最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（&lt; 40,000）。 特点及细节 在当前所有算法中，具有极好的准确率 能够有效地运行在大数据集上 能够处理具有高维特征的输入样本，而且不需要降维 能够评估各个特征在分类问题上的重要性 在生成过程中，能够获取到内部生成误差的一种无偏估计 对于缺省值问题也能够获得很好得结果 12345为什么要随机抽样训练集？如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；为什么要有放回地抽样？如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是"片面的"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。 12减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。 12345678910RF特征选择首先特征选择的目标有两个：1：找到与分类结果高度相关的特征变量。2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。特征选择的步骤：（1）对于每一棵决策树，计算其oob error（2）随机的修改OOB中的每个特征xi的值，计算oob error_2，再次计算重要性（3）按照特征的重要性排序，然后剔除后面不重要的特征（4）然后重复以上步骤，直到选出m个特征。 123456789RF特征重要性的度量方法（1）对于每一棵决策树，计算其oob error_0（2）选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1（3）特征的重要性 = ∑(oob error_1-oob error_0) / 随机森林中决策树的个数（4）对随机森林中的特征变量按照特征重要性降序排序。（5）然后重复以上步骤，直到选出m个特征。解释：用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。 参考 随机森林原理篇 随机森林（Random Forest） Introduction to Random forest (博主：爱67) 随即森林原理-简书 Introduction to Random forest – Simplified]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT & XGBoost]]></title>
    <url>%2F2018%2F04%2F12%2Fgbdtxgboost%2F</url>
    <content type="text"><![CDATA[GBDT & XGBoost目录 符号定义 GBDT = GBRT = MART XGBoost 正则化 GBDT与XGBoost的比较 1. 符号定义决策树 \[ f(x; \lbrace R_j, b_j\rbrace ^{J}_1) = \sum_{j=1}^J b_j I(x\in R_j) \] \( \lbrace R_j \rbrace ^{J}_1 \)和\( \lbrace b_j \rbrace ^{J}_1 \)表示决策树的参数，前者为分段空间(disjoint空间)，后者为这些空间上的输出值[其他地方称为打分值]，\(J\)是叶子结点的数量，下文中用\(f(x)\)省略表示\( f(x; \lbrace R_j, b_j\rbrace ^{J}_1) \) 决策树的Ensemble \[ F = \sum_{i=0}^K f_i \] 其中\(f_0\)是模型初始值，通常为统计样本计算出的常数[论文中为median]，同时定义\( F_k = \sum_{i=0}^k f_i \)。 \(D = \lbrace (x_i, y_i) \rbrace ^N_1\)，训练样本。 \[ \mathfrak{L} = \mathfrak{L}( \lbrace y_i, F(x_i) \rbrace ^N_1 ) = \underbrace{\sum_{i=1}^N L(y_i, F(x_i))}_{\text{训练误差项}} + \underbrace{\sum_{k=1}^K \Omega(f_k)}_{\text{正则化项}} \] 目标函数(损失函数)，第一项\(L\)是针对训练数据的\(Loss\)，可以选择：绝对值误差、平方误差、logistic loss等；第二项\( \Omega \)是正则化函数，惩罚\(f_k\)的复杂度。 2. GBDT2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]输入：\( \lbrace (x_i, y_i) \rbrace^N_1 , K, L, …\) 输出：\(F_k\) （1）初始化\(f_0\) （2）对k=1,2,…,K, 计算 （2.1）\( \tilde{y}_i = - \frac{\partial L(y_i, F_{k-1}(x_i))}{\partial F_{k-1}}, i=1,2,…,N \) 计算响应[response]\(\tilde{y}_i\)，它是一个和残差[residual, \(y_i-F_{k-1}(x_i)\)]正相关的变量。 （2.2）\( \lbrace R_j, b_j \rbrace^{J^{\ast}}_1 = \mathop{\arg\min}_{\lbrace R_j, b_j \rbrace^J_1} \sum_{i=1}^N [\tilde{y}_i - f_k(x_i;\lbrace R_j,b_j \rbrace^J_1)]^2 \) 使用平方误差训练一颗决策树\(f_k\)，拟合数据\( \lbrace (x_i, \tilde{y}_i) \rbrace^N_1 \) （2.3）\( \rho^{\ast} = \mathop{\arg\min}_{\rho} \mathfrak{L}(\lbrace y_i, F_{k-1}(x_i)+\rho f_k(x_i) \rbrace ^N_1)= \mathop{\arg\min}_{\rho} \sum_{i=1}^N L(y_i, F_{k-1}(x_i) + \rho f _k(x_i)) + \Omega(f_k) \) 求一个步长\(\rho^{\ast}\)，最小化损失。 （2.4）令\( f_k = \rho^{\ast}f_k, \quad F_k = F_{k-1}+f_k \) 将训练出来的\(f_k\)叠加到\(F\)。 总体来说，GBDT就是一个不断拟合响应(残差)并叠加到F上的过程，在这个过程中，响应不断变小，Loss不断接近最小值。 2.2 GBDT例子GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。 假设我们现在有一个训练集，训练集只有4个人A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果： 现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果： 在第一棵树分枝和第一张图片中一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1,-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 最后GBDT的预测结果为： A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14； B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16； C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24； D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26。 那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。 注：两张图片中的最终效果相同，为何还需要GBDT呢？答案是过拟合。 其他例子请见李航博士《统计学习方法》page.149 GBRT 的优点: 对混合型数据的自然处理（异构特征） 强大的预测能力 在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现) GBDT 的缺点: 可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBDT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行. 附GBDT论文Friedman J. - Greedy Function Approximation_A Gradient Boosting Machine 梯度提升树(GBDT)原理小结 - 刘建平Pinard 3. XGBoost 数据分析利器：XGBoost算法最佳解析 - 腾讯技术工程 xgboost特征重要性指标: weight, gain, cover xgboost中使用的正则化函数为： \[ \Omega(f_k) = \frac{\gamma}{2} J + \frac{\lambda}{2} \sum_{j=1}^J b^2_j \] 我们的目标是求\(f_k\)，它最小化目标函数[式\(3\)] \[ \begin{equation} \begin{aligned} \mathfrak{L}_k &amp; = \sum_{i=1}^N L(y_i, F_{k-1}(x_i)+ \rho f_k(x_i)) + \Omega(f_k) \\ &amp; = \sum_{i=1}^N L(y_i, F_{k-1}+\rho f_k) + \Omega(f_k) \quad \quad \quad [泰勒二阶展开可得下步] \\ &amp; \approx \sum_{i=1}^N \lgroup L(y_i, F_{k-1}) + \underbrace{\frac{\partial L(y_i, F_{k-1})}{\partial F_{k-1}}}_{\text{= \(g_i\)}} f_k + \frac{1}{2}\underbrace{\frac{\partial^2 L(y_i, F_{k-1})}{\partial F^2_{k-1}}}_{\text{=\(h_i\)}} f^2_k \rgroup + \Omega(f_k) \\ &amp; = \sum_{i=1}^N \lgroup L(y_i, F_{k-1}) + g_i f_k + \frac{1}{2}h_i f_k^2 \rgroup + \Omega(f_k) \\ &amp; = \sum_{i=1}^N \lgroup L(y_i, F_{k-1}) + g_i \sum_{j=1}^J b_j + \frac{1}{2} h_i \sum_{j=1}^J b_j^2 \rgroup + \frac{\gamma}{2} J + \frac{\lambda}{2} \sum_{j=1}^J b_j^2 \\ \end{aligned} \end{equation} \] 整理出和\(\lbrace R_j \rbrace^J_1, \lbrace b_j \rbrace^J_1\)有关的项： \[ \begin{equation} \begin{aligned} \mathfrak{L}(\lbrace R_j \rbrace^J_1, \lbrace b_j \rbrace^J_1) &amp; = \sum_{i=1}^N \lgroup g_i \sum_{j=1}^J b_j + \frac{1}{2}h_i \sum_{j=1}^J b_j^2 \rgroup + \frac{\gamma}{2} J + \frac{\lambda}{2} \sum_{j=1}^J b_j^2 \\ &amp; = \sum_{x_i \in R_j} \lgroup g_i \sum_{j=1}^J b_j + \frac{1}{2}h_i \sum_{j=1}^J b_j^2 \rgroup + \frac{\gamma}{2} J + \frac{\lambda}{2} \sum_{j=1}^J b_j^2 \\ &amp; = \sum_{j=1}^J \lgroup \sum_{x_i \in R_j} g_i b_j + \sum_{x_i \in R_j} \frac{1}{2} h_i b_j^2 \rgroup + \frac{\gamma}{2} J + \frac{\lambda}{2} \sum_{j=1}^J b_j^2 \\ &amp; = \sum_{j=1}^J \lgroup \underbrace{\sum_{x_i \in R_j}g_i}_{\text{\(=G_j\)}}b_j + \frac{1}{2} \lgroup \underbrace{\sum_{x_i \in R_j}h_i}_{\text{\(=H_j \)}} + \lambda \rgroup b_j^2 \rgroup + \frac{\gamma}{2} J \\ &amp; = \sum_{j=1}^J \lgroup G_jb_j + \frac{1}{2}(H_j + \lambda)b_j^2 \rgroup + \frac{\gamma}{2} J \end{aligned} \end{equation} \] \(式(6)\)对\(b_j\)求导，并令其等于零，得： \[ b_j^{\ast} = - \frac{G_j}{H_j + \lambda}, \quad j=1,2,…,J \] \(式(7)代入式(6)\)中，化简得最小的\(\mathfrak{L}\)： \[ \mathfrak{L}^{\ast}_k = - \frac{1}{2} \sum_{j=1}^J \frac{G_j^2}{H_j+\lambda} + \frac{\gamma}{2}J \] 求\(\lbrace R_j\rbrace^J_1\)与求\( \lbrace b_j \rbrace^J_1 \)的方法不同，前者它是对输入\(x\)所属空间的一种划分方法不连续，无法求导。 精确得到划分\(\lbrace R_j \rbrace^J_1\)是一个NP难问 题，取而代之使用贪心法，即分裂某节点时，只考虑对当前节点分裂后哪个分裂方案能得到最小的\(\mathfrak{L}_k\)。 像传统决策树一样，CART中的办法也是遍历\(x\)的每个维度的每个分裂点，选择具有最小 $\mathfrak{L}_k$的维度和分裂点进行。 那么定义：当前节点 $R_j$ 分裂成\( R_L \)和\( R_R \)使得分裂后整棵树的\( \mathfrak{L}_k \)最小。 从\(式(8)\)可知，整棵树的最小\(\mathfrak{L}_k\)等于每个叶子结点上(最小)Loss的和，由于整个分裂过程中只涉及到3个结点，其他任何结点的Loss在分裂过程中不变，这个问题等价于： \[ \mathop{\max}_{R_L, R_R} \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \frac{\gamma}{2} \] \(式(9)\)的含义是：前两项分别加上新生成的叶子结点的最小Loss，第三项是指减去被分裂的叶子结点的最小Loss，第四项是分裂后增加叶结点带来的模型复杂度。它是将结点\(R_j\)分裂成\(R_L和R_R\)后，整棵树最小\(\mathfrak{L}_k\)的降低量，这个量越大越好。 3.1 xgboost树的分裂算法 3.2 xgboost调参1-Complete Guide to Parameter Tuning in XGBoost 2-Hyperopt 3-GridSearchCV 4. 正则化GBDT有非常快降低Loss的能力，这也会造成一个问题：Loss迅速下降，模型低bias，高variance，造成过拟合。 下面一一介绍GBDT中抵抗过拟合的技巧： 限制树的复杂度。\(\Omega\)函数对树的节点数，和节点上预测值\( \lbrace b_j \rbrace^J_1 \)的平方和均有惩罚，除此之外，我们通常在终止条件上还会增加一条为树的深度。 采样。即训练每个树的时候只使用一部分的样本。 列采样。即训练每个树的时候只使用一部分的特征，这是Xgboost的创新，它将随机森林中的思想引入了GBDT。 Shrinkage。进一步惩罚\(\lbrace b_j \rbrace^J_1\)，给它们乘一个小于1的系数，也可以理解为设置了一个较低的学习率。 Early stop。因为GBDT的可叠加性我们使用的模型不一定是最终的ensemble，而根据测试集的测试情况，选择使用前若干棵树。 5. GBDT与XGBoost的比较以下来自知乎，作者：wepon 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。 为了限制树的生长，加入阈值gamma，当增益大于阈值时才让节点分裂，它是正则项里叶子节点数J的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性 参考 kimmyzhang - GBDT详解上 + 下 + 后补 Friedman - Greedy Function Approximation:A Gradient Boosting Machine GBDT(MART)迭代决策树入门教程简介 雪伦 - xgboost原理 机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？ Tianqi Chen - XGBoost_A Scalable Tree Boosting System]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adaboost & 前向分布算法]]></title>
    <url>%2F2018%2F04%2F10%2Fadaboost%2F</url>
    <content type="text"><![CDATA[Adaboost & 前向分布算法目录 Adaboost 前向分布算法 前向分布算法推导Adaboost Adaboost特点 1. AdaboostAdaboost提高那些被前一轮弱学习器错误分类样本的权值，而降低那些被正确分类样本的权值。这样没有正确分类的样本在下一轮学习中将获得更大的关注；之后Adaboost采用加权多数表决的方法，加大错误率低的弱学习器的权值，减小错误率高的弱学习器的权值。 1.1 Adaboost算法输入：训练数据集\(T =\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \rbrace \)，其中\( x_i \in \chi \subseteq R^n, y_i \in \mathcal{y} = \lbrace -1, +1 \rbrace \)； 弱学习器学习算法； 输出：最终的强学习器\(G(x)\) （1）初始化训练数据的权值分布 \[ D_1 = (w_{11}, w_{12}, …, w_{1n}), \quad w_{1i}=\frac{1}{n}, \quad i=1,2,…,n \] 假设训练数据集具有均匀分布的权值分布，保证第1步能在原始数据集上学习弱学习器\(G_1(x)\)。 （2）对\(m=1,2,…,M\) （2.1）使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器 \[ G_m(x):\chi \longrightarrow \lbrace -1,+1 \rbrace \] （2.2）计算\(G_m(x)\)在训练数据集上的误差率 \[ e_m = \sum_{i=1}^n P(G_m(x_i) \not= y_i) = \sum_{i=1}^n w_{mi}I(G_m(x_i) \not= y_i) \] （2.3）计算\(G_m(x_i)\)的系数 \[ \alpha_m = \frac{1}{2} ln \frac{1-e_m}{e_m} \] 这里可知，当\(e_m \leq \frac{1}{2}时，\alpha_m \geq 0\)，并且\(\alpha_m\)随着\(e_m\)的减小而增大，所以误差率小的弱学习器在最终的强学习器中具有更大的作用。当\( e_m &gt; \frac{1}{2} \)算法停止。 （2.4）更新训练数据集的权值分布 \[ D_{m+1} = (w_{m+1,1}, w_{m+1,2}, …, w_{m+1,n}) \\ w_{m+1,i} = \frac{w_{mi}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), \quad i=1,2,…,n \] 这里的\(Z_m\)是规范化因子，它使\(D_{m+1}\)成为一个概率分布。 \[ Z_m = \sum_{i=1}^n w_{mi} exp(-\alpha_m y_i G_m(x_i)) \] （3）构建弱学习器的线性组合 \[ f(x) = \sum_{m=1}^M \alpha_m G_m(x) \] 得到最终强学习器 \[ G(x) = sign(f(x)) = sign( \sum_{m=1}^M \alpha_m G_m(x)) \] Adaboost的例子见《统计学习方法》page,140. Adaboost的另外一个解释：认为Adaboost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法的二类分类学习方法。 2. 前向分布算法输入：训练数据集\(T=\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \rbrace\)；损失函数\(L(y,f(x))\)；基函数集\( \lbrace b(x;\gamma) \rbrace \)； 输出：加法模型\(f(x)\)。 （1）初始化\(f_0(x)=0\) （2）对\(m=1,2,…,M\) （2.1）极小化损失函数 \[ (\beta_m, \gamma_m) = \mathop{\arg\min_{\beta,\gamma}} \sum_{i=1}^n L(y_i, f_{m-1}(x_i)+\beta b(x_i;\gamma)) \] 得到参数\(\beta_m, \gamma_m\)。 （2.2）更新 \[ f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m) \] （3）得到加法模型 \[ f(x) = f_M(x) = \sum_{i=1}^M \beta_m b(x;\gamma_m) \] 这样，前向分布算法将同时求解\(m=1\)到\(M\)所有参数\(\beta_m, \gamma_m\)的优化问题简化为逐次求解各个\(\beta_m, \gamma_m\)的优化问题。 3. 前向分布算法推导Adaboost前向分布算法学习的是加法模型，所以该加法模型等价于Adaboost的最终的强学习器 \[ f(x) = \sum_{m=1}^M \alpha_m G_m(x) \] 前向分步算法逐一学习基函数与Adaboost逐一学习弱分类器的过程一致。 下面证明前向分布算法的损失函数是指数损失函数时，其学习的具体操作等价于Adaboost算法学习的具体操作。 \[ L(y, f(x)) = exp[-y f(x)] \] 假设经过\(m-1\)轮迭代，前向分布算法得到\(f_{m-1}(x)\)： \[ \begin{equation} \begin{aligned} f_{m-1}(x) = &amp; = f_{m-2}(x) + \alpha_{m-1}G_{m-1}(x) \\ &amp; = \alpha_1 G_1(x) + … + \alpha_{m-1} G_{m-1}(x) \end{aligned} \end{equation} \] 在第\(m\)次迭代得到\(\alpha_m, G_m(x)和f_m(x)\)。 \[ f_m(x) = f_{m-1}(x) + \alpha_m G_m(x) \] 我们的目标是使用前向分步算法，得到\(\alpha_m, G_m(x)\)使\(f(x)\)在训练数据集上的指数损失最小，即 \[ (\alpha_m, G_m(x)) = \mathop{\arg\min_{\alpha, G}} \sum_{i=1}^n exp[-y_i (f_{m-1}(x_i) + \alpha G(x_i))] \] 重新整理\(式(16)\)得： \[ (\alpha_m, G_m(x)) = \mathop{\arg\min_{\alpha, G}} \sum_{i=1}^n \bar{w}_{mi} exp[-y_i \alpha G(x_i)] \] 其中\(\bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)]\)，由于\(\bar{w}_{mi}\)既不依赖\(\alpha\)也不依赖于\(G\)，所以与最小化无关。 首先求\( G_m^{\ast} \)， 对任意\(\alpha &gt; 0\)，使\( 式(8.21) \)最小的\(G(x)\)由下式得到： \[ G_m^{\ast} = \mathop{\arg\min_G} \sum_{i=1}^n \bar{w}_{mi} I(y_i \not= G(x_i)) \] 之后求\(\alpha_m^{\ast}\)，令： \[ \begin{equation} \begin{aligned} W &amp; = \sum_{i=1}^n \bar{w}_{mi} exp[ - y_i \alpha G(x_i)] \\ &amp; = \sum_{y_i = G_m(x_i)} \bar{w}_{mi} e^{-\alpha} + \sum_{y_i \not= G_m(x_i)} \bar{w}_{mi} e^{\alpha} \\ &amp; = e^{-\alpha} \sum_{y_i = G_m(x_i)} \bar{w}_{mi} + e^{\alpha} \sum_{y_i \not= G_m(x_i)} \bar{w}_{mi} \\ &amp; = e^{-\alpha} \lbrace \sum_{i=1}^n \bar{w}_{mi} - \sum_{y_i \not= G_m(x_i)} \bar{w}_{mi} \rbrace + e^{\alpha} \sum_{y_i \not= G_m(x_i)} \bar{w}_{mi} \\ &amp; = e^{-\alpha} \sum_{i=1}^n \bar{w}_{mi} + (e^{\alpha} - e^{-\alpha}) \sum_{i=1}^n \bar{w}_{mi} I(y_i \not= G_m(x_i)) \end{aligned} \end{equation} \] 将式\((18)\)求得的\( G_m^{\ast}(x) \)代入上式，对\(\alpha\)求导并令导数为零，即得： \[ \frac{\partial W}{\partial \alpha} = - e^{-\alpha} \sum_{i=1}^n \bar{w}_{mi} + (e^{\alpha} + e^{- \alpha}) \sum_{i=1}^n \bar{w}_{mi} I(y_i \not= G_m^{\ast}(x_i)) = 0 \\ \Downarrow \\ e^{-\alpha} \sum_{i=1}^n \bar{w}_{mi} = (e^{\alpha} + e^{- \alpha}) \sum_{i=1}^n \bar{w}_{mi} I(y_i \not= G_m^{\ast}(x_i)) \\ \Downarrow \\ \alpha_m^{\ast} = \frac{1}{2} log \frac{1-e_m}{e_m} \\ where, \quad e_m = \frac{\sum_{i=1}^n \bar{w}_{mi} I(y_i \not= G_m(x_i))}{\sum_{i=1}^n \bar{w}_{mi}} \] 这里的\(\alpha_m^{\ast}\)与Adaboost算法第(2.3)步的\(\alpha_m\)完全一致。 接下来求权值更新公式： \[ \begin{cases} f_m(x) = f_{m-1}(x) + \alpha_m G_m(x) \\ \bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)] \end{cases} \quad \Longrightarrow \quad \bar{w}_{m+1,i} = \bar{w}_{m,i}exp[-y_i \alpha_m G_m(x)] \] 这与Adaboost算法的第(2.4)步的样本均值的更新，只相差规范化因子，因而等价。 4. Adaboost特点 优点：分类精度高；灵活的弱学习器；模型简单易解释；不易过拟合 缺点：异常值敏感 参考 李航 - 《统计学习方法》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用sklearn大规模机器学习]]></title>
    <url>%2F2018%2F04%2F03%2Fbigscaledataml%2F</url>
    <content type="text"><![CDATA[[转]使用sklearn大规模机器学习 目录 核外学习(out-of-core learning) 磁盘上数据流式化 sklearn 中的 SGD 流式数据中的特征工程 总结 转载自:吴良超的学习笔记核外学习(out-of-core learning)核外学习指的是机器的内存无法容纳训练的数据集，但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。 sampling（采样）采样可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。 解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。 mini-batch learning（小批量学习/增量学习）这种方法不同于 sampling，利用了全部的数据，只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。 通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息（如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负）。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。 磁盘上数据流式化文件读取这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现 123456with open(source_file, &apos;r&apos;) as fp: line = fp.readline() while line: # data processing # training line = fp.readline() 而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取， 下面以这个数据文件为例说明： Bike-Sharing-Dataset.zip 备用地址:Bike-Sharing-Dataset.zip, 1234567891011121314import csvSEP = &quot;,&quot;with open(source_file, &apos;r&apos;) as fp: iterator = csv.reader(fp, delimiter=SEP) for n, row in enumerate(iterator): if n == 0: header = row else: # data processing # training pass print(&apos;Total rows: %i&apos; % (n+1)) print(&apos;Header: %s&apos; % &apos;, &apos;.join(header)) print(&apos;Sample values: %s&apos; % &apos;, &apos;.join(row)) 输出为 123Total rows: 17380Header: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cntSample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49 在上面的例子中，每个样本是就是一个 row（list类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为 12345678910import csvSEP = &quot;,&quot;with open(source_file, &apos;r&apos;) as R: iterator = csv.DictReader(R, delimiter=SEP) for n, row in enumerate(iterator): # data processing # training pass print (&apos;Total rows: %i&apos; % (n+1)) print (&apos;Sample values: %s&apos; % row) 输出为 12Total rows: 17379Sample values: &#123;&apos;temp&apos;: &apos;0.26&apos;, &apos;cnt&apos;: &apos;49&apos;, &apos;yr&apos;: &apos;1&apos;, &apos;windspeed&apos;: &apos;0.1343&apos;, &apos;casual&apos;: &apos;12&apos;, &apos;registered&apos;: &apos;37&apos;, &apos;season&apos;: &apos;1&apos;, &apos;weathersit&apos;: &apos;1&apos;, &apos;dteday&apos;: &apos;2012-12-31&apos;, &apos;hr&apos;: &apos;23&apos;, &apos;weekday&apos;: &apos;1&apos;, &apos;atemp&apos;: &apos;0.2727&apos;, &apos;hum&apos;: &apos;0.65&apos;, &apos;holiday&apos;: &apos;0&apos;, &apos;instant&apos;: &apos;17379&apos;, &apos;mnth&apos;: &apos;12&apos;, &apos;workingday&apos;: &apos;1&apos;&#125; 除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示: 12345678910import pandas as pdCHUNK_SIZE = 1000with open(source_file, &apos;rb&apos;) as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, data_chunk in enumerate(iterator): print (&apos;Size of uploaded chunk: %i instances, %i features&apos; % (data_chunk.shape)) # data processing # training pass print (&apos;Sample values: \n%s&apos; % str(data_chunk.iloc[0])) 对应的输出为 12345678910111213141516171819202122232425262728293031323334353637Size of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 1000 instances, 17 featuresSize of uploaded chunk: 379 instances, 17 featuresSample values: instant 17001dteday 2012-12-16season 4yr 1mnth 12hr 3holiday 0weekday 0workingday 0weathersit 2temp 0.34atemp 0.3333hum 0.87windspeed 0.194casual 1registered 37cnt 38Name: 17000, dtype: object 数据库读取上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不仅能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。 假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下(未验证) 12345678910111213141516import sqlite3import pandas as pdDB_NAME = &apos;bikesharing.sqlite&apos;CHUNK_SIZE = 2500conn = sqlite3.connect(DB_NAME)conn.text_factory = str # allows utf-8 data to be stored sql = &quot;SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)&quot;DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)for j,data_chunk in enumerate(DB_stream): print (&apos;Chunk %i -&apos; % (j+1)), print (&apos;Size of uploaded chunk: %i istances, %i features&apos; % (data_chunk.shape)) # data processing # training pass 输出为 1234567Chunk 1 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 2 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 3 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 4 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 5 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 6 - Size of uploaded chunk: 2500 istances, 18 featuresChunk 7 - Size of uploaded chunk: 2379 istances, 18 features 样本的读取顺序上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。 假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。 这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。 假如内存能够容纳这些数据，那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接，下面是这两种 shuffle 方法的实现代码。 在内存中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下 1234567891011121314import zlibfrom random import shuffledef ram_shuffle(filename_in, filename_out, header=True): with open(filename_in, &apos;r&apos;) as f: zlines = [zlib.compress(line, 9) for line in f] if header: first_row = zlines.pop(0) shuffle(zlines) with open(filename_out, &apos;w&apos;) as f: if header: f.write(zlib.decompress(first_row)) for zline in zlines: f.write(zlib.decompress(zline)) 基于磁盘的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下 12345678910111213141516171819202122232425from random import shuffleimport pandas as pdimport numpy as npimport osdef disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=&apos;,&apos;): for i in range(iterations): with open(filename_in, &apos;rb&apos;) as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, df in enumerate(iterator): if n==0 and header: header_cols =SEP.join(df.columns)+&apos;\n&apos; df.iloc[np.random.permutation(len(df))].to_csv(str(n)+&apos;_chunk.csv&apos;, index=False, header=False, sep=SEP) ordering = list(range(0,n+1)) shuffle(ordering) with open(filename_out, &apos;wb&apos;) as W: if header: W.write(header_cols) for f in ordering: with open(str(f)+&apos;_chunk.csv&apos;, &apos;r&apos;) as R: for line in R: W.write(line) os.remove(str(f)+&apos;_chunk.csv&apos;) filename_in = filename_out CHUNK_SIZE = int(CHUNK_SIZE / 2) sklearn 中的 SGD通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。 需要注意的参数有： loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等 penalty：正则项，用于防止过拟合(默认为 L2 正则项) learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档 需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。 需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。 12345678910111213from sklearn import linear_modelimport pandas as pdCHUNK_SIZE = 1000n_iter = 10 # number of iteration over the whole datasetn_class = 7model = linear_model.SGDClassifier(loss = &apos;hinge&apos;, penalty =&apos;l1&apos;,)for _ in range(n_iter): with open(source_file, &apos;rb&apos;) as R: iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) for n, data_chunk in enumerate(iterator): model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class))) 流式数据中的特征工程feature scaling对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。 由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为(\(\mu\)为均值)： \[ \sigma ^2= \frac{1}{n} \sum_x (x−μ)^2 \] 只有知道均值才能求解，这意味着只有遍历一次求得\(\mu\)后才能求\(\sigma^2\)，这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得\(\mu和 \sigma^2 \)能够同时求出。假如当前有 n 个样本，当前的均值 \( \mu’ \) 可以简单求出，而当前的方差 \( \sigma’^2 \) 可通以下公式求解 \[ \sigma′^2 = \frac{1}{n} \sum_x (x^2 − 2 x \mu′+\mu′^2 ) = \frac{1}{n} \sum_x (x^2) − \frac{1}{n} (2n\mu′^2 − n \mu′^2)=\frac{1}{n}\sum_x (x^2) − \mu′^2 \] 通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。 12345678910111213141516171819202122232425262728293031323334# calculate the running mean,standard deviation, and range reporting the final resultimport os, csvraw_source = &apos;bikesharing/hour.csv&apos; # unshuffleshuffle_source = &apos;bikesharing/shuffled_hour.csv&apos;def running_statistic(source): SEP=&apos;,&apos; running_mean = list() running_std = list() with open(local_path+&apos;/&apos;+source, &apos;rb&apos;) as R: iterator = csv.DictReader(R, delimiter=SEP) x = 0.0 x_squared = 0.0 for n, row in enumerate(iterator): temp = float(row[&apos;temp&apos;]) if n == 0: max_x, min_x = temp, temp else: max_x, min_x = max(temp, max_x),min(temp, min_x) x += temp x_squared += temp**2 running_mean.append(x / (n+1)) running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5) # DATA PROCESSING placeholder # MACHINE LEARNING placeholder pass print (&apos;Total rows: %i&apos; % (n+1)) print (&apos;Feature \&apos;temp\&apos;: mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f&apos; \ % (running_mean[-1], max_x, min_x, running_std[-1])) return running_mean, running_stdprint &apos;===========raw data file===========&apos;raw_running_mean, raw_running_std = running_statistic(raw_source)print &apos;===========shuffle data file===========&apos;shuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source) 输出如下 123456===========raw data file===========Total rows: 17379Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193===========shuffle data file===========Total rows: 17379Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193 两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化 12345678910111213# plot how such stats changed as data was streamed from disk# get an idea about how many instances are required before getting a stable mean and standard deviation estimateimport matplotlib.pyplot as plt%matplotlib inlinefor mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)): plt.plot(mean,&apos;r-&apos;, label=&apos;mean&apos;) plt.plot(std,&apos;b-&apos;, label=&apos;standard deviation&apos;) plt.ylim(0.0,0.6) plt.xlabel(&apos;Number of training examples&apos;) plt.ylabel(&apos;Value&apos;) plt.legend(loc=&apos;lower right&apos;, numpoints= 1) plt.show()# The difference in the two charts reminds us of the importance of randomizing the order of the observations. 得到的结果如下 原始的文件 shuffle 后的文件 可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。 hasing trick对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。 hahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子 1234from sklearn.feature_extraction.text import HashingVectorizerh = HashingVectorizer(n_features=1000, binary=True, norm=None)sparse_vector = h.transform([&apos;A simple toy example will make clear how it works.&apos;])print(sparse_vector) 输出如下 123456789(0, 61) 1.0(0, 271) 1.0(0, 287) 1.0(0, 452) 1.0(0, 462) 1.0(0, 539) 1.0(0, 605) 1.0(0, 726) 1.0(0, 918) 1.0 这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。 总结本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新， 在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machines - Part 4]]></title>
    <url>%2F2018%2F03%2F28%2Fsvm-4%2F</url>
    <content type="text"><![CDATA[支持向量机 - SVM（Support Vector Machines）Part 4支持向量机 - SVM（Support Vector Machines）Part 1 线性可分支持向量机学习算法 - 最大间隔法 线性可分支持向量机的对偶算法 支持向量机 - SVM（Support Vector Machines）Part 2 线性支持向量机 核函数及非线性支持向量机 常用的核函数及其特点 支持向量机 - SVM（Support Vector Machines）Part 3 序列最小最优化(SMO)算法 支持向量机 - SVM（Support Vector Machines）Part 4 支持向量回归 - SVR 目录 支持向量回归 - SVR 支持向量回归 - SVR回归和分类从某种意义上讲，本质上是一回事。 SVM分类，就是找到一个平面，让两个分类集合的支持向量或者所有的数据离分类平面最远； SVR回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。 假设某个元素到回归平面的距离为$ r = d(x) − g(x) $ 。另外，由于数据不可能都在回归平面上，距离之和还是挺大，因此所有数据到回归平面的距离可以给定一个容忍值$ \epsilon$ 防止过拟合。该参数是经验参数，需要人工给定。如果数据元素到回归平面的距离小于$ \epsilon $，则代价为0。SVR的代价函数可以表示为： cost(x) = \max ( 0, \quad \| d(x)-g(x) \| - \epsilon )其中$d(x)$为样本的标签。考虑松弛变量$ \zeta_i, \zeta_i^{\ast} $，分别表示上下边界的松弛因子，约束条件即为： \begin{cases} d(x_i) - g(x_i) < \epsilon + \zeta_i, \qquad \zeta_i \geq 0 \\ g(x_i) - d(x_i) < \epsilon + \zeta_i^{\ast}, \quad \zeta_i^{\ast} \geq 0 \end{cases}实际上要最小化$ \zeta_i, \zeta_i^{\ast} $，为了获得$w$的稀疏解，且假设$w$的计算结果满足正态分布，根据贝叶斯线性回归模型，对$w$有$L_2$范数约束。 SVR可以转化为最优化问题： \psi (x) = \sum_i (\zeta_i + \zeta_i^{\ast}) + \frac{1}{2} C' w^T w \\ \Downarrow \\ \psi (x) = C \sum_i (\zeta_i + \zeta_i^{\ast}) + \frac{1}{2} w^T w其中$C$是惩罚因子，是人为给定的经验参数。考虑约束条件，引入拉格朗日算子$\alpha, \alpha^{\ast}, \beta, \beta^{\ast}$， 将最优化问题转化为对偶问题： J=\frac{1}{2} w^T w + C \sum_i (\zeta_i + \zeta_i^{\ast}) +\sum_i \alpha_i \left[ d(x_i) − g(x_i) − \epsilon − \zeta_i \right] + \sum_i \alpha_i^{\ast} \left[ g(x_i) − d(x_i) − \epsilon - \zeta_i^{\ast} \right] −\sum_i \beta_i \zeta_i − \sum_i \beta_i^{\ast} \zeta_i^{\ast}然后分别求导得到： \begin{equation*} \begin{aligned} \frac{\partial J}{\partial w} &= w − ( \sum_i \alpha_i x_i − \sum_i \alpha_i^{\ast} x_i) = 0 \\ \frac{\partial J}{\partial b} &= \sum_i ( \alpha_i − \alpha_i^{\ast} ) = 0 \\ \frac{\partial J}{\partial \zeta_i} &= C − \alpha_i − \beta_i = 0 \\ \frac{\partial J}{\partial \zeta_i^{\ast}} &= C − \alpha_i^{\ast} − \beta_i^{\ast} = 0 \\ C &= \alpha_i + \beta_i = \alpha_i^{\ast} + \beta_i^{\ast} \\ \end{aligned} \end{equation*}将上述式子代入$J$函数有： \begin{equation*} \begin{aligned} J &= \frac{1}{2}w^Tw − \sum_i (\alpha_i − \alpha_i^{\ast}) w x_i − b \sum_i (\alpha_i − \alpha_i^{\ast}) − \sum_i (\alpha_i + \alpha_i^{\ast}) \epsilon + \sum_i (\alpha_i − \alpha_i^{\ast})d(x_i) \\ &+ \underbrace{C \sum_i (\zeta_i + \zeta_i^{\ast})− \sum_i \alpha_i \zeta_i − \sum_i \alpha_i^{\ast} \zeta_i^{\ast} − \sum_i (C−\alpha_i)\zeta_i − \sum_i(C−\alpha_i^{\ast}) \zeta_i^{\ast}}_{这部分可以相互抵消，变为0} \\ &= \frac{1}{2} (\sum_i \alpha_i x_i - \sum_i \alpha_i^{\ast} x_i)(\sum_j \alpha_j x_j - \sum_j \alpha_j^{\ast} x_j) − (\sum_i \alpha_i x_i - \sum_i \alpha_i^{\ast} x_i)(\sum_j \alpha_j x_j - \sum_j \alpha_j^{\ast} x_j) − \sum_i ( \alpha_i + \alpha_i^{\ast}) \epsilon + \sum_i (\alpha_i − \alpha_i^{\ast}) d(x_i) \\ &= − \frac{1}{2} \sum_i \sum_j (\alpha_i − \alpha_i^{\ast} ) ( \alpha_j − \alpha_j^{\ast} ) x_i x_j − \sum_i (\alpha_i + \alpha_i^{\ast}) \epsilon + \sum_i (\alpha_i − \alpha_i^{\ast})d(x_i) \\ s.t. \qquad 0 \leq \quad \alpha_i, \alpha_i^{\ast} \quad \leq C \end{aligned} \end{equation*}其中$\zeta, \zeta^{\ast}, \beta, \beta^{\ast}$都在计算过程中抵消了。$\epsilon, C$则是人为给定的参数，是常量。如果要使用核函数，可以将上式写成： J = − \frac{1}{2} \sum_i \sum_j (\alpha_i − \alpha_i^{\ast} ) (\alpha_j − \alpha_j^{\ast}) K(x_i, x_j) − \sum_i ( \alpha_i + \alpha_i^{\ast} ) \epsilon + \sum_i ( \alpha_i − \alpha_i^{\ast})d(x_i)SVR的代价函数和SVM的很相似，但是最优化的对象却不同，对偶式有很大不同，解法同样都是基于拉格朗日的最优化问题解法。 求解这类问题的早期解法非常复杂，后来出来很多新的较为简单的解法，对数学和编程水平要求高，对大部分工程学人士来说还是颇为复杂和难以实现， 因此大牛们推出了一些SVM库。比较出名的有libSVM，该库同时实现了SVM和SVR。 &gt; SVR支持向量机回归 &gt;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machines - Part 2]]></title>
    <url>%2F2018%2F03%2F28%2Fsvm-2%2F</url>
    <content type="text"><![CDATA[支持向量机 - SVM（Support Vector Machines）Part 2支持向量机 - SVM（Support Vector Machines）Part 1 线性可分支持向量机学习算法 - 最大间隔法 线性可分支持向量机的对偶算法 支持向量机 - SVM（Support Vector Machines）Part 2 线性支持向量机 核函数及非线性支持向量机 常用的核函数及其特点 支持向量机 - SVM（Support Vector Machines）Part 3 序列最小最优化(SMO)算法 支持向量机 - SVM（Support Vector Machines）Part 4 支持向量回归 - SVR 目录 线性支持向量机 核函数及非线性支持向量机 常用的核函数及其特点 1. 线性支持向量机线性可分支持向量机的学习方法对于线性不可分训练数据是不适用的，因为问题中的不等式约束并不能都成立，所以我们要修改函数间隔。在支持向量机 - SVM（Support Vector Machines）Part 1中的线性可分支持向量机的约束条件为$y_i(w\cdot x_i +b) \geq 1$，称为硬间隔最大化；现在我们对每个样本$(x_i, y_i)$引入一个松弛变量$\xi_i \geq 0$,使得函数间隔加上松弛变量大于等于1，此时的约束条件就变为 y_i(w\cdot x_i +b) \geq 1 - \xi_i同时，对每个松弛变量也要支付一个代价$\xi_i$，目标函数也由原来的$\frac{1}{2} || w ||^2$变为 \frac{1}{2} || w ||^2 + C\sum_{i=1}^n \xi_i相对于硬间隔最大化，此时称为软间隔最大化。 线性不可分的线性支持向量机的学习问题变为凸二次规划问题(原始问题)： \mathop{\min_{w,b,\xi}} \frac{1}{2}||w||^2 + C \sum_{i=1}^n \xi_i s.j. \quad y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad i=1,2,...,n \xi_i \geq 0, \quad, i=1,2,...,n下面求原始问题的对偶问题： 原始问题的拉格朗日函数是 \begin{equation} \begin{aligned} L(w,b,\xi,\alpha,\mu) & = \frac{1}{2}||w||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i(y_i(w \cdot x_i + b)-1+\xi_i) - \sum_{i=1}^n \mu_i \xi_i \\ & = \frac{1}{2}||w||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i y_i (w \cdot x +b) + \sum_{i=1}^n \alpha_i - \sum_{i=1}^n \alpha_i \xi_i - \sum_{i=1}^n \mu_i \xi_i \\ s.t. \quad \alpha_i \geq 0, \quad \mu_i \geq 0 \end{aligned} \end{equation}原始问题为 \mathop{\min_{w,b,\xi} \max_{\alpha, \mu}} L(w,b,\xi, \alpha, \mu)对偶问题为 \mathop{\max_{\alpha, \mu} \min_{w,b,\xi}} L(w,b,\xi, \alpha, \mu)因此，先求$L(w,b,\xi,\alpha,\mu)$对$w,b,\xi$的极小 \nabla _w L(w,b,\xi,\alpha,\mu) = w - \sum_{i=1}^n \alpha_i y_i x_i = 0 \\ \nabla _b L(w,b,\xi,\alpha,\mu) = - \sum_{i=1}^n \alpha_i y_i = 0 \\ \nabla_{\xi_i} L(w,b,\xi,\alpha,\mu) = C - \alpha_i - \mu_i = 0得 w = \sum_{i=1}^n \alpha_i y_i x_i \sum_{i=1}^n \alpha_i y_i = 0 C - \alpha_i - \mu_i = 0将$ 式(10) \sim 式(12) $代入$式(6)$，得 \mathop{\min_{w,b,\xi}} L(w,b,\xi,\alpha,\mu) = - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^n \alpha_i再对$\mathop{\min_{w,b,\xi}} L(w,b,\xi,\alpha,\mu)$求$\alpha$的极大，即得对偶问题 \mathop{\max_{\alpha}} - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (x_i^T \cdot x_j) + \sum_{i=1}^n \alpha_i s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 \\ C - \alpha_i - \mu_i = 0 \\ \alpha_i \geq 0 \\ \mu_i \geq 0, \quad i=1,2,...,n调整化简约束条件，得 \begin{cases} C - \alpha_i - \mu_i = 0 \\ \alpha_i \geq 0 \\ \mu_i \geq 0 \end{cases} \Longrightarrow \begin{cases} \mu_i = C - \alpha_i \geq 0 \\ \alpha_i \geq 0 \end{cases} \Longrightarrow 0 \leq \alpha_i \leq C因此，原始问题的对偶问题为 \mathop{\min_{\alpha}} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (x_i^T \cdot x_j) - \sum_{i=1}^n \alpha_i s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 0 \leq \alpha_i \leq C, \quad i=1,2,...,n至此，我们得到一个和线性可分支持向量机对偶问题类似的问题，所以同样需要使用序列最小最优化(SMO)算法解决，详见Support Vector Machines - Part 3。 假设已经求出了$ \alpha^{\ast}=(\alpha_1 ^{\ast}, \alpha_2 ^{\ast}, …, \alpha_n ^{\ast})^T $是对偶问题的解， 若存在$0 \leq \alpha_i \leq C$, 则利用$\alpha^{\ast}$求$w^{\ast}$。 原始问题是凸二次规划问题，解满足$KKT$条件，即得(下列公式也见《统计学习方法》page.111) \nabla _w L(w^{\ast},b^{\ast},\xi,\alpha,\mu) = w^{\ast} - \sum_{i=1}^n \alpha_i^{\ast} y_i x_i = 0 \\ \nabla _b L(w^{\ast},b^{\ast},\xi,\alpha,\mu) = - \sum_{i=1}^n \alpha_i^{\ast} y_i = 0 \\ \nabla_{\xi_i} L(w^{\ast},b^{\ast},\xi,\alpha,\mu) = C - \alpha_i^{\ast} - \mu_i^{\ast} = 0 \alpha_i^{\ast} (y_i (w^{\ast} \cdot x_i + b^{\ast}) -1 + \xi_i^{\ast}) = 0 \mu_i^{\ast} \xi_i^{\ast} = (C - \alpha_i^{\ast}) \xi_i = 0 \\ y_i (w^{\ast}\cdot x_i + b^{\ast}) -1 + \xi_i^{\ast} \geq 0 \\ \xi_i^{\ast} \geq 0 \\ \alpha_i^{\ast} \geq 0 \\ \mu_i^{\ast} \geq 0, \quad i=1,2,...,n设我们找到一个支持向量$\alpha_i^{\ast} &gt; 0$，则由$ 式(21)$可以得到 y_i (w^{\ast}\cdot x_i + b^{\ast}) -1 + \xi_i^{\ast} = 0 \Longrightarrow b^{\ast} = y_i - \xi_i^{\ast} y_i - w^{\ast} \cdot x_i想要求$b^{\ast}$就必须先求$\xi_i^{\ast}$，没办法得到$\xi_i^{\ast}$，但是如果$\xi_i^{\ast}=0$的话，那么 b^{\ast} = y_i - w^{\ast} \cdot x_i所以为了让$\xi_i^{\ast}=0$，那么由$ 式(22) $可知，$C - \alpha_i^{\ast} \not= 0$，即$ \alpha_i^{\ast} \not= C \Rightarrow \alpha_i^{\ast} &lt;C $。 所以，要求解$ b^{\ast} $，则需要找到一个$ 0 &lt; \alpha_i^{\ast} &lt; C $，那么相应的$b^{\ast} $就可以用$式(24)$进行计算。 综合上面的内容，可以看到线性支持向量机(软间隔)和线性可分支持向量机(硬间隔)几乎是一样的。 相对于硬间隔来说，软间隔更灵活，可以通过调节$C$的值来控制，关心分隔超平面的间隔更大一些还是分类错误少一些，并且也不要求我们的数据是线性可分的，所以软间隔比硬间隔更加具有实际的应用价值。 对于$\alpha_i &gt; 0$的样本点$(x_i,y_i)$称为支持向量(软间隔的支持向量)，如下图所示，分离超平面由实线表示，间隔平面用虚线表示。 $0 &lt; \alpha_i &lt; C, 则\xi_i=0, \quad $分类正确，支持向量落在间隔边界上[类似线性可分支持向量机] $\alpha_i = C, 0 &lt; \xi_i &lt; 1, \quad $分类正确，支持向量落在间隔边界和分离超平面之间 $\alpha_i = C, \xi_i=1, \quad $，支持向量落在分离超平面上 $\alpha_i = C, \xi_i&gt;1, \quad $，分类错误，支持向量落在超平面另外一侧。 2. 核函数及非线性支持向量机目前为止，SVM还只能处理线性或者近似线性的情况，下面引入核函数，进而将SVM推广到非线性问题上。 对于一个数据点$x$分类，实际上就是通过把$x$带入到$f(x)=wx+b$中算出结果，然后根据结果的正负进行类别的划分，在之前的推导中，我们得到 w^{\ast} = \sum_{i=1}^n \alpha_i^{\ast} y_i x_i因此分类函数为 \begin{equation} \begin{aligned} f(x) & = (\sum_{i=1}^n \alpha_i^{\ast} y_i x_i)^T x + b \\ & = \sum_{i=1}^n \alpha_i^{\ast} y_i \langle x_i,x \rangle + b \end{aligned} \end{equation}其中$ \langle.,. \rangle $表示内积。这里比较有趣的地方是，对于新点$x$的预测，只需要计算它与训练数据点的内积即可，这是使用核函数$kernel$进行非线性推广的基本前提， 事实上，所有非支持向量的系数$\alpha^{\ast}$都是等于零的，因此对于新点的内积运算，不需要针对所有的训练数据，实际上只要针对少量的支持向量就可以了。 为什么非支持向量的系数为零? 直观解释：间隔边界后面的点对分离超平面的确定没有影响，所以这些无关的点不会参与到分类问题的计算中。 公式解释：在线性可分支持向量机(硬间隔)中，我们得到了拉格朗日函数 \begin{equation} \begin{aligned} \theta(w) & = \mathop{\max_{\alpha_i \geq 0}} L(w,b,\alpha) \\ & = \mathop{\max_{\alpha_i \geq 0}} \frac{1}{2}||w||^2 - \sum_{i=1}^n \alpha_i (y_i(wx_i +b)-1) \\ \end{aligned} \end{equation} \alpha_i \geq 0, \quad i=1,2,...,nNotice: 如果$ x_i $是支持向量的话，那么上式右面$ y_i(wx_i +b)-1$部分是等于零的，因为支持向量的函数间隔等于1； 而对于非支持向量，函数间隔大于零，所以$ y_i(wx_i +b)-1$部分是大于零的，所以为了满足拉格朗日最大化，$\alpha_i$必须等于零。 下面开始正式介绍核函数$Kernel$ 设$\chi$是输入空间(欧式空间$R^n$的子集或离散集合)，又设$\mathcal{H}$为特征空间(希尔伯特空间[完备的内积空间])，如果存在一个从$\chi$到$\mathcal{H}$的映射 \varphi(x): \chi \to \mathcal{H}使得对所有$x,z \in \chi $，函数$K(x,z)$满足条件 K(x,z) = \varphi(x) \cdot \varphi(z)则称$K(x,z)$为核函数，$\varphi(x)$为映射函数，式中$\varphi(x) \cdot \varphi(z)$为$\varphi(x)和\varphi(z)$的内积。 核函数的想法是，在学习和预测中只定义核函数$K(x,z)$，而不显式地定义映射函数$\varphi(x)$。通常直接计算$K(x,z)$比较容易，而通过$\varphi(x)和\varphi(z)$计算$K(x,z)$并不容易。 这样，处理非线性数据的情况时，线性支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中学习线性分离超平面，从而把原始空间中不好分的非线性数据分开。 这样分类决策函数变为 \begin{equation} \begin{aligned} f(x) & = (\sum_{i=1}^n \alpha_i y_i x_i)^T x + b \\ & = \sum_{i=1}^n \alpha_i y_i K(x_i, x) + b \end{aligned} \end{equation}[其余核函数的举例不再赘述] 维度爆炸的问题！！ 按照核函数的定义[先找映射函数，之后再在高维空间中计算内积]，这样是很容易出现维度爆炸问题的。具体例子如下： 设对向量$ x=(u_1, u_2)^T $，映射的结果为 \varphi(x)=u_1+u_1^2+u_2+u_2^2+u_1u_2又由于$ x_1=(\eta_1, \eta_2)^T, x_2=(\xi_1, \xi_2)^T $，则$x_1, x_2$的映射如下 \varphi(x_1) = (\eta_1, \eta_1^2, \eta_2, \eta_2^2, \eta_1\eta_2) \\ \varphi(x_2) = (\xi_1, \xi_2^2, \xi_2, \xi_2^2, \xi_1\xi_2)上面两式做内积运算即可得到 \langle \varphi(x_1), \varphi(x_2) \rangle = \eta_1 \xi_1 + \eta_1^2 \xi_2^2 + \eta_2 \xi_2 + \eta_2^2 \xi_2^2 + \eta_1\eta_2 \xi_1\xi_2另外，如果我们计算下式 (\langle x_1, x_2 \rangle + 1)^2 = 2 \eta_1 \xi_1 + \eta_1^2 \xi_2^2 + \eta_2 \xi_2 + \eta_2^2 \xi_2^2 + 2 \eta_1\eta_2 \xi_1\xi_2 + 1对比$式(33)和式(34)$，两者有很多相似的地方。实际上，我们只要把映射的某几个维度线性缩放一下，然后再加上一个常数维度，计算出来的结果和经过映射的两向量内积的结果是相等的。 具体来说，如果对于向量$x=(a,b)^T$，设置新映射如下 \phi = (\sqrt{2}a, a^2, \sqrt{2}b, b^2, \sqrt{2}ab, 1)^T则$式(34)$即$(\langle x_1, x_2 \rangle + 1)^2 $的结果和内积$ \langle \phi(x_1), \phi(x_2) \rangle $的结果是相等的。现在我们来研究一下二者的区别。 $(\langle x_1, x_2 \rangle + 1)^2 $: 直接在原来的低维空间中进行计算，不需要显示的写出映射后的结果 $ \langle \phi(x_1), \phi(x_2) \rangle $：先映射到高维空间中，然后再根据内积进行计算 回到刚才提到的维度爆炸，在$ \langle \phi(x_1), \phi(x_2) \rangle $的方法不能计算的情况下，另一种方法$(\langle x_1, x_2 \rangle + 1)^2 $却能从容处理，甚至无穷维度的情况也可以计算。 这也就是核函数不显式的求映射关系，直接在低维计算，高维学习分离超平面的原因之一。 这样，我们之前学习到的线性支持向量机，可以通过核函数学习非线性数据的情况： f(x) = \sum_{i=1}^n \alpha_i y_i \langle x_i, x \rangle + b \Longrightarrow f(x)= \sum_{i=1}^n \alpha_i y_i K(x_i, x) + b 3. 常用的核函数及其特点： 多项式核函数(polynomial kernel function) K(x_1, x_2) = (\langle x_1, x_2 \rangle + R)^d该空间的维度是$C_{m+d}^d$,其中$m$是原始空间的维度。 高斯核函数(Gaussian kernel function) / 高斯径向基函数(Radial basis function) K(x_1, x_2) = exp(- \frac{|| x_1 - x_2 ||^2}{2\sigma^2})这个核会将原始空间映射为无穷维空间。不过，如果$\sigma$选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间； 反过来，如果$\sigma$选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。 不过，总的来说，通过调控参数$\sigma$，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间。 线性核函数 K(x_1, x_2) = \langle x_1, x_2 \rangle这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了 (意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板 或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的) 更多核函数进阶请看这里 last but not least. 核函数的本质 实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去； 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(即维度爆炸)； 核函数登场，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。 核函数选择问题需要注意的是需要对数据归一化处理 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 参考 李航 - 《统计学习方法》 July - 支持向量机通俗导论（理解SVM的三层境界） 学习July博文总结——支持向量机(SVM)的深入理解 周志华 - 《机器学习》 _席达_ - SVM学习笔记]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machines - Part 1]]></title>
    <url>%2F2018%2F03%2F28%2Fsvm-1%2F</url>
    <content type="text"><![CDATA[支持向量机 - SVM（Support Vector Machines）Part 1支持向量机 - SVM（Support Vector Machines）Part 1 线性可分支持向量机学习算法 - 最大间隔法 线性可分支持向量机的对偶算法 支持向量机 - SVM（Support Vector Machines）Part 2 线性支持向量机 核函数及非线性支持向量机 常用的核函数及其特点 支持向量机 - SVM（Support Vector Machines）Part 3 序列最小最优化(SMO)算法 支持向量机 - SVM（Support Vector Machines）Part 4 支持向量回归 - SVR 目录 基本概念 线性可分支持向量机学习算法 - 最大间隔法 线性可分支持向量机的对偶算法 特点 1. 基本概念定义函数间隔(用$\hat{\gamma}$表示)为： \hat{\gamma} = y(w x + b) = yf(x)超平面$(w,b)$关于训练数据集$T$中所有样本点$ (x_i, y_i) $的函数间隔最小值，即为超平面$ (w,b) $关于训练数据集$T$的函数间隔： \hat{\gamma} = min \hat{\gamma_i}, \quad i=1,2,...,n其中$n$表示样本点数量。[问题：将$w$和$b$成比例改变为$\lambda w$和$\lambda b$，函数间隔的值也会变化为原来的$\lambda$倍]，因此自然地引出几何间隔的概念。 定义几何间隔(用$\gamma$表示)为： \gamma = \frac{y(wx+b)}{||w||} = \frac{yf(x)}{||w||} \\其中$||w||$为$w$的$L_2$范数。 由上可知， \gamma = \frac{\hat{\gamma}}{||w||}即，几何间隔等于函数间隔与$w$二阶范数的商。 回到原始的问题中，我们需要最大间隔(几何间隔)分离超平面，即： \mathop{\max_{w,b}} \quad \gamma s.t. \quad y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}) \geq \gamma, \quad i=1,2,...,n考虑几何间隔和函数间隔的关系，可以将上面的问题改写成： \mathop{\max_{w,b}} \quad \frac{\hat{\gamma}}{||w||} s.t. \quad y_i(w \cdot x_i+ b) \geq \hat{\gamma}, \quad i=1,2,...,n由于函数间隔$ \hat{\gamma} $的取值并不影响最优化问题的解[可以认为函数间隔通过$\lambda$放缩后变为1，这样最优化问题中含有一个常数因子]， 这样令函数间隔$\hat{\gamma}=1$代入上面的最优化问题，同时，$ \max \frac{\hat{\lambda}}{||w||} $、$ \max \frac{1}{|| \lambda w ||} $与$ \min \frac{1}{2} ||w||^2 $是等价的。 所以上述最优化问题最终可以归结为以下最优化问题： \mathop{\min_{w,b}} \frac{1}{2}||w||^2 s.t. \quad y_i(w \cdot x_i + b) - 1 \geq 0, \quad i=1,2,...,n现在目标函数和约束条件都是连续可微的凸函数，所以可以通过拉格朗日对偶性，通过求解与对偶问题等价的对偶问题得到原始问题的最优解， 这样的优点：一、对偶问题往往更容易求解；二、自然的引入核函数，推广到非线性分类问题。 2. 线性可分支持向量机学习算法 - 最大间隔法输入：线性可分训练数据集$T=\lbrace(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\rbrace $，其中$ x_i \in \chi=R^n, y_i \in \mathcal{y} = \lbrace -1, +1 \rbrace, i=1,2,…,n$ 输出：最大间隔分离超平面和分类决策函数 （1）构造并求解约束最优化问题 \mathop{\min_{w,b}} \frac{1}{2} ||w||^2 s.t. \quad y_i(w \cdot x_i +b) -1 \geq 0, \quad i=1,2,..,n求的最优解$ w^{\ast},b^{\ast} $。 （2）由此得到分离超平面： w^{\ast} \cdot x + b^{\ast} = 0分类决策函数 f(x) = sign(w^{\ast} \cdot x + b^{\ast}) 3. 线性可分支持向量机的对偶算法首先由最优化问题$式(9) \sim 式(10)$，构建拉格朗日函数，引入拉格朗日乘子$ \alpha_i \geq 0, i=1,2,…,n $ L(w,b,\alpha) = \frac{1}{2} ||w||^2 - \sum_{i=1}^n \alpha_i y_i (w \cdot x_i + b) + \sum_{i=i}^n \alpha_i其中$ \alpha = (\alpha_1, \alpha_2, …, \alpha_n)^T $。 根据拉格朗日对偶性，原始问题： \mathop{\min_{w,b} \max_\alpha} L(w,b,\alpha)[这个地方有点绕，注意内部的极大后等价与原约束问题，理解拉格朗日对偶性] 对偶问题： \mathop{\max_\alpha \min_{w,b}} L(w,b,\alpha)求解对偶问题（1）求 $ \min_{w,b} L(w,b,\alpha)$ 插播二范数的求导公式如下： \frac{\partial(|| w ||^2)}{\partial x} = \frac{\partial(w^T w)}{\partial x} = 2w \\ \frac{\partial(y^Tx)}{\partial x} = y \\ \frac{\partial(x^TAx)}{\partial x} = (A+A^T)x将拉格朗日函数$L(w,b,\alpha)$分别对$w,b$求偏导数并令其等于0. \nabla_w L(w,b,\alpha) = w - \sum_{i=1}^n \alpha_i y_i x_i = 0 \nabla_b L(w,b,\alpha) = - \sum_{i=1}^n \alpha_i y_i = 0得 w = \sum_{i=1}^n \alpha_i y_i x_i \sum_{i=1}^n \alpha_i y_i = 0将式(21)代入拉格朗日函数(15)，并利用式(22)，即得 \begin{equation} \begin{aligned} L(w,b,\alpha) & = \frac{1}{2} ||w||^2 - \sum_{i=1}^n \alpha_iy_i(w x_i+b) + \sum_{i=1}^n \alpha_i \\ & = \frac{1}{2} w^Tw - \sum_{i=1}^n \alpha_iy_iwx_i - \sum_{i=1}^n \alpha_i y_i b + \sum_{i=1}^n \alpha_i \\ & = \frac{1}{2} w^T \sum_{i=1}^n \alpha_i y_ix_i - \sum_{i=1}^n \alpha_iy_iwx_i - \sum_{i=1}^n \alpha_iy_ib + \sum_{i=1}^n \alpha_i \\ & = \frac{1}{2} w^T \sum_{i=1}^n \alpha_i y_ix_i - w^T \sum_{i=1}^n \alpha_iy_ix_i - \sum_{i=1}^n \alpha_i y_ib + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} w^T \sum_{i=1}^n \alpha_i y_i x_i - \sum_{i=1}^n \alpha_i y_i b + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} w^T \sum_{i=1}^n \alpha_i y_i x_i - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} (\sum_{i=1}^n \alpha_i y_i x_i)^T \sum_{i=1}^n \alpha_i y_i x_i - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} \sum_{i=1}^n \alpha_i y_i (x_i)^T \sum_{i=1}^n \alpha_i y_i x_i - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i y_i (x_i)^T \alpha_j y_j x_j - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ & = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - b \sum_{i=1}^n \alpha_i y_i + \sum_{i=1}^n \alpha_i \\ & = - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^n \alpha_i \end{aligned} \end{equation}倒数第5步推导倒数第4步利用了线性代数的转置运算，由于$\alpha_i$和$y_i$都是实数，因此转置后与自身一样。 倒数第4步到倒数第3步使用了$ (a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+… $的乘法运算规则。 即： \mathop{\min_{w,b}}L(w,b,\alpha) = - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^n \alpha_i（2）求$ \min_{w,b} l(w,b,\alpha) $对$\alpha$的极大，即是对偶问题： \mathop{\max_\alpha} - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^n \alpha_i s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 \quad \quad \alpha_i \geq 0, \quad i=1,2,..,n将式(25)的目标函数由求极大转化为求极小，就得到下面与之等价的对偶最优化问题： \mathop{\min_\alpha} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^n \alpha_i s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 \quad \quad \alpha_i \geq 0, \quad i=1,2,..,n$式(28) \sim 式(30)$要解决的是在参数$( \alpha_1, \alpha_2, …, \alpha_n )^T$上求极值的问题，我们需要利用序列最小最优化(SMO)算法解决。 限于篇幅及知识递进程度，具体SMO算法在Support Vector Machines - Part 3详细描述。 （3）求参数$w^{\ast}, b^{\ast}$ 这样假设已经求出了$ \alpha^{\ast}=(\alpha_1^{\ast}, \alpha_2^{\ast}, …, \alpha_n^{\ast})^T $后，从而根据$KKT$条件得： w^{\ast} = \sum_{i=1}^n \alpha_i^{\ast} y_i x_i在$y=-1, y=1$的类别中，支持向量处于边界上，根据$KKT$条件$ \alpha_i^{\ast}(y_i(w^{\ast}x_i+b)-1)=0 $， 至少有一个$\alpha_j^{\ast}&gt;0$[反证法，假设$\alpha^{\ast}=0$，则$w^{\ast}=0$，而$w^{\ast}=0$不是原始最优化问题的解，产生矛盾！]， 所以，对此$j$有 \begin{cases} y_j(w^{\ast}x_j+b^{\ast})-1=0 \\ y_j^2 = 1 \end{cases} \\ \Downarrow b^{\ast} = y_j - \sum _{i=1}^n \alpha_i^{\ast} y_i (x_i^T \cdot x_j)即可求出$w^{\ast},b^{\ast}$，最终得出分离超平面 w^{\ast} x + b^{\ast} = 0分类决策函数 f(x) = sign(w^{\ast}\cdot x + b^{\ast}) 4. 特点SVM 函数间隔(functional margin)为$ \hat{\gamma}=y(wx+b)=yf(x) $，其中的$y$是只取1和-1吗？$y$的唯一作用就是确保函数间隔(functional margin)的非负性？ （1）二分类问题中，$y$只取两个值，而且这两个值是可以任意取的； （2）求解的超平面分开的两侧的函数值的符号是刚好相反的； （3）为了问题简单化，取了$y$的值为$1$和$-1$。 在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)，支持向量是使约束条件$ y_i(w\cdot x_i +b) -1=0 $成立的点，如下图$H_1和H_2$上的点。 在决定分离超平面时，只有支持向量起作用，其他样本点并不起作用。如果移动支持向量，将会改变分离超平面，移动其他样本点则无影响。支持向量一般个数较少。 为什么令函数间隔为1？ 函数间隔可以表征样本被分到某一类的置信度，比如说$y_i=+1$时，如果$f(x_i) = w \cdot x_i +b &gt;0$且很大，说明$(x_i,y_i)$离分类边界很远，我们有理由相信$x_i$是正类。 另外我们知道成比例改变$w,b$，分离超平面不变，几何间隔也不改变 因此可以做变量替换，将最优化问题改变为函数间隔为1。这样，在不影响最优化问题的前提下，改变了最优化函数并简化了计算。 参考 July - 支持向量机通俗导论 李航 - 《统计学习方法》 SVM推导过程中的三个疑问]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machines - Part 3]]></title>
    <url>%2F2018%2F03%2F28%2Fsvm-3%2F</url>
    <content type="text"><![CDATA[支持向量机 - SVM（Support Vector Machines）Part 3支持向量机 - SVM（Support Vector Machines）Part 1 线性可分支持向量机学习算法 - 最大间隔法 线性可分支持向量机的对偶算法 支持向量机 - SVM（Support Vector Machines）Part 2 线性支持向量机 核函数及非线性支持向量机 常用的核函数及其特点 支持向量机 - SVM（Support Vector Machines）Part 3 序列最小最优化(SMO)算法 支持向量机 - SVM（Support Vector Machines）Part 4 支持向量回归 - SVR 目录 序列最小最优化(SMO)算法 SMO算法步骤 代码资料 1. 序列最小最优化(SMO)算法先看一下SMO要解决的问题: 线性可分支持向量机(硬间隔) \mathop{\min_{\alpha}} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^n \alpha_i \\ s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 \\ \quad \quad \alpha_i \geq 0, i=1,2,...,n线性支持向量机(软间隔) \mathop{\min_{\alpha}} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j - \sum_{i=1}^n \alpha_i \Longleftrightarrow \mathop{\min_{\alpha}} \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij} - \sum_{i=1}^n \alpha_i s.t. \quad \sum_{i=1}^n \alpha_i y_i = 0 \quad \quad 0 \leq \alpha_i \leq C, i=1,2,...,n其中$K_{ij}=K(x_i, x_j), i,j=1,2,…,n$ 下面要解决的问题是: $ \alpha = \lbrace \alpha_1, \alpha_2, …, \alpha_n \rbrace $上求上述目标函数的最小化。SMO的基本思路是： 如果所有的变量的解都满足此最优化问题的$KKT$条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题， 这个二次规划问题关于这两个变量的解更接近于原始二次规划问题的解。整个SMO算法包括两部分： 求解这两个变量二次规划的解析方法 选择变量的启发式方法 1.1 两个变量二次规划的求解方法随机选择两个变量$\alpha_1, \alpha_2$，其他变量固定。于是SMO的最优化问题$式(2)$可以写成： \mathop{\min_{\alpha_1, \alpha_2}} W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1 \alpha_2 - (\alpha_1+\alpha_2) + y_1\alpha_1\sum_{i=3}^n y_i \alpha_iK_{i1} + y_2\alpha_2\sum_{i=3}^ny_i\alpha_iK_{i2} s.t. \quad \alpha_1y_1 + \alpha_2y_2 = - \sum_{i=3}^n y_i \alpha_i = \varsigma \quad \quad 0 \leq \alpha_i \leq C, \quad i=1,2,...,n其中$K_{ij}=K(x_i, x_j), i,j=1,2,…,n, \quad \varsigma$是常数，另外式$(5)$中省略了不含$\alpha_1, \alpha_2$的常数项。 这里，我们引入新的变量 v_i = \sum_{j=3}^n \alpha_jy_jK_{ij} = f(x_i) - b - \alpha_1 y_1 K_{i1} - \alpha_2 y_2 K_{i2}其中$ f(x) = \sum_{i=1}^n \alpha_iy_i K(x_i, x) + b$，则式$(5)$中的目标函数可以重新写为 W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1 \alpha_2 - (\alpha_1+\alpha_2) + y_1v_1\alpha_1 + y_2v_2\alpha_2下面我们研究一下约束条件$式(6) \sim 式(7)$，由式$(7)$可知，两个变量均要落在[0,C]x[0,C]的一个矩阵中， 考虑$\alpha_2$单变量的最优化问题，设问题$式(5) \sim 式(7)$的初始可行解为$ \alpha_1^{old}, \alpha_2^{old}$，最优解为$ \alpha_1^{new}, \alpha_2^{new} $， 并且假设在沿着约束方向上未经剪辑时$\alpha_2$的最优解为$\alpha_2^{new,unc}$。假设$\alpha_2^{new}$的上下边界分别为$H和L$,那么有： L \leq \alpha_2^{new} \leq H接下来综合约束条件$0 \leq \alpha_i \leq C,i=1,2,…,n和\alpha_1^{new}y_i+\alpha_2^{new}y_2=\alpha_1^{old}y_1+\alpha_2^{old}y_2=\varsigma$，求取上下边界的值。 以$y_1 \not= y_2$为例，由$\alpha_1^{new}y_i+\alpha_2^{new}y_2=\alpha_1^{old}y_1+\alpha_2^{old}y_2=\varsigma$可得： \alpha_2 = \alpha_1 - \varsigma \quad [下方线段] \quad 或 \quad \quad \alpha_2 = - \alpha_1 + \varsigma \quad [上方线段]所以，当$\alpha_1=0$时，$ L = max(0, -\varsigma) = max(0, \alpha_2^{old} - \alpha_1^{old}) [上方线段] $； 所以，当$\alpha_1=C$时，$ H = min(C, C-\varsigma) = min(C, C - (\alpha_1^{old} - \alpha_2^{old})) [下方线段] $； 二维图像如下图左图所示，右图为$y_1=y_2$时的图像。 如此，根据$y_1和y_2$异号或同号，可以得到如下$\alpha_2^{new}$的上下界分别为： \begin{cases} L = max(0, \alpha_2^{old}-\alpha_1^{old}), \quad \quad \quad H = min(C, C+\alpha_2^{old}-\alpha_1^{old}), \quad if \quad y_1 \not= y_2 \\ L = max(0, \alpha_2^{old}+\alpha_1^{old}-C), \quad H = min(C, \alpha_2^{old}+\alpha_1^{old}), \quad \quad \quad if \quad y_1 = y_2 \end{cases}下面开始求沿着约束方向未经剪辑[即未考虑不等式约束$(7)$]时$\alpha_2$的最优解$\alpha_2^{new,unc}$；之后再求剪辑后$\alpha_2$的解$\alpha_2^{new}$。 f(x) = \sum_{i=1}^n \alpha_i y_i K(x_i, x) + b E_i = f(x_i) - y_i = (\sum_{i=1}^n \alpha_i y_i K(x_i, x) + b) - y_i由$ \alpha_1y_1 + \alpha_2y_2 = \varsigma及y_i^2 = 1 $可得， \alpha_1 = (\varsigma - \alpha_2y_2)y_1将$式(15)$代入$式(9)$中，得： W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1 \alpha_2 - (\alpha_1+\alpha_2) + y_1v_1\alpha_1 + y_2v_2\alpha_2\Downarrow W(\alpha_2) = \frac{1}{2}K_{11}(\varsigma-\alpha_2y_2)^2+\frac{1}{2}K_{22}\alpha_2^2+y_2K_{12}(\varsigma-\alpha_2y_2)\alpha_2 - (\varsigma-\alpha_2y_2)y_1 - \alpha_2 + v_1(\varsigma-\alpha_2y_2)+y_2v_2\alpha_2之后对$\alpha_2$求导数[Notice: $ y_i^2 = 1 $]并令其等于零： \begin{equation} \begin{aligned} \frac{\partial W}{\partial \alpha_2} & = K_{11}(\varsigma - \alpha_2y_2)(-y_2) + K_{22}\alpha_2 + y_2K_{12}\varsigma - 2y_2K_{12}y_2\alpha_2+y_1y_2 -1 - v_1y_2 + v_2y_2 \\ & = K_{11}\alpha_2 + K_{22}\alpha_2 - 2K_{12}\alpha_2 - K_{11}\varsigma y_2 + K_{12}\varsigma y_2 -1 -v_1y_2 + v_2y_2 \\ & = 0 \end{aligned} \end{equation}得： \begin{equation} \begin{aligned} (K_{11}+K_{22}-2K_{12})\alpha_2 & = y_2(y_2 - y_1 + \varsigma K_{11} - \varsigma K_{12}+v_1-v_2) \\ & = y_2 \lbrace y_2 - y_1 + \varsigma K_{11} - \varsigma K_{12}+ [f(x_1) - \sum_{j=1}^2 \alpha_jy_jK_{1j} - b] - [f(x_2) - \sum_{j=1}^2 \alpha_jy_jK_{2j} - b] \rbrace \\ \end{aligned} \end{equation}将$ \varsigma = \alpha_1^{old}y_1 + \alpha_2^{old}y_2 $代入式$(20)$，得： \begin{equation} \begin{aligned} (K_{11}+K_{22}-2K_{12})\alpha_2^{new,unc} & = y_2 \lbrace (K_{11}+K_{12} - 2K_{12})\alpha_2^{old}y_2 + y_2 - y_1 + f(x_1) - f(x_2) \rbrace \\ & = (K_{11}+K_{12} - 2K_{12})\alpha_2^{old} + y_2(E_1 - E_2) \\ \end{aligned} \end{equation}令$\eta = K_{11}+K_{22}-2K_{12}$，即得： \alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{\eta}之后剪辑$\alpha_2^{new,unc}$得： \alpha_2^{new} = \begin{cases} H, \quad \quad \quad \quad \alpha_2^{new,unc} > H \\ \alpha_2^{new,unc}, \quad \quad L \leq \alpha_2^{new,unc} \leq H \\ L, \quad \quad \quad \quad \alpha_2^{new,unc} < L \end{cases} \\ where, \quad \quad \begin{cases} L = max(0, \alpha_2^{old}-\alpha_1^{old}), \quad \quad \quad H = min(C, C+\alpha_2^{old}-\alpha_1^{old}), \quad if \quad y_1 \not= y_2 \\ L = max(0, \alpha_2^{old}+\alpha_1^{old}-C), \quad H = min(C, \alpha_2^{old}+\alpha_1^{old}), \quad \quad \quad if \quad y_1 = y_2 \end{cases}然后由$ \alpha_2^{new}求得 \alpha_1^{new} $为： \alpha_1^{new} = \alpha_1^{old} + y_1y_2(\alpha_2^{old} - \alpha_2^{new})至此，我们选择的两个变量，固定其他变量的最优化过程结束。 1.2 选择变量的启发式方法1.2.1 第1个变量的选择SMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选择违反$KKT$条件最严重的样本点，并将其对应的变量作为第一个变量。 具体地，检验样本点$ (x_i, y_i) $是否满足$KKT$条件，即： \alpha_i = 0 \Leftrightarrow y_if(x_i) \geq 1 \\ 0 < \alpha_i < C \Leftrightarrow y_if(x_i)=1 \\ \alpha_i = C \Leftrightarrow y_if(x_i) \leq 1其中$ f(x_i) = \sum_{j=1}^n \alpha_j y_j K(x_i, x_j) +b $。 以下几种情况出现将会导致$KKT$条件不满足： $y_if(x_i) \leq 1$但是$\alpha_i &lt;C$不满足，原本$ \alpha_i=C$； $y_if(x_i) \geq 1$但是$\alpha_i &gt; 0$不满足，原本$ \alpha_i=0$； $y_if(x_i) = 1$但是$\alpha_i=0或\alpha_i=C$不满足，原本$0 &lt; \alpha_i &lt; C$； 1.2.2 第2个变量的选择SMO称选择第二个变量的过程为内层循环，第二个变量选择的标准是希望能使$\alpha_2$有足够大的变化。 所以，对于第二个变变量，通常选择满足下式的样本点对应的变量： max\| E\_1 - E\_2 \|特殊情况下，如果内存循环通过以上方法选择的$ \alpha_2 $不能使目标函数有足够的下降，那么采用以下启发式规则继续选择$ \alpha_2 $。 遍历在间隔边界上的支持向量点，一次将其对应的变量作为$ \alpha_2 $试用，知道目标函数有足够的下降； 若找不到合适的$ \alpha_2 $，那么遍历训练数据集； 若仍找不到合适的$ \alpha_2 $，则放弃第一个$ \alpha_1 $，再通过外层循环寻求另外的$ \alpha_2 $。 1.2.3 计算阈值$b和差值E_i$每次完成两个变量的优化后，都要重新计算阈值$b$。当$0&lt;\alpha_i^{new}&lt;C$时，由$KKT$条件可知： \sum_{i=1}^n \alpha_i y_i K_{i1} +b = y_1于是， \begin{cases} b_1^{new} = y_1 - \sum_{i=3}^n \alpha_i y_i K_{i1} - \alpha_1^{new}y_1K_{11} - \alpha_2^{new} y_2K_{21} \\ E_1 = \sum_{i=3}^n\alpha_iy_iK_{i1} + \alpha_1^{old}y_1K_{11} + \alpha_2^{old}y_2K_{21} + b^{old} -y_1 \end{cases} \\ \Downarrow b_1^{new} = -E_1 - y_1K_{11}(\alpha_1^{new} - \alpha_1^{old}) - y_2K_{21}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}同样，如果$ 0&lt;\alpha_2^{new} &lt; C $，那么 b_2^{new} = -E_2 - y_1K_{12}(\alpha_1^{new} - \alpha_1^{old}) - y_2K_{22}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}如果$\alpha_1^{new}, \alpha_2^{new}是0或C$，那么$b_1^{new}和b_2^{new}$以及它们之间的数都是符合$KKT$条件的阈值，此时选择它们的中点作为$b^{new}$。同时，每次完成两个变量的优化之后，还必须更新对应的$E_i$值。 E_i^{new} = \sum_S y_j \alpha_j K(x_i, x_j) + b^{new} - y_i其中，$S$是所有支持向量$x_j$的集合。 以上： b = \begin{cases} b_1^{new} = -E_1 - y_1K_{11}(\alpha_1^{new} - \alpha_1^{old}) - y_2K_{21}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}, \quad 0 < \alpha_1^{new}]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日对偶性]]></title>
    <url>%2F2018%2F03%2F27%2Flagrange%2F</url>
    <content type="text"><![CDATA[拉格朗日对偶性 - Lagrange Duality目录 原始问题 对偶问题 原始问题与对偶问题的关系 1. 原始问题假设$ f(x),c_i(x),h_j(x) $是定义在$R^n$上的连续可微函数，考虑约束最优化问题 \mathop{\min}_{x \in R^n} f(x) s.t. c_i(x) \leq 0, \quad i=1,2,...,k \quad \quad h_j(x) = 0, \quad j=1,2,...,k称此约束最优化问题为原始最优化问题或原始问题。 引入广义拉格朗日函数 L(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_j h_j(x)这里，$x=(x^{(1)}, x^{(2)}, …, x^{(n)})^T \in R^n, \alpha_i, \beta_j$是拉格朗日乘子，$ \alpha_i \geq 0 $, [不要求$ \beta_j \geq 0 $，详细可以看这里]。 考虑$x$的函数： \theta_P(x) = \mathop{\max}_{\alpha, \beta: \alpha_i \geq 0} L(x, \alpha, \beta)这里下标$P$表示原始问题。 当有约束条件不满足时，可以调整该约束条件的拉格朗日乘子，可以使得$ \theta_P(x)=+\infty $,只有当所有的约束条件全都满足时，则可知$\theta_P(x) = f(x)$。 所以， \theta_P(x) = \begin{cases} f(x), & x满足原始问题约束 \\ +\infty, & 其他 \end{cases}所以，如果考虑极小化问题 \mathop{\min}_x \theta_P(x) = \mathop{\min_x\max_{\alpha, \beta: \alpha_i \geq 0}} L(x, \alpha, \beta)该问题与原始最优化问题$式(1) \sim 式(3)$等价，即它们具有相同解。问题$ \mathop{\min}_x \theta_P(x) = \mathop{\min_x\max_{\alpha, \beta: \alpha_i \geq 0}} L(x, \alpha, \beta) $ 称为广义拉格朗日函数的极小极大问题，这样，原始最优化问题表示为广义拉格朗日函数的极小极大问题。 定义原始问题的最优值： p^{\ast} = \mathop{\min_x} \theta_P(x)称为原始问题的值。 2. 对偶问题定义 \theta_D(\alpha, \beta) = \mathop{\min_x} L(x, \alpha, \beta)再考虑极大化 \mathop{\max_{\alpha,\beta:\alpha_i \geq 0}} \theta\_D(\alpha, \beta) = \mathop{\max\_{\alpha,\beta:\alpha\_i \geq 0} \min\_x} L(x,\alpha, \beta)问题$ \mathop{\max}_{\alpha,\beta:\alpha_i \geq 0} \mathop{\min_x} L(x,\alpha, \beta) $称为广义拉格朗日函数的极大极小问题。 可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题： \mathop{\max_{\alpha, \beta}} \theta_D(\alpha, \beta) = \mathop{\max_{\alpha, \beta} \min_x} L(x, \alpha, \beta) s.t. \quad \alpha_i \geq 0, \quad i=1,2,...,k称为原始问题的对偶问题。 定义对偶问题的最优解： d^{\ast} = \mathop{\max_{\alpha, \beta: \alpha_i \geq 0}} \theta_D(\alpha, \beta)称为对偶问题的值。 3. 原始问题与对偶问题的关系3.1 定理 1若原始问题和对偶问题都有最优值，则 d^{\ast} = \mathop{\max_{\alpha,\beta:\alpha_i \geq 0} \min_x}L(x,\alpha, \beta) \leq \mathop{\min_x \max_{\alpha,\beta:\alpha_i \geq 0}} L(x, \alpha, \beta) = p^{\ast}3.2 定理 2考虑原始问题$式(1) \sim 式(3)$和对偶问题$式(11) \sim 式(12)$， 假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数[最高次数为1的多项式函数,常数项为零的仿射函数称为线性函数]； 并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有$i$有$c_i(x)&lt;0$， 则存在$ x^{\ast}, \alpha^{\ast}, \beta^{\ast} $，使$x^{\ast}$是原始问题的解，$ \alpha^{\ast}, \beta^{\ast} $是对偶问题的解，并且 p^{\ast} = d^{\ast} = L(x^{\ast}, \alpha^{\ast}, \beta^{\ast})3.3 定理 3对原始问题$式(1) \sim 式(3)$和对偶问题$式(11) \sim 式(12)$， 假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是仿射函数，并且假设不等式约束$c_i(x)$是严格可行的，则$ x^{\ast} 和 \alpha^{\ast}, \beta^{\ast} $分别是原始问题和对偶问题的解的充分必要条件是$ x^{\ast}, \alpha^{\ast}, \beta^{\ast} $满足下面的$ Karush-Kuhn-Tucker(KKT) $条件： \nabla_x L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) = 0 \\ \nabla_\alpha L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) = 0 \\ \nabla_\beta L(x^{\ast}, \alpha^{\ast}, \beta^{\ast}) = 0 \alpha_i^{\ast}c_i(x^{\ast}) = 0, \quad i=1,2,...,k c_i(x^{\ast}) \leq 0, \quad i=1,2,...,k \alpha_i^{\ast} \geq 0, \quad i=1,2,...,k h_j(x^{\ast}) = 0, \quad i=1,2,...,l特别指出，$ 式(17) $称为$KKT$的对偶互补条件，由此条件可知：若$ \alpha^{\ast} &gt; 0 $，则$ c_i(x^{\ast})=0 $。 参考 李航 - 《统计学习方法》]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的正则化]]></title>
    <url>%2F2018%2F03%2F24%2Fregularization%2F</url>
    <content type="text"><![CDATA[机器学习中的正则化 - Regularization 目录 先验知识 L0正则[范数] L1正则[范数] L2正则[范数] L1和L2的区别 1. 先验知识监督机器学习问题是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。 因为参数太多，会导致我们的模型复杂度上升，容易过拟合。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。 所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化能力，而模型“简单”就是通过规则函数来实现的。 另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。 还有几种角度来看待规则化的。 规则化符合奥卡姆剃刀(Occam’s razor)原理：在所有可能选择的模型中，选择能够很好地解释已知数据并且简单的模型。 从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。 还有个说法，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。 一般来说，监督学习可以看作最小化下面的目标函数： w^{\ast} = \mathop{\arg\min}\_w \sum\_i L(y\_i, f(x\_i; w)) + \lambda \Omega(w)其中，第一项$L(yi,f(xi;w))$衡量我们的模型对第$i$个样本的预测值$f(x_i;w)$和真实的标签$y_i$之前的误差。 因为模型是要拟合训练样本，所以要求这一项最小，也就是要求模型尽量的拟合训练数据。 但正如上面说言，不仅要保证训练误差最小，更希望模型测试误差小，所以需要加上第二项，也就是对参数$w$的规则化函数$\Omega(w)$去约束模型尽量的简单。 对于第一项Loss函数，不同的loss函数，具有不同的拟合特性，具体问题具体分析: 如果是Square loss，是最小二乘 如果是Hinge Loss，是SVM 如果是exp-Loss，是Boosting 如果是log-Loss，是Logistic Regression 等等 规则化函数$\Omega(w)$也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。 零范数 一范数 二范数 迹范数 Frobenius范数(对应元素的平方和再开方) $ \sqrt{\sum_{i=1}^m \sum_{j=1}^n | a_{ij} | ^2} $ 核范数 $ tr(\sqrt{X^TX}) $ 等等 2. L0正则[范数]定义： L0范数是指向量中非0的元素的个数。 如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说就是让参数W是稀疏的。 所以L0范数非常适合机器学习中稀疏编码，特征选择。但是由于L0范数很难优化求解（NP难问题），而L1范数又是L0范数的最优凸近似[??]，故通常用L1范数来代替。 其他可以参考Emmanuel Candes的工作。 3. L1正则[范数]定义：L1范数是指向量中各个元素绝对值之和。 \|\| w \|\|\_1 = \sum\_i \| w\_i \|L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 3.1 参数稀疏的优势（1）特征选择 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。 一般来说，$x_i$的大部分元素（也就是特征）都是和最终的输出$y_i$没有关系或者不提供任何信息的，在最小化目标函数的时候考虑$x_i$这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确$y_i$的预测。 稀疏规则化算子的引入就是为了完成特征自动选择的任务，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 （2）可解释性 另一个理由是，模型更容易解释。 例如患某种病的概率是$y$，假设收集到的数据$x$是1000维的，也就是需要寻找这1000种因素到底是怎么影响患上这种病的概率的。 假设我们这个是个回归模型：$ y = w_1 x_1 + w_2 x_2 + … + w_1000 x_1000 + b$（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。 通过学习，如果最后学习到的$w^{\ast}$就只有很少的非零元素，例如只有5个非零的$w_i$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。 也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_i$都非0，医生面对这1000种因素就无从下手。 4. L2正则[范数]定义：欧式距离 \|\| w \|\|\_2 = \sqrt{\sum\_i w\_i^2}L2正则对于改善过拟合具有强大的功效。 让L2范数的规则项最小，可以使得$w$的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。 123为什么参数越小模型越简单？因为越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大，反之，简单的模型，参数值较小。 通过L2范数，可以实现对模型空间的限制，从而在一定程度上避免了过拟合。 说到可以避免过拟合，就要涉及到条件数（详见 条件数-维基百科, 矩阵条件数-百度百科） 从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。 优化有两大难题，一是：局部最小值，二是：病态(ill-condition)问题。前者找的是全局最小值，如果局部最小值太多，那优化算法就很容易陷入局部最小而不能自拔，这不是我们愿意看到的。 病态对应着良置(well-condition)，假设我们有个方程组$AX=b$，我们需要求解$X$。如果$A$或者$b$稍微的改变，会使得$X$的解发生很大的改变，那么这个方程组系统就是病态的， 反之就是良置的。 下面看个例子： 先看左边,第一行假设是$AX=b$，第二行稍微改变下$b$，得到的$x$和没改变前的差别很大，第三行稍微改变下系数矩阵$A$，可以看到结果的变化也很大。 换句话来说，这个系统的解对系数矩阵$A$或者$b$太敏感了。又因为一般系数矩阵$A$和$b$是从实验数据里面估计得到的，所以是存在误差的，如果系统对这个误差是可以容忍的就还好， 但系统对这个误差太敏感了，以至于解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是病态(ill-conditioned)的，不正常的，不稳定的，有问题的。右边那个就叫良置(well-condition)的系统。 上面的条件数就是拿来衡量病态系统的可信度的。条件数衡量的是输入发生微小变化的时候，输出会发生多大的变化，也就是系统对微小变化的敏感度。条件数值小的就是良置的，大的就是病态的。 对条件数来个总结：条件数是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的条件数在1附近，那么它就是良置的， 如果远大于1，那么它就是病态的，如果一个系统是病态的，它的输出结果可信度就很低。 回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。 因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为： w^{\ast} = (X^TX)^{-1}X^Ty然而，如果当样本$X$的数目比每个样本的维度还要小的时候，矩阵$X^TX$将会不是满秩的，也就是$X^TX$会变得不可逆，所以$w^{\ast}$就没办法直接计算出来了。 或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。 但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了： w^{\ast} = (X^TX+\lambda I)^{-1} X^T y要得到这个解，通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。 考虑没有规则项的时候，也就是$\lambda=0$的情况，如果矩阵$X^TX$的条件数很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善条件数。 另外，如果使用迭代优化的算法，条件数太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成$ \lambda-strongly convex, (\lambda$强凸)的了。 关于强凸和普通凸的对比，看下图[其他详细数学内容自行查阅资料]: 设最优解为$w^{\ast}$指向的地方。如果我们的函数$f(w)$，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算$w_t$和$w^{\ast}$离的比较近的时候，$f(wt)$和$f(w^{\ast})$的值差别还是挺大的，也就是会保证在最优解$w^{\ast}$附近的时候，还存在较大的梯度值，这样才可以在比较少的迭代次数内达到$w^{\ast}$。 但对于右图，红色的函数$f(w)$只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在$w_t$还离我们的最优点$w^{\ast}$很远的时候，我们的近似梯度$(f(w_t)-f(w^{\ast}))/(w_t-w^{\ast})$就已经非常小了，在$w_t$处的近似梯度$\partial f / \partial w $就更小了，这样通过梯度下降，我们得到的结果就是$w$的变化非常缓慢。 如果要获得强凸性质怎么做？最简单的就是往里面加入一项$ (\alpha/2)* || w ||^2 $。 一言以蔽之：L2范数不但可以防止过拟合，还可以让优化求解变得稳定和快速。 5. L1和L2的区别5.1 下降速度L1和L2都是规则化的方式，将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。 L1又称Lasso，L2又称Ridge。 5.2 模型空间的限制 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式： L1(Lasso): \mathop{\min}_w \frac{1}{n} \|\| y-w^T X \|\|^2, \quad s.t. \quad \|\| w \|\|_1 \leq C \\ L2(Ridge): \mathop{\min}_w \frac{1}{n} \|\| y-w^T X \|\|^2, \quad s.t. \quad \|\| w \|\|_2 \leq C \\也就是说，我们将模型空间限制在$w$的一个L1-ball中。 为了便于可视化，我们考虑两维的情况，在$ (w_1, w_2) $平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个norm ball。等高线与norm ball首次相交的地方就是最优解： 可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的等高线除非位置摆得非常好，大部分时候都会在角的地方相交。 注意到在角的位置就会产生稀疏性，例如图中的相交点就有$w_1=0$，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则能产生稀疏性，而L2正则不行的原因了。 因此总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 参考 深入理解L1、L2正则化 机器学习中的范数规则化 机器学习中为什么越小的参数说明模型越简单?]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卡方检验]]></title>
    <url>%2F2018%2F03%2F21%2Fchisquaredtest%2F</url>
    <content type="text"><![CDATA[卡方检验 - Chi-Squared Test 目录 定义 统计机器学习中的应用 举例 特点 1. 定义卡方检验（Chi-Squared Test）是一种统计量的分布在零假设成立时近似服从卡方分布（$\chi^2$分布）的假设检验。在没有其他的限定条件或说明时，卡方检验一般指代的是皮尔森卡方检验。在卡方检验的一般运用中，研究人员将观察量的值划分成若干互斥的分类，并且使用一套理论（或零假设）尝试去说明观察量的值落入不同分类的概率分布的模型。而卡方检验的目的就在于去衡量这个假设对观察结果所反映的程度。 2. 统计机器学习中的应用卡方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。 具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（指“如果两者确实独立”的情况下应该有的值）的偏差程度， 如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设； 如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。 那么用什么来衡量偏差程度呢？假设理论值为$E$（数学期望），实际值为$x$，如果仅仅使用所有样本的观察值与理论值的差值$x-E$之和 \sum_{i=1}^n (x_i - E)来衡量，单个的观察值还好说，当有多个观察值$x_1,x_2,x_3$的时候，很可能$x_1-E，x_2-E，x_3-E$的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！ 此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用 \sum_{i=1}^n (x_i - E)^2这时又引来了新的问题，对于$500$的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。 因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断。改进的式子如下： \sum_{i=1}^n \frac{(x_i - E)^2}{E}上面这个式子已经相当好了。实际上这个式子就是卡方检验使用的差值衡量公式。 当提供了数个样本的观察值$x_1, x_2, …, x_i, …, x_n$之后，代入到$式(3)$中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。 3. 举例在文本分类问题的特征选择阶段，我们主要关心一个词word（一个随机变量）与一个类别Ci（另一个随机变量）之间是否相互独立？ 如果独立，就可以说词word对类别Ci完全没有表征作用，即我们根本无法根据word出现与否来判断一篇文档是否属于Ci这个分类。 但与最普通的卡方检验不同，我们不需要设定阈值，因为很难说词word和类别Ci关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最相关的即可。 我们越倾向于认为原假设的反面情况是正确的，一般都使用”词word与类别Ci不相关“来做原假设。选择的过程也变成了为每个词计算它与类别Ci的卡方值，从大到小排个序（此时开方值越大越相关），取前k个就可以。 比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有四个观察值可以使用： 包含“篮球”且属于“体育”类别的文档数，命名为A 包含“篮球”但不属于“体育”类别的文档数，命名为B 不包含“篮球”但却属于“体育”类别的文档数，命名为C 既不包含“篮球”也不属于“体育”类别的文档数，命名为D 下面我们需要根据上面数据估算上面的ABCD四个值，根据估算值计算卡方值 对于A的估计值（上文说的是理论值）： A+B+C+D=NA+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。 好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。 这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近 \frac{A+B}{N}（因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中， 应该有$E_{11} = (A+C)\frac{A+B}{N}$篇包含“篮球”这个词（数量乘以概率），实际上是“体育类”中有A篇包含篮球这个词。 此时使用$ 式(3) $对这种情况的差值就可以计算出来： D_{11} = \frac{(A-E_{11})^2}{E_{11}}同样，我们还可以计算剩下三种情况的差值$D_{12},D_{21},D_{22}$。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的卡方值 \chi^2(篮球，体育) = D_{11}+D_{12}+D_{21}+D_{22} = \frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}因此，计算卡方值的一般公式的形式为： \chi^2(t，c) = \frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的卡方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。 4. 特点针对英文纯文本的实验结果表明： 作为特征选择方法时，卡方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）； 文档频率方法的性能同前两者大体相当，术语强度方法性能一般； 互信息方法的性能最差。 但卡方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词word，却不管word在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。 甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。 这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。 参考 卡方检验 特征选择之Chi卡方检验]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[泰勒级数]]></title>
    <url>%2F2018%2F03%2F20%2Ftaylorseries%2F</url>
    <content type="text"><![CDATA[泰勒级数 - Taylor Series 1. 通项公式 f(x) = \sum_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f^{(1)}(a)(x-a)^1 + \frac{1}{2!}f^{(2)}(a)(x-a)^2 + \frac{1}{3!}f^{(3)}(a)(x-a)^3+... 2. 特殊函数的泰勒展开 e^x = 1 + x + \frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+... \frac{1}{1-x} = 1+x+x^2+x^3+...]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树 Decision Tree]]></title>
    <url>%2F2018%2F03%2F18%2Fdecision-tree%2F</url>
    <content type="text"><![CDATA[决策树 - Decision Tree目录 ID3 C45 决策树的剪枝 CART 特点 代码实现 决策树学习的3个步骤： 特征选择 决策树生成 决策树修剪 决策树的损失函数通常为正则化的极大似然函数。1. ID3ID3算法使用信息增益作为特征选择的标准，信息增益越大越好。 1.1 信息增益算法输入： 训练数据集 $ D $ 和特征 $ A $; 输出： 特征$A$对训练数据集$D$的信息增益 $ g(D,A) $ （1）计算数据集 $ D $ 的经验熵 $ H(D) $ H(D) = - \sum_{k=1}^K \frac{ | C_k |}{| D |}log_2 \frac{| C_k |}{| D |}（2）计算特征$A$对数据集$D$的经验条件熵$ H(D | A) $ H(D | A) = \sum_{i=1}^n \frac{| D_i |}{| D |} H(D_i) = - \sum_{i=1}^n \frac{ | D_i | }{ | D | } \sum_{k=1}^K \frac{| D_{ik} |}{ | D_i |} log_2 \frac{ | D_{ik} |}{| D_i |}（3）计算信息增益 g(D,A) = H(D) - H(D | A)其中$K$为类别的数量$ \lbrace C_1, C_2, …, C_k \rbrace $，$n$为特征$A$的取值数量$ \lbrace a_1, a_2, …, a_n \rbrace $。 2. C45信息增益作为标准时，存在偏向于取值数量较多的特征的问题，因此C45算法选择信息增益比作为划分的标准，信息增益比越大越好。 2.1 信息增益比(基于信息增益)定义为信息增益$g(D,A)$与关于特征$A$的值的熵$H_A(D)$之比，即： g_R(D,A) = \frac{g(D,A)}{H_A(D)} H_A(D) = - \sum_{i=1}^n \frac{ | D_i | } { | D | } log_2 \frac{ | D_i | }{ | D |}其中$n$为特征$A$取值的个数。 3. 决策树的剪枝设树$ T $的叶结点个数为$ | T | $，$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,…,K$，$H_t(T)$为叶结点上的经验熵，则决策树的损失函数可以定义为： \begin{equation} \begin{aligned} C_{\alpha}(T) & = \sum_{t=1}^{ | T | } N_t H_t(T) + \alpha | T | \\ & = - \sum_{t=1}^{ | T | } \sum_{k=1}^K N_{tk} log \frac{N_{tk}}{N_t} + \alpha | T | \\ & = C(T) + \alpha | T | \end{aligned} \end{equation}通过 $ \alpha \geq 0 $控制模型与训练数据拟合度和模型复杂度。 3.1 ID3&amp;C45树的剪枝算法输入：生成算法产生的整个树 $ T $，参数 $ \alpha $； 输出：修剪后的子树$ T_{\alpha} $。 （1）计算每个结点的经验熵, 经验熵计算公式； H_t(T) = - \sum_k \frac{N_{tk}}{N_t} log \frac{N_{tk}}{N_t}（2）递归地从树的叶结点向上回缩； 设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$，其对应的损失函数值分别为$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果 C_{\alpha}(T_B) \leq C_{\alpha}(T_A)则进行剪枝，即将父结点变为新的叶结点。 （3）返回（2），直至不能继续为止，得到损失函数最小的子树$T_{\alpha}$。 4. CARTCART假设决策树是二叉树，内部结点特征取值为“是”和“否”，递归二分每个特征。 回归数使用平方误差最小化准则，越小越好。 分类数使用基尼指数最小化准则，越小越好。 4.1 回归树输入：训练数据集$ D=\lbrace (x_1, y_1), (x_2, y_2), …, (x_N, y_N) \rbrace $, 并且$Y$是连续型变量； 输出：回归树 $ f(x) $ 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。 （1）选择最优切分变量$j$与切分点$s$，求解 \mathop{\min}_{j,s} [\mathop{\min}_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \mathop{\min}_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2]遍历$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。 （2）用选定的对$(j,s)$划分区域并决定相应的输出值(均值)： R_1(j,s) = \lbrace x | x^{(j)} \leq s \rbrace, \quad R_2(j,s)=\lbrace x | x^{(j)} > s \rbrace \\ \hat{c}_m = \frac{1}{N_m} \sum_{x_i \in R_m(j,s)} y_i, \quad x \in R_m, \quad m=1,2（3）继续对两个子区域调用步骤(1),(2)，直至满足停止条件； （4）将输入空间划分为$M$个区域$ R_1, R_2, …, R_M $，生成决策树： f(x) = \sum_{m=1}^M \hat{c}_m I(x \in R_m)4.2 分类树在特征$ A $的条件下，集合$ D $的基尼指数定义为 \begin{equation} \begin{aligned} Gini(D, A) & = \frac{ | D_1 | }{ | D | } Gini(D_1) + \frac{ | D_2 | }{ | D | } Gini(D_2) \\ & = \frac{ | D_1 | }{ | D | } [1 - \sum_{k=1}^K(\frac{ | C_k | }{ | D_1 | })^2 ] + \frac{ | D_2 | }{ | D | } [1 - \sum_{k=1}^K(\frac{ | C_k | }{ | D_2 | })^2 ] \end{aligned} \end{equation}其中$ C_k $是$ D_i $中属于第$k$类的样本子集，$K$是类的个数。 输入：训练集$D$,停止计算的条件 输出：CART决策树 根据训练集，从跟结点开始递归地对每个结点进行一下操作，构建二叉决策树。 （1）根据每一个特征以及每个特征的取值，计算相应二叉划分时的基尼指数； （2）在所有可能的特征及可能的特征值中，选择基尼指数最小的特征及相应特征值作为划分切分特征及切分点，并将训练集划分到两个子结点中； （3）对两个子节点递归调用(1),(2)，直至满足停止条件； （4）生成CART决策树。 4.3 CART的剪枝算法 剪枝，形成一个子树序列 在剪枝过程中，计算子树的损失函数： C_{\alpha}(T) = C(T) + \alpha | T |其中，$T$为任意子树，$C(T)$表示训练数据的预测误差，$| T |$为子树的叶结点数目，$ \alpha \geq 0 $为参数。 具体地，从整体树$T_0$开始剪枝，对$T_0$的任意内部结点$t$，以$t$为单结点树的损失函数是 C_{\alpha}(t) = C(t) + \alpha以$t$为根节点的子树$T_t$的损失函数是 C_{\alpha}(T_t) = C(T_t) + \alpha | T_t | 当$ \alpha=0 $及$\alpha$充分小时，有不等式 C_{\alpha}(T_t) < C_{\alpha}(t)当$ \alpha $增大时，在某一$ \alpha $有 C_{\alpha}(T_t) = C_{\alpha} (t)当$\alpha$增大时，不等式反向。只要$ \alpha=\frac{C(t)-C(T_t)}{| T_t | - 1} $，$T_t$与$t$具有相同的损失，但是$t$的结点数量更少，因此$t$更可取，所以剪枝。 对$T_0$中每一个内部结点$t$，计算 g(t) = \frac{C(t)-C(T_t)}{| T_t | - 1}它表示间之后整体损失函数减少的程度，在$T_0$中减去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha$， $T_1$为区间[$\alpha_1,\alpha_2$]的最优子树。 这个地方理解为：最小的g(t)是一个阈值，选择$\alpha=\mathop{\min}\lbrace g(t) \rbrace \Longleftrightarrow$ [其他g(t)的情况是-剪枝比不剪枝的损失大，即式（16）不等号反向的情况]，所以在最小g(t)处剪枝 如此剪枝下去，直至得到根节点，在这个过程中，不断增加$\alpha$的值，产生新的区间。 在剪枝得到的子树序列中通过交叉验证选取最优子树 利用独立的验证数据集，测试子树序列$ T_0,T_1,…,T_n $中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优决策树，在子树序列中，每棵子树$ T_1,T_2,…,T_n $都对应着一个参数$ \alpha_1, \alpha_2, …, \alpha_n $，所以当最优子树$T_k$确定时，对应的$\alpha_k$也确定了，即得到最优决策树$T_\alpha$。 输入：CART算法生成的决策树$T_0$； 输出：最优决策树$T_\alpha$。 （1）设$k=0, \quad T=T_0$ （2）设$\alpha=0$ （3）自下而上地对各内部结点$t$计算$C(T_t)$，$| T_t | $以及 g(t)=\frac{C(t)-C(T_t)}{| T_t | - 1} \alpha = \mathop{\min}(\alpha, g(t))这里，$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$ | T_t | $是$T_t$的叶结点个数。 （4）对$g(t)=\alpha$的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$。 （5）设$ k=k+1, \quad \alpha_k=\alpha, \quad T_k=T $。 （6）如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令$T_k=T_n$。 （7）采用交叉验证法在子树序列$T_0,T_1,…,T_n$中选择最优子树$T_\alpha$。 5. 特点 优点：可解释性；可处理多种数值类型；没有复杂的参数设置；运算快 缺点：易过拟合；不适合高维数据；异常值敏感；泛化能力差 控制过拟合的方式 1234（1）树的深度（2）一个结点被拆分出子结点所需要的包含最少的样本个数（3）最底层结点所需要好汉的最小样本个数（4）集成学习的方法(随机森林，Xgboost等) 连续值如何划分？ 12C45：基于阈值的信息增益(比)CART：最优切分变量和最优切分点 缺失值如何处理？ 12概率权重(Probability Weights)：C45、ID3替代法(Alternate/Surrogate Splits)：CART 不完整数据如何处理？ 决策树是如何处理不完整数据的？-知乎 12345678（1）抛弃缺失值抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。（2）补充缺失值缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多,这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。（3）概率化缺失值对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,“F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和”,即F表示所给属性具有已知值样本的概率。 6. 代码实现代码中的公式均指李航老师的《统计学习方法》中的公式。 Decision_Tree.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# coding:utf-8import numpy as npimport pandas as pdclass DecisionTree(object): def __init__(self, feature_names, threshold, principle="information gain"): self.feature_names = feature_names self.threshold = threshold self.principle = principle # formula 5.7 def __calculate_entropy__(self, y): datalen = len(y) labelprob = &#123;l: 0 for l in set(y)&#125; entropy = 0.0 for l in y: labelprob[l] += 1 for l in labelprob.keys(): thisfrac = labelprob[l] / datalen entropy -= thisfrac * np.log2(thisfrac) return entropy # formula 5.8 def __calculate_conditional_entropy__(self, X, y, axis): datalen = len(y) featureset = set([x[axis] for x in X]) sub_y = &#123;f:list() for f in featureset&#125; for i in range(datalen): sub_y[X[i][axis]].append(y[i]) conditional_entropy = 0.0 for key in sub_y.keys(): prob = len(sub_y[key]) / datalen entropy = self.__calculate_entropy__(sub_y[key]) conditional_entropy += prob * entropy return conditional_entropy # formula 5.9 def calculate_information_gain(self, X, y, axis): hd = self.__calculate_entropy__(y) hda = self.__calculate_conditional_entropy__(X, y, axis) gda = hd - hda return gda def __most_class__(self, y): labelset = set(y) labelcnt = &#123;l:0 for l in labelset&#125; for y_i in y: labelcnt[y_i] += 1 st = sorted(labelcnt.items(), key=lambda x: x[1], reverse=True) return st[0][0] # formula 5.10 def calculate_information_gain_ratio(self, X, y, axis): gda = self.calculate_information_gain(X, y, axis) had = self.__calculate_entropy__(X[:, axis]) grda = gda / had return grda def __split_dataset__(self, X, y, axis, value): rstX = list() rsty = list() for i in range(len(X)): if X[i][axis] == value: tmpfeature = list(X[i][:axis]) tmpfeature.extend(list(X[i][axis+1:])) rstX.append(tmpfeature) rsty.append(y[i]) return np.asarray(rstX), np.asarray(rsty) def __best_split_feature__(self, X, y, feature_names): best_feature = -1 max_principle = -1.0 for feature_n in feature_names: axis = feature_names.index(feature_n) if self.principle == "information gain": this_principle = self.calculate_information_gain(X, y, axis) else: this_principle = self.calculate_information_gain_ratio(X, y, axis) print("%s\t%f\t%s" % (feature_n, this_principle, self.principle)) if this_principle &gt; max_principle: best_feature = axis max_principle = this_principle print("-----") return best_feature, max_principle def _fit(self, X, y, feature_names): # 所有实例属于同一类 labelset = set(y) if len(labelset) == 1: return labelset.pop() # 如果特征集为空集，置T为单结点树，实例最多的类作为该结点的类，并返回T if len(feature_names) == 0: return self.__most_class__(y) # 计算准则,选择特征 best_feature, max_principle = self.__best_split_feature__(X, y, feature_names) # 如果小于阈值，置T为单结点树，实例最多的类作为该结点的类，并返回T if max_principle &lt; self.threshold: return self.__most_class__(y) best_feature_label = feature_names[best_feature] del feature_names[best_feature] tree = &#123;best_feature_label: &#123;&#125;&#125; bestfeature_values = set([x[best_feature] for x in X]) for value in bestfeature_values: sub_X, sub_y = self.__split_dataset__(X, y, best_feature, value) tree[best_feature_label][value] = self._fit(sub_X, sub_y, feature_names) return tree def fit(self, X, y): feature_names = self.feature_names[:] self.tree = self._fit(X, y, feature_names) def _predict(self, tree, feature_names, x): firstStr = list(tree.keys())[0] secondDict = tree[firstStr] featIndex = feature_names.index(firstStr) key = x[featIndex] valueOfFeat = secondDict[key] if isinstance(valueOfFeat, dict): classLabel = self._predict(valueOfFeat, feature_names, x) else: classLabel = valueOfFeat return classLabel def predict(self, X): preds = list() for x in X: preds.append(self._predict(self.tree, self.feature_names, x)) return preds def output_tree(self): import treePlot # cite: https://gitee.com/orayang_admin/ID3_decisiontree/tree/master import importlib importlib.reload(treePlot) treePlot.createPlot(self.tree)def load_data(): dt = pd.read_csv("./credit.csv") # from lihang - "statistic learning method" - page59, table 5.1 # dt = pd.read_csv("./titanic.csv") data = dt.values feature_names = dt.columns[:-1] # delete label column return data, list(feature_names)def run_ID3(): data, feature_names = load_data() print("ID3 Descision Tree ... ") ml = DecisionTree(feature_names=feature_names, threshold=0, principle="information gain") ml.fit(data[:, :-1], data[:, -1]) test = [["mid", "yes", "no", "good"]] preds = ml.predict(test) print("ID3 predict:", preds) ml.output_tree()def run_C45(): data, feature_names = load_data() print("C45 Descision Tree ... ") ml = DecisionTree(feature_names=feature_names, threshold=0, principle="information gain ratio") ml.fit(data[:, :-1], data[:, -1]) test = [["mid", "yes", "no", "good"]] preds = ml.predict(test) print("C45 predict:", preds) ml.output_tree()if __name__ == '__main__': # run_ID3() run_C45() 结果： ID3 Descision Tree ... age 0.083007 information gain job 0.323650 information gain house 0.419973 information gain credit 0.362990 information gain ----- age 0.251629 information gain job 0.918296 information gain credit 0.473851 information gain ----- ID3 predict: [&#39;yes&#39;] C45 Descision Tree ... age 0.052372 information gain ratio job 0.352447 information gain ratio house 0.432538 information gain ratio credit 0.231854 information gain ratio ----- age 0.164411 information gain ratio job 1.000000 information gain ratio credit 0.340374 information gain ratio ----- C45 predict: [&#39;yes&#39;] 7. 参考 李航 - 《统计学习方法》 zergzzlun - cart树怎么进行剪枝？ 巩固,张虹 - 决策树算法中属性缺失值的研究 周志华 - 《机器学习》 OraYang的博客]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑斯谛回归 Logistic Regression]]></title>
    <url>%2F2018%2F03%2F11%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[逻辑斯谛回归 - Logistic Regression目录 逻辑斯谛分布 逻辑斯谛回归模型 逻辑斯谛模型的参数估计 逻辑斯谛模型的特点 代码实现 逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。1. 逻辑斯谛分布 设 $ X $ 是连续随机变量，$ X $ 服从逻辑斯谛分布是指 $ X $ 具有下列分布函数和密度函数： F(x)=P(X\le{x})=\frac{1}{1+e^{-(x-\mu)/\gamma}} f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^{2}}密度函数与分布函数图像如下,其中分布函数以$ (\mu,\frac{1}{2}) $中心对称。 2. 逻辑斯谛回归模型 P(Y=1|x)=\frac{exp(w\cdot x)}{1+exp(w\cdot x)} P(Y=0|x)=\frac{1}{1+exp(w\cdot x)} 3. 逻辑斯谛模型的参数估计设 P(Y=1|x)=\frac{exp(w\cdot x)}{1+exp(w\cdot x)}=\pi(x) P(Y=0|x)=\frac{1}{1+exp(w\cdot x)}=1-\pi(x)似然函数(N为样本数量)： l(w)=\prod_{i=1}^{N}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}对数似然： L(w)=logl(w)=\sum_{i=1}^N[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]对$ L(w) $求极值 其中$ x_i $表示第$ i $个训练样例；$x_{ij} $表示第$ i $个训练样例的第$ j $个维度。 $ N $表示样本数量； $ D $表示样本维度。 4. 逻辑斯谛模型的特点 优点： 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。 资源占用小，尤其是内存。因为只需要存储各个维度的特征值。 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。 缺点： 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。 5. 代码实现使用sklearn中的load_breast_cancer()作为二分类数据集。 Binary Logistic Regression 代码 # coding:utf-8 import time import numpy as np from datetime import timedelta from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.datasets import load_breast_cancer def time_consume(s_t): diff = time.time()-s_t return timedelta(seconds=int(diff)) class BinaryLR(object): def __init__(self): self.weight = None self.learning_rate = 0.001 self.max_iteration = 3000 def predict_single_sample(self, feature): feature = list(feature) feature.append(1.0) wx = np.sum(np.matmul(self.weight, feature)) exp_wx = np.exp(wx) if exp_wx/(1+exp_wx) &gt; 0.5: return 1 else: return 0 def fit(self, X, y): self.nlen = X.shape[0] self.ndim = X.shape[1] self.weight = np.zeros(shape=self.ndim+1) # add bias to weight correct = 0 exec_times = 0 while exec_times &lt; self.max_iteration: index = np.random.randint(0, self.nlen) feature = list(X[index]) label = self.predict_single_sample(feature) if label == y[index]: correct += 1 if correct &gt; self.max_iteration: break continue exec_times += 1 correct = 0 feature.append(1.0) wx = np.sum(np.matmul(self.weight, feature)) exp_wx = np.exp(wx) # update weight for i in range(self.weight.shape[0]): self.weight[i] -= self.learning_rate * (label - exp_wx / (1.0 + exp_wx)) * feature[i] if exec_times % 100 == 0: print(&quot;Times:%d TrainAcc:%.4f Timeusage:%s&quot; % (exec_times, self.accuracy(train_y, self.predict(train_x)), time_consume(start_time))) def predict(self, X): if self.weight is None: raise ValueError(&quot;Please train model first.&quot;) labels = list() for i in range(X.shape[0]): d = X[i] labels.append(self.predict_single_sample(d)) return labels def accuracy(self, y_true, y_pred): return accuracy_score(y_true, y_pred) def load_dataset(): data = load_breast_cancer() return data.data, data.target if __name__ == &#39;__main__&#39;: start_time = time.time() data, target = load_dataset() print(&quot;Data Shape:&quot;, data.shape, target.shape) train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=1024, shuffle=True) ml = BinaryLR() ml.fit(train_x, train_y) y_pred = ml.predict(test_x) accuracy = ml.accuracy(test_y, y_pred) print(&quot;Accuracy is &quot;, accuracy) print(&quot;Timeusage: %.2f s&quot; % (time.time()-start_time)) 运行结果如下：（不同的划分会产生不同的结果，且差异比较大） Times:100 TrainAcc:0.8518 Timeusage:0:00:00 Times:200 TrainAcc:0.8518 Timeusage:0:00:00 Times:300 TrainAcc:0.8518 Timeusage:0:00:00 Times:400 TrainAcc:0.8518 Timeusage:0:00:00 Times:500 TrainAcc:0.8518 Timeusage:0:00:00 Times:600 TrainAcc:0.8518 Timeusage:0:00:00 Times:700 TrainAcc:0.8518 Timeusage:0:00:00 Times:800 TrainAcc:0.8518 Timeusage:0:00:00 Times:900 TrainAcc:0.8518 Timeusage:0:00:00 Times:1000 TrainAcc:0.8518 Timeusage:0:00:00 Times:1100 TrainAcc:0.8518 Timeusage:0:00:00 Times:1200 TrainAcc:0.8518 Timeusage:0:00:00 Times:1300 TrainAcc:0.8518 Timeusage:0:00:00 Times:1400 TrainAcc:0.8518 Timeusage:0:00:00 Times:1500 TrainAcc:0.8518 Timeusage:0:00:00 Times:1600 TrainAcc:0.8518 Timeusage:0:00:00 Times:1700 TrainAcc:0.8518 Timeusage:0:00:00 Times:1800 TrainAcc:0.8518 Timeusage:0:00:00 Times:1900 TrainAcc:0.8518 Timeusage:0:00:00 Times:2000 TrainAcc:0.8518 Timeusage:0:00:00 Times:2100 TrainAcc:0.8518 Timeusage:0:00:00 Times:2200 TrainAcc:0.8518 Timeusage:0:00:00 Times:2300 TrainAcc:0.8518 Timeusage:0:00:00 Times:2400 TrainAcc:0.8518 Timeusage:0:00:00 Times:2500 TrainAcc:0.8518 Timeusage:0:00:00 Times:2600 TrainAcc:0.8518 Timeusage:0:00:00 Times:2700 TrainAcc:0.8518 Timeusage:0:00:00 Times:2800 TrainAcc:0.8518 Timeusage:0:00:00 Times:2900 TrainAcc:0.8518 Timeusage:0:00:00 Times:3000 TrainAcc:0.8518 Timeusage:0:00:00 Accuracy is 0.877192982456 Timeusage: 0.71 s 面试常问 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？ 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。 将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。 \theta_j = \theta_j - (y_i - h_{\theta}(x_i)) \cdot x_{ij}更新速度只和$ x_{ij}，y_i $相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 为什么不选平方损失函数的呢？ 其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和 sigmod 函数本身的梯度是很相关的。 sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？ 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。 为什么我们还是会在训练的过程当中将高度相关的特征去掉？ 去掉高度相关的特征会让模型的可解释性更好 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。 其次是特征多了，本身就会增大训练的时间。 参考： 李航 - 《统计学习方法》 周志华 - 《机器学习》 scikit-learn WenDesi’s Github]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯 Naive Bayes]]></title>
    <url>%2F2018%2F03%2F08%2Fnaive-bayes%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯 - Naive Bayes目录 基本思想 算法 后验概率最大化的意义 特点 代码实现 1. 基本思想朴素贝叶斯法通过训练数据集学习联合概率分布$ P(X,Y) $。具体地，学习以下先验概率分布及条件概率分布。先验概率分布：P(Y=c_k),\quad k=1,2,...,K 条件概率分布: P(X=x | Y=c\_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\_k),\quad k=1,2,...,K于是通过贝叶斯公式$P(X,Y) = P(X | Y)\cdot P(Y)$， 可以学习到联合概率分布$P(X,Y)$。 但是，条件概率分布$P(X=x | Y=c_k)$有指数级数量的参数，其估计实际是不可行的。事实上，假设$x^{(j)}$ 可取值有$S_j$个，$j=1,2,…,n$，$Y$可取值有$K$个，那么参数个数为$K\prod_{j=1}^nS_j$。 朴素贝叶斯法对条件概率分布作了条件独立性的假设，即假设特征之间相互独立，公式表达如下： P(X=x | Y=c\_k) \quad = \quad P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\_k) \quad = \quad \prod\_{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c\_k) 2. 算法输入： 训练数据$ T={((x_1, y_1)),…,(x_N,y_N)} $， 其中$ x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T $, $x_i^{(j)}$是第$i$个样本的第$j$个特征， $x_i^{(j)} \in {a_{j1},a_{j2},…,a_{jS_j}}$, $a_{jl}$是第$j$个特征可能取的第$l$个值， $j=1,2,…,n$, $l=1,2,…,S_j$, $y_i \in {c_1, c_2, …, c_k}$; 输出： 实例$x$的分类 （1）计算先验概率及条件概率 P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i=c_k)}{N}, \quad k=1,2,...,KP(X^{(j)}=a_{jl} \| Y=c_k)=\frac{\sum_{i=1}^N I(x_i^{(j)}, y_i=c_k)}{\sum_{i=1}^N I(y_i=c_k)}j=1,2,...,n; \quad l=1,2,...,S_j; \quad k=1,2,...,K（2）对于给定的实例$ x=(x^{(1)}, x^{(2)},.., x^{(n)})^T $，计算 P(Y=c_k) \prod_{j=1}^n P(X^{(j)}=x^{(j)} \| Y=c_k), \quad k=1,2,...,K（3）确定实例$ x $的类 y = \mathop{\arg\max}_{c_k} P(Y=c_k) \prod_{j=1}^n P(X^{(j)} \| Y=c_k) 3. 后验概率最大化的意义朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数： L(Y,f(X)) = \begin{cases} 1, & Y \neq f(X) \\ 0, & Y = f(X) \end{cases}式中$ f(X) $是决策函数，这时期望风险函数为 R_{exp}(f)=E[L(Y,f(X)]期望是对联合分布 $ P(X,Y) $取的，由此取条件期望 \begin{equation} \begin{aligned} R_{exp}(f) & = \iint_{D_{xy}}L(y,f(x)) \cdot P(x,y) dxdy \\ & = \int_{D_x} \int_{D_y} L(y, f(x)) \cdot P(y\|x) \cdot P(x) dxdy \\ & = \int_{D_x} [\int_{D_y} L(x,f(x)) \cdot P(y\|x) dy] P(x) dx \\ & = E_X \sum_{k=1}^K [L(c_k, f(X)]P(c_k|X) \end{aligned} \end{equation}为了使期望风险最小化，只需要对 $ X=x $逐个极小化，由此得到： \begin{equation} \begin{aligned} f(x) & = \mathop{\arg\min}_{y\in \mathbb{y}} \sum_{k=1}^K L(c_k,y)P(c_k|X=x) \\ & = \mathop{\arg\min}_{y\in \mathbb{y}} \sum_{k=1}^K P(y \neq c_k| X=x) \\ & = \mathop{\arg\min}_{y\in \mathbb{y}} (1-P(y=c_k|X=x) \\ & = \mathop{\arg\max}_{y\in \mathbb{y}} P(y=c_k|X=x) \end{aligned} \end{equation}这样，根据期望奉献最小化准则就得到了后验概率最大化准则： f(x)=\mathop{\arg\max}_{y=c_k} P(c_k|X=x)即朴素贝叶斯法采用的原理。 4. 特点 优点：源于古典数学理论，有稳定的分类效率；小数据好，可增量训练；对缺失数据不敏感，算法简单。 缺点：独立性假设强，实际中根据属性间的关系强弱，效果不同；须知先验概率，但先验有时取决于假设；对数据的输入形式敏感。 5. 代码实现Naive Bayes 代码 # coding:utf-8 import numpy as np import time from datetime import timedelta from sklearn.datasets import load_breast_cancer from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def time_consume(s_t): diff = time.time()-s_t return timedelta(seconds=int(diff)) class NaiveBayes(object): def __init__(self, class_num=-1, features_dim=-1): self.class_num = class_num self.features_dim = features_dim def fit(self, X, y): self.nlen = X.shape[0] # shape meaning: class_num prior_prob = np.zeros(shape=self.class_num) # shape meaning: class_num, feature_dim, feature_value conditional_prob = np.zeros(shape=(self.class_num, self.features_dim, 2)) for i in range(self.nlen): prior_prob[y[i]] += 1.0 for j in range(self.features_dim): conditional_prob[y[i]][j][X[i][j]] += 1.0 for i in range(self.class_num): for j in range(self.features_dim): p_0 = conditional_prob[i][j][0] p_1 = conditional_prob[i][j][1] prob_0 = p_0 / (p_0 + p_1) prob_1 = p_1 / (p_0 + p_1) conditional_prob[i][j][0] = prob_0 conditional_prob[i][j][1] = prob_1 self.prior_prob = prior_prob # 没有除self.nlen，对结果无影响 self.conditional_prob = conditional_prob def __calculate_prob__(self, sample, label): prob = int(self.prior_prob[label]) for i in range(self.features_dim): prob *= self.conditional_prob[label][i][sample[i]] return prob def predict(self, X): if self.class_num == -1: raise ValueError(&quot;Please fit first.&quot;) y_pred = np.zeros(shape=X.shape[0]) for i in range(X.shape[0]): label = 0 prob = self.__calculate_prob__(X[i], 0) for j in range(1, self.class_num): this_prob = self.__calculate_prob__(X[i], j) if prob &lt; this_prob: prob = this_prob label = j y_pred[i] = label return y_pred def accuracy(self, y_true, y_pred): return accuracy_score(y_true, y_pred) def load_data(): &quot;&quot;&quot;normalize value of data to {0, 1}&quot;&quot;&quot; data, target = load_breast_cancer(return_X_y=True) nlen = data.shape[0] ndim = data.shape[1] X = np.zeros(shape=data.shape, dtype=np.int32) for i in range(ndim): mean = np.mean(data[:, i]) for j in range(nlen): if data[j][i] &gt; mean: X[j][i] = 1 return X, target if __name__ == &#39;__main__&#39;: start_time = time.time() data, target = load_data() train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=2048, shuffle=True) ml = NaiveBayes(class_num=2, features_dim=30) ml.fit(train_x, train_y) y_pred = ml.predict(test_x) accuracy = ml.accuracy(test_y, y_pred) print(&quot;Accuracy is &quot;, accuracy) print(&quot;Timeusage: %s&quot; % (time_consume(start_time))) 结果如下： Accuracy is 0.947368421053 Timeusage: 0:00:00 6. 参考 李航 - 《统计学习方法》 刘建平Pinard’s Blog scikit-learn WenDesi’s Github]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>模型算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017开放学术精准画像大赛]]></title>
    <url>%2F2018%2F03%2F01%2F2017aca%2F</url>
    <content type="text"><![CDATA[队伍名：ICRC@HITSZ，获决赛第三名(3/412)，大赛主页本文是对比赛的思路等的整理，包含后期参考其他队的思路目录 Task 1 学者画像信息抽取 Task 2 学者兴趣标签预测 Task 3 学者未来影响力预测 评测方案 Task 1 学者画像信息抽取 Task 1 赛题学者画像信息具体包括学者的主页地址、性别、职位等。随着互联网越来越普及，与学者相关的网页的数量和内容的丰富度和复杂度都大大增加，其中包含了学者的大量冗余信息，通过整合互联网上多种来源的学者数据，采用合适的机器学习模型，获得学者的精准信息是一项潜在有效的学者画像技术。 给定学者姓名、学者所属机构，以及谷歌搜索关键字“学者姓名＋机构”返回的第一页搜索结果的缓存页面的URL（静态页面，一般包含10条搜索结果），同时允许参赛者访问其中每条搜索结果的网页链接以及该网页内的链接，要求抽取学者的个人主页URL、学者头像URL、邮箱地址、性别、职称/职位列表，以及学者当前所属机构所在国家。 示例: 123456789101112输入：scholar name: Jiawei Hanorganization name: UIUCsearch results page: http://xxx.xxx/xxx.html输出：homepage: http://hanj.cs.illinois.edu/ (注释: 从搜索结果中找)pic: http://hanj.cs.illinois.edu/images/hanj_tour.jpg (注释: 从个人主页中找)email: hanj[at]cs.uiuc.edu (注释: email的格式跟学者个人主页中保持一致)gender: m (注释: m 代表 male, f 代表 female)position: Abel Bliss Professor (注释: 需要跟个人主页中的格式保持一致)location: USA (注释: 需要跟个人主页中的格式保持一致) Task 1 思路个人主页URL 对于个人主页搜索结果的每条记录提取特征后进行是否是主页的分类，其中概率最高的被判断为个人主页。 特征如下： 网站在搜索结果中的排名；-&gt; 1 学者所在机构是否为大学/学校；-&gt; 1 搜索结果标题/摘要以及学者所在机构的文本长度 -&gt; 3 url中是否包含给定关键词(社交类linkedin……、论文类spring……、正向词homepage……)，对每个部分做平均值为特征； -&gt; 35 搜索结果摘要中是否包含mail，address； -&gt; 2 搜索结果的标题/摘要中的学者姓名得分。这里将学者姓名分词为n份，如果其中m份在文本中出现，那么姓名的得分就是 m/n，同时增加完整名字在标题摘要中出现的得分； -&gt; 1 123456789101112131415一级负向词&apos;facebook&apos;, &apos;linkedin&apos;, &apos;twitter&apos;, &apos;youtube&apos;, &apos;instagram&apos;, &apos;quora&apos;, &apos;blots&apos;, &apos;dl.acm&apos;, &apos;arxiv&apos;, &apos;ieeexplore&apos;, &apos;springer&apos;, &apos;webofknowledge&apos;, &apos;library&apos;, &apos;researchgate&apos;, &apos;books.google&apos;, &apos;netprofile&apos;, &apos;cnki&apos;, &apos;ted.com&apos;, &apos;antpedia&apos;, &apos;academictree&apos;, &apos;phdtree&apos;, &apos;shguang.net&apos;, &apos;airbnb&apos;, &apos;bloomberg&apos;, &apos;pinterest&apos;, &apos;weibo&apos;, &apos;hmetmer&apos;, &apos;peekyou&apos;, &apos;mitbbs&apos;, &apos;schneier.com&apos;, &apos;directory.ubc.ca&apos;, &apos;citation&apos;, &apos;videolectures&apos;, &apos;slideshare&apos;, &apos;brokercheck.finra.org&apos;,二级负向词&apos;pdf&apos;, &apos;news&apos;, &apos;report&apos;, &apos;scholarmate&apos;, &apos;journals&apos;, &apos;ratemyprofessors&apos;,&apos;doc&apos;, &apos;conference&apos;, &apos;wiki&apos;, &apos;scholar&apos;, &apos;book&apos;, &apos;publication&apos;, &apos;github&apos;, &apos;crunchbase&apos;,&apos;group&apos;, &apos;scholarmate&apos;, &apos;google&apos;, &apos;stories&apos;, &apos;story&apos;, &apos;relationshipscience&apos;,正向词&apos;~&apos;, &apos;edu&apos;, &apos;home&apos;, &apos;faculty&apos; 由于正负样本不均衡(1:9)，所以首先尝试通过过采样的方式进行处理。具体如下： 重复增加正样本的数量，尝试通过XGBoost模型及SVM模型进行分类，最终在训练集中的效果有提升，但在测试集上变化不大； (未实现)尝试通过SMOTE算法进行过采样。 鉴于第一种方式的效果并不明显，分析由于样本特征比较少(43维)，导致出现了过拟合的现象出现。 接下来尝试通过负采样的方式进行解决，具体如下： 因为负样本数量多，进行随机选择，构造新的训练数据集； 通过XGBoost模型及SVM模型进行分类，效果明显得到提升。 其中XGBoost效果比SVM效果稍微好一点。 XGBoost可以自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线性化需要基于核函数，要求学习之前就要定好核函数，然而确定核函数并不是一件容易的事； 多分类器隐含正则项：XGBoost自带正则化项（包含叶结点数量等），SVM通常需要增加L2正则 真实世界的数据具有特征多样性的特点，SVM本质是一个几何模型，需要人为去定义样例之间的核函数或者相似性； 多种可调的参数，效率、灵活、易用方面，XGBoost都有比较好的交互使用。 最终我们在个人主页上的测试集取得了0.74的准确率，考虑到数据噪声以及搜索结果中多个正确主页的存在，真实的准确率应该接近0.85。 性别 主要通过学者的名字进行判断。通过网上的开源姓名性别数据，训练了一个朴素贝叶斯分类器，其中用到的特征是名字的后2位到后7位字母(每一位为一维特征)，以及名字长度，名字分词长度。如果学者名字长度不足的，用空格填充。 结果上能取得0.9的准确率。 邮箱 主要通过正则表达式进行识别。根据RFC2822中的邮箱标准，并结合训练集中邮箱的格式，我们写了一个很复杂的正则表达式，另外针对仍然无法获取到邮箱的，我们捕获网页中的mailto的链接来确定该学者的邮箱。 这样仍然有一部分的图片邮件或者通过加密技术加密的邮箱无法获得，针对图片邮箱，我们通过捕获图片的名称，判断是否和邮件相关，相关即取该图片的url。 对于加密邮箱，暂未想到比较合适的方法，而且这种邮箱数量比较少，所以对最终结果应该不大。 照片 首先利用正则表达式+xpath等手段，获取到网页中的所有图片url及图片的名称，对于图片名称是学者名字或名字一部分的图片，直接判断该图片为学者的照片； 若没有获得结果，使用开源工具face_recognition对网页的图片进行人脸识别，当该网页中的人脸数量少于阈值4的时候，选择第一个人脸照片为该学者的照片，对人脸数量大于阈值且没有获得邮箱的，认为对学者的个人主页选择错误，并将该学者的信息留空。 职称 通过统计训练集中的职称，并做了一些加工之后，使用正则表达式对网页的内容进行匹配。 国家 主要根据获取的邮箱、以及网页中出现的地址、电话、传真等地点指向性词进行判断；另外学者的名字有时也具有一定的指向性。 当无法获得有效信息时，将该信息留空。 其他因为该任务需要联网获取数据，且学者的网站服务器良莠不齐，所以实现了一个多进程并行的爬虫。主要用到的Python库如下： requests selenium urllib.request lxml multiprocessing bs4 Task 2 学者兴趣标签预测Task 2 赛题研究兴趣是学者画像的重要组成部分，其不仅是学者本身的研究心得或研究拓展方向的集中体现，也能从中窥视不同背景的学者对研究领域热点或学科研究趋势的关注度、敏感度的集体反映。与学者画像信息抽取类似，通过整合互联网上的大规模多源信息，可以对学者的研究兴趣进行判断。 给定学者已发表的论文信息和合著关系网络，参赛者需为每位学者标注5个最合适的兴趣标签。 示例: 12345输入：name: Jiawei Han输出：research interests: data mining, database, information networks, knowledge discovery, machine learning Task 2 思路本任务主要用到论文信息中的title以及期刊，首先将论文title分别以作者和标签为单位收集在一起并打上标签 主要是基于词袋模型，以训练集作者为单位收集的标题文本，分词去停用次后使用卡方检验进一步筛选特征，按卡方值筛选了前10%的特征词作为特征， 以标签为单位收集所有相关作者的标题文本，筛选了50%的特征词作为特征。 确定文档特征词后，采用词的tf-idf作为文档向量相应位置的权重。 考虑到文本分类大多都是线性可分的最终采用sklean中的SGDClassifier，分别对上述构建的两个数据集（以作者为单位收集的文档和以标签为单位收集的文档）训练了两个模型；尝试过Stacking，sklearn中的BaggingClassifier， GridSearchCV 5 folds来寻找最优参数； 因为本任务是多标签预测，所以我们根据训练的模型为每个作者输出概率最大的n个标签以及概率(以作者为单位的模型输出24个标签做候选标签集合，以标签为单位的模型输出23个标签做候选标签集合) 训练两个模型(以作者为单位和以标签为单位)从不同的角度提取的特征，是一个相互补充，相互修正的作用；将两个模型的候选标签集合取并集，遍历候选标签集合，评价的量为每个标签两个模型预测的概率的乘积， 期间统计作者发过的论文，记录title中出现该标签的频数(限定该作者在合著名单里为2作或者3作以内)，最后在每个标签评价量上加频数 * 评价量。最终排序取top5个标签为当前作者的标签。 Task 3 学者未来影响力预测Task 3 赛题学术影响力用来衡量学者在专业理论及技术方面的影响，常用的评价指标有论文被引量，期刊影响因子、作者H指数等，其中论文被引量是一个重要而直观的指标。本任务的目的是基于学者当前的相关学术数据预测其未来某段时间内的总论文被引量。 给定学者截止到2013年底的全部论文数据（包括论文详细引用关系，详见数据描述部分），参赛者需预测学者截止至2017年6月的总被引用数. 示例: 12345输入：name: Jiawei Han输出：citation number: 126147 Task 3 思路预处理 papers.txt包含了学者截止2016年所有论文的数据，其中需要预测的学者隐去了2013年后所发表的论文。所以任务三只用到2013(包含2013)年前的论文数据， 对paper.txt中的学者进行统计引用数, 去除实际引用数小于统计引用数训练集中的学者。 统计发现，学者论文引用数为0的，约占整个训练集的24%，故尝试首先通过二分类，判断学者是否具有论文引用量，之后对具有论文引用量的学者进行回归预测(两次使用同样的特征)。 特征 学者距今0,1,2,3,4,5,6,7,8,9年的文章数被引用数(cited)； 学者文章被引用数的平均值，最小值，最大值； 学者距今0,1,2,3,4,5,6,7,8,9,10,20,30,40,50年的引用数(reference)； 学者学术年龄、学者发表的会议期刊的数量； 学者的hindex以及近两年的变化值； 学者的共同作者每四年中hindex的最小值,最大值,平均值,和； 会议期刊文章总数，hindex，引用他人文章的最小值，最大值，平均值以及近两年的变化值； 会议期刊被引用数的中最小值，最大值，平均值，和； 会议期刊按被引用数排序的特征； 以共同作者关系建边图的pagerank值，以及使用共同作者次数加权的weighted_pagerank(作者合作的越多，引用数越接近)； 使用每篇论文title建模LDA模型求出每篇论文困惑度(perplexity)和主题(topic)权值，和每个topic的被引用数，得到每个作者论文按主题计算的引用数和排序结果； 通过有label的训练集中作者的真实被引用数估计每篇论文的权重，然后使用每篇文章的权重来预测未知作者的权重weight1(作者的权重由其论文权重决定)； 根据训练集的label和论文被引用数关系迭代训练得出每篇论文的权值，从而每个作者得到权重weight2(比起weight1使用了paper.txt统计的论文被引用数)； 模型 分类模型：XGBClassifier(训练集上统计分类准确率为92.3%) 回归模型：XGBoost中XGBRegressor, sklearn中的RandomForestRegressor, ExtraTreesRegressor, SVR, LinearRegressor，并对结果进行Stacking集成 其他trick 学者的论文被引用数的跨度很大，有0，有10W+，所以在预测的时候对标签使用了log进行平滑； 评测方案Task 1任务1中，每个学者共有 k (在该项任务中 k = 6)项个人画像数据需要预测。这 k 个中除了职称/职位外都通过完全匹配的方式进行评测，完全匹配上得1分，否则得0分；而职称/职位由于是一个集合，所以通过Jaccard index (即两个集合交集的元素个数除以两个集合并集的元素个数) 来进行计算提取出来的集合跟标注答案给出的集合的相似度 (介于0 ~ 1之间)。一个学者的画像数据的预测值的最终得分为这 k 个的平均分。一个参赛选手在该任务上的最终得分为对各个学者的画像数据的预测得分的平均分。即： score1 = \frac{1}{kN} \sum_{i=1}^N \sum_{j=1}^k s_i其中$ s_i $表示 k 项画像数据中第 i 项的得分，取值0 ~ 1。 Task 2任务2的得分score2为参赛队伍计算生成的学者兴趣与给定的学者兴趣完全相同的比例，即： score2 = \frac{1}{N} \sum_{i=1}^N \frac{\| T_i \bigcap T_i^{\ast} \|}{\| T_i^{\ast} \|}其中，N为任务2的评测集样本个数，$T_i$为计算生成的用户i 的兴趣集合,$T_i^{\ast}$为给定的用户i 的兴趣集合。 Task 3任务3的得分score3用参赛队伍预测的学者被引数与给定的学者真实被引数之间的相对误差来计算来计算 score3 = 1 - \frac{1}{N} \sum_{i=1}^N \begin{cases} 0, \qquad \qquad \quad v_i=0, v_i^{\ast}=0 \\ \| v_i - v_i^{\ast} \| / max(v_i, v_i^{\ast}), \quad otherwise \end{cases}其中，N 为任务3的评测集样本个数，$v_i$为用户i 的预测被引数，$v_i^{\ast}$为用户i 的真实被引数。 最终得分 score = score1 + score2 + score3]]></content>
      <categories>
        <category>个人小结</category>
      </categories>
      <tags>
        <tag>比赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分查找 - BinarySearch]]></title>
    <url>%2F2017%2F12%2F23%2Fbinarysearch%2F</url>
    <content type="text"><![CDATA[详细原文这里：特别好用的二分查找法模板 1. 二分查找的基本思想二分查找的基本思想是：夹逼法或者叫排除法在每一轮循环中，都可以排除候选区间里将近一半的元素，进而使得候选区间越来越小，直至有限个数（通常为1个），而这个数就有可能是我们要找的数（在一些情况下，还需要单独做判断）。 2. 编码的细节 思考左右边界，如果左右边界不能包括目标数值，二分查找法是怎么都写不对的； 先写逻辑上容易想到的分支逻辑，这个分支逻辑通常是排除中位数的逻辑； 只写两个分支，一个分支排除中位数，另一个分支不排除中位数； 根据分支的逻辑选择中位数的类型，可能是左中位数，也可能是右中位数，标准是避免出现死循环； 退出循环的时候，可能需要对夹逼法剩下的那个数做一次判断，这一步叫做后处理 3. 细节思考 参考代码 1：重点理解为什么候选区间的索引范围是 [0, size]。 1234567891011121314151617181920212223from typing import Listclass Solution: def searchInsert(self, nums: List[int], target: int) -&gt; int: # 返回大于等于 target 的索引，有可能是最后一个 size = len(nums) # 特判 if size == 0: return 0 left = 0 # 如果target比nums里所有的数都大，则最后一个数的索引+1就是候选值，因此右边界应该是数组的长度 right = size # 二分的逻辑一定要写对，否则会出现死循环或者数组下标越界 while left &lt; right: mid = left + (right - left) // 2 if nums[mid] &lt; target: left = mid + 1 else: assert nums[mid] &gt;= target # [1,5,7] 2 right = mid return left 参考代码 2：对于是否接在原有序数组后面单独判断，不满足的时候再在候选区间的索引范围 [0, size-1] 内使用二分查找法进行搜索。 12345678910111213141516171819202122232425from typing import Listclass Solution: def searchInsert(self, nums: List[int], target: int) -&gt; int: # 返回大于等于 target 的索引，有可能是最后一个 size = len(nums) # 特判 1 if size == 0: return 0 # 特判 2：如果比最后一个数字还要大，直接接在它后面就可以了 if target &gt; nums[-1]: return size left = 0 right = size - 1 # 二分的逻辑一定要写对，否则会出现死循环或者数组下标越界 while left &lt; right: mid = left + (right - left) // 2 if nums[mid] &lt; target: left = mid + 1 else: assert nums[mid] &gt;= target right = mid return left 4. 技巧&amp;调试方法&amp;注意事项 先来看int mid = left + (right - left) / 2， 其中left和right都是可以取到的数组index 当 left 和 right 是很大的整数的时候，如果写int mid = (left + right) / 2; 这里 left + right 的值就有可能超过 int 类型能表示的最大值， 因此使用 mid = left + (right - left) // 2 可以避免这种情况。 事实上，mid = left + (right - left) // 2 在 right 很大、 left 是负数且很小的时候， right - left 也有可能超过 int 类型能表示的最大值， 只不过一般情况下 left 和 right 表示的是数组索引值，left 是非负数，因此 right - left 溢出的可能性很小。 建议在Java中用 (left + right) &gt;&gt;&gt; 1，在Python中用 (left + right) &gt;&gt; 1，都是无符号右移。 当数组的元素个数是偶数的时候，中位数有左中位数和右中位数之分： 当数组的元素个数是偶数的时候： 使用 mid = left + (right - left) // 2 得到左中位数的索引； 使用 mid = left + (right - left + 1) // 2 得到右中位数的索引。 当数组的元素个数是奇数的时候，以上二者都能选到最中间的那个中位数。 编写分支逻辑的时候，先写 “排除逻辑” 所在的分支。 先考虑能把 “中位数” 排除在外的逻辑，而不能排除 “中位数” 的逻辑放在 else 分支里，这样做的理由有 2点： 可以排除 “中位数” 的逻辑，通常比较好想，但并不绝对，这一点视情况而定 分支条数变成 2 条，比原来 3 个分支要考虑的情况少，好处是: 不用在每次循环开始单独考虑中位数是否是目标元素，节约了时间，我们只要在退出循环的时候，即左右区间压缩成一个数（索引）的时候，去判断这个索引表示的数是否是目标元素，而不必在二分的逻辑中单独做判断。 根据分支编写的情况，选择使用左中位数还是右中位数 先写判断分支，根据分支的逻辑选中位数，选左中位数还是右中位数，这要做的理由是为了防止出现死循环。死循环容易发生在区间只有 2 个元素时候，此时中位数的选择尤为关键。 当出现死循环的时候的调试方法：打印输出左右边界、中位数的值和目标值、分支逻辑等必要的信息 5. 使用总结 无脑地写 while left &lt; right: ，这样你就不用判断，在退出循环的时候你应该返回 left 还是 right，因为返回 left 或者 right 都对； 先写分支逻辑，并且先写排除中位数的逻辑分支（因为更多时候排除中位数的逻辑容易想），另一个分支的逻辑你就不用想了，写出第1个分支的反面代码即可，再根据分支的情况选择使用左中位数还是右中位数；左右分支的规律就如下两点： 如果第 1 个分支的逻辑是 “左边界排除中位数”（left = mid + 1），那么第 2 个分支的逻辑就一定是 “右边界不排除中位数”（right = mid），反过来也成立；这种情况的时候选择左中位数，即加一选左。 如果第 2 个分支的逻辑是 “右边界排除中位数”（right = mid - 1），那么第 2 个分支的逻辑就一定是 “左边界不排除中位数”（left = mid），反之也成立；这种情况的时候选择右中位数，即减一选右。 分支条数只有 2 条，代码执行效率更高，不用在每一轮循环中单独判断中位数是否符合题目要求，写分支的逻辑的目的是尽量排除更多的候选元素，而判断中位数是否符合题目要求我们放在最后进行； 注意事项 左中位数还是右中位数选择的标准根据分支的逻辑而来，标准是每一次循环都应该让区间收缩，当候选区间只剩下 2 个元素的时候，为了避免死循环发生，选择正确的中位数类型。 如果实在很晕，不防使用有 2 个元素的测试用例，另外在代码出现死循环的时候，建议可以将左边界、右边界、你选择的中位数的值，还有分支逻辑都打印输出一下，出现死循环的原因就一目了然了； 如果能确定要找的数就在候选区间里，那么退出循环的时候，区间最后收缩成为 1 个数后，直接把这个数返回即可； 如果你要找的数有可能不在候选区间里，区间最后收缩成为 1 个数后，还要单独判断一下这个数是否符合题意。 6. 模版1234567891011121314151617# Python: 当分支逻辑不能排除右边界时，选左中位数；如果选右中位数会出现死循环def binary_search_template1(left, right): # 如果选择右中位数，当区间只剩下2个元素时， # 一旦进入 right=mid 这个分支，右边界不会收缩，会进入死循环 while left &lt; right: # 选择左中位数，无符号右移 mid = (left + right) &gt;&gt; 1 if check(mid): # 先写排除中位数的逻辑 left = mid + 1 else: # 右边界不能排除 right = mid # 退出循环时一定有 left == right # 视情况分析是否需要单独判断 left(或者right) 这个索引表示的元素是否符合题意 pass 12345678910111213141516# Python: 当分支逻辑不能排除左边界时，选右中位数；如果选左中位数会出现死循环def binary_search_template2(left, right): # 如果选择左中位数，当区间只剩下2个元素时， # 一旦进入 left=mid 这个分支，右边界不会收缩，会进入死循环 while left &lt; right: # 选右中位数 mid = (left + right + 1) &gt;&gt; 1 if check(mid): # 先写排除中位数的逻辑 right = mid - 1 else: # 左边界不能排除 left = mid # 退出循环时一定有 left == right # 视情况分析是否需要单独判断 left(或者right) 这个索引表示的元素是否符合题意 pass 虽说是两个模板，区别在于选中位数，中位数根据分支逻辑来选，原则是区间要收缩，且不出现死循环，退出循环的时候，视情况，有可能需要对最后剩下的数单独做判断。 7. 二分排序1234567891011121314151617181920212223242526// 时间复杂度：O(n) * O(logn) = O(nlogn)void binarysort(vector&lt;int&gt; &amp;arr) &#123; int start, end, tmp = 0, mid, j; // 每次将 arr[i] 插入到排好序的 arr[0:i-1] 中，时间复杂度 O(n) for (int i = 1; i &lt; arr.size(); i++) &#123; start = 0; // arr[i] 可能就是较大的，要放在arr[i]的位置，因此 end = i end = i; // 待插入的值tmp tmp = arr[i]; // 二分查找到要插入的index，时间复杂度 O(logn) while (start &lt; end) &#123; mid = (start + end) &gt;&gt; 1; if (arr[mid] &lt; tmp) start = mid + 1; else end = mid; &#125; // 往后挪值 for (j = i - 1; j &gt;= start; j--) &#123; arr[j + 1] = arr[j]; &#125; // 插入 arr[start] = tmp; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归并排序 - MergeSort]]></title>
    <url>%2F2017%2F12%2F22%2Fmergesort%2F</url>
    <content type="text"><![CDATA[归并排序 - MergeSort 合并函数1234567891011121314151617181920void merge(int *arr, int left, int mid, int right)&#123; int *tmparr = new int[right - left + 1]; int leftindex = left, rightindex = mid + 1; int tmpindex = 0; while (leftindex &lt;= mid &amp;&amp; rightindex &lt;= right) &#123; if (arr[leftindex] &lt; arr[rightindex]) tmparr[tmpindex++] = arr[leftindex++]; else tmparr[tmpindex++] = arr[rightindex++]; &#125; while (leftindex &lt;= mid) tmparr[tmpindex++] = arr[leftindex++]; while (rightindex &lt;= right) tmparr[tmpindex++] = arr[rightindex++]; tmpindex = 0; while (left &lt;= right) arr[left++] = tmparr[tmpindex++];&#125; 递归123456789void mergesort_recursive(int *arr, int left, int right)&#123; if (left &gt;= right) return; int mid = (left + right) / 2; mergesort_recursive(arr, left, mid); mergesort_recursive(arr, mid + 1, right); merge(arr, left, mid, right);&#125; 非递归12345678910111213141516void mergesort_nonerecursive(int *arr, int left, int right)&#123; int step = 2, i = 0; while (step &lt;= right + 1) &#123; i = 0; while (i + step &lt;= right) &#123; merge(arr, i, i + step / 2 - 1, i + step - 1); i += step; &#125; merge(arr, i, i + step / 2 - 1, right); step *= 2; &#125; merge(arr, 0, step / 2 - 1, right);&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序 - QSort]]></title>
    <url>%2F2017%2F12%2F21%2Fqsort%2F</url>
    <content type="text"><![CDATA[快速排序 - QSort 第一种Partition的方法1234567891011121314151617181920212223242526272829int Partition(int* data, int length, int start, int end)&#123; if (data == nullptr || length &lt;= 0 || start &lt; 0 || end &gt;= length) return -1; int small = start - 1; for (int index = start; index &lt; end; index++) &#123; if (data[index] &lt; data[end]) &#123; small++; if (small != index) swap(data[index], data[small]); &#125; &#125; small++; swap(data[small], data[end]); return small;&#125;void Qsort(int* data, int length, int start, int end)&#123; if (start == end) return; int index = Partition(data, length, start, end); if (index &gt; start) Qsort(data, length, start, index - 1); if (index &lt; end) Qsort(data, length, index + 1, end);&#125; 第二种Partition的方法(更容易理解)12345678910111213141516171819202122232425int myPartition(int* data, int length, int start, int end)&#123; int left = start, right = end - 1; while (left &lt; right) &#123; while (data[left] &lt; data[end] &amp;&amp; left &lt; right) left++; while (data[right] &gt;= data[end] &amp;&amp; left &lt; right) right--; swap(data[left], data[right]); &#125; if (data[left] &gt;= data[end]) swap(data[left], data[end]); else swap(data[++left], data[end]); return left;&#125;void myQsort(int* data, int length, int start, int end)&#123; if (data == nullptr || length &lt;= 0 || start &gt;= end) return; int index = myPartition(data, length, start, end); myQsort(data, length, start, index - 1); myQsort(data, length, index + 1, end);&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex公式语法]]></title>
    <url>%2F2017%2F12%2F12%2Flatex%2F</url>
    <content type="text"><![CDATA[KaTex用法如下： KATEX: https://katex.org/docs/supported.html LaTex用法如下：目录 1.上标与下标 2.分式 3.根式 4.求和与积分 5.公式中的空格 6.公式中的定界符 7.矩阵 8.排版数组 9.数学模式重音符号 10.希腊字母 11.二元关系 12.“大”运算符 13.箭头 14.定界符 15.大定界符 16.其他符号 17.非数学符号 18.AMS定界符 19.AMS希腊和希伯来字母 20.AMS二元关系 21.AMS箭头 22.AMS二元否定关系符和箭头 23.AMS二元运算符 24.AMS其他符号 25.数学字母 1.上标与下标上标命令是 ^{角标}，下标命令是 _{角标}。当角标是单个字符时可以不用花括号(在 LaTeX 中，花括号是用于分组，即花括号内部文本为一组)。 1234$$x_1$$$$x_1^2$$$$x_&#123;22&#125;^&#123;(n)&#125;$$$$&#123;&#125;^*\!x^*$$ x_1x_1^2x_{22}^{(n)}{}^*\!x^*2.分式输入带有水平分数线的公式，可用命令：\frac{分子}{分母}； 12$$\frac&#123;x+y&#125;&#123;2&#125;$$$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$ \frac{x+y}{2}\frac{1}{1+\frac{1}{2}}3.根式123$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$ \sqrt{2}]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown语法]]></title>
    <url>%2F2017%2F12%2F12%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[主要内容 Markdown是什么？ 谁创造了它？ 为什么要使用它？ 怎么使用？ 谁在用？ 正文1. Markdown是什么？Markdown是一种轻量级标记语言，它以纯文本形式(易读、易写、易更改)编写文档，并最终以HTML格式发布。Markdown也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。 2. 为什么要使用它？ 它是易读（看起来舒服）、易写（语法简单）、易更改纯文本。处处体现着极简主义的影子。 兼容HTML，可以转换为HTML格式发布。 跨平台使用。 越来越多的网站支持Markdown。 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail） 摆脱Word（我不是认真的）。 3. 怎么使用？如果不算扩展，Markdown的语法绝对简单到让你爱不释手。 Markdown语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \，符号’`’。 3.1 标题两种形式：1）使用=和-标记一级和二级标题。 一级标题=========二级标题--------- 效果： 一级标题 二级标题 2）使用#，可表示1-6级标题。 # 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 效果： 一级标题二级标题三级标题四级标题五级标题六级标题 3.2 段落段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。 3.3 区块引用在段落的每行或者只在第一行使用符号&gt;,还可使用多个嵌套引用，如： > 区块引用>&gt; 嵌套引用 效果： 区块引用 嵌套引用 3.4 代码区块代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如普通段落： void main(){ printf(“Hello, Markdown.”);} 代码区块： void main() { printf(&quot;Hello, Markdown.&quot;); } 注意:需要和普通段落之间存在空行。 3.5 强调在强调内容两侧分别加上*或者_，如： *斜体*，_斜体_**粗体**，__粗体__ 效果： 斜体，斜体粗体，粗体 3.6 列表使用·、+、或-标记无序列表，如： -（+*） 第一项 -（+*） 第二项 - （+*）第三项 注意：标记后面最少有一个空格或制表符。若不在引用区块中，必须和前方段落之间存在空行。 效果： 第一项 第二项 第三项 有序列表的标记方式是将上述的符号换成数字,并辅以.，如： 1 . 第一项2 . 第二项3 . 第三项 效果： 第一项 第二项 第三项 3.7 分割线分割线最常使用就是三个或以上*，还可以使用-和_。 3.8 链接链接可以由两种形式生成：行内式和参考式。行内式： [younghz的Markdown库](https:://github.com/younghz/Markdown “Markdown”)。 效果： younghz的Markdown库。 参考式： [younghz的Markdown库1][1][younghz的Markdown库2][2][1]:https:://github.com/younghz/Markdown “Markdown”[2]:https:://github.com/younghz/Markdown “Markdown” 效果： younghz的Markdown库1younghz的Markdown库2 注意：上述的[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;不出现在区块中。 3.9 图片添加图片的形式和链接相似，只需在链接的基础上前方加一个！。 3.10 反斜杠\相当于反转义作用。使符号成为普通符号。 3.11 符号’`’起到标记作用。如： `ctrl+a` 效果： ctrl+a 4. 谁在用？Markdown的使用者： GitHub 简书 Stack Overflow Apollo Moodle Reddit 等等 5. 尝试一下 Chrome下的插件诸如stackedit与markdown-here等非常方便，也不用担心平台受限。 在线的dillinger.io评价也不错 Windowns下的MarkdownPad也用过，不过免费版的体验不是很好。 Mac下的Mou是国人贡献的，口碑很好。 Linux下的ReText不错。 当然，最终境界永远都是笔下是语法，心中格式化 :)。 注意：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。 虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber] (http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。 以上基本是所有traditonal markdown的语法。 其它：列表的使用(非traditonal markdown)： 用|表示表格纵向边界，表头和表内容用-隔开，并可用:进行对齐设置，两边都有:则表示居中，若不加:则默认左对齐。 代码库 链接 MarkDown https://github.com/younghz/Markdown MarkDownCopy https://github.com/younghz/Markdown 关于其它扩展语法可参见具体工具的使用说明。]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda解决Python2和Python3共存]]></title>
    <url>%2F2017%2F11%2F13%2Fanaconda%2F</url>
    <content type="text"><![CDATA[Anaconda解决Python2和Python3共存下载Anaconda直接从官网下载即可，https://www.anaconda.com/download/。建议选择Python3版本的Anaconda进行下载，下载完成安装即可。conda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令 多版本切换# 基于 python3.6 创建一个名为env_py3 的环境 conda create --name env_py3 python=3.6 # 基于 python2.7 创建一个名为env_py2 的环境 conda create --name env_py2 python=2.7 # 激活 env 环境 activate env_py2 # windows source activate env_py2 # linux/mac # 退出 env 环境 deactivate # windows source deactivate # linux/mac]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Jekyll With Mathjax]]></title>
    <url>%2F2017%2F11%2F12%2Fjekywithmathjax%2F</url>
    <content type="text"><![CDATA[设置Jekyll和Github上的Mathjax如何做呢？很简单。 1.到你的Jekyll目录下，默认路径为username.github.io/_includes/head.html 2.将下列代码复制粘贴到里面（注意要放到 … &lt;/head>之间） 12345678910111213&lt;head&gt; ... &lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; TeX: &#123; equationNumbers: &#123; autoNumber: "all" &#125; &#125; &#125;); &lt;/script&gt; &lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; tex2jax: &#123; inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true &#125; &#125;); &lt;/script&gt; &lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;&lt;/head&gt; 3.完成！ 4.测试一下吧！ E = m\cdot c^2 \label{eq:mc2}Thanks csega]]></content>
      <categories>
        <category>小工具</category>
      </categories>
  </entry>
</search>
