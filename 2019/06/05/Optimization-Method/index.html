<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/github-logo.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/github-logo.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    save_scroll: false,
    copycode: {"enable":true,"show_result":true,"style":"default"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="本文是从各个论文、博客、专栏等学习整理所得,如有任何错误疏漏等问题,欢迎评论或邮箱提出,大家一起学习进步！0 优化算法框架 计算目标函数关于当前参数的梯度  g_t = \nabla f(w_t) \tag{0-1} 根据历史梯度计算一阶动量和二阶动量  m_t = \phi (g_1, g_2, \cdots, g_t) \tag{0-2} V_t = \psi (g_1, g_2, \cdot">
<meta name="keywords" content="优化算法">
<meta property="og:type" content="article">
<meta property="og:title" content="优化算法">
<meta property="og:url" content="https://weiguozhao.github.io/2019/06/05/Optimization-Method/index.html">
<meta property="og:site_name" content="Weiguo&#39;s Station">
<meta property="og:description" content="本文是从各个论文、博客、专栏等学习整理所得,如有任何错误疏漏等问题,欢迎评论或邮箱提出,大家一起学习进步！0 优化算法框架 计算目标函数关于当前参数的梯度  g_t = \nabla f(w_t) \tag{0-1} 根据历史梯度计算一阶动量和二阶动量  m_t = \phi (g_1, g_2, \cdots, g_t) \tag{0-2} V_t = \psi (g_1, g_2, \cdot">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/5.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/1.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/6.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/7.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/8.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/9.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/10.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/2.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/3.png">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/4.png">
<meta property="og:updated_time" content="2021-03-22T10:53:44.476Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="优化算法">
<meta name="twitter:description" content="本文是从各个论文、博客、专栏等学习整理所得,如有任何错误疏漏等问题,欢迎评论或邮箱提出,大家一起学习进步！0 优化算法框架 计算目标函数关于当前参数的梯度  g_t = \nabla f(w_t) \tag{0-1} 根据历史梯度计算一阶动量和二阶动量  m_t = \phi (g_1, g_2, \cdots, g_t) \tag{0-2} V_t = \psi (g_1, g_2, \cdot">
<meta name="twitter:image" content="https://weiguozhao.github.io/posts_res/2019-06-05-Optimization-Method/5.png">
  <link rel="canonical" href="https://weiguozhao.github.io/2019/06/05/Optimization-Method/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>优化算法 | Weiguo's Station</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ff6106e096f9eb6ace966de311be4b1a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weiguo's Station</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>博客首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>文章归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类专栏</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>各种标签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>站点搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    
  <div class="popup search-popup">
  <div class="search-header">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <div class="search-input-wrapper">
      <input autocomplete="off" autocorrect="off" autocapitalize="none"
             placeholder="搜索..." spellcheck="false"
             type="text" id="search-input">
    </div>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>
  <div id="search-result"></div>
</div>


  </div>
</div>
    </header>

    

  <a href="https://github.com/weiguozhao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://weiguozhao.github.io/2019/06/05/Optimization-Method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="WeiguoZHAO">
      <meta itemprop="description" content="Welcome to my blog~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weiguo's Station">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">优化算法

          
        </h1>

        <div class="post-meta">
        
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-06-05 18:05:24" itemprop="dateCreated datePublished" datetime="2019-06-05T18:05:24+08:00">2019-06-05</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-22 18:53:44" itemprop="dateModified" datetime="2021-03-22T18:53:44+08:00">2021-03-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><em>本文是从各个论文、博客、专栏等学习整理所得,如有任何错误疏漏等问题,欢迎评论或邮箱提出,大家一起学习进步！</em></p><h2 id="0-优化算法框架"><a href="#0-优化算法框架" class="headerlink" title="0 优化算法框架"></a>0 优化算法框架</h2><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{0-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
m_t = \phi (g_1, g_2, \cdots, g_t)
\tag{0-2}</script><script type="math/tex; mode=display">
V_t = \psi (g_1, g_2, \cdots, g_t)
\tag{0-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t
\tag{0-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
w_{t+1} = w_t - \eta_t
\tag{0-5}</script></li>
</ol><a id="more"></a>

<p><strong>核心区别是第3步执行的下降方向,在这个式子中,前半部分是实际的学习率（也即下降步长）,后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将重要的地方标红显示</strong></p>
<h2 id="1-Gradient-Descent-Variants"><a href="#1-Gradient-Descent-Variants" class="headerlink" title="1. Gradient Descent Variants"></a>1. Gradient Descent Variants</h2><h3 id="1-1-Batch-Gradient-Descent"><a href="#1-1-Batch-Gradient-Descent" class="headerlink" title="1.1 Batch Gradient Descent"></a>1.1 Batch Gradient Descent</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{1.1-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red} {m_t = g_t}
\tag{1.1-2}</script><script type="math/tex; mode=display">
\color{red} {V_t = 1}
\tag{1.1-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
\tag{1.1-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\
&= w_t - \alpha \cdot g_t
\end{align*}
\tag{1.1-5}</script></li>
</ol>
<h3 id="1-2-Stochastic-Gradient-Descent"><a href="#1-2-Stochastic-Gradient-Descent" class="headerlink" title="1.2 Stochastic Gradient Descent"></a>1.2 Stochastic Gradient Descent</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
\color{red}  { g_t = \nabla f(w_t; x^{(i)}, y^{(i)}) } 
\tag{1.2-1}</script><p>其中$(x^{(i)}, y^{(i)})$表示第$i$个样本；</p>
</li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red} {m_t = g_t}
\tag{1.2-2}</script><script type="math/tex; mode=display">
\color{red} {V_t = 1}
\tag{1.2-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
\tag{1.2-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t
\end{align*}
\tag{1.2-5}</script></li>
</ol>
<h3 id="1-3-Mini-Bach-Gradient-Descent"><a href="#1-3-Mini-Bach-Gradient-Descent" class="headerlink" title="1.3 Mini-Bach Gradient Descent"></a>1.3 Mini-Bach Gradient Descent</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
\color{red}  { g_t = \nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } 
\tag{1.3-1}</script><p>其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本,$n$表示mini-batch的大小；</p>
</li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red} {m_t = g_t}
\tag{1.3-2}</script><script type="math/tex; mode=display">
\color{red} {V_t = 1}
\tag{1.3-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\eta_t = \frac{\alpha}{\sqrt{V_t}} \cdot m_t = \alpha \cdot g_t
\tag{1.3-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - \alpha \cdot g_t
\end{align*}
\tag{1.3-5}</script></li>
</ol>
<p><strong>上述算法存在的问题：</strong></p>
<ul>
<li>很难调整出一个合适的learning_rate</li>
<li>learning_rate的变化规则往往是预定义的,很难适应不同的数据</li>
<li>所有的特征共享相同的learning_rate</li>
<li>局部最有解的问题</li>
</ul>
<hr>
<h2 id="2-Gradient-Descent-Optimization-Algorithm"><a href="#2-Gradient-Descent-Optimization-Algorithm" class="headerlink" title="2. Gradient Descent Optimization Algorithm"></a>2. Gradient Descent Optimization Algorithm</h2><h3 id="2-1-Gradient-Descent-with-Momentum"><a href="#2-1-Gradient-Descent-with-Momentum" class="headerlink" title="2.1 Gradient Descent with Momentum"></a>2.1 Gradient Descent with Momentum</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{2.1-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量,计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>
<script type="math/tex; mode=display">
\color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t }
\tag{2.1-2&3}</script><script type="math/tex; mode=display">
\color{red} { \eta_t = m_t }
\tag{2.1-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t )
\end{align*}
\tag{2.1-5}</script></li>
</ol>
<p>一阶动量是移动平均值,这里 $\gamma $ 的经验值为<code>0.9</code>。
以历史动量的和为主,适当加上一点当前的偏移,即考虑惯性的因素。</p>
<h3 id="2-2-Nesterov-Accelerated-Gradient"><a href="#2-2-Nesterov-Accelerated-Gradient" class="headerlink" title="2.2 Nesterov Accelerated Gradient"></a>2.2 Nesterov Accelerated Gradient</h3><ol>
<li><p>计算目标函数关于当前参数+下次变化的梯度</p>
<script type="math/tex; mode=display">
\color{red} { g_t = \nabla f(w_t - \gamma m_{t-1}) }
\tag{2.2-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量,计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>
<script type="math/tex; mode=display">
\color{red} {m_t = \gamma \cdot m_{t-1} + \alpha \cdot g_t }
\tag{2.2-2&3}</script><script type="math/tex; mode=display">
\color{red} { \eta_t = m_t }
\tag{2.2-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - ( \gamma \cdot m_{t-1} + \alpha \cdot g_t )
\end{align*}
\tag{2.2-5}</script></li>
</ol>
<p>这里 $\gamma $ 的经验值为<code>0.9</code>,在对参数求导时,不再和之前方法一直,而是对下次参数求导,即多向前看一步可以用来指导当前怎么走。</p>
<h3 id="2-3-AdaGrad"><a href="#2-3-AdaGrad" class="headerlink" title="2.3 AdaGrad"></a>2.3 AdaGrad</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{2.3-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
m_t = g_t
\tag{2.3-2}</script><script type="math/tex; mode=display">
\color{red} {V_t = \sum_{\tau = 1}^t g_{\tau}^2 }
\tag{2.3-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\begin{align*}
\eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t
\end{align*}
\tag{2.3-4}</script><p>这里$\epsilon$是为了避免分母为$0$,通常设置为$1e-8$。</p>
</li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{\alpha}{\sqrt{ \sum_{\tau = 1}^t g_{\tau}^2 + \epsilon }} \cdot m_t
\end{align*}
\tag{2.3-5}</script></li>
</ol>
<p>不同的参数具有不同的梯度,从而达到了不同参数具有不同学习率的目的,这样梯度更新快的参数,学习率(步长)会渐渐变小。
同样最终会面临学习率过小,模型无法继续学习的问题。</p>
<h3 id="2-4-AdaDelta"><a href="#2-4-AdaDelta" class="headerlink" title="2.4 AdaDelta"></a>2.4 AdaDelta</h3><p>首先定义动态平均值 $\color{red}{ E[ g^2 ]_t = \gamma E[ g^2 ]_{t-1} + (1 - \gamma) g_t^2 }$,该值仅取决于当前梯度值与上一时刻的动态平均值,其中$\gamma$通常设置成$0.9$。</p>
<ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{2.4-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
m_t = g_t
\tag{2.4-2}</script><script type="math/tex; mode=display">
\color{red} {V_t = E[ g^2 ]_t }
\tag{2.4-3}</script></li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\begin{align*}
\eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t \\ &= \frac{\alpha}{\sqrt{ E[ g^2 ]_t + \epsilon }} \cdot m_t
\end{align*}
\tag{2.4-4.1}</script><p>这里$\epsilon$是为了避免分母为$0$。将分母$ \sqrt{ E[ g^2 ]_t + \epsilon} $ 记为 $\color{red}{ RMS[g]_t} $,定义</p>
<script type="math/tex; mode=display">
E[ \Delta g^2 ] _t = \gamma E[ \Delta g^2 ] _{t-1} + (1 - \gamma) \Delta g _t^2</script><p>则：</p>
<script type="math/tex; mode=display">
RMS[\Delta g] _t = \sqrt{ E[ \Delta g^2 ] _t + \epsilon }</script><p>用$RMS[\Delta g]_{t-1}$代替学习率$\alpha$,则式$(2.4-4.1)$可以转化为:</p>
<script type="math/tex; mode=display">
\begin{align*}
\eta_t &= \frac{\alpha}{RMS[g] _t} \cdot g_t \\ &= \frac{ RMS[\Delta g] _{t-1} }{ RMS[g] _t } \cdot g _t
\end{align*}
\tag{2.4-4.2}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ &= w_t - \frac{RMS[\Delta g] _{t-1}}{RMS[g] _t} \cdot g_t
\end{align*}
\tag{2.4-5}</script></li>
</ol>
<p>$\color{red}{为什么}$用$RMS[\Delta g]_{t-1}$代替学习率$\alpha$??</p>
<h3 id="2-5-RMSprop"><a href="#2-5-RMSprop" class="headerlink" title="2.5 RMSprop"></a>2.5 RMSprop</h3><p>RMSprop是AdaDelta算法的一个特例。</p>
<script type="math/tex; mode=display">
E[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t</script><script type="math/tex; mode=display">
w_{t+1} = w_t - \frac{\alpha}{ \sqrt{ E[g^2] _t + \epsilon } } g_t</script><p>Hinton建议$\gamma$设置成$0.9$,学习率设置成$0.001$。</p>
<h3 id="2-6-Adam"><a href="#2-6-Adam" class="headerlink" title="2.6 Adam"></a>2.6 Adam</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{2.6-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red}{ \hat{m_t} = \beta_1 \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\
\color{red}{ m_t = \frac{\hat{m_t}}{1 - \beta_1^t} }
\tag{2.6-2}</script><script type="math/tex; mode=display">
\color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\
\color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } }
\tag{2.6-3}</script><p>其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量；</p>
</li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\begin{align*}
\eta_t &= \frac{\alpha}{\sqrt{V_t}} \cdot m_t
\end{align*}
\tag{2.6-4}</script><p>其中增加的$\epsilon$为了防止分母等于$0$；</p>
</li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t
\end{align*}
\tag{2.6-5}</script><p>作者建议默认值$\beta_1 = 0.9$,$\beta_2 = 0.999$,$\epsilon = 1e-8$。</p>
</li>
</ol>
<h3 id="2-7-AdaMax"><a href="#2-7-AdaMax" class="headerlink" title="2.7 AdaMax"></a>2.7 AdaMax</h3><p>Adamax是Adam的一种变体,此方法对学习率的上限提供了一个更简单的范围。</p>
<ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
g_t = \nabla f(w_t)
\tag{2.7-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red}{ m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t }
\tag{2.7-2}</script><script type="math/tex; mode=display">
\color{red}{ 
\begin{align*}
V_t &= \beta_2^{\infty} V_{t-1} + (1 - \beta_2^{\infty}) \| g_t \| ^{\infty} \\
&= max( \beta_2 \cdot V_{t-1}, \| g_t \| )
\end{align*}
}
\tag{2.7-3}</script><p>其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量；</p>
</li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\color{red}{
\begin{align*}
\eta_t &= \frac{\alpha}{V_t} \cdot m_t \\
&= \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \}
\end{align*}
}
\tag{2.7-4}</script></li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t \\ 
&= w_t - \frac{\alpha}{ max( \beta_2 \cdot V_{t-1}, \| g_t \| ) } \cdot \{ \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t \}
\end{align*}
\tag{2.7-5}</script><p>论文说合适的默认值为$\alpha = 0.002, \beta_1 = 0.9, \beta_2 = 0.999$。</p>
</li>
</ol>
<h3 id="2-8-Nadam"><a href="#2-8-Nadam" class="headerlink" title="2.8 Nadam"></a>2.8 Nadam</h3><ol>
<li><p>计算目标函数关于当前参数的梯度</p>
<script type="math/tex; mode=display">
\color{red}{ g_t = \nabla f(w_t - \frac{\alpha}{\sqrt{V_t}} \cdot m_{t-1}) }
\tag{2.8-1}</script></li>
<li><p>根据历史梯度计算一阶动量和二阶动量</p>
<script type="math/tex; mode=display">
\color{red}{ \hat{m_t} = \gamma \cdot \hat{m_{t-1}} + (1 - \beta_1) \cdot g_t } \\
\color{red}{ m_t = \frac{\hat{m_t}}{ 1 - \beta_1^t } }
\tag{2.8-2}</script><script type="math/tex; mode=display">
\color{red}{ \hat{V_t} = \beta_2 \cdot \hat{V_{t-1}} + (1 - \beta_2) \cdot g_t^2 } \\
\color{red}{ V_t = \frac{\hat{V_t}}{ 1 - \beta_2^t } }
\tag{2.8-3}</script><p>其中的$\beta_1$控制一阶动量,$\beta_2$控制二阶动量；</p>
</li>
<li><p>计算当前时刻的下降梯度</p>
<script type="math/tex; mode=display">
\begin{align*}
\eta_t &= \frac{\alpha}{\sqrt{V_t} + \epsilon } \cdot m_t
\end{align*}
\tag{2.8-4}</script><p>其中增加的$\epsilon$为了防止分母等于$0$；</p>
</li>
<li><p>根据下降梯度进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
w_{t+1} &= w_t - \eta_t
\end{align*}
\tag{2.8-5}</script></li>
</ol>
<hr>
<h2 id="3-Online-Optimization-Algorithm"><a href="#3-Online-Optimization-Algorithm" class="headerlink" title="3. Online Optimization Algorithm"></a>3. Online Optimization Algorithm</h2><p>上面描述的主要是批量训练的优化方法，批量训练有自身的局限性：面对高维高数据量的时候，批量处理的方式就显得笨重和不够高效。因此需要有在线处理的方法(Online)来解决相同的问题。在线学习算法的特点是：每来一个训练样本，就用该样本产生的loss和梯度对模型迭代一次，一个一个数据地进行训练，因此可以处理大数据量训练和在线训练。</p>
<h3 id="3-1-Truncated-Gradient"><a href="#3-1-Truncated-Gradient" class="headerlink" title="3.1 Truncated Gradient"></a>3.1 Truncated Gradient</h3><h4 id="3-1-1-L1正则法"><a href="#3-1-1-L1正则法" class="headerlink" title="3.1.1 L1正则法"></a>3.1.1 L1正则法</h4><script type="math/tex; mode=display">
w^{(t+1)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} - \eta^{(t)} \cdot \lambda \cdot sgn( w^{(t)} )
\tag{3.1-1}</script><p>其中,</p>
<ul>
<li>$\lambda \in \mathbb{R} $是一个标量,且$\lambda \ge 0$,为L1正则化参数；</li>
<li>$sgn(v)$为符号函数；</li>
<li>$\eta^{(t)}$为学习率,通常将其设置为$1/\sqrt{t}$的函数；</li>
<li>$G^{(t)} = \nabla_w f(w^{(t)}, z^{(t)}) $代表了第$t$次迭代中损失函数的梯度；</li>
</ul>
<h4 id="3-1-2-简单截断法"><a href="#3-1-2-简单截断法" class="headerlink" title="3.1.2 简单截断法"></a>3.1.2 简单截断法</h4><p>以$k$为窗口,当$t/k$不为整数时采用标准SGD进行迭代；当$t/k$为整数时,采用如下权重更新方式；</p>
<script type="math/tex; mode=display">
w^{(t+1)} = T_0 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \theta \right)</script><script type="math/tex; mode=display">
T_0 (v_i, \theta) = 
\begin{cases}
0 & \quad \mid v_i \mid \le \theta \\
v_i & \quad otherwise
\end{cases}
\tag{3.1-2}</script><p>$\theta \in \mathbb{R}$是一个标量,且$\theta \ge 0$；</p>
<h4 id="3-1-3-截断梯度法-TG"><a href="#3-1-3-截断梯度法-TG" class="headerlink" title="3.1.3 截断梯度法(TG)"></a>3.1.3 截断梯度法(TG)</h4><p>简单截断法太过激进,因此TG在此基础上进行了改进：</p>
<script type="math/tex; mode=display">
w^{(t+1)} = T_1 \left( w^{(t)} - \eta^{(t)} G^{(t)}, \quad \eta^{(t)} \lambda^{(t)}, \quad \theta \right)</script><script type="math/tex; mode=display">
T_1 (v_i, \alpha, \theta) = 
\begin{cases}
max(0, \quad v_i - \alpha ) & \quad v_i \in [0, \theta] \\
max(0, \quad v_i + \alpha ) & \quad v_i \in [- \theta, 0 ] \\
v_i & \quad otherwise
\end{cases}
\tag{3.1-3}</script><ul>
<li>其中$\lambda^{(t)} \in \mathbb{R}$,且$\lambda^{(t)} \ge 0$；</li>
<li>TG 同样是以$k$为窗口,每$k$步进行一次截断。<ul>
<li>当$t/k$不为整数时,$\lambda^{(t)} = 0$；</li>
<li>当$t/k$为整数时,$\lambda^{(t)} = k \cdot \lambda$。</li>
</ul>
</li>
<li>从公式$(3.1-3)$可以看出,超参数$\lambda$和$\theta$决定了$w$的稀疏程度,这两个值越大,则稀疏性越强；尤其令$\lambda = \theta$时,只需要通过调节一个参数就能控制稀疏性。</li>
</ul>
<p>根据公式$(3.1-3)$，我们很容易写出 TG 的算法逻辑:</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/5.png" alt="TG"></p>
<h4 id="3-1-4-截断公式对比"><a href="#3-1-4-截断公式对比" class="headerlink" title="3.1.4 截断公式对比"></a>3.1.4 截断公式对比</h4><p><img src="/posts_res/2019-06-05-Optimization-Method/1.png" alt="compare"></p>
<p>其中左侧是简单截断法的截断公式，右侧是截断梯度的截断公式。公式$(3.1-3)$进行改写，描述特征权重每个维度的更新方式:</p>
<script type="math/tex; mode=display">
w^{(t+1)}_i = 
\begin{cases}
Trnc \{ ( w^{(t)}_i - \eta^{(t)} g^{(t)}_i ), \lambda^{(t)}_{TG}, \theta \} & if \quad mod(t,k) = 0 \\
w^{(t)}_i - \eta^{(t)} g^{(t)}_i & otherwise
\end{cases}</script><script type="math/tex; mode=display">
\lambda^{(t)}_{TG} = \eta^{(t)} \lambda k
\tag{3.1-4}</script><script type="math/tex; mode=display">
Trnc( w, \lambda^{(t)}_{TG}, \theta ) = 
\begin{cases}
0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\
w - \lambda^{(t)}_{TG} sgn(w) & if \quad \lambda^{(t)}_{TG} \le \mid w \mid \le 0 \\
w & otherwise
\end{cases}</script><p>如果令$\lambda^{(t)}_{TG} = \theta$, 截断公式$Trnc(w, \lambda^{(t)}_{TG}, \theta)$变成:</p>
<script type="math/tex; mode=display">
Trnc(w, \theta, \theta) = 
\begin{cases}
0 & if \quad \mid w \mid \le 0 \\
w & otherwise
\end{cases}</script><p><strong>此时$TG$退化成简单截断法</strong>。</p>
<p>如果令$\theta = \infty$截断公式$Trnc(w, \lambda^{(t)}_{TG}, \theta)$变成:</p>
<script type="math/tex; mode=display">
Trnc(w, \lambda^{(t)}_{TG}, \infty) = 
\begin{cases}
0 & if \quad \mid w \mid \le \lambda^{(t)}_{TG} \\
w & otherwise
\end{cases}</script><p>如果再令$k=1$,那么特征权重维度更新公式变成:</p>
<script type="math/tex; mode=display">
\begin{align*}
w^{(t+1)}_i &= Trnc\{ (w^{(t)}_i - \eta^{(t)} g^{(t)}_i), \eta^{(t)} \lambda, \infty \} \\
&= w^{(t)}_i - \eta^{(t)} g^{(t)}_i - \eta^{(t)} \cdot \lambda \cdot sgn(w^{(t)}_i) 
\end{align*}</script><p><strong>此时 $TG$ 退化成 L1正则化法</strong>。</p>
<h3 id="3-2-FOBOS前向后向切分"><a href="#3-2-FOBOS前向后向切分" class="headerlink" title="3.2 FOBOS前向后向切分"></a>3.2 FOBOS前向后向切分</h3><h4 id="3-2-1-FOBOS算法原理"><a href="#3-2-1-FOBOS算法原理" class="headerlink" title="3.2.1 FOBOS算法原理"></a>3.2.1 FOBOS算法原理</h4><p>在 FOBOS 中, 将权重的更新分为两个步骤:</p>
<script type="math/tex; mode=display">
w^{(t+0.5)} = w^{(t)} - \eta^{(t)} \cdot G^{(t)} \\
w^{(t+1)} = argmin_w \{ \frac{1}{2} \| w - w^{(t+0.5)} \|^2 + \eta^{(t+0.5)} \cdot \psi (w) \}
\tag{3.2-1}</script><ul>
<li>前一个步骤实际上是一个标准的梯度下降步骤;</li>
<li>后一个步骤可以理解为对梯度下降的结果进行微调;<ul>
<li>前一部分保证微调发生在梯度下降结果的附近</li>
<li>后一部分则用于处理正则化，产生稀疏性</li>
</ul>
</li>
</ul>
<p>如果将公式 $(3.2-1)$ 中的两个步骤合二为一, 即将 $w^{(t+0.5)}$ 的计算带入到 $w^{(t+1)}$ 中, 有:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin _w \{ \frac{1}{2} \| w - w^{(t)} + \eta^{(t)} G^{(t)} \|^2 + \eta^{(t+0.5)} \psi (w) \}</script><p>令$F(w) = \frac{1}{2} || w - w^{(t)} + \eta^{(t)} G^{(t)} ||^2 + \eta^{(t+0.5)} \psi (w)$, 
如果$w^{(t+1)}$存在一个最优解, 那么就可以推断$0$向量一定属于$F(w)$的次梯度集合:</p>
<script type="math/tex; mode=display">
0 \in \partial F(w) = w - w^{(t)} + \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w)</script><p>由于$w^{(t+1)} = argmin_w F(w)$, 那么有:</p>
<script type="math/tex; mode=display">
0 = \{ w - w^{(t)} - \eta^{(t)} G^{(t)} + \eta^{(t+0.5)} \partial \psi (w) \} \mid_{w = w^{(t+1)}}</script><p>上式实际上给出了 FOBOS 中权重更新的另一种形式:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = w^{(t)} - \eta^{(t)} G^{(t)} - \eta^{(t+0.5)} \partial \psi (w^{(t+1)})</script><p>我们这里可以看到, $w^{(t+1)}$不仅仅与迭代前的状态$w^{(t)}$有关，而且与迭代后的$\psi (w^{(t+1)})$有关。</p>
<h4 id="3-2-2-L1-FOBOS"><a href="#3-2-2-L1-FOBOS" class="headerlink" title="3.2.2 L1-FOBOS"></a>3.2.2 L1-FOBOS</h4><p>在 L1 正则化下，有$\psi (w) = \lambda || w ||_1$, 
为了简化描述, 用向量$v= [ v_1, v_2, \cdots, v_N ] \in \mathbb{R}^N$ 来表示$w^{(t+0.5)}$, 
用标量$\hat{\lambda} \in \mathbb{R}$ 来表示 $\eta^{(t+0.5)} \lambda$, 并将公式$(3.2-1)$等号右边按维度展开:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \sum_{i=1}^N ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )
\tag{3.2-2}</script><p>可以看到,在求和公式$\sum_{i=1}^N ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )$ 中的每一项都是大于等于 $0$ 的,
所以公式$(3.2-2)$可以拆解成对特征权重$w$每一维度单独求解:</p>
<script type="math/tex; mode=display">
w^{(t+1)}_i = argmin_{w_i} ( \frac{1}{2} (w_i - v_i)^2 + \hat{\lambda} \mid w_i \mid )</script><p><img src="/posts_res/2019-06-05-Optimization-Method/6.png" alt="derived"></p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/7.png" alt="derived"></p>
<p>因此, 综合上面的分析得到在 FOBOS 在 L1 正则化条件下，特征权重的各个维度更新的方式为:</p>
<script type="math/tex; mode=display">
\begin{align*}
w^{(t+1)}_i &= sgn(v_i) max (0, \mid v_i \mid - \hat{\lambda}) \\
&= sgn(w_i^{(t)} - \eta^{(t)} g^{(t)}_i ) max \{ 0, \mid w^{(t)}_i - \eta_t \cdot g_i^{(t)} \mid - \eta^{(t+0.5)} \lambda \}
\end{align*}
\tag{3.2-3}</script><p>其中, $g_i^{(t)}$ 为梯度 $G^{(t)}$ 在维度 $i$ 上的取值。</p>
<p>根据公式$(3.2-3)$，我们很容易就可以设计出 L1-FOBOS 的算法逻辑:</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/8.png" alt="fobos"></p>
<h4 id="3-2-3-L1-FOBOS与TG的关系"><a href="#3-2-3-L1-FOBOS与TG的关系" class="headerlink" title="3.2.3 L1-FOBOS与TG的关系"></a>3.2.3 L1-FOBOS与TG的关系</h4><p>对于 L1-FOBOS 特征权重的各个维度更新公式$(3.2-3)$，也可以写作如下形式:</p>
<script type="math/tex; mode=display">
w^{(t+1)}_i = 
\begin{cases}
0 & \quad \mid w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \mid \le \eta^{(t+0.5)} \lambda \\
\left( w^{(t)}_i - \eta^{(t)} \cdot g^{(t)}_i \right) - \eta^{(t+0.5)} \cdot \lambda \cdot sgn(w^{(t)}_i - \eta^{(t)} g^{(t)}_i) & \quad otherwise
\end{cases}</script><p>上式截断的含义是：当一条样本产生的梯度不足以令对应维度上的权重值发生足够大的变化 $( \eta^{(t+0.5)} \cdot \lambda )$, 
则认为在本次更新过程中该维度不够重要，应当令其权重为$0$。</p>
<p>同时上式与TG的特征权重更新公式$(3.1-4)$对比, 发现如果令$ \theta = \infty, k = 1, \lambda^{(t)}_{TG} = \eta^{(t+0.5} \lambda $,则L1-FOBOS与TG完全一致，因此可以认为L1-FOBOS是TG在特定条件下的特殊形式。</p>
<h3 id="3-3-RDA正则对偶平均"><a href="#3-3-RDA正则对偶平均" class="headerlink" title="3.3 RDA正则对偶平均"></a>3.3 RDA正则对偶平均</h3><p>简单截断、TG、FOBOS 都是建立在 SGD 的基础之上的，属于梯度下降类型的方法，这类型方法的优点就是<strong>精度比较高</strong>，
并且TG、FOBOS也都能在稀疏性上得到提升。但是有些其它类型的算法，例如RDA，是从另一个方面来求解 Online Optimization,
并且更有效地提升了特征权重的稀疏性。</p>
<h4 id="3-3-1-RDA算法原理"><a href="#3-3-1-RDA算法原理" class="headerlink" title="3.3.1 RDA算法原理"></a>3.3.1 RDA算法原理</h4><p>在RDA中，特征权重的更新策略为:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \psi (w) + \frac{\beta^{(t)}}{t} h(w) \rbrace
\tag{3.3-1}</script><p>其中，$&lt; G^{(r)}, w &gt;$表示梯度 $G^{(r)}$对 $w$ 的积分平均值(积分中值); $\psi (w)$ 为正则项；$h(w)$为一个辅助的严格凸函数；
$ \lbrace \beta^{(t)} | t \ge 1 \rbrace $是一个非负且非自减序列。本质上，公式$(3.3-1)$中包含了3个部分：</p>
<ul>
<li>线性函数 $ \frac{1}{t} \sum_{r=1}^t &lt; G^{(t)}, w &gt; $, 包含了之前所有梯度(或次梯度)的平均值(dual average);</li>
<li>正则项 $\psi (w)$;</li>
<li>额外正则项 $\frac{\beta^{(t)}}{t} h(w)$, 这是个严格凸函数;</li>
</ul>
<h4 id="3-3-2-L1-RDA"><a href="#3-3-2-L1-RDA" class="headerlink" title="3.3.2 L1-RDA"></a>3.3.2 L1-RDA</h4><p>令$\psi (w) = \lambda || w ||_1$，并且由于$h(w)$是一个关于$w$的严格凸函数，不妨令 $h(w) = \frac{1}{2} || w ||_2^2 $,
此外，将非负非自减序列 $ \lbrace \beta^{(t)} | t \ge 1 \rbrace $ 定义为 $ \beta^{(t)} = \gamma \sqrt{t} $，将L1正则化代入公式$(3.3-1)$有:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace \frac{1}{t} \sum_{r=1}^t < G^{(r)}, w > + \lambda \| w \|_1 + \frac{\gamma}{2 \sqrt{t}} \| w \| _2^2 \rbrace
\tag{3.3-2}</script><p>针对特征权重的各个维度将其拆解成 N 个独立的标量最小化问题:</p>
<script type="math/tex; mode=display">
minimize_{w_i \in \mathbb{R}} \lbrace \bar{g}_i^{(t)} w_i + \lambda \mid w_i \mid + \frac{\gamma}{2\sqrt{t}} w_i^2 \rbrace
\tag{3.3-3}</script><p>这里 $ \lambda &gt; 0, \frac{\gamma}{\sqrt{t}} &gt; 0, \bar{g}_i^{(t)} = \frac{1}{t} \sum_{r=1}^t g_i^{(r)} $, 
公式$(3.3-3)$就是一个无约束的非平滑最优化问题。其中第2项 $\lambda | w_i |$ 在 $w_i$ 处不可导。</p>
<p>假设 $w_i^{\ast}$ 是其最优解，并且定义 $\xi \in \partial | w_i^{\ast} $ 为 $ | w_i | $ 在 $w_i^{\ast}$ 的次导数，那么有：</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/9.png" alt="derived"></p>
<p>之后可以得到L1-RDA特征权重的各个维度更新的方式为：</p>
<script type="math/tex; mode=display">
w_i^{(t+1)} = 
\begin{cases}
0 & \quad if \quad \mid \bar{g}_i^{(t)} \mid < \lambda \\
-\frac{\sqrt{t}}{\gamma} ( \bar{g}_i^{(t)} - \lambda \cdot sgn( \bar{g}_i^{(t)} ) ) & \quad otherwise
\end{cases}
\tag{3.3-6}</script><p>这里发现，当某个维度上累积梯度平均值的绝对值 $ | g_i^{(t)} | $ 小于阈值𝜆的时候，该维度权重将被置 $0$，特征权重的稀疏性由此产生。</p>
<p>根据公式 $(3.3-6)$，可以设计出 L1-RDA 的算法逻辑:</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/10.png" alt="rda"></p>
<h4 id="3-3-3-L1-RDA与L1-FOBOS的比较"><a href="#3-3-3-L1-RDA与L1-FOBOS的比较" class="headerlink" title="3.3.3 L1-RDA与L1-FOBOS的比较"></a>3.3.3 L1-RDA与L1-FOBOS的比较</h4><p>在 $3.2.2$ 中我们看到了 L1-FOBOS 实际上是 TG 的一种特殊形式, 在 L1-FOBOS 中，进行 <strong>截断</strong> 的判定条件是
$ | w_i^{(t)} - \eta^{(t)} g_i^{(t)} | \le \lambda^{(t)}_{TG} = \eta^{(t+0.5)} \lambda $ 。
通常会定义 $\eta$ 为与 $1$ 正相关的函数 $(\eta = \Theta (\frac{1}{\sqrt{t}}))$, 
因此 L1-FOBOS 的<strong>截断阈值</strong>为 $\Theta (\frac{1}{\sqrt{t}})) \lambda $, 随着 $t$ 的增加, 这个阈值会逐渐降低。</p>
<p>相比较而言，从$(3.3-6)$可以看出, L1-RDA 的<strong>截断阈值</strong>为 $\lambda$, 是一个常数，并不随着 $t$ 而变化, 
因此可以认为 L1-RDA 比 L1-FOBOS 在截断判定上更加激进, 这种性质使得 L1-RDA 更容易产生稀疏性;
此外，RDA 中判定对象是梯度的累加平均值 $\bar{g}_i^{(t)}$, 不同于 TG 或 L1-FOBOS 中针对单次梯度计算的结果进行判定,
避免了由于某些维度由于训练不足导致截断的问题。并且通过调节 $\lambda$ 一个参数，很容易在精度和稀疏性上进行权衡</p>
<h3 id="3-4-FTRL"><a href="#3-4-FTRL" class="headerlink" title="3.4 FTRL"></a>3.4 FTRL</h3><p>FTRL综合了L1-FOBOS基于梯度下降方法具有较高的精度、L1-RDA能在损失一定精度的情况下产生更好的稀疏性。</p>
<h4 id="3-4-1-L1-FOBOS和L1-RDA在形式上的统一"><a href="#3-4-1-L1-FOBOS和L1-RDA在形式上的统一" class="headerlink" title="3.4.1 L1-FOBOS和L1-RDA在形式上的统一"></a>3.4.1 L1-FOBOS和L1-RDA在形式上的统一</h4><p>L1-FOBOS在形式上，令 $ \eta^{(t+0.5)} = \eta^{(t)} = \Theta ( \frac{1}{\sqrt{t}} ) $ 是一个随 $t$变化的非增正序列, 
每次迭代都可以表示为：</p>
<script type="math/tex; mode=display">
w^{(t+0.5)} = w^{(t)} - \eta^{(t)} G^{(t)}</script><script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t + 0.5)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace</script><p>把这两个公式合并到一起，有:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace \frac{1}{2} | w - w^{(t)} + \eta^{(t)} G^{(t)} |_2^2 + \eta^{(t)} \lambda | w |_1 \rbrace</script><p>通过这个公式很难直接求出 $w^{(t+1)}$ 的解析解，但是我们可以按维度将其分解为 N 个独立的最优化步骤:</p>
<script type="math/tex; mode=display">
\begin{align*}
最优化 &= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)} + \eta^{(t)} g_i^{(t)} )^2 + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\
&= minimize_{w_i \in \mathbb{R}} \lbrace \frac{1}{2} (w_i - w_i^{(t)})^2 + \frac{1}{2} ( \eta^{(t)} g_i^{(t)} )^2 + w_i \eta^{(t)} g_i^{(t)} + w_i^{(t)} \eta^{(t)} g_i^{(t)} + \eta^{(t)} \lambda \mid w_i \mid \rbrace \\
&= minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid + \frac{1}{2} \eta^{(t)} (w_i - w_i^{(t)})^2 + \lbrack \frac{ \eta^{(t)} }{2} (g_i^{(t)})^2 + w_i^{(t)} g_i^{(t)} \rbrack  \rbrace
\end{align*}</script><p>由于 $\frac{ \eta^{(t)} }{2} (g_i^{(t)})^2 + w_i^{(t)} g_i^{(t)}$ 与变量 $ w_i $ 无关，因此上式可以等价于:</p>
<script type="math/tex; mode=display">
minimize_{w_i \in \mathbb{R}} \lbrace w_i g_i^{(t)} + \lambda \mid w_i \mid \frac{1}{2 \eta^{(t)} (w_i - w_i^{(t)})^2 }  \rbrace</script><p>再将这 N 个独立最优化子步骤合并，那么 L1-FOBOS 可以写作:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2 \eta^{(t)}} | w - w^{(t)} |_2^2 \rbrace</script><p>而对于 L1-RDA 的公式$(3.3.2-1)$，我们可以写作:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + t \lambda |w|_1 +  \frac{1}{2 \eta^{(t)}} | w - 0 |_2^2 \rbrace</script><p>这里 $G^{(1:t)} = \sum_{s=1}^t G^{(s)}$; 如果令 $ \sigma^{(s)} = \frac{1}{\eta^{(s)}} - \frac{1}{\eta^{(s-1)}}, \sigma^{(1:t)} = \frac{1}{\eta^{(t)}} $, 上面两个式子可以写做:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- w^{(t)} | _2^2  \rbrace
\tag{3.4.1-1}</script><script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace G^{(t)} \cdot w + t \lambda | w |_1 + \frac{1}{2} sigma^{(1:t)} | w- 0 | _2^2  \rbrace
\tag{3.4.1-2}</script><p>比较$(3.4.1-1)$和$(3.4.1-2)$这两个公式，可以看出 L1-FOBOS 和 L1-RDA 的区别在于:</p>
<ul>
<li>(1) 前者对计算的是累加梯度以及 L1 正则项只考虑当前模的贡献，而后者采用了累加的处理方式;</li>
<li>(2) 前者的第三项限制$ w $的变化不能离已迭代过的解太远，而后者则限制 $w$ 不能离 0 点太远;</li>
</ul>
<h4 id="3-4-2-FTRL算法原理"><a href="#3-4-2-FTRL算法原理" class="headerlink" title="3.4.2 FTRL算法原理"></a>3.4.2 FTRL算法原理</h4><p>FTRL 综合考虑了 FOBOS 和 RDA 对于正则项和$w$限制的区别，其特征权重的更新公式为:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace G^{(1:t)} \cdot w + \lambda_1 |w|_1 + \lambda_2 |w|_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} | w - w^{(s)} |_2^2 \rbrace
\tag{3.4.2-1}</script><p>注意，公式 $(3.4.2-1)$ 中出现了L2正则项 $ \frac{1}{2} || w ||_2^2 $，
L2正则项的引入仅仅相当于对最优化过程多了一个约束，使得结果求解结果更加“平滑”。</p>
<p>公式$(3.4.2-1)$看上去很复杂，更新特征权重貌似非常困难的样子。不妨将其进行改写，将最后一项展开，等价于求下面这样一个最优化问题:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace (G^{(1:t)} - \sum_{s=1}^t \sigma^{(s)} w^{(s)} ) \cdot w + \lambda_1 |w|_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) \cdot | w |_2^2 + \frac{1}{2} \sum_{s=1}^t \sigma^{(s)} |w^{(s)}|_2^2 \rbrace</script><p>由于 $\frac{1}{2} \sum_{s=1}^t \sigma^{(s)} || w^{(s)} ||_2^2$ 相对于 $w$ 来说是一个常数，并且令 $ z^{(t)} = G^{(1:t)} - \sum_{s=1}^t \sigma^{(s)} w^{(s)} $, 上式等价于:</p>
<script type="math/tex; mode=display">
w^{(t+1)} = argmin_w \lbrace z^{(t)} \cdot w + \lambda_1 |w|_1 + \frac{1}{2} (\lambda_2 + \sum_{s=1}^t \sigma^{(s)} )  |w|_2^2 \rbrace</script><p>针对特征权重的各个维度将其拆解成N个独立的标量最小化问题:</p>
<script type="math/tex; mode=display">
minimize_{w_i \in \mathbb{R}} \lbrace z_i^{(t)} w_i + \lambda_1 \mid w \mid_1 + \frac{1}{2} ( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} ) w_i^2 \rbrace</script><p>到这里，我们遇到了与式$(3.3.2-2)$类似的优化问题，用相同的分析方法可以得到:</p>
<script type="math/tex; mode=display">
w_i^{t+1} = 
\begin{cases}
0 & \quad if \mid z_i^{(t)} \mid < \lambda \\
-( \lambda_2 + \sum_{s=1}^t \sigma^{(s)} )^{-1} ( z_i^{(t)} - \lambda_1 sng( z_i^{(t)} ) ) & \quad otherwise
\end{cases}
\tag{3.4.2-2}</script><p>从式 $(3.4.2-2)$ 可以看出，引入 L2 正则化并没有对 FTRL 结果的稀疏性产生任何影响。</p>
<h4 id="3-4-3-Per-Coordinate-Learning-Rates"><a href="#3-4-3-Per-Coordinate-Learning-Rates" class="headerlink" title="3.4.3 Per-Coordinate Learning Rates"></a>3.4.3 Per-Coordinate Learning Rates</h4><p>前面介绍了 FTRL 的基本推导，但是这里还有一个问题是一直没有被讨论到的:关于学习率 $\eta^{(t)}$ 的选择和计算。
事实上在 FTRL 中，每个维度上的学习率都是单独考虑的 (Per-Coordinate Learning Rates)。</p>
<p>在一个标准的OGD里面使用的是一个全局的学习率策略$\eta^{(t)} = \frac{1}{\sqrt{t}}$，这个策略保证了学习率是一个正的非增长序列，
对于每一个特征维度都是一样的。考虑特征维度的变化率: 如果特征1 比特征2 的变化更快，那么在维度1 上的学习率应该下降得更快。
我们很容易就可以想到可以用某个维度上梯度分量来反映这种变化率。在 FTRL 中，维度 $i$ 上的学习率是这样计算的:</p>
<script type="math/tex; mode=display">
\eta^{(t)}_i = \frac{\alpha}{ \beta + \sqrt{ \sum_{s=1}^t (g_i^{(s)})^2 } }
\tag{3.4.3-1}</script><p>由于 $\sigma^{(1:t)} = \frac{1}{\eta^{(t)}}$，所以公式$(3.4.2-2)$中 $\sum_{s=1}^t \sigma^{(s)} = \frac{1}{\eta^{(t)}} = (\beta + \sqrt{ \sum_{s=1}^t (g_i^{(s)})^2 }) / \alpha$。
这里的 $\alpha$ 和 $\beta$ 是需要输入的参数, 公式 $(3.4.2-2)$ 中学习率写成累加的形式，是为了方便理解后面 FTRL 的迭代计算逻辑。</p>
<h4 id="3-4-4-FTRL算法逻辑"><a href="#3-4-4-FTRL算法逻辑" class="headerlink" title="3.4.4 FTRL算法逻辑"></a>3.4.4 FTRL算法逻辑</h4><p>到现在为止，我们已经得到了 FTRL 的特征权重维度的更新方法$公式(3.4.2-2)$，每个特征维度的学习率计算方法公式$(3.4.3-1)$，
那么很容易写出 FTRL 的算法逻辑, 这里是根据$(3.4.2-2)$ 和$(3.4.3-1)$ 写的 L1&amp;L2-FTRL 求解最优化的算法逻辑，如下：</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/2.png" alt="fengyang"></p>
<p>而论文<code>Ad Click Prediction: a View from the Trenches</code>中 Algorithm 1 给出的是 L1&amp;L2-FTRL 针对 Logistic Regression 的算法逻辑:</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/3.png" alt="Ad Click Prediction"></p>
<h3 id="3-5-Online总结"><a href="#3-5-Online总结" class="headerlink" title="3.5 Online总结"></a>3.5 Online总结</h3><p>从类型上来看，简单截断法、TG、FOBOS 属于同一类，都是梯度下降类的算法,并且TG在特定条件可以转换成简单截断法和FOBOS;
RDA属于简单对偶平均的扩展应用;FTRL 可以视作 RDA 和 FOBOS 的结合，同时具备二者的优点。
目前来看， RDA 和 FTRL 是最好的稀疏模型 Online Training 的算法。</p>
<p>谈到高维高数据量的最优化求解，不可避免的要涉及到并行计算的问题, <a href="http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html" target="_blank" rel="noopener">冯扬(8119)的博客</a>讨论了 batch 模式下的并行逻辑回归，其实只要修改损失函数，就可以用于其它问题的最优化求解。
另外，对于 Online 下，<a href="http://martin.zinkevich.org/publications/nips2010.pdf" target="_blank" rel="noopener">Parallelized Stochastic Gradient Descent</a>给出了一种很直观的方法:</p>
<p><img src="/posts_res/2019-06-05-Optimization-Method/4.png" alt="Parallelized Stochastic Gradient Descent"></p>
<p>对于 Online 模式的并行化计算，一方面可以参考 ParallelSGD 的思路，另一方面也可以借鉴 batch 模式下对高维向量点乘以及梯度分量并行计算的思路。
总之，在理解算法原理的 基础上将计算步骤进行拆解，使得各节点能独自无关地完成计算最后汇总结果即可。</p>
<hr>
<blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">一个框架看懂优化算法之异同 SGD/AdaGrad/Adam</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32488889" target="_blank" rel="noopener">SGD、Momentum、RMSprop、Adam区别与联系</a></li>
<li><a href="https://arxiv.org/pdf/1609.04747.pdf" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
<li><a href="/posts_res/2019-06-05-Optimization-Method/online_optimization_fengyang.pdf">在线最优化求解(Online Optimization)-冯扬</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD,Adagrad,Adadelta,Adam,Adamax,Nadam）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/29920135" target="_blank" rel="noopener">Deep Learning 最优化方法之AdaGrad</a></li>
<li><a href="/posts_res/2019-06-05-Optimization-Method/Ad Click Prediction- a View from the Trenches.pdf">Ad_Click_Prediction_a_View_from_the_Trenches</a></li>
<li><a href="https://www.cnblogs.com/EE-NovRain/p/3810737.html" target="_blank" rel="noopener">各大公司广泛使用的在线学习算法FTRL详解</a></li>
<li><a href="https://liam.page/2019/08/31/a-not-so-simple-introduction-to-FTRL/" target="_blank" rel="noopener">FTRL 不太简短之介绍</a> 这个文章讲得也挺好</li>
</ul>
</blockquote>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/优化算法/" rel="tag"># 优化算法</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/04/24/pymongo基础/" rel="next" title="pymongo基础">
                  <i class="fa fa-chevron-left"></i> pymongo基础
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/09/21/Java修饰符总结/" rel="prev" title="Java修饰符总结">
                  Java修饰符总结 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80MTI1Mi8xNzgwMA"></div>
  </div>
  
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="WeiguoZHAO">
  <p class="site-author-name" itemprop="name">WeiguoZHAO</p>
  <div class="site-description motion-element" itemprop="description">Welcome to my blog~</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">86</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">47</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/weiguozhao" title="GitHub &rarr; https://github.com/weiguozhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:aszhaoweiguo@gmail.com" title="E-Mail &rarr; mailto:aszhaoweiguo@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-inline">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      大牛们
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://colah.github.io/" title="http://colah.github.io/" rel="noopener" target="_blank">colah's blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://github.com/wzhe06/Reco-papers" title="https://github.com/wzhe06/Reco-papers" rel="noopener" target="_blank">王喆的Github</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://github.com/ljpzzz/machinelearning" title="https://github.com/ljpzzz/machinelearning" rel="noopener" target="_blank">刘建平的Github</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://tech.meituan.com/" title="https://tech.meituan.com/" rel="noopener" target="_blank">美团技术团队</a>
        </li>
      
    </ul>
  </div>


        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-优化算法框架"><span class="nav-text">0 优化算法框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Gradient-Descent-Variants"><span class="nav-text">1. Gradient Descent Variants</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Batch-Gradient-Descent"><span class="nav-text">1.1 Batch Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Stochastic-Gradient-Descent"><span class="nav-text">1.2 Stochastic Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Mini-Bach-Gradient-Descent"><span class="nav-text">1.3 Mini-Bach Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Gradient-Descent-Optimization-Algorithm"><span class="nav-text">2. Gradient Descent Optimization Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Gradient-Descent-with-Momentum"><span class="nav-text">2.1 Gradient Descent with Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Nesterov-Accelerated-Gradient"><span class="nav-text">2.2 Nesterov Accelerated Gradient</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-AdaGrad"><span class="nav-text">2.3 AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-AdaDelta"><span class="nav-text">2.4 AdaDelta</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-RMSprop"><span class="nav-text">2.5 RMSprop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Adam"><span class="nav-text">2.6 Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-AdaMax"><span class="nav-text">2.7 AdaMax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-Nadam"><span class="nav-text">2.8 Nadam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Online-Optimization-Algorithm"><span class="nav-text">3. Online Optimization Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Truncated-Gradient"><span class="nav-text">3.1 Truncated Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-L1正则法"><span class="nav-text">3.1.1 L1正则法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-简单截断法"><span class="nav-text">3.1.2 简单截断法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-截断梯度法-TG"><span class="nav-text">3.1.3 截断梯度法(TG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-4-截断公式对比"><span class="nav-text">3.1.4 截断公式对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-FOBOS前向后向切分"><span class="nav-text">3.2 FOBOS前向后向切分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-FOBOS算法原理"><span class="nav-text">3.2.1 FOBOS算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-L1-FOBOS"><span class="nav-text">3.2.2 L1-FOBOS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-L1-FOBOS与TG的关系"><span class="nav-text">3.2.3 L1-FOBOS与TG的关系</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-RDA正则对偶平均"><span class="nav-text">3.3 RDA正则对偶平均</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-RDA算法原理"><span class="nav-text">3.3.1 RDA算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-L1-RDA"><span class="nav-text">3.3.2 L1-RDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-L1-RDA与L1-FOBOS的比较"><span class="nav-text">3.3.3 L1-RDA与L1-FOBOS的比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-FTRL"><span class="nav-text">3.4 FTRL</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-L1-FOBOS和L1-RDA在形式上的统一"><span class="nav-text">3.4.1 L1-FOBOS和L1-RDA在形式上的统一</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-FTRL算法原理"><span class="nav-text">3.4.2 FTRL算法原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-Per-Coordinate-Learning-Rates"><span class="nav-text">3.4.3 Per-Coordinate Learning Rates</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-4-FTRL算法逻辑"><span class="nav-text">3.4.4 FTRL算法逻辑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Online总结"><span class="nav-text">3.5 Online总结</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WeiguoZHAO</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
      <div class="reading-progress-bar"></div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>

<script src="/js/schemes/pisces.js?v=7.3.0"></script>



<script src="/js/next-boot.js?v=7.3.0"></script>




  















  <script src="/js/local-search.js?v=7.3.0"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script><script src="/js/post-details.js?v=7.3.0"></script>


<script>
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

</body>
</html>
