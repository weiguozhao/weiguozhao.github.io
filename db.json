{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":1,"renderable":0},{"_id":"source/images/github-logo.png","path":"images/github-logo.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/18.png","path":"posts_res/2017-12-12-latex/18.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/19.png","path":"posts_res/2017-12-12-latex/19.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-08-native-bayes/native_bayes.py","path":"posts_res/2018-03-08-native-bayes/native_bayes.py","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-11-logistic-regression/1-1.png","path":"posts_res/2018-03-11-logistic-regression/1-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-11-logistic-regression/logistic_regression.py","path":"posts_res/2018-03-11-logistic-regression/logistic_regression.py","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/cart_cut.png","path":"posts_res/2018-03-18-decision-tree/cart_cut.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/credit.csv","path":"posts_res/2018-03-18-decision-tree/credit.csv","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-11-logistic-regression/1-4.gif","path":"posts_res/2018-03-11-logistic-regression/1-4.gif","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/decision_tree.py","path":"posts_res/2018-03-18-decision-tree/decision_tree.py","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/titanic.csv","path":"posts_res/2018-03-18-decision-tree/titanic.csv","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/dt.png","path":"posts_res/2018-03-18-decision-tree/dt.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-21-chisquaredtest/3-1.png","path":"posts_res/2018-03-21-chisquaredtest/3-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-18-decision-tree/treePlot.py","path":"posts_res/2018-03-18-decision-tree/treePlot.py","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-24-regularization/2-1.jpg","path":"posts_res/2018-03-24-regularization/2-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-24-regularization/5-1.png","path":"posts_res/2018-03-24-regularization/5-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-24-regularization/4-2.png","path":"posts_res/2018-03-24-regularization/4-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-28-svm/1-supportvector.png","path":"posts_res/2018-03-28-svm/1-supportvector.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-28-svm/2-supportvector.png","path":"posts_res/2018-03-28-svm/2-supportvector.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/1-1.png","path":"posts_res/2018-04-12-gbdtxgboost/1-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/1-2.png","path":"posts_res/2018-04-12-gbdtxgboost/1-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-19-bigscaledataml/1.png","path":"posts_res/2018-04-19-bigscaledataml/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-16-languagestlset/1.jpg","path":"posts_res/2018-04-16-languagestlset/1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-19-bigscaledataml/2.png","path":"posts_res/2018-04-19-bigscaledataml/2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-1.png","path":"posts_res/2018-04-21-randomforest/3-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-2.png","path":"posts_res/2018-04-21-randomforest/3-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-3.png","path":"posts_res/2018-04-21-randomforest/3-3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-6.png","path":"posts_res/2018-04-21-randomforest/3-6.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-5.png","path":"posts_res/2018-04-21-randomforest/3-5.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-21-randomforest/3-4.png","path":"posts_res/2018-04-21-randomforest/3-4.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-23-tuneparameters/1.png","path":"posts_res/2018-04-23-tuneparameters/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-28-emalgorithm/1.jpg","path":"posts_res/2018-04-28-emalgorithm/1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-10-LongestPalindromicSubstring/1.png","path":"posts_res/2018-05-10-LongestPalindromicSubstring/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-10-LongestPalindromicSubstring/2.png","path":"posts_res/2018-05-10-LongestPalindromicSubstring/2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-18-dimreduction/1-1.png","path":"posts_res/2018-05-18-dimreduction/1-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-18-dimreduction/1-2.png","path":"posts_res/2018-05-18-dimreduction/1-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-18-dimreduction/3-1.jpg","path":"posts_res/2018-05-18-dimreduction/3-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-18-dimreduction/4-1.jpg","path":"posts_res/2018-05-18-dimreduction/4-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-20-gradientdescent/0-1.png","path":"posts_res/2018-05-20-gradientdescent/0-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-23-backpropagation/2-1.jpg","path":"posts_res/2018-05-23-backpropagation/2-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-20-gradientdescent/3-1.png","path":"posts_res/2018-05-20-gradientdescent/3-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-23-backpropagation/2-2.jpg","path":"posts_res/2018-05-23-backpropagation/2-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-18-smote/smote.jpg","path":"posts_res/2018-06-18-smote/smote.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/1-1.jpg","path":"posts_res/2018-06-20-rnn1/1-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/1-2.jpg","path":"posts_res/2018-06-20-rnn1/1-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/2-1.jpg","path":"posts_res/2018-06-20-rnn1/2-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/2-2.jpg","path":"posts_res/2018-06-20-rnn1/2-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-1.jpg","path":"posts_res/2018-06-20-rnn1/3-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-2.jpg","path":"posts_res/2018-06-20-rnn1/3-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-3.jpg","path":"posts_res/2018-06-20-rnn1/3-3.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-5.jpg","path":"posts_res/2018-06-20-rnn1/3-5.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-4.jpg","path":"posts_res/2018-06-20-rnn1/3-4.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-8.jpg","path":"posts_res/2018-06-20-rnn1/3-8.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-7.jpg","path":"posts_res/2018-06-20-rnn1/3-7.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-6.jpg","path":"posts_res/2018-06-20-rnn1/3-6.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/3-9.jpg","path":"posts_res/2018-06-20-rnn1/3-9.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/4-2.jpg","path":"posts_res/2018-06-20-rnn1/4-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/4-3.jpg","path":"posts_res/2018-06-20-rnn1/4-3.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-20-rnn1/4-1.jpg","path":"posts_res/2018-06-20-rnn1/4-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-1.png","path":"posts_res/2018-06-28-cnn1/2-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-2.png","path":"posts_res/2018-06-28-cnn1/2-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-10.png","path":"posts_res/2018-06-28-cnn1/2-10.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-11.png","path":"posts_res/2018-06-28-cnn1/2-11.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-3.png","path":"posts_res/2018-06-28-cnn1/2-3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-4.png","path":"posts_res/2018-06-28-cnn1/2-4.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-6.png","path":"posts_res/2018-06-28-cnn1/2-6.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-5.png","path":"posts_res/2018-06-28-cnn1/2-5.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/3-1.png","path":"posts_res/2018-06-28-cnn1/3-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-1.png","path":"posts_res/2018-06-28-cnn1/4-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-10.png","path":"posts_res/2018-06-28-cnn1/4-10.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-12.png","path":"posts_res/2018-06-28-cnn1/4-12.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-13.png","path":"posts_res/2018-06-28-cnn1/4-13.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-16.png","path":"posts_res/2018-06-28-cnn1/4-16.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-17.png","path":"posts_res/2018-06-28-cnn1/4-17.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-3.png","path":"posts_res/2018-06-28-cnn1/4-3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-2.png","path":"posts_res/2018-06-28-cnn1/4-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-4.png","path":"posts_res/2018-06-28-cnn1/4-4.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-6.png","path":"posts_res/2018-06-28-cnn1/4-6.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-7.png","path":"posts_res/2018-06-28-cnn1/4-7.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-8.png","path":"posts_res/2018-06-28-cnn1/4-8.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-9.png","path":"posts_res/2018-06-28-cnn1/4-9.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/2.png","path":"posts_res/2018-07-08-doc2vectutorial/2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/1.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/4.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/4.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/2.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/5.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/5.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/3.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/3.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-10-09-kmeans/kmeans.py","path":"posts_res/2018-10-09-kmeans/kmeans.py","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/6.jpg","path":"posts_res/2018-07-30-rnngradientdisappear/6.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-10-09-kmeans/res.jpg","path":"posts_res/2018-10-09-kmeans/res.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-12-tSNE/2-2.png","path":"posts_res/2018-11-12-tSNE/2-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-12-tSNE/2-6-2.jpg","path":"posts_res/2018-11-12-tSNE/2-6-2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-12-tSNE/2-3.png","path":"posts_res/2018-11-12-tSNE/2-3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg","path":"posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png","path":"posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg","path":"posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg","path":"posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png","path":"posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png","path":"posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-29-git基础/1.jpg","path":"posts_res/2019-03-29-git基础/1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-29-git基础/2.png","path":"posts_res/2019-03-29-git基础/2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-29-git基础/3.png","path":"posts_res/2019-03-29-git基础/3.png","modified":1,"renderable":0},{"_id":"source/posts_res/2019-03-29-git基础/4.png","path":"posts_res/2019-03-29-git基础/4.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/scrollspy.js","path":"js/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/post-details.js","path":"js/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"source/posts_res/2017-12-12-latex/10.png","path":"posts_res/2017-12-12-latex/10.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/15.png","path":"posts_res/2017-12-12-latex/15.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/17.png","path":"posts_res/2017-12-12-latex/17.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/9.png","path":"posts_res/2017-12-12-latex/9.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-01-2017aca/results.png","path":"posts_res/2018-03-01-2017aca/results.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-24-regularization/4-1.png","path":"posts_res/2018-03-24-regularization/4-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-24-regularization/5-2.png","path":"posts_res/2018-03-24-regularization/5-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-03-word2vec/1-1.jpg","path":"posts_res/2018-05-03-word2vec/1-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-03-word2vec/2-1.jpg","path":"posts_res/2018-05-03-word2vec/2-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-20-gradientdescent/2-1.png","path":"posts_res/2018-05-20-gradientdescent/2-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/1-1.png","path":"posts_res/2018-06-28-cnn1/1-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-7.png","path":"posts_res/2018-06-28-cnn1/2-7.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-8.png","path":"posts_res/2018-06-28-cnn1/2-8.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/3-2.png","path":"posts_res/2018-06-28-cnn1/3-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-11.gif","path":"posts_res/2018-06-28-cnn1/4-11.gif","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-15.png","path":"posts_res/2018-06-28-cnn1/4-15.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-18.png","path":"posts_res/2018-06-28-cnn1/4-18.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-5.png","path":"posts_res/2018-06-28-cnn1/4-5.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/1.png","path":"posts_res/2018-07-08-doc2vectutorial/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-10-17-rnn2/gru.jpg","path":"posts_res/2018-10-17-rnn2/gru.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg","path":"posts_res/2018-11-26-推荐系统HandBook/2-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-12-tSNE/2-6.jpg","path":"posts_res/2018-11-12-tSNE/2-6.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg","path":"posts_res/2018-11-26-推荐系统HandBook/3-1.jpg","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/12.png","path":"posts_res/2017-12-12-latex/12.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/14.png","path":"posts_res/2017-12-12-latex/14.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/16.png","path":"posts_res/2017-12-12-latex/16.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/25.png","path":"posts_res/2017-12-12-latex/25.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-20-gradientdescent/1-1.png","path":"posts_res/2018-05-20-gradientdescent/1-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/2-9.png","path":"posts_res/2018-06-28-cnn1/2-9.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-28-cnn1/4-14.png","path":"posts_res/2018-06-28-cnn1/4-14.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-13-LINE/1.png","path":"posts_res/2018-07-13-LINE/1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-08-08-resume/4-1.png","path":"posts_res/2018-08-08-resume/4-1.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"source/posts_res/2017-12-12-latex/23.png","path":"posts_res/2017-12-12-latex/23.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-28-svm/3-variables.png","path":"posts_res/2018-03-28-svm/3-variables.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/Distributed representations of sentences and documents.pdf","path":"posts_res/2018-07-08-doc2vectutorial/Distributed representations of sentences and documents.pdf","modified":1,"renderable":0},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/posts_res/2017-12-12-latex/13.png","path":"posts_res/2017-12-12-latex/13.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/11-2.png","path":"posts_res/2017-12-12-latex/11-2.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/24.png","path":"posts_res/2017-12-12-latex/24.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-03-word2vec/source-archive.zip","path":"posts_res/2018-05-03-word2vec/source-archive.zip","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"source/posts_res/2017-12-12-latex/22.png","path":"posts_res/2017-12-12-latex/22.png","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/11-1.png","path":"posts_res/2017-12-12-latex/11-1.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip","path":"posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"source/posts_res/2017-12-12-latex/21.png","path":"posts_res/2017-12-12-latex/21.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-28-svm/2-gaussian.png","path":"posts_res/2018-03-28-svm/2-gaussian.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-28-emalgorithm/2.png","path":"posts_res/2018-04-28-emalgorithm/2.png","modified":1,"renderable":0},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/GBDT详解上.pdf","path":"posts_res/2018-04-12-gbdtxgboost/GBDT详解上.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2017-12-12-latex/20.png","path":"posts_res/2017-12-12-latex/20.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/Greedy Function Approximation_A Gradient Boosting Machine.pdf","path":"posts_res/2018-04-12-gbdtxgboost/Greedy Function Approximation_A Gradient Boosting Machine.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf","path":"posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-23-machinelearningcompare/abstract.png","path":"posts_res/2018-07-23-machinelearningcompare/abstract.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-08-08-resume/my_resume.pdf","path":"posts_res/2018-08-08-resume/my_resume.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/XGBoost_A Scalable Tree Boosting System.pdf","path":"posts_res/2018-04-12-gbdtxgboost/XGBoost_A Scalable Tree Boosting System.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-07-13-LINE/LINE Large-scale Information Network Embedding.pdf","path":"posts_res/2018-07-13-LINE/LINE Large-scale Information Network Embedding.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png","path":"posts_res/2018-04-12-gbdtxgboost/3-treesplit.png","modified":1,"renderable":0},{"_id":"source/posts_res/2018-03-28-svm/支持向量机通俗导论理解SVM的三层境界LaTeX最新版_2015.1.9.pdf","path":"posts_res/2018-03-28-svm/支持向量机通俗导论理解SVM的三层境界LaTeX最新版_2015.1.9.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf","path":"posts_res/2018-05-03-word2vec/Word2Vec详解.pdf","modified":1,"renderable":0},{"_id":"source/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf","path":"posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"787f8c743e148651a5d78c8b4ae56ef1016bad71","modified":1566127216709},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1566023038873},{"_id":"themes/next/.all-contributorsrc","hash":"76833610150976c7349d4fc8f1142366eaaa09fa","modified":1566023038872},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1566023038873},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1566023038873},{"_id":"themes/next/.DS_Store","hash":"7f6b000f82f2c095a0957373dca6b820e1d732f1","modified":1566098808645},{"_id":"themes/next/.gitignore","hash":"b80cec1d5e6a73d1cec382aad8046d1352a1e963","modified":1566023038878},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1566023038879},{"_id":"themes/next/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1566023038879},{"_id":"themes/next/_config.yml","hash":"0ae3c00a8f39a742aec9bad97c50ee4df46bdfd4","modified":1566215142623},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1566023038881},{"_id":"themes/next/gulpfile.coffee","hash":"2ba4aeddc257a74a209edf5218137010662cd5fb","modified":1566023038886},{"_id":"themes/next/package.json","hash":"170e67ca35a7c1ddc7dae50a90f6d4270143331f","modified":1566023038928},{"_id":"source/_posts/2017-11-12-jekywithmathjax.md","hash":"c5066979ef90444ddc20e2235a45d59b788ede61","modified":1566034083583},{"_id":"source/_posts/.DS_Store","hash":"93ceacaa7718e14eb22a7e31a23a9c6b884707d5","modified":1566033809743},{"_id":"source/_posts/2017-11-13-anaconda.md","hash":"20d7b4375bcaef6bb1652c99439eacbc7a20a820","modified":1566034106632},{"_id":"source/_posts/2017-12-21-qsort.md","hash":"6df8f99c4ebbfdcfad9de47493a44512bc1e0c1f","modified":1566034192777},{"_id":"source/_posts/2017-12-12-markdown.md","hash":"74471b8c49882b04cac3378f32abefac1aa5fbb3","modified":1566034147097},{"_id":"source/_posts/2017-12-22-mergesort.md","hash":"58bc90d08d3dea2fbe2e8a085803de2f9a7d5229","modified":1566034200271},{"_id":"source/_posts/2017-12-12-latex.md","hash":"37bfe621e761d54c5b0d68e14ef8aed1265afe80","modified":1566034125540},{"_id":"source/_posts/2017-12-23-binarysearch.md","hash":"d687219fd2435688278b6207716ce7b6bcd9406e","modified":1566034233060},{"_id":"source/_posts/2018-03-08-naive-bayes.md","hash":"42964566a5d3a18d3753a48a0e0f33bc17c61e23","modified":1566034292460},{"_id":"source/_posts/2018-03-01-2017aca.md","hash":"64c2e1cdbae741a96fb5e458683a75ee1da8ddb6","modified":1566034256237},{"_id":"source/_posts/2018-03-11-logistic-regression.md","hash":"3ad9b3ad77cbc308190ebed469a81c3407a0f223","modified":1566034303902},{"_id":"source/_posts/2018-03-20-taylorseries.md","hash":"d86c1fc5a1d3b2f73c1ce08e370860f596f9d34d","modified":1566034328302},{"_id":"source/_posts/2018-03-21-chisquaredtest.md","hash":"f0d68113cb45b1aa40a1fce9fb6376c53ea7a4a4","modified":1566034359997},{"_id":"source/_posts/2018-03-18-decision-tree.md","hash":"976db177661b3c274049d5ca922605789ca85e6a","modified":1566034311718},{"_id":"source/_posts/2018-03-24-regularization.md","hash":"dde18af0b1912cd3fac2897c7cc17c8a1afa011e","modified":1566034387314},{"_id":"source/_posts/2018-03-27-lagrange.md","hash":"f8597758cb356f7147cb9560fa1f410244e42a24","modified":1566034402633},{"_id":"source/_posts/2018-03-28-svm-1.md","hash":"e858fed7053e14ed5809f907b5baa27322c876f3","modified":1566034410856},{"_id":"source/_posts/2018-03-28-svm-2.md","hash":"f62e624daa47e0571a59d514e7a2ee5bc3c0a7b3","modified":1566034420643},{"_id":"source/_posts/2018-03-28-svm-3.md","hash":"2598f6eb31ccf6cb866c13faf24f88f818c32d8f","modified":1566034426881},{"_id":"source/_posts/2018-03-28-svm-4.md","hash":"823ccac384fd97c9c1d2f803863fc4c61faa73ca","modified":1566034431041},{"_id":"source/_posts/2018-04-10-adaboost.md","hash":"5945f1680c8a06d1100a04d73c634a32798bc6bf","modified":1566034436782},{"_id":"source/_posts/2018-04-12-gbdtxgboost.md","hash":"c473b5bb59cca978a9894091810951d344f998ec","modified":1566034453035},{"_id":"source/_posts/2018-04-19-bigscaledataml.md","hash":"480fa275f24ad8708f5ad6b630cc0a6e4ce329aa","modified":1566034502664},{"_id":"source/_posts/2018-04-28-emalgorithm.md","hash":"0908f21e48fcc05aa61dc98918c532aac88ed5e0","modified":1566034555251},{"_id":"source/_posts/2018-04-23-tuneparameters.md","hash":"d87a7a1b063ce25a2d7cbd0d045a794202f84619","modified":1566034544502},{"_id":"source/_posts/2018-04-21-randomforest.md","hash":"842987e4f63f64ae172867e18c9ca158af5d7af6","modified":1566034519050},{"_id":"source/_posts/2018-05-03-word2vec.md","hash":"733b6657f0da02d055719431838f33ea212aa3a3","modified":1566034049315},{"_id":"source/_posts/2018-05-10-LongestPalindromicSubstring.md","hash":"da1d58c83cbd7553cf8ba1fe41db12e8362cff66","modified":1566034574655},{"_id":"source/_posts/2018-05-20-gradientdescent.md","hash":"629ff45d2de52b92767e1001791320c68197a868","modified":1566034611493},{"_id":"source/_posts/2018-05-18-dimreduction.md","hash":"bc51bbf4928709d40a518dcedd69688cdfacedf3","modified":1566034596860},{"_id":"source/_posts/2018-05-23-backpropagation.md","hash":"cbebd385e6541ab4776bf850fc2f082ec559eb0c","modified":1566034627759},{"_id":"source/_posts/2018-06-03-newstonmethod.md","hash":"92adac12c7907c736934db6b58aa522637ab6cac","modified":1566034639157},{"_id":"source/_posts/2018-06-18-smote.md","hash":"95502df4796699fe4b86dc834fbc40f1c9786885","modified":1566034654560},{"_id":"source/_posts/2018-06-20-rnn1.md","hash":"3a81fb93912d969782e1919568659ee57c6fe52c","modified":1566034666506},{"_id":"source/_posts/2018-06-28-cnn1.md","hash":"21a8743fb35906b533a85efb33ac73d6c83294c2","modified":1566034691742},{"_id":"source/_posts/2018-07-13-LINE.md","hash":"8beb866d5465c30b6a660122197f76e316c2d50f","modified":1566034731517},{"_id":"source/_posts/2018-07-08-doc2vectutorial.md","hash":"469c2949919bf0198cab92f75ff1891628d8b705","modified":1566034711598},{"_id":"source/_posts/2018-07-30-rnngradientdisappear.md","hash":"afc24a2314bdf0bacefe3bf05aa5cc946409ecba","modified":1566034744973},{"_id":"source/_posts/2018-09-29-machinelearningevaluate.md","hash":"dae399f78951210b0403fd92a4d89cb0569580df","modified":1566034785865},{"_id":"source/_posts/2018-10-09-kmeans.md","hash":"eea7e764d2b27f87af90d743abd7899633b9c052","modified":1566034799950},{"_id":"source/_posts/2018-10-17-rnn2.md","hash":"bed76df6ed33dbc6d8509229fe5efcba4d64b5fb","modified":1566034810283},{"_id":"source/_posts/2018-08-08-resume.md","hash":"78fe8c47d00c0b07e351efdaef161aca5aa79c79","modified":1566034756973},{"_id":"source/_posts/2018-11-26-hexo常用命令.md","hash":"de347cf06e0e96d71b5973d79ded18891ceff898","modified":1566034863145},{"_id":"source/_posts/2018-11-12-tSNE.md","hash":"f25ac2a6145ad1d3b95b77bc95e210507cf322a9","modified":1566034837459},{"_id":"source/_posts/2018-11-28-推荐系统HandBook-Chapter3.md","hash":"43f0363e22f593dab573e57577a9fc519c747048","modified":1566034873271},{"_id":"source/_posts/2018-11-28-hexo博客同步管理及迁移.md","hash":"4a7a38bb7d8e947437936d0c6d7aedd0c7babfdc","modified":1566100099364},{"_id":"source/_posts/2018-11-26-推荐系统HandBook-Chapter1.md","hash":"0cd0e6e953d956d6fee449ce5ec22cf125da994c","modified":1566034843429},{"_id":"source/_posts/2018-11-27-推荐系统HandBook-Chapter2.md","hash":"fb79ea8fbe858ad3014a94901769fbfd0a371f2d","modified":1566034868506},{"_id":"source/_posts/2018-6-21-unbalancedata.md","hash":"c1d4a159b5748b2a58364c30ad2bce65896f5428","modified":1566034678851},{"_id":"source/_posts/2019-02-14-pig基础.md","hash":"7cf18f6f9d5e3594f83b24fe0d1649c8f96fb04d","modified":1566034906486},{"_id":"source/_posts/2019-03-07-Spark基础.md","hash":"b86eb81b7816cdb91b42065984e422f9a1bd7ca1","modified":1566207225932},{"_id":"source/_posts/2019-03-13-python常用实例.md","hash":"03d421d12cc723b1669c50795bb76543ffc37b41","modified":1566034943335},{"_id":"source/_posts/2019-03-27-Vim-Tutorial.md","hash":"5cef62ed166c608b64ef1ee24bfcdfd45953c4f3","modified":1566034965205},{"_id":"source/_posts/2019-06-05-Optimization-Method.md","hash":"75a6dd94c6582d034b556b7cefd9278213da61a4","modified":1566377800776},{"_id":"source/_posts/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录.md","hash":"e953986378af811f50ba948ac5a07e2c9aedc149","modified":1566034980723},{"_id":"source/_posts/2019-03-29-git基础.md","hash":"dcee4406f89681dca06b2d41c61aae082f0cda37","modified":1566034996586},{"_id":"source/_posts/2019-04-24-pymongo基础.md","hash":"0e27de5a6655f11d79e15ce6e8962d5bdecfd43c","modified":1566127279381},{"_id":"source/categories/index.md","hash":"bd4f0f41584baa4e787aafed06ab93ca046b171e","modified":1566097476126},{"_id":"source/tags/index.md","hash":"ae26dcd0f0035f52fecaf930c332f121c8a5a2b1","modified":1566097496229},{"_id":"source/images/.DS_Store","hash":"d5df4b4938b29b8c714200b610d622f37b8fbc40","modified":1566022089714},{"_id":"source/images/avatar.jpg","hash":"a7832453f0a2be3fb155f3b5e3dace03fcba67c6","modified":1561299535625},{"_id":"source/images/github-logo.png","hash":"5c45c326d1c756542e2bfda274ca28a804f69a73","modified":1561299535627},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"c149f003d03501565e7688915cd8f2e99fbf8f42","modified":1566023038874},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"d91296a4c9facf6e2dc15815820b6444262f575c","modified":1566023038875},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"3239625bb2573e61f7bcce27a74882a9ff7021e9","modified":1566023038876},{"_id":"themes/next/.github/auto_assign.yml","hash":"cb68a1dca1c4623448c2ca899614a9f21df1b036","modified":1566023038877},{"_id":"themes/next/.github/config.yml","hash":"8a5cbf5aa9529390fe0a782758aca9c3a02f9dcf","modified":1566023038877},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1566023038875},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"b795f819d4e85197c31ac8892aab55e19bca970a","modified":1566023038877},{"_id":"themes/next/.github/lock.yml","hash":"585d2c471047be320aa62f2b74dad797bf09c530","modified":1566023038877},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1566023038877},{"_id":"themes/next/.github/release-drafter.yml","hash":"fbd6210c4454c2db6cad1960dd03c89488d68c87","modified":1566023038878},{"_id":"themes/next/.github/eslint-disable-bot.yml","hash":"16541fb7b80f5ab90135db96285badb63c4d7d3e","modified":1566023038877},{"_id":"themes/next/.github/stale.yml","hash":"41bf97ee86b8940a0b2e754499ec77fd2b44b717","modified":1566023038878},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1566023038878},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"4094dab82cbdfdd0de117e94b508bbd5ceb3d363","modified":1566023038882},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1566023038881},{"_id":"themes/next/docs/AUTHORS.md","hash":"e387f4859131983a4b29d0f61c6082d5b8fb0edd","modified":1566023038882},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"1fb2c852fad03b73882069f1ec0659db2e1f2da0","modified":1566023038883},{"_id":"themes/next/docs/MATH.md","hash":"6a6be5a98a44b063993eba755c280ac00b926c63","modified":1566023038883},{"_id":"themes/next/docs/DATA-FILES.md","hash":"a85ae0f3a9655eea9acd3f6209b72eb002f2058d","modified":1566023038882},{"_id":"themes/next/docs/INSTALLATION.md","hash":"5385246ab4756e3e26ec9fe1f6a6759812177cc4","modified":1566023038882},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"a07ca23c38f6e4dddd4b74016b30e88cd3796f75","modified":1566023038883},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1566023038883},{"_id":"themes/next/languages/default.yml","hash":"4de207d89824422ccec5bc754a81af032d5e30f9","modified":1566023038889},{"_id":"themes/next/languages/en.yml","hash":"4de207d89824422ccec5bc754a81af032d5e30f9","modified":1566023038889},{"_id":"themes/next/languages/de.yml","hash":"7d49f193a8f5adaf4f92e59190f30e0defff6750","modified":1566023038887},{"_id":"themes/next/languages/es.yml","hash":"170a623afb40566565a805461a030f649ec9a803","modified":1566023038889},{"_id":"themes/next/languages/fr.yml","hash":"03f5cf2aa02ca4ed6ba639a38e8cf5d6d873b442","modified":1566023038890},{"_id":"themes/next/languages/hu.yml","hash":"1e7e7eaa12fa7eb8ba45b228f1ff47f39a841c72","modified":1566023038890},{"_id":"themes/next/languages/fa.yml","hash":"bdd06c1ff301a32d1d5940a234c938924265ef71","modified":1566023038889},{"_id":"themes/next/languages/id.yml","hash":"32152e32a3b6da6ae81c37c13d1f8950a4d76ade","modified":1566023038890},{"_id":"themes/next/languages/it.yml","hash":"ffaaa9c07981b525762af48b0c78e92443659f93","modified":1566023038890},{"_id":"themes/next/languages/ko.yml","hash":"3be6b7695565777e4428c4934a43930517fb1d74","modified":1566023038891},{"_id":"themes/next/languages/ja.yml","hash":"8ec35aa9d54f201d17e322882735018e63c6120d","modified":1566023038891},{"_id":"themes/next/languages/nl.yml","hash":"d1b5dc6236dd6c0c5e32b9f5e9d8d60a77e06de6","modified":1566023038891},{"_id":"themes/next/languages/pt-BR.yml","hash":"ae238667c73a074cf383960ceabcaf98f4c3b7a5","modified":1566023038891},{"_id":"themes/next/languages/pt.yml","hash":"8f4b951c62e47c5dea0253bba87f0a2f3f31c8f3","modified":1566023038892},{"_id":"themes/next/languages/ru.yml","hash":"3a2e2fbfdd68183bcf33253d3a94f254cea6444f","modified":1566023038893},{"_id":"themes/next/languages/tr.yml","hash":"93975c9152d8d9f3715b6f4ae9d28bc96bc168b5","modified":1566023038893},{"_id":"themes/next/languages/uk.yml","hash":"5d994a97fe5725c6b3a1d63043c26f76bf52bba3","modified":1566023038894},{"_id":"themes/next/languages/zh-CN.yml","hash":"bf1d66fc60be3e349c0118f3b33ef464b4814c99","modified":1566099224500},{"_id":"themes/next/languages/vi.yml","hash":"a00c281324d182f147345ee6634c26450efc3160","modified":1566023038894},{"_id":"themes/next/languages/zh-TW.yml","hash":"d1b69b3075b0fa0444ee32b4e25ac52886b1388a","modified":1566023038895},{"_id":"themes/next/languages/zh-HK.yml","hash":"f78690189d8dccf95e5a5cf9f25d865eea19d85e","modified":1566023038895},{"_id":"themes/next/layout/_layout.swig","hash":"bb1b2ec6fbc2dab4c0d8456e3b42a1c163c5b04c","modified":1566095502645},{"_id":"themes/next/layout/category.swig","hash":"3020f7646dc3c79d44e3b5f1ce24b06d5b9fc13a","modified":1566023038926},{"_id":"themes/next/layout/index.swig","hash":"8cf3451dd2d5727920b351a8f5eefdddb5bf3638","modified":1566023038927},{"_id":"themes/next/layout/archive.swig","hash":"9941337701a8c0377a7af0024d948fcbb3a29a58","modified":1566023038926},{"_id":"themes/next/layout/page.swig","hash":"dc48085a0c69c60155d57aa546ded835a1ff767e","modified":1566023038927},{"_id":"themes/next/layout/post.swig","hash":"4291b34b17fc6f1c06eb69852c894913e889a774","modified":1566023038927},{"_id":"themes/next/layout/tag.swig","hash":"02b5dfdfa1d1ff61f161f592d38b09454c941036","modified":1566023038928},{"_id":"themes/next/source/.DS_Store","hash":"62cb573b6bfecfb00ffe3607ebaf017f3181573c","modified":1566098814058},{"_id":"source/posts_res/2017-12-12-latex/18.png","hash":"265cf19014600c394f5a84d315db1d246d7fe3e6","modified":1566015427321},{"_id":"source/posts_res/2017-12-12-latex/19.png","hash":"42a7d7cf28034f2def83364e66b4348895cce503","modified":1566015427323},{"_id":"source/posts_res/2018-03-08-native-bayes/native_bayes.py","hash":"216265851f8abac49bef0a460f3a41ecfacdb751","modified":1566015427284},{"_id":"source/posts_res/2018-03-11-logistic-regression/1-1.png","hash":"37829ee22acdd33893fed8b842455f916b69f225","modified":1566015427395},{"_id":"source/posts_res/2018-03-11-logistic-regression/logistic_regression.py","hash":"3fc946b8c9337deee57154be3d28ff53b57ba94d","modified":1566015427395},{"_id":"source/posts_res/2018-03-18-decision-tree/cart_cut.png","hash":"7c4c6b53bb11f32c7972d98efa32c908d44efac2","modified":1566015427384},{"_id":"source/posts_res/2018-03-18-decision-tree/credit.csv","hash":"74f2f0eabf0497961d4b1dd9b73887ff4558a4e7","modified":1566015427381},{"_id":"source/posts_res/2018-03-11-logistic-regression/1-4.gif","hash":"e3af77ee5228cbbe8baf60472fee710e36010205","modified":1566015427394},{"_id":"source/posts_res/2018-03-18-decision-tree/decision_tree.py","hash":"30348d26717af8b3e4719b36ecfcc5272d6133c0","modified":1566015427383},{"_id":"source/posts_res/2018-03-18-decision-tree/titanic.csv","hash":"b45a3c53ef15d4c66f94d459348ed74f904d6d8c","modified":1566015427383},{"_id":"source/posts_res/2018-03-18-decision-tree/dt.png","hash":"cbdc378cc88a9a9e020b30e1fb0025b9a67d7f50","modified":1566015427382},{"_id":"source/posts_res/2018-03-21-chisquaredtest/3-1.png","hash":"384fdfb5bb3897bcebf5f230358fd915ec12b1c0","modified":1566015427393},{"_id":"source/posts_res/2018-03-18-decision-tree/treePlot.py","hash":"c45b625d2afd5f4980c9c7a2d22f334ad02c13b2","modified":1566015427382},{"_id":"source/posts_res/2018-03-24-regularization/2-1.jpg","hash":"e38966f85250a6b4831e39f5d85f46edfe08037a","modified":1566015427377},{"_id":"source/posts_res/2018-03-24-regularization/5-1.png","hash":"4039fe5e599e505271d5cc7cf2212a83ad870465","modified":1566015427380},{"_id":"source/posts_res/2018-03-24-regularization/4-2.png","hash":"40fabf0810723bff643cb9d8caa5863a21aedf0e","modified":1566015427378},{"_id":"source/posts_res/2018-03-28-svm/1-supportvector.png","hash":"2288b9887025faec995df417c953d38e08eb9e36","modified":1566015427349},{"_id":"source/posts_res/2018-03-28-svm/2-supportvector.png","hash":"9616f8539fc8f8f8c3c9e083813443d75270be57","modified":1566015427355},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/1-1.png","hash":"a9767370fce1909c67f764751d063bd9976c2ad8","modified":1566015427373},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/1-2.png","hash":"0bf0ffa963f7cd53dffb62a195cb7ea8ed170a94","modified":1566015427375},{"_id":"source/posts_res/2018-04-19-bigscaledataml/1.png","hash":"0d4b57ae0d1e2a1a78baeb40e000c3f2e619a918","modified":1566015427330},{"_id":"source/posts_res/2018-04-16-languagestlset/1.jpg","hash":"08b5ab09c7fa27deb662ae2219e8a2de132e240c","modified":1566015427328},{"_id":"source/posts_res/2018-04-19-bigscaledataml/2.png","hash":"0d3591078e5715d10861ae9215bf3c281f639367","modified":1566015427330},{"_id":"source/posts_res/2018-04-21-randomforest/3-1.png","hash":"5b1356b4dfd6a65dcaaa8dc920f8b8d6e17091dc","modified":1566015427326},{"_id":"source/posts_res/2018-04-21-randomforest/3-2.png","hash":"d045e77e9b61638d75c0057f363fffad8b635721","modified":1566015427326},{"_id":"source/posts_res/2018-04-21-randomforest/3-3.png","hash":"54af23d18ae5c8f566053b504f1a537b751d3418","modified":1566015427325},{"_id":"source/posts_res/2018-04-21-randomforest/3-6.png","hash":"4b7cde787cd1bbd7e323a43c7f51ee438c9c1601","modified":1566015427327},{"_id":"source/posts_res/2018-04-21-randomforest/3-5.png","hash":"854ad1caaeaea78e1c11c031418059595a242448","modified":1566015427327},{"_id":"source/posts_res/2018-04-21-randomforest/3-4.png","hash":"c3b689df944c1bcf5f58c2c09b3f5de4cd6d0ddf","modified":1566015427327},{"_id":"source/posts_res/2018-04-23-tuneparameters/1.png","hash":"b8ff9c52727a4eb9d3550ff296b25d760b66d805","modified":1566015427358},{"_id":"source/posts_res/2018-04-28-emalgorithm/1.jpg","hash":"2ad2f8e71996ac1ff5d596ce396fac7be77511dd","modified":1566015427324},{"_id":"source/posts_res/2018-05-10-LongestPalindromicSubstring/1.png","hash":"62d0168093aea045e773fe136b276d892170d792","modified":1566015427393},{"_id":"source/posts_res/2018-05-10-LongestPalindromicSubstring/2.png","hash":"36e8dce4b293f139c8f0c24af91d009d890d99dc","modified":1566015427392},{"_id":"source/posts_res/2018-05-18-dimreduction/1-1.png","hash":"692da042ed3d89132d553ed900987584f48737ad","modified":1566015427361},{"_id":"source/posts_res/2018-05-18-dimreduction/1-2.png","hash":"4bfc04a8722c644495ef97944c1ef1c3f6754c4b","modified":1566015427361},{"_id":"source/posts_res/2018-05-18-dimreduction/3-1.jpg","hash":"15de6b3f33d3d4abd6ba11d1910afce48380c716","modified":1566015427362},{"_id":"source/posts_res/2018-05-18-dimreduction/4-1.jpg","hash":"1c4e640b7f36f0474529fdce0fb981a0e7e00657","modified":1566015427360},{"_id":"source/posts_res/2018-05-20-gradientdescent/0-1.png","hash":"406d1525c1821731909e926c62e035be549c23a2","modified":1566015427387},{"_id":"source/posts_res/2018-05-23-backpropagation/2-1.jpg","hash":"f22ebf2e9e1628cea50893ad68e91a730c817315","modified":1566015427342},{"_id":"source/posts_res/2018-05-20-gradientdescent/3-1.png","hash":"3cf07203d742348a6c2de2fcf47a379063131672","modified":1566015427389},{"_id":"source/posts_res/2018-05-23-backpropagation/2-2.jpg","hash":"81308c6253b0efc03f32bc39e8ddcf94d09bd76f","modified":1566015427343},{"_id":"source/posts_res/2018-06-18-smote/smote.jpg","hash":"196195958b7cbc3e730045eafb0dfec129c21f08","modified":1566015427286},{"_id":"source/posts_res/2018-06-20-rnn1/1-1.jpg","hash":"7e20e753c33c9bac42297d35d2213123b1db58cd","modified":1566015427278},{"_id":"source/posts_res/2018-06-20-rnn1/1-2.jpg","hash":"4a7fdd264525356f44053c45411b6f0cd75728d5","modified":1566015427280},{"_id":"source/posts_res/2018-06-20-rnn1/2-1.jpg","hash":"a4bae4774faddd7a5dc60d81fc74a3dde51a791c","modified":1566015427275},{"_id":"source/posts_res/2018-06-20-rnn1/2-2.jpg","hash":"9baebf7a990c0bfee2865f32eba9a33c80c9e42e","modified":1566015427276},{"_id":"source/posts_res/2018-06-20-rnn1/3-1.jpg","hash":"25e94e6d35299ddf49bcac4d1f1b21b93c3f3c84","modified":1566015427281},{"_id":"source/posts_res/2018-06-20-rnn1/3-2.jpg","hash":"b8905a58579922ff78294c7ffcaf2c1e932b668a","modified":1566015427279},{"_id":"source/posts_res/2018-06-20-rnn1/3-3.jpg","hash":"5c177d98f105885fca9afbc7ca4532cd34fc1348","modified":1566015427278},{"_id":"source/posts_res/2018-06-20-rnn1/3-5.jpg","hash":"662c746c9de6f280b950bbbeee038c04d0f864d4","modified":1566015427281},{"_id":"source/posts_res/2018-06-20-rnn1/3-4.jpg","hash":"c3dda1ec78ce9b51fa66bd37b45c835dd87e58d8","modified":1566015427282},{"_id":"source/posts_res/2018-06-20-rnn1/3-8.jpg","hash":"6eb43f0215365715b6274707d94870e75683cf76","modified":1566015427274},{"_id":"source/posts_res/2018-06-20-rnn1/3-7.jpg","hash":"15431160546ed5f8999b7e71820bed5d3cd4b409","modified":1566015427283},{"_id":"source/posts_res/2018-06-20-rnn1/3-6.jpg","hash":"cc0a7367566d4efafd6d65878df76a9d7f9d940f","modified":1566015427282},{"_id":"source/posts_res/2018-06-20-rnn1/3-9.jpg","hash":"62c8301caff1d78ae0b26afbe83e1a61c9493936","modified":1566015427274},{"_id":"source/posts_res/2018-06-20-rnn1/4-2.jpg","hash":"3f33dcb9c608451620df8984d33d2d59dc746434","modified":1566015427277},{"_id":"source/posts_res/2018-06-20-rnn1/4-3.jpg","hash":"3f33dcb9c608451620df8984d33d2d59dc746434","modified":1566015427277},{"_id":"source/posts_res/2018-06-20-rnn1/4-1.jpg","hash":"0581d323c2b130368e4d8cc7c6dc61a80efdd4cc","modified":1566015427276},{"_id":"source/posts_res/2018-06-28-cnn1/2-1.png","hash":"ff4b0a0ec829a8b131f146ab1b62f898c16c89be","modified":1566015427297},{"_id":"source/posts_res/2018-06-28-cnn1/2-2.png","hash":"9812de99dbe3d5c5e7c131e95d780730950c4a00","modified":1566015427299},{"_id":"source/posts_res/2018-06-28-cnn1/2-10.png","hash":"96e045b6d339b705d8ba25bb84e4a79ead905da2","modified":1566015427294},{"_id":"source/posts_res/2018-06-28-cnn1/2-11.png","hash":"ce7df48a1fcdb29a180c02a4c89a5db660495760","modified":1566015427292},{"_id":"source/posts_res/2018-06-28-cnn1/2-3.png","hash":"31d0a7c665b9353ae84ddbbd4493180103d9bc0d","modified":1566015427298},{"_id":"source/posts_res/2018-06-28-cnn1/2-4.png","hash":"4e4a0203a2c6de9b8c9be4c98085e7dda8d96f10","modified":1566015427304},{"_id":"source/posts_res/2018-06-28-cnn1/2-6.png","hash":"22243309df58aa4c101680fed65f14b7c7630d9b","modified":1566015427301},{"_id":"source/posts_res/2018-06-28-cnn1/2-5.png","hash":"4fd6e8ab0f0865c77a6688bafaac2fb0bca5500f","modified":1566015427303},{"_id":"source/posts_res/2018-06-28-cnn1/3-1.png","hash":"a437d0e1251d77546159737f89adc5eaf200f3b0","modified":1566015427307},{"_id":"source/posts_res/2018-06-28-cnn1/4-1.png","hash":"c2b22413298577cd901e24785cacd0ea2e8b55f3","modified":1566015427302},{"_id":"source/posts_res/2018-06-28-cnn1/4-10.png","hash":"03216d29dfd4f52a9fabfca5c1318ddb3419d754","modified":1566015427289},{"_id":"source/posts_res/2018-06-28-cnn1/4-12.png","hash":"06984f275ff3b2edf4f863d48dafaa69c4b55ee8","modified":1566015427288},{"_id":"source/posts_res/2018-06-28-cnn1/4-13.png","hash":"00b69bdea8b8dc1e1d6ada203bacd946eb17dc96","modified":1566015427287},{"_id":"source/posts_res/2018-06-28-cnn1/4-16.png","hash":"a437d0e1251d77546159737f89adc5eaf200f3b0","modified":1566015427291},{"_id":"source/posts_res/2018-06-28-cnn1/4-17.png","hash":"aacbd1c0e40a2cbc39676b6c851338951cd8ae1e","modified":1566015427292},{"_id":"source/posts_res/2018-06-28-cnn1/4-3.png","hash":"9d6a251bf95ae1887a2c4f97192ada2ef195da05","modified":1566015427303},{"_id":"source/posts_res/2018-06-28-cnn1/4-2.png","hash":"ecefbc4cb98ba47d79a85c7cb2acf24d36025425","modified":1566015427304},{"_id":"source/posts_res/2018-06-28-cnn1/4-4.png","hash":"0786e861c0b1f9a5ad17d5818d2475a44a71b1c6","modified":1566015427300},{"_id":"source/posts_res/2018-06-28-cnn1/4-6.png","hash":"051f32a02fbaad4ad051fa92c20f7684497f0abe","modified":1566015427296},{"_id":"source/posts_res/2018-06-28-cnn1/4-7.png","hash":"f2fdb7b46058f6bd5e9669b560598078d98ac958","modified":1566015427297},{"_id":"source/posts_res/2018-06-28-cnn1/4-8.png","hash":"40f9e8232443e9b001b1e68518f80c75f5288139","modified":1566015427296},{"_id":"source/posts_res/2018-06-28-cnn1/4-9.png","hash":"a485ace5f8557a4c349c25b5bb8bf419b79114cf","modified":1566015427295},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/2.png","hash":"52aeee4766a85619229734077c5a75ed836eb4c1","modified":1566015427367},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/1.jpg","hash":"cf4ced0fdbb06270b8fe2a9c8fd03dfc2524fef1","modified":1566015427399},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/4.jpg","hash":"ae9e2093fa1cef7caa2260eb5b88391612bb4a92","modified":1566015427396},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/2.jpg","hash":"f271dcbe29bf4a6331c65d2d7b5264ad1c66a4d7","modified":1566015427398},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/5.jpg","hash":"2a308e387bb1f9be8c9a6269a49cdc71a73a104e","modified":1566015427397},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/3.jpg","hash":"3928d7a5e40e74693d6cfd172f6a7d81d1e3a973","modified":1566015427399},{"_id":"source/posts_res/2018-10-09-kmeans/kmeans.py","hash":"2d4c9bea312e65237b674bcaf9df84a7f4756258","modified":1566015427362},{"_id":"source/posts_res/2018-07-30-rnngradientdisappear/6.jpg","hash":"34b2f7476f92f2abe2a48cf39587d8d3f7e583fa","modified":1566015427398},{"_id":"source/posts_res/2018-10-09-kmeans/res.jpg","hash":"3639c8e88694ef6443492718c521c3a406a99fce","modified":1566015427363},{"_id":"source/posts_res/2018-11-12-tSNE/2-2.png","hash":"99fcef182fbaa007dfde70273537d22a6bcd12b2","modified":1566015427369},{"_id":"source/posts_res/2018-11-12-tSNE/2-6-2.jpg","hash":"279f0508fd0cb3e9dea07951f70d3661b2764542","modified":1566015427369},{"_id":"source/posts_res/2018-11-12-tSNE/2-3.png","hash":"9a5fd912660eeb801eca3a8f3f56b66957793f77","modified":1566015427368},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg","hash":"17c5c2ce7a67ce446c40e5476b232ed874e3de12","modified":1566015427401},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png","hash":"b9840d66f16ec4cd3b8529b1e51c243bd58c7158","modified":1566015427332},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg","hash":"4e0e582c3c33dd9ff35811b2cfa25da6f950a330","modified":1566015427400},{"_id":"source/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg","hash":"e04ac7e60876f42497c91c03588a19ccc59f32aa","modified":1566015427400},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png","hash":"e8ad024e82eda9ba3128a6a37ce42b435e511da5","modified":1566015427331},{"_id":"source/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png","hash":"78c9a27ed02c93e56bd222867dc2cf391bdc727c","modified":1566015427332},{"_id":"source/posts_res/2019-03-29-git基础/1.jpg","hash":"9452cdd7f9ebbf71739bfdaeb562c7301f4379c0","modified":1566015427386},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"c37a60580c901c79ccb22564b228a46e06207445","modified":1566023038876},{"_id":"themes/next/.github/ISSUE_TEMPLATE/custom-issue-template.md","hash":"57e1e06e845193e80c7df4a4454af28352526f7a","modified":1566023038876},{"_id":"source/posts_res/2019-03-29-git基础/2.png","hash":"6a1a4e6d81b9d5bfa87d59a920ee61ef5c25c4d4","modified":1566015427385},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"07c423cce4157b8e2dbf60907ccbf3f18c4cf98a","modified":1566023038876},{"_id":"source/posts_res/2019-03-29-git基础/3.png","hash":"cf981952a2ed1a3fb4be484962b46c7fae150313","modified":1566015427385},{"_id":"themes/next/.github/ISSUE_TEMPLATE/non-english.md","hash":"0b0727ff4d5180ae67f930fb4f8e9488e33eda9f","modified":1566023038876},{"_id":"source/posts_res/2019-03-29-git基础/4.png","hash":"108edcf3ca29aca927543f59a06491167ac5d50d","modified":1566015427385},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1566023038884},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"caa624092175d44e3d3a8c6ca23922718da2354c","modified":1566023038884},{"_id":"themes/next/docs/ru/README.md","hash":"932d3965d8b1a1ff653c07a0cafcdbf5892d6945","modified":1566023038884},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1566023038884},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"0dfb458370a0ffbbe37c00f53c15e3aa3e79b125","modified":1566023038884},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"2949ccf2cba5238f01a1386a9e91e646c7a22260","modified":1566023038885},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"c13546dd70a99a7d2d409cddf5d59f2f4d4d44a4","modified":1566023038885},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"c46f3f7496fc422207cc5954e10a2bc972b650aa","modified":1566023038885},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"e568a1b3c532e0ecf33cc237e1425b2579bd006d","modified":1566023038885},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"a658a72589738e11aefaa3a714b188ffd5aac822","modified":1566023038885},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"ba918bbd8faf32c28335693f03ff37c6acaff7f3","modified":1566023038886},{"_id":"themes/next/docs/zh-CN/README.md","hash":"79a73361b24e7fb7022992702961faacd6a2f9fe","modified":1566023038886},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"67d04e3e29a2675d1a9f87d1ae6ea0769e963ff1","modified":1566023038884},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"76350742ccddd23b3a790f932004c34eaa407c8e","modified":1566023038897},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"c2aa4574aa032418103e3cfd07c9996e1ce25733","modified":1566023038898},{"_id":"themes/next/layout/_macro/post.swig","hash":"24be6c04a23aa7fa0d26555f0a04ee9d58b9353c","modified":1566023038898},{"_id":"themes/next/layout/_partials/github-banner.swig","hash":"40e940c3213b74933e529cd64f64c1f6f512b5ca","modified":1566023038900},{"_id":"themes/next/layout/_partials/comments.swig","hash":"e0f59221b6010bd3e05d34fac915556f51b8a458","modified":1566023038899},{"_id":"themes/next/layout/_partials/footer.swig","hash":"ab5ae49e8f2590b5bfacce95dc89f50d19baebdc","modified":1566023038899},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"769a983fe2f77908bfc8590ff37a0d8958b47598","modified":1566023038902},{"_id":"themes/next/layout/_partials/pjax.swig","hash":"2016027da7af496cb584c41affecf4b167c0f5a9","modified":1566023038902},{"_id":"themes/next/layout/_scripts/next-boot.swig","hash":"78b55b869382b99111bb2186244d07c84f694b08","modified":1566023038909},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"5707ec9e019a0af59b898bace53fac4fa92e8a27","modified":1566023038909},{"_id":"themes/next/layout/_partials/post-edit.swig","hash":"4b0c972c44bfa7ab07604280563deb9cc3974657","modified":1566023038902},{"_id":"themes/next/layout/_scripts/index.swig","hash":"3807bed8ae3f632019f26e282ef613d2275287ee","modified":1566023038909},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"edaff4766e0c05fd5c889d9dd32884d376bef9d9","modified":1566023038910},{"_id":"themes/next/layout/_scripts/three.swig","hash":"930535b03f7c0c6f31b788503201a0df29dc011e","modified":1566023038912},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"dd90ed9e8f5b8aae4111fdc28a25036c5cb9548c","modified":1566023038912},{"_id":"themes/next/layout/_third-party/bookmark.swig","hash":"b4ede5d2fc639dd9e703adcd7ca51ed3e9545975","modified":1566023038916},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"8627c8c8b031ecee16c522433b66fa4d6979b8ea","modified":1566023038915},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"f53bd0f161f9b8d13d50212b838d35236dd03fba","modified":1566023038923},{"_id":"themes/next/layout/_third-party/index.swig","hash":"56133a4237a5cdee2bd874a2b3c0be2ac967dd2b","modified":1566023038921},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"da6a9d14ed10203e378c6e2c00a7b5e7afabca58","modified":1566023038923},{"_id":"themes/next/layout/_third-party/facebook-sdk.swig","hash":"ae5a96ad90f314da50bb6057bdd3a4a0a3106827","modified":1566023038921},{"_id":"themes/next/layout/_third-party/vkontakte-api.swig","hash":"e0d012bdbae5545d90143030116114c34219bd37","modified":1566023038926},{"_id":"themes/next/scripts/events/core.js","hash":"a25690d8b03779f7e318108c8eace9baf6d4283a","modified":1566023038928},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"834599d2b83f06e90f8cf618cddc106fe900ba5c","modified":1566023038932},{"_id":"themes/next/scripts/filters/exturl.js","hash":"9c32675f64121ffbe454189b8e418b408f79d9c2","modified":1566023038933},{"_id":"themes/next/scripts/filters/excerpt.js","hash":"827c3733fa14b018dd1f07e41686c207880e823a","modified":1566023038933},{"_id":"themes/next/scripts/filters/lazyload.js","hash":"51672320fff6b39f56fe6a2881c611afe4e79b8e","modified":1566023038933},{"_id":"themes/next/scripts/filters/minify.js","hash":"cd66d19f85dffcf6d2624ebd2371e44ffa2c696c","modified":1566023038934},{"_id":"themes/next/scripts/helpers/engine.js","hash":"896a581feb9432e58a325ad04d66d6310a12374e","modified":1566023038934},{"_id":"themes/next/scripts/helpers/font.js","hash":"53169e2d04f0c21e68beb07c212be0a6c2a9aaea","modified":1566023038934},{"_id":"themes/next/scripts/helpers/next-inject.js","hash":"33eb6f38de575b973fed237032c34d878594fdc1","modified":1566023038935},{"_id":"themes/next/scripts/helpers/next-js.js","hash":"b28a8ed6d033946acbb5b91f6d8b884857410dd9","modified":1566023038935},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"799a042bbf497a4c7a2981aa2014ff28fa1bb382","modified":1566023038935},{"_id":"themes/next/scripts/helpers/next-vendors.js","hash":"d512f44195f68b0ff93f3c82afff9161cdf1b7d6","modified":1566023038936},{"_id":"themes/next/scripts/tags/button.js","hash":"1d1d25f7e579d92fa563778dd0f163e8eda190da","modified":1566023038936},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"20e392b8583ba6ae5037449c2c7e191d3927641b","modified":1566023038936},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"f13430d9d1c9773b390787c2f046bb1f12a79878","modified":1566023038937},{"_id":"themes/next/scripts/tags/full-image.js","hash":"70b1d6ed969143e3912daf051ccc3585d984c146","modified":1566023038937},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"8fc05f22b88553bc1d96e0c925799cd97920fc6a","modified":1566023038937},{"_id":"themes/next/scripts/tags/include-raw.js","hash":"60f880c1e11a7df1aee4f2e01d65451fecc0372c","modified":1566023038937},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1566023038937},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1566023038938},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1566023038938},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1566023038938},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1566023038939},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1566023038939},{"_id":"themes/next/source/css/.DS_Store","hash":"548b0114469fafa017b4b1d66b35536290293379","modified":1566098829292},{"_id":"themes/next/source/css/main.styl","hash":"68c3377b643162aeaae2b60c196486fdb3b509c3","modified":1566023038972},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1566023038972},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1566023038972},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1566023038972},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1566023038973},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1566023038973},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1566023038973},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1566023038973},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1566023038974},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1566023038974},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1566023038973},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1566023038974},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1566023038974},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1566023038975},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1566023038974},{"_id":"themes/next/source/js/algolia-search.js","hash":"394936c360ba6d6e27ea8d21a6d0cf6f9c92b6f9","modified":1566023038975},{"_id":"themes/next/source/js/local-search.js","hash":"fb87ccb496f5be6fd988d7419c53dea11284d8f5","modified":1566023038975},{"_id":"themes/next/source/js/motion.js","hash":"91abd5737797aa165266c8e7ff9ff9a12742e244","modified":1566023038975},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1566023038975},{"_id":"themes/next/source/js/next-boot.js","hash":"20864354f063d2a7d675cfa5182576dacfc75fee","modified":1566023038976},{"_id":"themes/next/source/js/scrollspy.js","hash":"e630d9b05ab6bcc9ebab92435587d5a554eb0e3b","modified":1566023038977},{"_id":"themes/next/source/js/post-details.js","hash":"1798f25822ac6dd7bbb0f7cddd1f6783e919346c","modified":1566023038976},{"_id":"themes/next/source/js/utils.js","hash":"3a73c8055252a6c08fb153002a977b380b205f1a","modified":1566023038977},{"_id":"source/posts_res/2017-12-12-latex/10.png","hash":"2fb4aa3866ae143a5c74bd56b721bcfdc409564d","modified":1566015427316},{"_id":"source/posts_res/2017-12-12-latex/15.png","hash":"a8770a6ccdae013ea34f9fe90c81e79a93fa58ab","modified":1566015427312},{"_id":"source/posts_res/2017-12-12-latex/17.png","hash":"a56836c96294581763ae0478cad9def2d3fe2aef","modified":1566015427312},{"_id":"source/posts_res/2017-12-12-latex/9.png","hash":"4ff044446a2846398988970f9753de733e449904","modified":1566015427310},{"_id":"source/posts_res/2018-03-01-2017aca/results.png","hash":"5481339fe5c7f379539557355ef1683800d34a33","modified":1566015427405},{"_id":"source/posts_res/2018-03-24-regularization/4-1.png","hash":"55b1a2ef65cc3ead1a299867805ea5e3e9ee9ba0","modified":1566015427378},{"_id":"source/posts_res/2018-03-24-regularization/5-2.png","hash":"2e9a50e037ede6921e76afc7d289d0252753085d","modified":1566015427379},{"_id":"source/posts_res/2018-05-03-word2vec/1-1.jpg","hash":"570612994158bfdc31e5195d00ff4dbeaada0625","modified":1566015427341},{"_id":"source/posts_res/2018-05-03-word2vec/2-1.jpg","hash":"41fc701b2638d61fe0b1d216f5ae1ad8fc34be83","modified":1566015427339},{"_id":"source/posts_res/2018-05-20-gradientdescent/2-1.png","hash":"c116508b1de1fc4692e3bde5bac214d5b4df1196","modified":1566015427387},{"_id":"source/posts_res/2018-06-28-cnn1/1-1.png","hash":"edbfa793bd1b5c32d7999937b54163a6398eb4ed","modified":1566015427305},{"_id":"source/posts_res/2018-06-28-cnn1/2-7.png","hash":"6c04c9dfe9a30d42dc1ed2b72bae36d312b80d39","modified":1566015427301},{"_id":"source/posts_res/2018-06-28-cnn1/2-8.png","hash":"edbfa793bd1b5c32d7999937b54163a6398eb4ed","modified":1566015427294},{"_id":"source/posts_res/2018-06-28-cnn1/3-2.png","hash":"24f823e747a148d9d47302ca4cb1ebea6e0e8a80","modified":1566015427306},{"_id":"source/posts_res/2018-06-28-cnn1/4-11.gif","hash":"3c59ccecf89600bd3f2aa0c649f213da2d97dd39","modified":1566015427299},{"_id":"source/posts_res/2018-06-28-cnn1/4-15.png","hash":"610f9699df15f32fab74959770eef1f752d19986","modified":1566015427289},{"_id":"source/posts_res/2018-06-28-cnn1/4-18.png","hash":"24f823e747a148d9d47302ca4cb1ebea6e0e8a80","modified":1566015427307},{"_id":"source/posts_res/2018-06-28-cnn1/4-5.png","hash":"e3654fad2356c1f609981937bf21b03eace6cf2f","modified":1566015427298},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/1.png","hash":"1e4a81b642d64c109cbc5fa6b12100a41fa617bd","modified":1566015427368},{"_id":"source/posts_res/2018-10-17-rnn2/gru.jpg","hash":"d84b6ee541bbe14db2cbcde42b7b95f9d769fbcf","modified":1566015427273},{"_id":"source/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg","hash":"04e0a415cacd2114f71a1d0322eb9b6e872ef43a","modified":1566015427390},{"_id":"source/posts_res/2018-11-12-tSNE/2-6.jpg","hash":"b74dca003f6edf9838c39f37d95c401e02d5ff75","modified":1566015427370},{"_id":"source/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg","hash":"3dbe1a8d5c7db194eef99761a31ab66e3cb241d2","modified":1566015427391},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1566023038965},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1566023038965},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1566023038971},{"_id":"source/posts_res/2017-12-12-latex/12.png","hash":"cc89592591190c84cf4d544e38ef77eff7c85c16","modified":1566015427314},{"_id":"source/posts_res/2017-12-12-latex/14.png","hash":"378f2d1f68b89c7781b0d811fa1890b86b2b3a5b","modified":1566015427311},{"_id":"source/posts_res/2017-12-12-latex/16.png","hash":"e6996a688dd04e6751ad0ae8de75beb896c68d6e","modified":1566015427313},{"_id":"source/posts_res/2017-12-12-latex/25.png","hash":"147bac132e38bfcbdbb32cab776aeeddaf933876","modified":1566015427322},{"_id":"source/posts_res/2018-05-20-gradientdescent/1-1.png","hash":"74bd6d9a601f9d6e1a654d544508713e69e95cc6","modified":1566015427388},{"_id":"source/posts_res/2018-06-28-cnn1/2-9.png","hash":"57a3cc8a8b0e2dddf2797151e1357d07dc6c499b","modified":1566015427293},{"_id":"source/posts_res/2018-06-28-cnn1/4-14.png","hash":"b6e2a2f2b74712433d18625fba8fcdbeb8cbceae","modified":1566015427290},{"_id":"source/posts_res/2018-07-13-LINE/1.png","hash":"c754d2f21deff4790b96f99ec8452f518a798606","modified":1566015427365},{"_id":"source/posts_res/2018-08-08-resume/4-1.png","hash":"eaa16adbce8ac53f1e01fea1b1d0ef00a7ef7643","modified":1566015427404},{"_id":"themes/next/layout/_macro/menu/menu-item.swig","hash":"1dc3875981923aee38b7b1e3ca3813b75ed6ab9c","modified":1566023038896},{"_id":"themes/next/layout/_macro/menu/menu-badge.swig","hash":"9e959646899a1dd381c0bea870408cd392c93c70","modified":1566023038896},{"_id":"themes/next/layout/_partials/analytics/busuanzi-counter.swig","hash":"b35b2bf2ce3d73cabe8c443a45c2a82d45baf151","modified":1566023038898},{"_id":"themes/next/layout/_partials/analytics/index.swig","hash":"2925f6353934b1b2adc2808059e64dd7d220edd7","modified":1566023038899},{"_id":"themes/next/layout/_partials/analytics/tencent-analytics.swig","hash":"493a1ef6c8f475d4dd3156a6ab37690dc75695ea","modified":1566023038899},{"_id":"themes/next/layout/_partials/analytics/tencent-mta.swig","hash":"198813a3f382bda4278fe3759bf0f18a5769bb33","modified":1566023038899},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"6f02724e24945d7e5f0dcd351deebc90996ad38d","modified":1566023038900},{"_id":"themes/next/layout/_partials/analytics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1566023038898},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"75fd5e416cd584890da6de723b9f51cac8812c64","modified":1566023038900},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"883ea1d024a8262eb8d3213de71c8ccb30daf095","modified":1566023038901},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"d3c25d70f0fd76121e4cacb9f8af4ccbe2a3c74d","modified":1566023038902},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"8dd7ac5ccc52848b3d0fb0f102874c1a8c1c45b6","modified":1566023038900},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"1ba4cd71b826186b7f782bd06258f2ec52a2b491","modified":1566023038901},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"3e768faf37730891754f9817a01c04d754022017","modified":1566023038901},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"a72e33e11df9d3639cdebab4f480e397c70deacb","modified":1566023038901},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"3ed15de413b9ca83641bbd0b740f94b450630357","modified":1566023038903},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"38683b340b199ab204a43f3d2cd41e21f01ceb06","modified":1566023038903},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"e2549106ea26ac48019aa3f8023b8fdc78c0f6db","modified":1566023038904},{"_id":"themes/next/layout/_partials/post/wechat-subscriber.swig","hash":"2edfaf1579ea58e15cb5583fb7962dfee9af2257","modified":1566023038905},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"40483139657cde24356c6a4033a6386b909327a9","modified":1566023038904},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"25f40a74449615b27aa8877029f860315c3c9a36","modified":1566023038905},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"7bce91d429216724db5e337ec64cf7f05b022c97","modified":1566023038905},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"ecc31fb422d9ad85ac82f60bd138c313426917f5","modified":1566023038906},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"15b542f5b06b7532234af367340b9ed9fcebb0ac","modified":1566023038906},{"_id":"themes/next/layout/_partials/share/index.swig","hash":"609ce79f47756977b90bce365614d4f8d1020196","modified":1566023038907},{"_id":"themes/next/layout/_partials/share/likely.swig","hash":"c6a8687c73e4f16a092fdece76edc71cc27af42c","modified":1566023038907},{"_id":"themes/next/layout/_partials/share/needsharebutton.swig","hash":"75be9526c57ecee2351df0d7eca29488f4f56c92","modified":1566023038908},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"84a4201867c034afbf25862b63af4a582f7086b1","modified":1566023038908},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"5258d3dbfbb6d585f798c697463f8b67b7dea02b","modified":1566023038911},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"b117a00807d0622d4041fc4a17947ae5b8187667","modified":1566023038910},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"108b157fbd1ac3baaf19ae87234fa8728ab79556","modified":1566023038912},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"ffc8e8836714ea79abeb77b75859634615652877","modified":1566023038911},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1566023038912},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1566023038912},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-jquery.swig","hash":"b485797073495e24bf2f0b85880d513257bfed72","modified":1566023038913},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"7fd0459d2f1cfe1254f2c2b8087dc094c8cb0e84","modified":1566023038913},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1566023038913},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"67e63c25d509f02a6057ee9724f1b6efd647f72f","modified":1566023038914},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"3b9c50e3cbfa2c4d6cb0ffd443a994a212f3dbfc","modified":1566023038914},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"20fcbed44aede1b908bd17d7a0279b03639068b1","modified":1566023038915},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"ca94a27904c4aa61dda840fa1bbe0d4c6e0ae8f6","modified":1566023038915},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"fdcf006e1ba2e53eab65e901b6c63159538307ef","modified":1566023038916},{"_id":"themes/next/layout/_third-party/chat/index.swig","hash":"5ab4a8b4306c837d2f8c211a9d7fdaed76fa254d","modified":1566023038917},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1566023038917},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"051fadd4cb777769fc4c0fe3cb0b309e3236e5c2","modified":1566023038914},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"caaa90ee888abb0a2a82d6df320b7037dde1e406","modified":1566023038918},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"1130b8bd60f3b84397974486e6586d7f0afaf836","modified":1566023038918},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"df8413cba22deffca94cf4edd6d9f8d35d2bb58f","modified":1566023038919},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"261c4f1ab42f692279a310943b807ed34516df7c","modified":1566023038920},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"6f95bc4d7ffaddc3c0df0ef3eeeaac58ff83f560","modified":1566023038920},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"feba64b1246acc88461b9126bd22e2d75ee10ec9","modified":1566023038921},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"bc23c9eb9c06b0b377695f44373c14c5cde1d0e3","modified":1566023038922},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"6a05a8847cbd915029a511e995d5ee1ce571292c","modified":1566023038922},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"dff6835d0b03ecc2db6919ca27cfb7b63949e6a8","modified":1566023038922},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"ad7a22ee0f183481c7843ecd881b9f9e8e037e31","modified":1566023038923},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"b2fd17688f0433571b37704bc8df798e175e7811","modified":1566023038923},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1566023038924},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1566023038924},{"_id":"themes/next/layout/_third-party/tags/index.swig","hash":"83d0a8f71878c6b96a04ba08567b9064de6cce83","modified":1566023038925},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"a54308f934c33b12f6bf28e4ce90e517f38cab57","modified":1566023038925},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"7597800b6ce2f545baea797d95d1b8c7cf624299","modified":1566023038925},{"_id":"themes/next/scripts/events/lib/config.js","hash":"867b23bb231224064a79df5278ba0a02215044eb","modified":1566023038929},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1566023038929},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"251dcc22ed213da9b6135e1b958010e3352408d4","modified":1566023038929},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"f4aafd279e43a53ce9a72b8f455efa008f3f5177","modified":1566023038930},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"0803d4f4d3d02c24417c163ad0b27b60fda79250","modified":1566023038930},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"8f4643a844f90ca0ad853de1fe7c687c7569503d","modified":1566023038930},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"379e3673263d62e1f92ede09df1275a0419e4b51","modified":1566023038931},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"a9ebe8e80edb5e19ec4b845090d260ae9de1566a","modified":1566023038931},{"_id":"themes/next/scripts/filters/comment/facebook-comments-plugin.js","hash":"f7a8a0fc86ed2656de4fc61444c4a45d0c8d47f0","modified":1566023038931},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"4505867a0598b83583f9740ecba3e4f053d18ebe","modified":1566023038931},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"71ee48847a1e54962640a4b64298ecc3e76565f8","modified":1566023038932},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"85502e9f2499f3ee02ff9d1afa368db578cdf14a","modified":1566023038932},{"_id":"themes/next/scripts/filters/comment/vkontakte.js","hash":"2aa608a4bc664e87e4e56b016a28dc7d798d9966","modified":1566023038932},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"2315dd8a7a2c7aabd29efa6193df08e805cb15fc","modified":1566023038965},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"2bb4c78b5da482751085beb73ab01ecd2dcbccda","modified":1566023038966},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"b82c5b13ca6d1a6424dcc1245fb7722cac090579","modified":1566023038966},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"451f1f904df8b39015ccff5ce804c047f132b1f4","modified":1566023038971},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"4c101c21b4f2712e47dba869ccc116b245fb0909","modified":1566023038971},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"db52bce619bb848189e83b7bfabd1b2a06fd5578","modified":1566023038971},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ea5adf4e5a9ba6dd07a74613f4b62f726fc639b7","modified":1566023038972},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1566023038978},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"4438abc724321ddc7f24500aedce4c02899726f3","modified":1566023038976},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1566023038977},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1566023038978},{"_id":"themes/next/source/js/schemes/muse.js","hash":"240e9b61244556884ac13c863eedba776276f98f","modified":1566023038976},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1566023038978},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1566023038978},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1566023038985},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1566023038985},{"_id":"source/posts_res/2017-12-12-latex/23.png","hash":"659b30749c4528f7e9e7a035c78fd6ba7c5d2dcc","modified":1566015427320},{"_id":"source/posts_res/2018-03-28-svm/3-variables.png","hash":"fffda9ec3f77b245af223e30701761f6c5fea7ec","modified":1566015427350},{"_id":"source/posts_res/2018-07-08-doc2vectutorial/Distributed representations of sentences and documents.pdf","hash":"36738c5b87c62c52b6724f3129a1d5ef3e026f04","modified":1566015427366},{"_id":"themes/next/source/lib/jquery/index.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1566023038984},{"_id":"source/posts_res/2017-12-12-latex/13.png","hash":"e7238601d1b4c6934cc7f7f530fad364b613e844","modified":1566015427315},{"_id":"source/posts_res/2017-12-12-latex/11-2.png","hash":"44a13036830b99b1ab1adee7b5a856d003f206ce","modified":1566015427309},{"_id":"source/posts_res/2017-12-12-latex/24.png","hash":"9eddc2ea51f589d810c41eb299b6ade570be11dd","modified":1566015427321},{"_id":"source/posts_res/2018-05-03-word2vec/source-archive.zip","hash":"a9e2b202dc34816f4b322ad3d760f82aec608025","modified":1566015427340},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"2b14d2a3f814c9dbd92c00835878925f9f10ca28","modified":1566023038941},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"58a80925a845d590d4a9736ae92a6b03dc7c575f","modified":1566023038940},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"1c178041435de8076a1ce31304184007a837fcbb","modified":1566023038940},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"6c61a120d61dec145c8db3a244cae0e724f6f3ef","modified":1566023038941},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"dc658f44f4cda38684a584c95a6823e96c970a6d","modified":1566023038941},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"390993b47942a47d23ed8904c92e0186b6e9af29","modified":1566023038947},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"0dd11537a4b9a4d2faf3ad5cdae0f0f770a99ccf","modified":1566023038953},{"_id":"themes/next/source/css/_common/components/scrollbar.styl","hash":"202ee1f1f8e64d8292df16f40ee176ff4e5d7e71","modified":1566023038959},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"4bec027d48ebb95acd5f5786456ee1eb19a23ee4","modified":1566023038952},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"bdf464ee7972a007949acbe555d82e57621b3714","modified":1566023038963},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"442f7598991b92ec8bc3ce5343e18730da92e4fb","modified":1566023038964},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"4c4986b704c3cc3167c3c2a69c454446cf152528","modified":1566023038964},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"c3c0b1dcc29c9d4868573424cc2d975faf625da8","modified":1566023038964},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"e9e3bcef3f9a3541a11e5f3edfed2a3d13d22b80","modified":1566023038965},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"173fe462331ba12d612f7ade8f18c0f9e259fa5d","modified":1566023038965},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"4f7140cc0db08b47a25c4c7aad71047d433051a1","modified":1566023038964},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"65328fbc073901d78729463e46cb28a4c37a6fac","modified":1566023038966},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"71d75fa309cf5b96d876da536f57c047e57afcff","modified":1566023038966},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"87d19a45db10689aae9a09736efd1eb896a35b3d","modified":1566023038967},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"3462ffd270f3e218e0a8362262466e79d731f975","modified":1566023038967},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"f87b7a7d30e9111f7af269dcf7bd8c1893343fe8","modified":1566023038967},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"d46c61153bdba976ea785431d5d4b2c2a47c39b2","modified":1566023038967},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"49c6c656ed6e7348f967c01f8302fe1f6a5e3cd9","modified":1566023038967},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"54c5e23ec3dcbbc77848ededba76e861cf0f4719","modified":1566023038968},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"bdc0ada1eec3c1d625bfe2c148cdfc1b3460fa32","modified":1566023038968},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"0138165021be463f19e092f8972ac41aea3fb68a","modified":1566023038968},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"45f03b3bb80e32fd5648ce723055d7552b87d97c","modified":1566023038968},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"2fa573d7c04b8fa4717fc50265d7e4273f4325ea","modified":1566023038970},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"cbeb23488b707418a60b59ef8d6abc4f0d671959","modified":1566023038970},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"2e0681fa265a55f0259969109a3b9e3788bf516c","modified":1566023038970},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"7fad214ad640f9cd35fd3357e3bab03bbe45a2da","modified":1566023038970},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"4b976940b728a78428e88f40acf9a801223af88a","modified":1566023038970},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"10b4794ba62726fc052c74d786d962ed79d0780b","modified":1566023038971},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1566023038979},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1566023038979},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1566023038980},{"_id":"source/posts_res/2017-12-12-latex/22.png","hash":"ecbfc2226005a1261f631d7c61a8a5ea7c98624d","modified":1566015427319},{"_id":"source/posts_res/2017-12-12-latex/11-1.png","hash":"941944877735f48f7fcb77f46cdf54f3a590bb33","modified":1566015427309},{"_id":"source/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip","hash":"82e0d464bee3633a7a472c7317c56d23527cc1f8","modified":1566015427329},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1566023038984},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1566023038983},{"_id":"source/posts_res/2017-12-12-latex/21.png","hash":"9a3f6fb214aecbc682d4383a6857030428ccbf53","modified":1566015427317},{"_id":"source/posts_res/2018-03-28-svm/2-gaussian.png","hash":"14e026a3a8fe77511ec1179d7c956ff8409170fe","modified":1566015427351},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"0442cccb1111eec0d38357fba810319d781b7ef9","modified":1566023038942},{"_id":"themes/next/source/css/_common/components/header/github-banner.styl","hash":"172a83db23cb3f8a95e7cd6046befbc841caa869","modified":1566023038943},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"4c7140804d1ca65396959a3e8f8b79b9c4d3233f","modified":1566023038943},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1566023038943},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"cf37dc0ef20a8423a28b056458e1fa609875789a","modified":1566023038944},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"dada73ae88f01fab2ab4d2c83eddfc558de2110a","modified":1566023038944},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"080f22922d89ca714a2463d6eaedfd4cb68e8aef","modified":1566023038944},{"_id":"themes/next/source/css/_common/components/highlight/copy-code.styl","hash":"e13f52684dab8add24e90f1373ac8af578671c65","modified":1566023038944},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1566023038945},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"65fa4eb4aaf254be86b407f616ffc7340de9fedd","modified":1566023038945},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"a67dc85f6d158dac20e951ab55988ff7e9e1739b","modified":1566023038945},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"1f217159862b12a89350e11b5097a8f3031af28f","modified":1566023038946},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"d416014be2accb53931ac0b4c60a0c5e3cf2d180","modified":1566023038946},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1566023038946},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"621dc55fe935afe68a19f67b7447524292bc7e56","modified":1566023038946},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"207019d9e4911060dc23f01f821727fe0fe4717c","modified":1566023038947},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"61ca40856e5cacd48e0fa9728fde4605c7dd4c94","modified":1566023038947},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1566023038948},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"83afee4c02f63642267a4d1487d028dc83c7739c","modified":1566023038948},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"7cee1dc30f323e9f46e1fd4cf61c1a3d4cebb8b0","modified":1566023038948},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"f96b241dea210e494b9173be76cce65faee7157a","modified":1566023038949},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"bb892e5934a3b525afd1e578db991b747ea4c1eb","modified":1566023038949},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"8b0dde1df553a5a2f5e0765e4e4ce48e9ab2c90e","modified":1566023038949},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"c60afb603d58cad3ad13e1a16c66832538510063","modified":1566023038950},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"fdebe803d61e781883453d6301b7e2bcf5a8ec05","modified":1566023038950},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"2e2a09dddd2a394a635bcefb6207b6cddcb784c7","modified":1566023038950},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1566023038950},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"70d13702896e03013f694f89d5a68adbf0fff4af","modified":1566023038951},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"7581c1a77831ac128798e8c293923be763dea68c","modified":1566023038951},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"dcf801bd2478e125dffc0e23c0e720d048cf580f","modified":1566023038951},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"1fa8ab5d7692228302ff0c094edaa8d4f476926f","modified":1566023038952},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2ef546b4cfa5038ebb050c592c5135bbeb51af41","modified":1566023038959},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"9b3ff4aa24069eab0e9771437013f45e450d4217","modified":1566023038959},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"cd7ab38366a73edf3e7576d2629339660080b6f6","modified":1566023038959},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"8fc3facb9e9d4dae2578d0a271648d997a7f9bf3","modified":1566023038960},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"b2eca1259b9df8df81c5768bbb9b7ef65826e360","modified":1566023038960},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-button.styl","hash":"26226247288e1b2e8eca7af9caa7086b28578a42","modified":1566023038960},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"92381084e1aaf6798f058668aacef15bf1721739","modified":1566023038961},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"f3108a247549fbdf11071d023add3f1b34ad2b23","modified":1566023038960},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"0527bc64708a9446dc7bbb2c7e39ab3a61251c26","modified":1566023038961},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"bfa93a958115ddd562cd10f74b2eb887b6d3f187","modified":1566023038960},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"78d309e22206a4addda3c393282518dae02250ed","modified":1566023038961},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"02e5a998748a67fa885fb06fae242bcf3f2786fb","modified":1566023038961},{"_id":"themes/next/source/css/_common/components/tags/pdf.styl","hash":"2dc2a5b7becb11de1d4bdab6b5195588ae878cfc","modified":1566023038962},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"5d63f7479ee0f645e7538c60e827036fad74766d","modified":1566023038961},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1566023038962},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"278c31ed96474cd8a2ba465ed4131327e9d598bd","modified":1566023038962},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"ef66c0a08e4243a25e41408d70ca66682b8dcea1","modified":1566023038963},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"f211554f09a9005fad30b3e8f031be7f3fb7016a","modified":1566023038962},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1566023038962},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"c2fc1a5105d6691d680f7b77b9301db416ebac8b","modified":1566023038963},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"85b8606763f4177d2f9a11c1909a21ab1ae2011c","modified":1566023038963},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"8d840bea4c88f5f38293f956c15e0ba9e541982f","modified":1566023038963},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"e475eecfc7922c514731604f09980e8b016e32c9","modified":1566023038963},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"dd55feb43312489b5ec5692cc9d83ad150d7b389","modified":1566023038968},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"a29de6af7503a854e962eca8a3b8b911525ef2e5","modified":1566023038969},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/_sidebar.styl","hash":"1c3371551dadb86a4bf662c1adaa0dc2d2654ff7","modified":1566023038969},{"_id":"source/posts_res/2018-04-28-emalgorithm/2.png","hash":"8607589d4e3954af40825a9a183b8db0b02dba11","modified":1566015427324},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1566023038982},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/GBDT详解上.pdf","hash":"fde487dc0958fb5b5a5fa55c98c4bf5eba185de6","modified":1566015427374},{"_id":"source/posts_res/2017-12-12-latex/20.png","hash":"d8471bec7a1a922f5eabedab70a1ebc279d399f8","modified":1566015427318},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/Greedy Function Approximation_A Gradient Boosting Machine.pdf","hash":"12840952f0e3ca2f354bf038b26ca69c29228750","modified":1566015427371},{"_id":"source/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf","hash":"ccc071559dc55848652b203367c42795e1a7aeaa","modified":1566015427285},{"_id":"source/posts_res/2018-07-23-machinelearningcompare/abstract.png","hash":"19fe8f2aa34aaa0f18d9c1b22e13c494caf324b8","modified":1566015427359},{"_id":"source/posts_res/2018-08-08-resume/my_resume.pdf","hash":"9109b2e097409c918ca79d30ecb2669505d362c0","modified":1566015427403},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/XGBoost_A Scalable Tree Boosting System.pdf","hash":"f6f3ff590d248137835607bf454a00b5fdc296bd","modified":1566015427373},{"_id":"source/posts_res/2018-07-13-LINE/LINE Large-scale Information Network Embedding.pdf","hash":"25be77d69f0f692d8822a345c95b9c46e4bdb594","modified":1566015427365},{"_id":"source/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png","hash":"a1cd0a9c00e295a362367d3b959953faab59c23b","modified":1566015427377},{"_id":"source/posts_res/2018-03-28-svm/支持向量机通俗导论理解SVM的三层境界LaTeX最新版_2015.1.9.pdf","hash":"ab94233f72464f20ee710adef60814e37dbbf8ae","modified":1566015427357},{"_id":"source/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf","hash":"5c14c70c8315f7dbb4220a6ce55853d0d05530fa","modified":1566015427338},{"_id":"source/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf","hash":"22912d865bf1c131ba29bad2a59d246d39b97f7d","modified":1566015427347}],"Category":[{"name":"小工具","_id":"cjzl0wnud00022qwpwagiq2f4"},{"name":"LeetCode","_id":"cjzl0wnur000b2qwpq399l8q1"},{"name":"基础知识","_id":"cjzl0wnux000i2qwp730c52u9"},{"name":"机器学习","_id":"cjzl0wnve00162qwp4ogz8dhm"},{"name":"个人小结","_id":"cjzl0wnvk001f2qwprvdcqybu"},{"name":"深度学习","_id":"cjzl0wnw4002h2qwprp6e6wrn"},{"name":"推荐系统HandBook读书笔记","_id":"cjzl0wnww00432qwpc3rw9le4"},{"name":"大数据","_id":"cjzl0wnxf004t2qwpjdhdydxb"}],"Data":[],"Page":[{"title":"标签","date":"2018-11-25T05:11:27.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2018-11-25 13:11:27\ntype: \"tags\"\ncomments: false\n---\n","updated":"2019-08-18T03:04:56.229Z","path":"tags/index.html","layout":"page","_id":"cjzl0wo0n005y2qwpbt30c4t2","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"分类","date":"2018-11-25T05:12:00.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2018-11-25 13:12:00\ntype: \"categories\"\ncomments: false\n---\n","updated":"2019-08-18T03:04:36.126Z","path":"categories/index.html","layout":"page","_id":"cjzl0wo0o00602qwpzu87fe2m","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"layout":"post","title":"Jekyll With Mathjax","date":"2017-11-12T02:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>设置Jekyll和Github上的Mathjax</center>\n\n如何做呢？\n\n很简单。\n\n* 1.到你的Jekyll目录下，默认路径为`username.github.io/_includes/head.html`\n\n* 2.将下列代码复制粘贴到里面（**注意要放到<head\\> ... </head\\>之间**）\n\n```js\n<head>\n    ...\n    <script type=\"text/x-mathjax-config\"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"all\" } } }); </script>\n    <script type=\"text/x-mathjax-config\">\n        MathJax.Hub.Config({\n            tex2jax: {\n                inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n                processEscapes: true\n            }\n        });\n        </script>\n    <script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n</head>\n```\n\n\n* 3.完成！\n\n* 4.测试一下吧！\n\n$$ E = m\\cdot c^2 \\label{eq:mc2}$$\n\n\nThanks\n> [csega](http://csega.github.io/mypost/2017/03/28/how-to-set-up-mathjax-on-jekyll-and-github-properly.html)","source":"_posts/2017-11-12-jekywithmathjax.md","raw":"---\nlayout: post\ntitle: Jekyll With Mathjax\ndate: 2017-11-12 10:10 +0800\ncategories: 小工具\nmathjax: true\ncopyright: true\n---\n\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>设置Jekyll和Github上的Mathjax</center>\n\n如何做呢？\n\n很简单。\n\n* 1.到你的Jekyll目录下，默认路径为`username.github.io/_includes/head.html`\n\n* 2.将下列代码复制粘贴到里面（**注意要放到<head\\> ... </head\\>之间**）\n\n```js\n<head>\n    ...\n    <script type=\"text/x-mathjax-config\"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"all\" } } }); </script>\n    <script type=\"text/x-mathjax-config\">\n        MathJax.Hub.Config({\n            tex2jax: {\n                inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n                processEscapes: true\n            }\n        });\n        </script>\n    <script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n</head>\n```\n\n\n* 3.完成！\n\n* 4.测试一下吧！\n\n$$ E = m\\cdot c^2 \\label{eq:mc2}$$\n\n\nThanks\n> [csega](http://csega.github.io/mypost/2017/03/28/how-to-set-up-mathjax-on-jekyll-and-github-properly.html)","slug":"jekywithmathjax","published":1,"updated":"2019-08-17T09:28:03.583Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnu500002qwpxgi9kejv","content":"<h2 id=\"设置Jekyll和Github上的Mathjax\"><a href=\"#设置Jekyll和Github上的Mathjax\" class=\"headerlink\" title=\"设置Jekyll和Github上的Mathjax\"></a><center>设置Jekyll和Github上的Mathjax</center></h2><p>如何做呢？</p><p>很简单。</p><ul>\n<li><p>1.到你的Jekyll目录下，默认路径为<code>username.github.io/_includes/head.html</code></p>\n</li>\n<li><p>2.将下列代码复制粘贴到里面（<strong>注意要放到<head\\> … &lt;/head>之间</head\\></strong>）</p>\n</li>\n</ul><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &lt;script type=<span class=\"string\">\"text/x-mathjax-config\"</span>&gt; MathJax.Hub.Config(&#123; <span class=\"attr\">TeX</span>: &#123; <span class=\"attr\">equationNumbers</span>: &#123; <span class=\"attr\">autoNumber</span>: <span class=\"string\">\"all\"</span> &#125; &#125; &#125;); &lt;/script&gt;</span><br><span class=\"line\">    &lt;script type=<span class=\"string\">\"text/x-mathjax-config\"</span>&gt;</span><br><span class=\"line\">        MathJax.Hub.Config(&#123;</span><br><span class=\"line\">            tex2jax: &#123;</span><br><span class=\"line\">                inlineMath: [ [<span class=\"string\">'$'</span>,<span class=\"string\">'$'</span>], [<span class=\"string\">\"\\\\(\"</span>,<span class=\"string\">\"\\\\)\"</span>] ],</span><br><span class=\"line\">                processEscapes: <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        &lt;<span class=\"regexp\">/script&gt;</span></span><br><span class=\"line\"><span class=\"regexp\">    &lt;script src=\"https:/</span><span class=\"regexp\">/cdn.mathjax.org/m</span>athjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML<span class=\"string\">\" type=\"</span>text/javascript<span class=\"string\">\"&gt;&lt;/script&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;/head&gt;</span></span><br></pre></td></tr></table></figure><a id=\"more\"></a><!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n\n\n\n<ul>\n<li><p>3.完成！</p>\n</li>\n<li><p>4.测试一下吧！</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">E = m\\cdot c^2 \\label{eq:mc2}</script><p>Thanks</p>\n<blockquote>\n<p><a href=\"http://csega.github.io/mypost/2017/03/28/how-to-set-up-mathjax-on-jekyll-and-github-properly.html\" target=\"_blank\" rel=\"noopener\">csega</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"设置Jekyll和Github上的Mathjax\"><a href=\"#设置Jekyll和Github上的Mathjax\" class=\"headerlink\" title=\"设置Jekyll和Github上的Mathjax\"></a><center>设置Jekyll和Github上的Mathjax</center></h2><p>如何做呢？</p><p>很简单。</p><ul>\n<li><p>1.到你的Jekyll目录下，默认路径为<code>username.github.io/_includes/head.html</code></p>\n</li>\n<li><p>2.将下列代码复制粘贴到里面（<strong>注意要放到<head\\> … &lt;/head>之间</head\\></strong>）</p>\n</li>\n</ul><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;head&gt;</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    &lt;script type=<span class=\"string\">\"text/x-mathjax-config\"</span>&gt; MathJax.Hub.Config(&#123; <span class=\"attr\">TeX</span>: &#123; <span class=\"attr\">equationNumbers</span>: &#123; <span class=\"attr\">autoNumber</span>: <span class=\"string\">\"all\"</span> &#125; &#125; &#125;); &lt;/script&gt;</span><br><span class=\"line\">    &lt;script type=<span class=\"string\">\"text/x-mathjax-config\"</span>&gt;</span><br><span class=\"line\">        MathJax.Hub.Config(&#123;</span><br><span class=\"line\">            tex2jax: &#123;</span><br><span class=\"line\">                inlineMath: [ [<span class=\"string\">'$'</span>,<span class=\"string\">'$'</span>], [<span class=\"string\">\"\\\\(\"</span>,<span class=\"string\">\"\\\\)\"</span>] ],</span><br><span class=\"line\">                processEscapes: <span class=\"literal\">true</span></span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        &lt;<span class=\"regexp\">/script&gt;</span></span><br><span class=\"line\"><span class=\"regexp\">    &lt;script src=\"https:/</span><span class=\"regexp\">/cdn.mathjax.org/m</span>athjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML<span class=\"string\">\" type=\"</span>text/javascript<span class=\"string\">\"&gt;&lt;/script&gt;</span></span><br><span class=\"line\"><span class=\"string\">&lt;/head&gt;</span></span><br></pre></td></tr></table></figure>","more":"<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n\n\n\n<ul>\n<li><p>3.完成！</p>\n</li>\n<li><p>4.测试一下吧！</p>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">E = m\\cdot c^2 \\label{eq:mc2}</script><p>Thanks</p>\n<blockquote>\n<p><a href=\"http://csega.github.io/mypost/2017/03/28/how-to-set-up-mathjax-on-jekyll-and-github-properly.html\" target=\"_blank\" rel=\"noopener\">csega</a></p>\n</blockquote>\n"},{"layout":"post","title":"Anaconda解决Python2和Python3共存","date":"2017-11-13T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>Anaconda解决Python2和Python3共存</center>\n\n\n----------------------------\n\n### 下载Anaconda\n\n直接从官网下载即可，[https://www.anaconda.com/download/](https://www.anaconda.com/download/)。建议选择Python3版本的Anaconda进行下载，下载完成安装即可。\n\nconda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令\n\n\n-------------------------------\n\n#### 多版本切换\n\n    # 基于 python3.6 创建一个名为env_py3 的环境\n    conda create --name env_py3 python=3.6 \n    \n    # 基于 python2.7 创建一个名为env_py2 的环境\n    conda create --name env_py2 python=2.7\n    \n    # 激活 env 环境\n    activate env_py2  # windows\n    source activate env_py2 # linux/mac\n    \n    # 退出 env 环境\n    deactivate  # windows\n    source deactivate # linux/mac\n\n\n\n\n\n\n\n\n\n","source":"_posts/2017-11-13-anaconda.md","raw":"---\nlayout: post\ntitle: Anaconda解决Python2和Python3共存\ndate: 2017-11-13 21:10 +0800\ncategories: 小工具\nmathjax: true\ncopyright: true\n---\n\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>Anaconda解决Python2和Python3共存</center>\n\n\n----------------------------\n\n### 下载Anaconda\n\n直接从官网下载即可，[https://www.anaconda.com/download/](https://www.anaconda.com/download/)。建议选择Python3版本的Anaconda进行下载，下载完成安装即可。\n\nconda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令\n\n\n-------------------------------\n\n#### 多版本切换\n\n    # 基于 python3.6 创建一个名为env_py3 的环境\n    conda create --name env_py3 python=3.6 \n    \n    # 基于 python2.7 创建一个名为env_py2 的环境\n    conda create --name env_py2 python=2.7\n    \n    # 激活 env 环境\n    activate env_py2  # windows\n    source activate env_py2 # linux/mac\n    \n    # 退出 env 环境\n    deactivate  # windows\n    source deactivate # linux/mac\n\n\n\n\n\n\n\n\n\n","slug":"anaconda","published":1,"updated":"2019-08-17T09:28:26.632Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnua00012qwprgvez3qi","content":"<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n<h2 id=\"Anaconda解决Python2和Python3共存\"><a href=\"#Anaconda解决Python2和Python3共存\" class=\"headerlink\" title=\"Anaconda解决Python2和Python3共存\"></a><center>Anaconda解决Python2和Python3共存</center></h2><hr>\n<h3 id=\"下载Anaconda\"><a href=\"#下载Anaconda\" class=\"headerlink\" title=\"下载Anaconda\"></a>下载Anaconda</h3><p>直接从官网下载即可，<a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">https://www.anaconda.com/download/</a>。建议选择Python3版本的Anaconda进行下载，下载完成安装即可。</p>\n<p>conda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令</p>\n<hr>\n<h4 id=\"多版本切换\"><a href=\"#多版本切换\" class=\"headerlink\" title=\"多版本切换\"></a>多版本切换</h4><pre><code># 基于 python3.6 创建一个名为env_py3 的环境\nconda create --name env_py3 python=3.6 \n\n# 基于 python2.7 创建一个名为env_py2 的环境\nconda create --name env_py2 python=2.7\n\n# 激活 env 环境\nactivate env_py2  # windows\nsource activate env_py2 # linux/mac\n\n# 退出 env 环境\ndeactivate  # windows\nsource deactivate # linux/mac\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n<h2 id=\"Anaconda解决Python2和Python3共存\"><a href=\"#Anaconda解决Python2和Python3共存\" class=\"headerlink\" title=\"Anaconda解决Python2和Python3共存\"></a><center>Anaconda解决Python2和Python3共存</center></h2><hr>\n<h3 id=\"下载Anaconda\"><a href=\"#下载Anaconda\" class=\"headerlink\" title=\"下载Anaconda\"></a>下载Anaconda</h3><p>直接从官网下载即可，<a href=\"https://www.anaconda.com/download/\" target=\"_blank\" rel=\"noopener\">https://www.anaconda.com/download/</a>。建议选择Python3版本的Anaconda进行下载，下载完成安装即可。</p>\n<p>conda 是 Anaconda 下用于包管理和环境管理的命令行工具，是 pip 和 vitualenv 的组合。安装成功后 conda 会默认加入到环境变量中，因此可直接在命令行窗口运行 conda 命令</p>\n<hr>\n<h4 id=\"多版本切换\"><a href=\"#多版本切换\" class=\"headerlink\" title=\"多版本切换\"></a>多版本切换</h4><pre><code># 基于 python3.6 创建一个名为env_py3 的环境\nconda create --name env_py3 python=3.6 \n\n# 基于 python2.7 创建一个名为env_py2 的环境\nconda create --name env_py2 python=2.7\n\n# 激活 env 环境\nactivate env_py2  # windows\nsource activate env_py2 # linux/mac\n\n# 退出 env 环境\ndeactivate  # windows\nsource deactivate # linux/mac\n</code></pre>"},{"layout":"post","title":"快速排序 - QSort","date":"2017-12-21T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 快速排序 - QSort </center>\n\n-------\n\n### 第一种Partition的方法\n\n```cpp\nint Partition(int* data, int length, int start, int end)\n{\n\tif (data == nullptr || length <= 0 || start < 0 || end >= length)\n\t\treturn -1;\n\n\tint small = start - 1;\n\tfor (int index = start; index < end; index++)\n\t{\n\t\tif (data[index] < data[end])\n\t\t{\n\t\t\tsmall++;\n\t\t\tif (small != index)\n\t\t\t\tswap(data[index], data[small]);\n\t\t}\n\t}\n\tsmall++;\n\tswap(data[small], data[end]);\n\treturn small;\n}\nvoid Qsort(int* data, int length, int start, int end)\n{\n\tif (start == end)\n\t\treturn;\n\tint index = Partition(data, length, start, end);\n\tif (index > start)\n\t\tQsort(data, length, start, index - 1);\n\tif (index < end)\n\t\tQsort(data, length, index + 1, end);\n}\n```\n\n\n-------\n\n### 第二种Partition的方法(更容易理解)\n\n```cpp\nint myPartition(int* data, int length, int start, int end)\n{\n\tint left = start, right = end - 1;\n\twhile (left < right)\n\t{\n\t\twhile (data[left] < data[end] && left < right)\n\t\t\tleft++;\n\t\twhile (data[right] >= data[end] && left < right)\n\t\t\tright--;\n\t\tswap(data[left], data[right]);\n\t}\n\tif (data[left] >= data[end])\n\t\tswap(data[left], data[end]);\n\telse\n\t\tswap(data[++left], data[end]);\n\treturn left;\n}\nvoid myQsort(int* data, int length, int start, int end)\n{\n\tif (data == nullptr || length <= 0 || start >= end)\n\t\treturn;\n\tint index = myPartition(data, length, start, end);\n\tmyQsort(data, length, start, index - 1);\n\tmyQsort(data, length, index + 1, end);\n}\n```\n\n\n\n","source":"_posts/2017-12-21-qsort.md","raw":"---\nlayout: post\ntitle: 快速排序 - QSort\ndate: 2017-12-21 21:10 +0800\ncategories: LeetCode\ntags:\n- 排序\nmathjax: true\ncopyright: true\n---\n\n## <center> 快速排序 - QSort </center>\n\n-------\n\n### 第一种Partition的方法\n\n```cpp\nint Partition(int* data, int length, int start, int end)\n{\n\tif (data == nullptr || length <= 0 || start < 0 || end >= length)\n\t\treturn -1;\n\n\tint small = start - 1;\n\tfor (int index = start; index < end; index++)\n\t{\n\t\tif (data[index] < data[end])\n\t\t{\n\t\t\tsmall++;\n\t\t\tif (small != index)\n\t\t\t\tswap(data[index], data[small]);\n\t\t}\n\t}\n\tsmall++;\n\tswap(data[small], data[end]);\n\treturn small;\n}\nvoid Qsort(int* data, int length, int start, int end)\n{\n\tif (start == end)\n\t\treturn;\n\tint index = Partition(data, length, start, end);\n\tif (index > start)\n\t\tQsort(data, length, start, index - 1);\n\tif (index < end)\n\t\tQsort(data, length, index + 1, end);\n}\n```\n\n\n-------\n\n### 第二种Partition的方法(更容易理解)\n\n```cpp\nint myPartition(int* data, int length, int start, int end)\n{\n\tint left = start, right = end - 1;\n\twhile (left < right)\n\t{\n\t\twhile (data[left] < data[end] && left < right)\n\t\t\tleft++;\n\t\twhile (data[right] >= data[end] && left < right)\n\t\t\tright--;\n\t\tswap(data[left], data[right]);\n\t}\n\tif (data[left] >= data[end])\n\t\tswap(data[left], data[end]);\n\telse\n\t\tswap(data[++left], data[end]);\n\treturn left;\n}\nvoid myQsort(int* data, int length, int start, int end)\n{\n\tif (data == nullptr || length <= 0 || start >= end)\n\t\treturn;\n\tint index = myPartition(data, length, start, end);\n\tmyQsort(data, length, start, index - 1);\n\tmyQsort(data, length, index + 1, end);\n}\n```\n\n\n\n","slug":"qsort","published":1,"updated":"2019-08-17T09:29:52.777Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnug00032qwphbgl4208","content":"<h2 id=\"快速排序-QSort\"><a href=\"#快速排序-QSort\" class=\"headerlink\" title=\" 快速排序 - QSort \"></a><center> 快速排序 - QSort </center></h2><hr><h3 id=\"第一种Partition的方法\"><a href=\"#第一种Partition的方法\" class=\"headerlink\" title=\"第一种Partition的方法\"></a>第一种Partition的方法</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">Partition</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data == <span class=\"literal\">nullptr</span> || length &lt;= <span class=\"number\">0</span> || start &lt; <span class=\"number\">0</span> || end &gt;= length)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> small = start - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> index = start; index &lt; end; index++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (data[index] &lt; data[end])</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tsmall++;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (small != index)</span><br><span class=\"line\">\t\t\t\tswap(data[index], data[small]);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tsmall++;</span><br><span class=\"line\">\tswap(data[small], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> small;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">Qsort</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (start == end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> index = Partition(data, length, start, end);</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (index &gt; start)</span><br><span class=\"line\">\t\tQsort(data, length, start, index - <span class=\"number\">1</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (index &lt; end)</span><br><span class=\"line\">\t\tQsort(data, length, index + <span class=\"number\">1</span>, end);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><a id=\"more\"></a>\n\n<hr>\n<h3 id=\"第二种Partition的方法-更容易理解\"><a href=\"#第二种Partition的方法-更容易理解\" class=\"headerlink\" title=\"第二种Partition的方法(更容易理解)\"></a>第二种Partition的方法(更容易理解)</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">myPartition</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> left = start, right = end - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (left &lt; right)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (data[left] &lt; data[end] &amp;&amp; left &lt; right)</span><br><span class=\"line\">\t\t\tleft++;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (data[right] &gt;= data[end] &amp;&amp; left &lt; right)</span><br><span class=\"line\">\t\t\tright--;</span><br><span class=\"line\">\t\tswap(data[left], data[right]);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data[left] &gt;= data[end])</span><br><span class=\"line\">\t\tswap(data[left], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\tswap(data[++left], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> left;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">myQsort</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data == <span class=\"literal\">nullptr</span> || length &lt;= <span class=\"number\">0</span> || start &gt;= end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> index = myPartition(data, length, start, end);</span><br><span class=\"line\">\tmyQsort(data, length, start, index - <span class=\"number\">1</span>);</span><br><span class=\"line\">\tmyQsort(data, length, index + <span class=\"number\">1</span>, end);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h2 id=\"快速排序-QSort\"><a href=\"#快速排序-QSort\" class=\"headerlink\" title=\" 快速排序 - QSort \"></a><center> 快速排序 - QSort </center></h2><hr><h3 id=\"第一种Partition的方法\"><a href=\"#第一种Partition的方法\" class=\"headerlink\" title=\"第一种Partition的方法\"></a>第一种Partition的方法</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">Partition</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data == <span class=\"literal\">nullptr</span> || length &lt;= <span class=\"number\">0</span> || start &lt; <span class=\"number\">0</span> || end &gt;= length)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> small = start - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> index = start; index &lt; end; index++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (data[index] &lt; data[end])</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tsmall++;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (small != index)</span><br><span class=\"line\">\t\t\t\tswap(data[index], data[small]);</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tsmall++;</span><br><span class=\"line\">\tswap(data[small], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> small;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">Qsort</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (start == end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> index = Partition(data, length, start, end);</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (index &gt; start)</span><br><span class=\"line\">\t\tQsort(data, length, start, index - <span class=\"number\">1</span>);</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (index &lt; end)</span><br><span class=\"line\">\t\tQsort(data, length, index + <span class=\"number\">1</span>, end);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"\n\n<hr>\n<h3 id=\"第二种Partition的方法-更容易理解\"><a href=\"#第二种Partition的方法-更容易理解\" class=\"headerlink\" title=\"第二种Partition的方法(更容易理解)\"></a>第二种Partition的方法(更容易理解)</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">myPartition</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> left = start, right = end - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (left &lt; right)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (data[left] &lt; data[end] &amp;&amp; left &lt; right)</span><br><span class=\"line\">\t\t\tleft++;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (data[right] &gt;= data[end] &amp;&amp; left &lt; right)</span><br><span class=\"line\">\t\t\tright--;</span><br><span class=\"line\">\t\tswap(data[left], data[right]);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data[left] &gt;= data[end])</span><br><span class=\"line\">\t\tswap(data[left], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\tswap(data[++left], data[end]);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> left;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">myQsort</span><span class=\"params\">(<span class=\"keyword\">int</span>* data, <span class=\"keyword\">int</span> length, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (data == <span class=\"literal\">nullptr</span> || length &lt;= <span class=\"number\">0</span> || start &gt;= end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> index = myPartition(data, length, start, end);</span><br><span class=\"line\">\tmyQsort(data, length, start, index - <span class=\"number\">1</span>);</span><br><span class=\"line\">\tmyQsort(data, length, index + <span class=\"number\">1</span>, end);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","title":"MarkDown语法","date":"2017-12-12T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n## 主要内容\n\n- Markdown*是什么*？\n- *谁*创造了它？\n- *为什么*要使用它？\n- *怎么*使用？\n- *谁*在用？\n\n## 正文\n### 1. Markdown*是什么*？\n**Markdown**是一种轻量级**标记语言**，它以纯文本形式(*易读、易写、易更改*)编写文档，并最终以HTML格式发布。    \n**Markdown**也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。    \n\n### 2. *为什么*要使用它？\n+ 它是易读（看起来舒服）、易写（语法简单）、易更改**纯文本**。处处体现着**极简主义**的影子。\n+ 兼容HTML，可以转换为HTML格式发布。\n+ 跨平台使用。\n+ 越来越多的网站支持Markdown。\n+ 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail）\n+ 摆脱Word（我不是认真的）。\n\n### 3. *怎么*使用？\n如果不算**扩展**，Markdown的语法绝对**简单**到让你爱不释手。\n\nMarkdown语法主要分为如下几大部分：\n**标题**，**段落**，**区块引用**，**代码区块**，**强调**，**列表**，**分割线**，**链接**，**图片**，**反斜杠 `\\`**，**符号'`'**。\n\n#### 3.1 标题\n两种形式：  \n1）使用`=`和`-`标记一级和二级标题。\n> 一级标题   \n> `=========`   \n> 二级标题    \n> `---------`\n\n效果：\n> 一级标题   \n> =========   \n> 二级标题\n> ---------  \n\n2）使用`#`，可表示1-6级标题。\n> \\# 一级标题   \n> \\## 二级标题   \n> \\### 三级标题   \n> \\#### 四级标题   \n> \\##### 五级标题   \n> \\###### 六级标题    \n\n效果：\n> # 一级标题   \n> ## 二级标题   \n> ### 三级标题   \n> #### 四级标题   \n> ##### 五级标题   \n> ###### 六级标题\n\n#### 3.2 段落\n段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用**两个以上**空格加上回车（引用中换行省略回车）。\n\n#### 3.3 区块引用\n在段落的每行或者只在第一行使用符号`>`,还可使用多个嵌套引用，如：\n> \\> 区块引用  \n> \\>> 嵌套引用  \n\n效果：\n> 区块引用  \n>> 嵌套引用\n\n#### 3.4 代码区块\n代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如    \n普通段落：\n\nvoid main()    \n{    \n    printf(\"Hello, Markdown.\");    \n}    \n\n代码区块：\n\n    void main()\n    {\n        printf(\"Hello, Markdown.\");\n    }\n\n**注意**:需要和普通段落之间存在空行。\n\n#### 3.5 强调\n在强调内容两侧分别加上`*`或者`_`，如：\n> \\*斜体\\*，\\_斜体\\_    \n> \\*\\*粗体\\*\\*，\\_\\_粗体\\_\\_\n\n效果：\n> *斜体*，_斜体_    \n> **粗体**，__粗体__\n\n#### 3.6 列表\n使用`·`、`+`、或`-`标记无序列表，如：\n> \\-（+\\*） 第一项\n> \\-（+\\*） 第二项\n> \\- （+\\*）第三项\n\n**注意**：标记后面最少有一个_空格_或_制表符_。若不在引用区块中，必须和前方段落之间存在空行。\n\n效果：\n> + 第一项\n> + 第二项\n> + 第三项\n\n有序列表的标记方式是将上述的符号换成数字,并辅以`.`，如：\n> 1 . 第一项   \n> 2 . 第二项    \n> 3 . 第三项    \n\n效果：\n> 1. 第一项\n> 2. 第二项\n> 3. 第三项\n\n#### 3.7 分割线\n分割线最常使用就是三个或以上`*`，还可以使用`-`和`_`。\n\n#### 3.8 链接\n链接可以由两种形式生成：**行内式**和**参考式**。    \n**行内式**：\n> \\[younghz的Markdown库\\]\\(https:://github.com/younghz/Markdown \"Markdown\"\\)。\n\n效果：\n> [younghz的Markdown库](https:://github.com/younghz/Markdown \"Markdown\")。\n\n**参考式**：\n> \\[younghz的Markdown库1\\]\\[1\\]    \n> \\[younghz的Markdown库2\\]\\[2\\]    \n> \\[1\\]:https:://github.com/younghz/Markdown \"Markdown\"    \n> \\[2\\]:https:://github.com/younghz/Markdown \"Markdown\"    \n\n效果：\n> [younghz的Markdown库1][1]    \n> [younghz的Markdown库2][2]\n\n[1]: https:://github.com/younghz/Markdown \"Markdown\"\n[2]: https:://github.com/younghz/Markdown \"Markdown\"\n\n**注意**：上述的`[1]:https:://github.com/younghz/Markdown \"Markdown\"`不出现在区块中。\n\n#### 3.9 图片\n添加图片的形式和链接相似，只需在链接的基础上前方加一个`！`。\n#### 3.10 反斜杠`\\`\n相当于**反转义**作用。使符号成为普通符号。\n#### 3.11 符号'`'\n起到标记作用。如：\n>\\`ctrl+a\\`\n\n效果：\n>`ctrl+a`    \n\n#### 4. *谁*在用？\nMarkdown的使用者：\n+ GitHub\n+ 简书\n+ Stack Overflow\n+ Apollo\n+ Moodle\n+ Reddit\n+ 等等\n\n#### 5. 尝试一下\n+ **Chrome**下的插件诸如`stackedit`与`markdown-here`等非常方便，也不用担心平台受限。\n+ **在线**的dillinger.io评价也不错   \n+ **Windowns**下的MarkdownPad也用过，不过免费版的体验不是很好。    \n+ **Mac**下的Mou是国人贡献的，口碑很好。\n+ **Linux**下的ReText不错。    \n\n**当然，最终境界永远都是笔下是语法，心中格式化 :)。**\n\n****\n**注意**：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。\n虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber]\n(http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。\n****\n以上基本是所有traditonal markdown的语法。\n\n### 其它：\n列表的使用(非traditonal markdown)：\n\n用`|`表示表格纵向边界，表头和表内容用`-`隔开，并可用`:`进行对齐设置，两边都有`:`则表示居中，若不加`:`则默认左对齐。\n\n|代码库                              |链接                                |\n|:------------------------------------:|------------------------------------|\n|MarkDown                              |[https://github.com/younghz/Markdown](https://github.com/younghz/Markdown \"Markdown\")|\n|MarkDownCopy                              |[https://github.com/younghz/Markdown](https://github.com/younghz/Markdown \"Markdown\")|\n\n\n关于其它扩展语法可参见具体工具的使用说明。\n\n","source":"_posts/2017-12-12-markdown.md","raw":"---\nlayout: post\ntitle: MarkDown语法\ndate: 2017-12-12 12:10 +0800\ncategories: 基础知识\ntags:\n- MarkDown\nmathjax: true\ncopyright: true\n---\n\n\n## 主要内容\n\n- Markdown*是什么*？\n- *谁*创造了它？\n- *为什么*要使用它？\n- *怎么*使用？\n- *谁*在用？\n\n## 正文\n### 1. Markdown*是什么*？\n**Markdown**是一种轻量级**标记语言**，它以纯文本形式(*易读、易写、易更改*)编写文档，并最终以HTML格式发布。    \n**Markdown**也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。    \n\n### 2. *为什么*要使用它？\n+ 它是易读（看起来舒服）、易写（语法简单）、易更改**纯文本**。处处体现着**极简主义**的影子。\n+ 兼容HTML，可以转换为HTML格式发布。\n+ 跨平台使用。\n+ 越来越多的网站支持Markdown。\n+ 更方便清晰地组织你的电子邮件。（Markdown-here, Airmail）\n+ 摆脱Word（我不是认真的）。\n\n### 3. *怎么*使用？\n如果不算**扩展**，Markdown的语法绝对**简单**到让你爱不释手。\n\nMarkdown语法主要分为如下几大部分：\n**标题**，**段落**，**区块引用**，**代码区块**，**强调**，**列表**，**分割线**，**链接**，**图片**，**反斜杠 `\\`**，**符号'`'**。\n\n#### 3.1 标题\n两种形式：  \n1）使用`=`和`-`标记一级和二级标题。\n> 一级标题   \n> `=========`   \n> 二级标题    \n> `---------`\n\n效果：\n> 一级标题   \n> =========   \n> 二级标题\n> ---------  \n\n2）使用`#`，可表示1-6级标题。\n> \\# 一级标题   \n> \\## 二级标题   \n> \\### 三级标题   \n> \\#### 四级标题   \n> \\##### 五级标题   \n> \\###### 六级标题    \n\n效果：\n> # 一级标题   \n> ## 二级标题   \n> ### 三级标题   \n> #### 四级标题   \n> ##### 五级标题   \n> ###### 六级标题\n\n#### 3.2 段落\n段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用**两个以上**空格加上回车（引用中换行省略回车）。\n\n#### 3.3 区块引用\n在段落的每行或者只在第一行使用符号`>`,还可使用多个嵌套引用，如：\n> \\> 区块引用  \n> \\>> 嵌套引用  \n\n效果：\n> 区块引用  \n>> 嵌套引用\n\n#### 3.4 代码区块\n代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如    \n普通段落：\n\nvoid main()    \n{    \n    printf(\"Hello, Markdown.\");    \n}    \n\n代码区块：\n\n    void main()\n    {\n        printf(\"Hello, Markdown.\");\n    }\n\n**注意**:需要和普通段落之间存在空行。\n\n#### 3.5 强调\n在强调内容两侧分别加上`*`或者`_`，如：\n> \\*斜体\\*，\\_斜体\\_    \n> \\*\\*粗体\\*\\*，\\_\\_粗体\\_\\_\n\n效果：\n> *斜体*，_斜体_    \n> **粗体**，__粗体__\n\n#### 3.6 列表\n使用`·`、`+`、或`-`标记无序列表，如：\n> \\-（+\\*） 第一项\n> \\-（+\\*） 第二项\n> \\- （+\\*）第三项\n\n**注意**：标记后面最少有一个_空格_或_制表符_。若不在引用区块中，必须和前方段落之间存在空行。\n\n效果：\n> + 第一项\n> + 第二项\n> + 第三项\n\n有序列表的标记方式是将上述的符号换成数字,并辅以`.`，如：\n> 1 . 第一项   \n> 2 . 第二项    \n> 3 . 第三项    \n\n效果：\n> 1. 第一项\n> 2. 第二项\n> 3. 第三项\n\n#### 3.7 分割线\n分割线最常使用就是三个或以上`*`，还可以使用`-`和`_`。\n\n#### 3.8 链接\n链接可以由两种形式生成：**行内式**和**参考式**。    \n**行内式**：\n> \\[younghz的Markdown库\\]\\(https:://github.com/younghz/Markdown \"Markdown\"\\)。\n\n效果：\n> [younghz的Markdown库](https:://github.com/younghz/Markdown \"Markdown\")。\n\n**参考式**：\n> \\[younghz的Markdown库1\\]\\[1\\]    \n> \\[younghz的Markdown库2\\]\\[2\\]    \n> \\[1\\]:https:://github.com/younghz/Markdown \"Markdown\"    \n> \\[2\\]:https:://github.com/younghz/Markdown \"Markdown\"    \n\n效果：\n> [younghz的Markdown库1][1]    \n> [younghz的Markdown库2][2]\n\n[1]: https:://github.com/younghz/Markdown \"Markdown\"\n[2]: https:://github.com/younghz/Markdown \"Markdown\"\n\n**注意**：上述的`[1]:https:://github.com/younghz/Markdown \"Markdown\"`不出现在区块中。\n\n#### 3.9 图片\n添加图片的形式和链接相似，只需在链接的基础上前方加一个`！`。\n#### 3.10 反斜杠`\\`\n相当于**反转义**作用。使符号成为普通符号。\n#### 3.11 符号'`'\n起到标记作用。如：\n>\\`ctrl+a\\`\n\n效果：\n>`ctrl+a`    \n\n#### 4. *谁*在用？\nMarkdown的使用者：\n+ GitHub\n+ 简书\n+ Stack Overflow\n+ Apollo\n+ Moodle\n+ Reddit\n+ 等等\n\n#### 5. 尝试一下\n+ **Chrome**下的插件诸如`stackedit`与`markdown-here`等非常方便，也不用担心平台受限。\n+ **在线**的dillinger.io评价也不错   \n+ **Windowns**下的MarkdownPad也用过，不过免费版的体验不是很好。    \n+ **Mac**下的Mou是国人贡献的，口碑很好。\n+ **Linux**下的ReText不错。    \n\n**当然，最终境界永远都是笔下是语法，心中格式化 :)。**\n\n****\n**注意**：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。\n虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber]\n(http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。\n****\n以上基本是所有traditonal markdown的语法。\n\n### 其它：\n列表的使用(非traditonal markdown)：\n\n用`|`表示表格纵向边界，表头和表内容用`-`隔开，并可用`:`进行对齐设置，两边都有`:`则表示居中，若不加`:`则默认左对齐。\n\n|代码库                              |链接                                |\n|:------------------------------------:|------------------------------------|\n|MarkDown                              |[https://github.com/younghz/Markdown](https://github.com/younghz/Markdown \"Markdown\")|\n|MarkDownCopy                              |[https://github.com/younghz/Markdown](https://github.com/younghz/Markdown \"Markdown\")|\n\n\n关于其它扩展语法可参见具体工具的使用说明。\n\n","slug":"markdown","published":1,"updated":"2019-08-17T09:29:07.097Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnuh00042qwplgvaa7go","content":"<h2 id=\"主要内容\"><a href=\"#主要内容\" class=\"headerlink\" title=\"主要内容\"></a>主要内容</h2><ul>\n<li>Markdown<em>是什么</em>？</li>\n<li><em>谁</em>创造了它？</li>\n<li><em>为什么</em>要使用它？</li>\n<li><em>怎么</em>使用？</li>\n<li><em>谁</em>在用？</li>\n</ul><h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-Markdown是什么？\"><a href=\"#1-Markdown是什么？\" class=\"headerlink\" title=\"1. Markdown是什么？\"></a>1. Markdown<em>是什么</em>？</h3><p><strong>Markdown</strong>是一种轻量级<strong>标记语言</strong>，它以纯文本形式(<em>易读、易写、易更改</em>)编写文档，并最终以HTML格式发布。<br><strong>Markdown</strong>也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。    </p><h3 id=\"2-为什么要使用它？\"><a href=\"#2-为什么要使用它？\" class=\"headerlink\" title=\"2. 为什么要使用它？\"></a>2. <em>为什么</em>要使用它？</h3><ul>\n<li>它是易读（看起来舒服）、易写（语法简单）、易更改<strong>纯文本</strong>。处处体现着<strong>极简主义</strong>的影子。</li>\n<li>兼容HTML，可以转换为HTML格式发布。</li>\n<li>跨平台使用。</li>\n<li>越来越多的网站支持Markdown。</li>\n<li>更方便清晰地组织你的电子邮件。（Markdown-here, Airmail）</li>\n<li>摆脱Word（我不是认真的）。</li>\n</ul><a id=\"more\"></a>\n\n\n<h3 id=\"3-怎么使用？\"><a href=\"#3-怎么使用？\" class=\"headerlink\" title=\"3. 怎么使用？\"></a>3. <em>怎么</em>使用？</h3><p>如果不算<strong>扩展</strong>，Markdown的语法绝对<strong>简单</strong>到让你爱不释手。</p>\n<p>Markdown语法主要分为如下几大部分：\n<strong>标题</strong>，<strong>段落</strong>，<strong>区块引用</strong>，<strong>代码区块</strong>，<strong>强调</strong>，<strong>列表</strong>，<strong>分割线</strong>，<strong>链接</strong>，<strong>图片</strong>，<strong>反斜杠 <code>\\</code></strong>，<strong>符号’`’</strong>。</p>\n<h4 id=\"3-1-标题\"><a href=\"#3-1-标题\" class=\"headerlink\" title=\"3.1 标题\"></a>3.1 标题</h4><p>两种形式：<br>1）使用<code>=</code>和<code>-</code>标记一级和二级标题。</p>\n<blockquote>\n<p>一级标题<br><code>=========</code><br>二级标题<br><code>---------</code></p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题   \"></a>一级标题   </h1><h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2></blockquote>\n<p>2）使用<code>#</code>，可表示1-6级标题。</p>\n<blockquote>\n<p># 一级标题<br>## 二级标题<br>### 三级标题<br>#### 四级标题<br>##### 五级标题<br>###### 六级标题    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<h1 id=\"一级标题-1\"><a href=\"#一级标题-1\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><h2 id=\"二级标题-1\"><a href=\"#二级标题-1\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><h5 id=\"五级标题\"><a href=\"#五级标题\" class=\"headerlink\" title=\"五级标题\"></a>五级标题</h5><h6 id=\"六级标题\"><a href=\"#六级标题\" class=\"headerlink\" title=\"六级标题\"></a>六级标题</h6></blockquote>\n<h4 id=\"3-2-段落\"><a href=\"#3-2-段落\" class=\"headerlink\" title=\"3.2 段落\"></a>3.2 段落</h4><p>段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用<strong>两个以上</strong>空格加上回车（引用中换行省略回车）。</p>\n<h4 id=\"3-3-区块引用\"><a href=\"#3-3-区块引用\" class=\"headerlink\" title=\"3.3 区块引用\"></a>3.3 区块引用</h4><p>在段落的每行或者只在第一行使用符号<code>&gt;</code>,还可使用多个嵌套引用，如：</p>\n<blockquote>\n<p>> 区块引用<br>>&gt; 嵌套引用  </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p>区块引用  </p>\n<blockquote>\n<p>嵌套引用</p>\n</blockquote>\n</blockquote>\n<h4 id=\"3-4-代码区块\"><a href=\"#3-4-代码区块\" class=\"headerlink\" title=\"3.4 代码区块\"></a>3.4 代码区块</h4><p>代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如<br>普通段落：</p>\n<p>void main()<br>{<br>    printf(“Hello, Markdown.”);<br>}    </p>\n<p>代码区块：</p>\n<pre><code>void main()\n{\n    printf(&quot;Hello, Markdown.&quot;);\n}\n</code></pre><p><strong>注意</strong>:需要和普通段落之间存在空行。</p>\n<h4 id=\"3-5-强调\"><a href=\"#3-5-强调\" class=\"headerlink\" title=\"3.5 强调\"></a>3.5 强调</h4><p>在强调内容两侧分别加上<code>*</code>或者<code>_</code>，如：</p>\n<blockquote>\n<p>*斜体*，_斜体_<br>**粗体**，__粗体__</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><em>斜体</em>，<em>斜体</em><br><strong>粗体</strong>，<strong>粗体</strong></p>\n</blockquote>\n<h4 id=\"3-6-列表\"><a href=\"#3-6-列表\" class=\"headerlink\" title=\"3.6 列表\"></a>3.6 列表</h4><p>使用<code>·</code>、<code>+</code>、或<code>-</code>标记无序列表，如：</p>\n<blockquote>\n<p>-（+*） 第一项\n-（+*） 第二项\n- （+*）第三项</p>\n</blockquote>\n<p><strong>注意</strong>：标记后面最少有一个<em>空格</em>或<em>制表符</em>。若不在引用区块中，必须和前方段落之间存在空行。</p>\n<p>效果：</p>\n<blockquote>\n<ul>\n<li>第一项</li>\n<li>第二项</li>\n<li>第三项</li>\n</ul>\n</blockquote>\n<p>有序列表的标记方式是将上述的符号换成数字,并辅以<code>.</code>，如：</p>\n<blockquote>\n<p>1 . 第一项<br>2 . 第二项<br>3 . 第三项    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<ol>\n<li>第一项</li>\n<li>第二项</li>\n<li>第三项</li>\n</ol>\n</blockquote>\n<h4 id=\"3-7-分割线\"><a href=\"#3-7-分割线\" class=\"headerlink\" title=\"3.7 分割线\"></a>3.7 分割线</h4><p>分割线最常使用就是三个或以上<code>*</code>，还可以使用<code>-</code>和<code>_</code>。</p>\n<h4 id=\"3-8-链接\"><a href=\"#3-8-链接\" class=\"headerlink\" title=\"3.8 链接\"></a>3.8 链接</h4><p>链接可以由两种形式生成：<strong>行内式</strong>和<strong>参考式</strong>。<br><strong>行内式</strong>：</p>\n<blockquote>\n<p>[younghz的Markdown库](https:://github.com/younghz/Markdown “Markdown”)。</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库</a>。</p>\n</blockquote>\n<p><strong>参考式</strong>：</p>\n<blockquote>\n<p>[younghz的Markdown库1][1]<br>[younghz的Markdown库2][2]<br>[1]:https:://github.com/younghz/Markdown “Markdown”<br>[2]:https:://github.com/younghz/Markdown “Markdown”    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库1</a><br><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库2</a></p>\n</blockquote>\n<p><strong>注意</strong>：上述的<code>[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;</code>不出现在区块中。</p>\n<h4 id=\"3-9-图片\"><a href=\"#3-9-图片\" class=\"headerlink\" title=\"3.9 图片\"></a>3.9 图片</h4><p>添加图片的形式和链接相似，只需在链接的基础上前方加一个<code>！</code>。</p>\n<h4 id=\"3-10-反斜杠\"><a href=\"#3-10-反斜杠\" class=\"headerlink\" title=\"3.10 反斜杠\\\"></a>3.10 反斜杠<code>\\</code></h4><p>相当于<strong>反转义</strong>作用。使符号成为普通符号。</p>\n<h4 id=\"3-11-符号’-’\"><a href=\"#3-11-符号’-’\" class=\"headerlink\" title=\"3.11 符号’`’\"></a>3.11 符号’`’</h4><p>起到标记作用。如：</p>\n<blockquote>\n<p>`ctrl+a`</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><code>ctrl+a</code>    </p>\n</blockquote>\n<h4 id=\"4-谁在用？\"><a href=\"#4-谁在用？\" class=\"headerlink\" title=\"4. 谁在用？\"></a>4. <em>谁</em>在用？</h4><p>Markdown的使用者：</p>\n<ul>\n<li>GitHub</li>\n<li>简书</li>\n<li>Stack Overflow</li>\n<li>Apollo</li>\n<li>Moodle</li>\n<li>Reddit</li>\n<li>等等</li>\n</ul>\n<h4 id=\"5-尝试一下\"><a href=\"#5-尝试一下\" class=\"headerlink\" title=\"5. 尝试一下\"></a>5. 尝试一下</h4><ul>\n<li><strong>Chrome</strong>下的插件诸如<code>stackedit</code>与<code>markdown-here</code>等非常方便，也不用担心平台受限。</li>\n<li><strong>在线</strong>的dillinger.io评价也不错   </li>\n<li><strong>Windowns</strong>下的MarkdownPad也用过，不过免费版的体验不是很好。    </li>\n<li><strong>Mac</strong>下的Mou是国人贡献的，口碑很好。</li>\n<li><strong>Linux</strong>下的ReText不错。    </li>\n</ul>\n<p><strong>当然，最终境界永远都是笔下是语法，心中格式化 :)。</strong></p>\n<hr>\n<p><strong>注意</strong>：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。\n虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber]\n(<a href=\"http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。\" target=\"_blank\" rel=\"noopener\">http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。</a></p>\n<hr>\n<p>以上基本是所有traditonal markdown的语法。</p>\n<h3 id=\"其它：\"><a href=\"#其它：\" class=\"headerlink\" title=\"其它：\"></a>其它：</h3><p>列表的使用(非traditonal markdown)：</p>\n<p>用<code>|</code>表示表格纵向边界，表头和表内容用<code>-</code>隔开，并可用<code>:</code>进行对齐设置，两边都有<code>:</code>则表示居中，若不加<code>:</code>则默认左对齐。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">代码库</th>\n<th>链接</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">MarkDown</td>\n<td><a href=\"https://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">https://github.com/younghz/Markdown</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MarkDownCopy</td>\n<td><a href=\"https://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">https://github.com/younghz/Markdown</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>关于其它扩展语法可参见具体工具的使用说明。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"主要内容\"><a href=\"#主要内容\" class=\"headerlink\" title=\"主要内容\"></a>主要内容</h2><ul>\n<li>Markdown<em>是什么</em>？</li>\n<li><em>谁</em>创造了它？</li>\n<li><em>为什么</em>要使用它？</li>\n<li><em>怎么</em>使用？</li>\n<li><em>谁</em>在用？</li>\n</ul><h2 id=\"正文\"><a href=\"#正文\" class=\"headerlink\" title=\"正文\"></a>正文</h2><h3 id=\"1-Markdown是什么？\"><a href=\"#1-Markdown是什么？\" class=\"headerlink\" title=\"1. Markdown是什么？\"></a>1. Markdown<em>是什么</em>？</h3><p><strong>Markdown</strong>是一种轻量级<strong>标记语言</strong>，它以纯文本形式(<em>易读、易写、易更改</em>)编写文档，并最终以HTML格式发布。<br><strong>Markdown</strong>也可以理解为将以MARKDOWN语法编写的语言转换成HTML内容的工具。    </p><h3 id=\"2-为什么要使用它？\"><a href=\"#2-为什么要使用它？\" class=\"headerlink\" title=\"2. 为什么要使用它？\"></a>2. <em>为什么</em>要使用它？</h3><ul>\n<li>它是易读（看起来舒服）、易写（语法简单）、易更改<strong>纯文本</strong>。处处体现着<strong>极简主义</strong>的影子。</li>\n<li>兼容HTML，可以转换为HTML格式发布。</li>\n<li>跨平台使用。</li>\n<li>越来越多的网站支持Markdown。</li>\n<li>更方便清晰地组织你的电子邮件。（Markdown-here, Airmail）</li>\n<li>摆脱Word（我不是认真的）。</li>\n</ul>","more":"\n\n\n<h3 id=\"3-怎么使用？\"><a href=\"#3-怎么使用？\" class=\"headerlink\" title=\"3. 怎么使用？\"></a>3. <em>怎么</em>使用？</h3><p>如果不算<strong>扩展</strong>，Markdown的语法绝对<strong>简单</strong>到让你爱不释手。</p>\n<p>Markdown语法主要分为如下几大部分：\n<strong>标题</strong>，<strong>段落</strong>，<strong>区块引用</strong>，<strong>代码区块</strong>，<strong>强调</strong>，<strong>列表</strong>，<strong>分割线</strong>，<strong>链接</strong>，<strong>图片</strong>，<strong>反斜杠 <code>\\</code></strong>，<strong>符号’`’</strong>。</p>\n<h4 id=\"3-1-标题\"><a href=\"#3-1-标题\" class=\"headerlink\" title=\"3.1 标题\"></a>3.1 标题</h4><p>两种形式：<br>1）使用<code>=</code>和<code>-</code>标记一级和二级标题。</p>\n<blockquote>\n<p>一级标题<br><code>=========</code><br>二级标题<br><code>---------</code></p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<h1 id=\"一级标题\"><a href=\"#一级标题\" class=\"headerlink\" title=\"一级标题   \"></a>一级标题   </h1><h2 id=\"二级标题\"><a href=\"#二级标题\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2></blockquote>\n<p>2）使用<code>#</code>，可表示1-6级标题。</p>\n<blockquote>\n<p># 一级标题<br>## 二级标题<br>### 三级标题<br>#### 四级标题<br>##### 五级标题<br>###### 六级标题    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<h1 id=\"一级标题-1\"><a href=\"#一级标题-1\" class=\"headerlink\" title=\"一级标题\"></a>一级标题</h1><h2 id=\"二级标题-1\"><a href=\"#二级标题-1\" class=\"headerlink\" title=\"二级标题\"></a>二级标题</h2><h3 id=\"三级标题\"><a href=\"#三级标题\" class=\"headerlink\" title=\"三级标题\"></a>三级标题</h3><h4 id=\"四级标题\"><a href=\"#四级标题\" class=\"headerlink\" title=\"四级标题\"></a>四级标题</h4><h5 id=\"五级标题\"><a href=\"#五级标题\" class=\"headerlink\" title=\"五级标题\"></a>五级标题</h5><h6 id=\"六级标题\"><a href=\"#六级标题\" class=\"headerlink\" title=\"六级标题\"></a>六级标题</h6></blockquote>\n<h4 id=\"3-2-段落\"><a href=\"#3-2-段落\" class=\"headerlink\" title=\"3.2 段落\"></a>3.2 段落</h4><p>段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用<strong>两个以上</strong>空格加上回车（引用中换行省略回车）。</p>\n<h4 id=\"3-3-区块引用\"><a href=\"#3-3-区块引用\" class=\"headerlink\" title=\"3.3 区块引用\"></a>3.3 区块引用</h4><p>在段落的每行或者只在第一行使用符号<code>&gt;</code>,还可使用多个嵌套引用，如：</p>\n<blockquote>\n<p>> 区块引用<br>>&gt; 嵌套引用  </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p>区块引用  </p>\n<blockquote>\n<p>嵌套引用</p>\n</blockquote>\n</blockquote>\n<h4 id=\"3-4-代码区块\"><a href=\"#3-4-代码区块\" class=\"headerlink\" title=\"3.4 代码区块\"></a>3.4 代码区块</h4><p>代码区块的建立是在每行加上4个空格或者一个制表符（如同写代码一样）。如<br>普通段落：</p>\n<p>void main()<br>{<br>    printf(“Hello, Markdown.”);<br>}    </p>\n<p>代码区块：</p>\n<pre><code>void main()\n{\n    printf(&quot;Hello, Markdown.&quot;);\n}\n</code></pre><p><strong>注意</strong>:需要和普通段落之间存在空行。</p>\n<h4 id=\"3-5-强调\"><a href=\"#3-5-强调\" class=\"headerlink\" title=\"3.5 强调\"></a>3.5 强调</h4><p>在强调内容两侧分别加上<code>*</code>或者<code>_</code>，如：</p>\n<blockquote>\n<p>*斜体*，_斜体_<br>**粗体**，__粗体__</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><em>斜体</em>，<em>斜体</em><br><strong>粗体</strong>，<strong>粗体</strong></p>\n</blockquote>\n<h4 id=\"3-6-列表\"><a href=\"#3-6-列表\" class=\"headerlink\" title=\"3.6 列表\"></a>3.6 列表</h4><p>使用<code>·</code>、<code>+</code>、或<code>-</code>标记无序列表，如：</p>\n<blockquote>\n<p>-（+*） 第一项\n-（+*） 第二项\n- （+*）第三项</p>\n</blockquote>\n<p><strong>注意</strong>：标记后面最少有一个<em>空格</em>或<em>制表符</em>。若不在引用区块中，必须和前方段落之间存在空行。</p>\n<p>效果：</p>\n<blockquote>\n<ul>\n<li>第一项</li>\n<li>第二项</li>\n<li>第三项</li>\n</ul>\n</blockquote>\n<p>有序列表的标记方式是将上述的符号换成数字,并辅以<code>.</code>，如：</p>\n<blockquote>\n<p>1 . 第一项<br>2 . 第二项<br>3 . 第三项    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<ol>\n<li>第一项</li>\n<li>第二项</li>\n<li>第三项</li>\n</ol>\n</blockquote>\n<h4 id=\"3-7-分割线\"><a href=\"#3-7-分割线\" class=\"headerlink\" title=\"3.7 分割线\"></a>3.7 分割线</h4><p>分割线最常使用就是三个或以上<code>*</code>，还可以使用<code>-</code>和<code>_</code>。</p>\n<h4 id=\"3-8-链接\"><a href=\"#3-8-链接\" class=\"headerlink\" title=\"3.8 链接\"></a>3.8 链接</h4><p>链接可以由两种形式生成：<strong>行内式</strong>和<strong>参考式</strong>。<br><strong>行内式</strong>：</p>\n<blockquote>\n<p>[younghz的Markdown库](https:://github.com/younghz/Markdown “Markdown”)。</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库</a>。</p>\n</blockquote>\n<p><strong>参考式</strong>：</p>\n<blockquote>\n<p>[younghz的Markdown库1][1]<br>[younghz的Markdown库2][2]<br>[1]:https:://github.com/younghz/Markdown “Markdown”<br>[2]:https:://github.com/younghz/Markdown “Markdown”    </p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库1</a><br><a href=\"https:://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">younghz的Markdown库2</a></p>\n</blockquote>\n<p><strong>注意</strong>：上述的<code>[1]:https:://github.com/younghz/Markdown &quot;Markdown&quot;</code>不出现在区块中。</p>\n<h4 id=\"3-9-图片\"><a href=\"#3-9-图片\" class=\"headerlink\" title=\"3.9 图片\"></a>3.9 图片</h4><p>添加图片的形式和链接相似，只需在链接的基础上前方加一个<code>！</code>。</p>\n<h4 id=\"3-10-反斜杠\"><a href=\"#3-10-反斜杠\" class=\"headerlink\" title=\"3.10 反斜杠\\\"></a>3.10 反斜杠<code>\\</code></h4><p>相当于<strong>反转义</strong>作用。使符号成为普通符号。</p>\n<h4 id=\"3-11-符号’-’\"><a href=\"#3-11-符号’-’\" class=\"headerlink\" title=\"3.11 符号’`’\"></a>3.11 符号’`’</h4><p>起到标记作用。如：</p>\n<blockquote>\n<p>`ctrl+a`</p>\n</blockquote>\n<p>效果：</p>\n<blockquote>\n<p><code>ctrl+a</code>    </p>\n</blockquote>\n<h4 id=\"4-谁在用？\"><a href=\"#4-谁在用？\" class=\"headerlink\" title=\"4. 谁在用？\"></a>4. <em>谁</em>在用？</h4><p>Markdown的使用者：</p>\n<ul>\n<li>GitHub</li>\n<li>简书</li>\n<li>Stack Overflow</li>\n<li>Apollo</li>\n<li>Moodle</li>\n<li>Reddit</li>\n<li>等等</li>\n</ul>\n<h4 id=\"5-尝试一下\"><a href=\"#5-尝试一下\" class=\"headerlink\" title=\"5. 尝试一下\"></a>5. 尝试一下</h4><ul>\n<li><strong>Chrome</strong>下的插件诸如<code>stackedit</code>与<code>markdown-here</code>等非常方便，也不用担心平台受限。</li>\n<li><strong>在线</strong>的dillinger.io评价也不错   </li>\n<li><strong>Windowns</strong>下的MarkdownPad也用过，不过免费版的体验不是很好。    </li>\n<li><strong>Mac</strong>下的Mou是国人贡献的，口碑很好。</li>\n<li><strong>Linux</strong>下的ReText不错。    </li>\n</ul>\n<p><strong>当然，最终境界永远都是笔下是语法，心中格式化 :)。</strong></p>\n<hr>\n<p><strong>注意</strong>：不同的Markdown解释器或工具对相应语法（扩展语法）的解释效果不尽相同，具体可参见工具的使用说明。\n虽然有人想出面搞一个所谓的标准化的Markdown，[没想到还惹怒了健在的创始人John Gruber]\n(<a href=\"http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。\" target=\"_blank\" rel=\"noopener\">http://blog.codinghorror.com/standard-markdown-is-now-common-markdown/)。</a></p>\n<hr>\n<p>以上基本是所有traditonal markdown的语法。</p>\n<h3 id=\"其它：\"><a href=\"#其它：\" class=\"headerlink\" title=\"其它：\"></a>其它：</h3><p>列表的使用(非traditonal markdown)：</p>\n<p>用<code>|</code>表示表格纵向边界，表头和表内容用<code>-</code>隔开，并可用<code>:</code>进行对齐设置，两边都有<code>:</code>则表示居中，若不加<code>:</code>则默认左对齐。</p>\n<div class=\"table-container\">\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">代码库</th>\n<th>链接</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">MarkDown</td>\n<td><a href=\"https://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">https://github.com/younghz/Markdown</a></td>\n</tr>\n<tr>\n<td style=\"text-align:center\">MarkDownCopy</td>\n<td><a href=\"https://github.com/younghz/Markdown\" title=\"Markdown\" target=\"_blank\" rel=\"noopener\">https://github.com/younghz/Markdown</a></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>关于其它扩展语法可参见具体工具的使用说明。</p>\n"},{"layout":"post","title":"归并排序 - MergeSort","date":"2017-12-22T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 归并排序 - MergeSort </center>\n\n------\n\n### 合并函数\n\n```\nvoid merge(int *arr, int left, int mid, int right)\n{\n\tint *tmparr = new int[right - left + 1];\n\tint leftindex = left, rightindex = mid + 1;\n\tint tmpindex = 0;\n\twhile (leftindex <= mid && rightindex <= right)\n\t{\n\t\tif (arr[leftindex] < arr[rightindex])\n\t\t\ttmparr[tmpindex++] = arr[leftindex++];\n\t\telse\n\t\t\ttmparr[tmpindex++] = arr[rightindex++];\n\t}\n\twhile (leftindex <= mid)\n\t\ttmparr[tmpindex++] = arr[leftindex++];\n\twhile (rightindex <= right)\n\t\ttmparr[tmpindex++] = arr[rightindex++];\n\ttmpindex = 0;\n\twhile (left <= right)\n\t\tarr[left++] = tmparr[tmpindex++];\n}\n```\n\n--------\n\n### 递归\n\n```cpp\nvoid mergesort_recursive(int *arr, int left, int right)\n{\n\tif (left >= right)\n\t\treturn;\n\tint mid = (left + right) / 2;\n\tmergesort_recursive(arr, left, mid);\n\tmergesort_recursive(arr, mid + 1, right);\n\tmerge(arr, left, mid, right);\n}\n```\n\n--------\n\n### 非递归\n\n```\nvoid mergesort_nonerecursive(int *arr, int left, int right)\n{\n\tint step = 2, i = 0;\n\twhile (step <= right + 1)\n\t{\n\t\ti = 0;\n\t\twhile (i + step <= right)\n\t\t{\n\t\t\tmerge(arr, i, i + step / 2 - 1, i + step - 1);\n\t\t\ti += step;\n\t\t}\n\t\tmerge(arr, i, i + step / 2 - 1, right);\n\t\tstep *= 2;\n\t}\n\tmerge(arr, 0, step / 2 - 1, right);\n}\n```\n","source":"_posts/2017-12-22-mergesort.md","raw":"---\nlayout: post\ntitle: 归并排序 - MergeSort\ndate: 2017-12-22 21:10 +0800\ncategories: LeetCode\ntags:\n- 排序\nmathjax: true\ncopyright: true\n---\n\n## <center> 归并排序 - MergeSort </center>\n\n------\n\n### 合并函数\n\n```\nvoid merge(int *arr, int left, int mid, int right)\n{\n\tint *tmparr = new int[right - left + 1];\n\tint leftindex = left, rightindex = mid + 1;\n\tint tmpindex = 0;\n\twhile (leftindex <= mid && rightindex <= right)\n\t{\n\t\tif (arr[leftindex] < arr[rightindex])\n\t\t\ttmparr[tmpindex++] = arr[leftindex++];\n\t\telse\n\t\t\ttmparr[tmpindex++] = arr[rightindex++];\n\t}\n\twhile (leftindex <= mid)\n\t\ttmparr[tmpindex++] = arr[leftindex++];\n\twhile (rightindex <= right)\n\t\ttmparr[tmpindex++] = arr[rightindex++];\n\ttmpindex = 0;\n\twhile (left <= right)\n\t\tarr[left++] = tmparr[tmpindex++];\n}\n```\n\n--------\n\n### 递归\n\n```cpp\nvoid mergesort_recursive(int *arr, int left, int right)\n{\n\tif (left >= right)\n\t\treturn;\n\tint mid = (left + right) / 2;\n\tmergesort_recursive(arr, left, mid);\n\tmergesort_recursive(arr, mid + 1, right);\n\tmerge(arr, left, mid, right);\n}\n```\n\n--------\n\n### 非递归\n\n```\nvoid mergesort_nonerecursive(int *arr, int left, int right)\n{\n\tint step = 2, i = 0;\n\twhile (step <= right + 1)\n\t{\n\t\ti = 0;\n\t\twhile (i + step <= right)\n\t\t{\n\t\t\tmerge(arr, i, i + step / 2 - 1, i + step - 1);\n\t\t\ti += step;\n\t\t}\n\t\tmerge(arr, i, i + step / 2 - 1, right);\n\t\tstep *= 2;\n\t}\n\tmerge(arr, 0, step / 2 - 1, right);\n}\n```\n","slug":"mergesort","published":1,"updated":"2019-08-17T09:30:00.271Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnuj00062qwpplm6n3uq","content":"<h2 id=\"归并排序-MergeSort\"><a href=\"#归并排序-MergeSort\" class=\"headerlink\" title=\" 归并排序 - MergeSort \"></a><center> 归并排序 - MergeSort </center></h2><hr><h3 id=\"合并函数\"><a href=\"#合并函数\" class=\"headerlink\" title=\"合并函数\"></a>合并函数</h3><figure class=\"highlight vbscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void merge(<span class=\"built_in\">int</span> *arr, <span class=\"built_in\">int</span> <span class=\"built_in\">left</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">mid</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">right</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> *tmparr = <span class=\"keyword\">new</span> <span class=\"built_in\">int</span>[<span class=\"built_in\">right</span> - <span class=\"built_in\">left</span> + <span class=\"number\">1</span>];</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> leftindex = <span class=\"built_in\">left</span>, rightindex = <span class=\"built_in\">mid</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> tmpindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (leftindex &lt;= <span class=\"built_in\">mid</span> &amp;&amp; rightindex &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (arr[leftindex] &lt; arr[rightindex])</span><br><span class=\"line\">\t\t\ttmparr[tmpindex++] = arr[leftindex++];</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\ttmparr[tmpindex++] = arr[rightindex++];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (leftindex &lt;= <span class=\"built_in\">mid</span>)</span><br><span class=\"line\">\t\ttmparr[tmpindex++] = arr[leftindex++];</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rightindex &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\ttmparr[tmpindex++] = arr[rightindex++];</span><br><span class=\"line\">\ttmpindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"built_in\">left</span> &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\tarr[<span class=\"built_in\">left</span>++] = tmparr[tmpindex++];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><a id=\"more\"></a>\n\n<hr>\n<h3 id=\"递归\"><a href=\"#递归\" class=\"headerlink\" title=\"递归\"></a>递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mergesort_recursive</span><span class=\"params\">(<span class=\"keyword\">int</span> *arr, <span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (left &gt;= right)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\tmergesort_recursive(arr, left, mid);</span><br><span class=\"line\">\tmergesort_recursive(arr, mid + <span class=\"number\">1</span>, right);</span><br><span class=\"line\">\tmerge(arr, left, mid, right);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"非递归\"><a href=\"#非递归\" class=\"headerlink\" title=\"非递归\"></a>非递归</h3><figure class=\"highlight vbscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void mergesort_nonerecursive(<span class=\"built_in\">int</span> *arr, <span class=\"built_in\">int</span> <span class=\"built_in\">left</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">right</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> <span class=\"keyword\">step</span> = <span class=\"number\">2</span>, i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"keyword\">step</span> &lt;= <span class=\"built_in\">right</span> + <span class=\"number\">1</span>)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\ti = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (i + <span class=\"keyword\">step</span> &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmerge(arr, i, i + <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, i + <span class=\"keyword\">step</span> - <span class=\"number\">1</span>);</span><br><span class=\"line\">\t\t\ti += <span class=\"keyword\">step</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tmerge(arr, i, i + <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, <span class=\"built_in\">right</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">step</span> *= <span class=\"number\">2</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tmerge(arr, <span class=\"number\">0</span>, <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, <span class=\"built_in\">right</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h2 id=\"归并排序-MergeSort\"><a href=\"#归并排序-MergeSort\" class=\"headerlink\" title=\" 归并排序 - MergeSort \"></a><center> 归并排序 - MergeSort </center></h2><hr><h3 id=\"合并函数\"><a href=\"#合并函数\" class=\"headerlink\" title=\"合并函数\"></a>合并函数</h3><figure class=\"highlight vbscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void merge(<span class=\"built_in\">int</span> *arr, <span class=\"built_in\">int</span> <span class=\"built_in\">left</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">mid</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">right</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> *tmparr = <span class=\"keyword\">new</span> <span class=\"built_in\">int</span>[<span class=\"built_in\">right</span> - <span class=\"built_in\">left</span> + <span class=\"number\">1</span>];</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> leftindex = <span class=\"built_in\">left</span>, rightindex = <span class=\"built_in\">mid</span> + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> tmpindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (leftindex &lt;= <span class=\"built_in\">mid</span> &amp;&amp; rightindex &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (arr[leftindex] &lt; arr[rightindex])</span><br><span class=\"line\">\t\t\ttmparr[tmpindex++] = arr[leftindex++];</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\ttmparr[tmpindex++] = arr[rightindex++];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (leftindex &lt;= <span class=\"built_in\">mid</span>)</span><br><span class=\"line\">\t\ttmparr[tmpindex++] = arr[leftindex++];</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (rightindex &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\ttmparr[tmpindex++] = arr[rightindex++];</span><br><span class=\"line\">\ttmpindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"built_in\">left</span> &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\tarr[<span class=\"built_in\">left</span>++] = tmparr[tmpindex++];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"\n\n<hr>\n<h3 id=\"递归\"><a href=\"#递归\" class=\"headerlink\" title=\"递归\"></a>递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">mergesort_recursive</span><span class=\"params\">(<span class=\"keyword\">int</span> *arr, <span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (left &gt;= right)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid = (left + right) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\tmergesort_recursive(arr, left, mid);</span><br><span class=\"line\">\tmergesort_recursive(arr, mid + <span class=\"number\">1</span>, right);</span><br><span class=\"line\">\tmerge(arr, left, mid, right);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"非递归\"><a href=\"#非递归\" class=\"headerlink\" title=\"非递归\"></a>非递归</h3><figure class=\"highlight vbscript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">void mergesort_nonerecursive(<span class=\"built_in\">int</span> *arr, <span class=\"built_in\">int</span> <span class=\"built_in\">left</span>, <span class=\"built_in\">int</span> <span class=\"built_in\">right</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">int</span> <span class=\"keyword\">step</span> = <span class=\"number\">2</span>, i = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (<span class=\"keyword\">step</span> &lt;= <span class=\"built_in\">right</span> + <span class=\"number\">1</span>)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\ti = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (i + <span class=\"keyword\">step</span> &lt;= <span class=\"built_in\">right</span>)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmerge(arr, i, i + <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, i + <span class=\"keyword\">step</span> - <span class=\"number\">1</span>);</span><br><span class=\"line\">\t\t\ti += <span class=\"keyword\">step</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tmerge(arr, i, i + <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, <span class=\"built_in\">right</span>);</span><br><span class=\"line\">\t\t<span class=\"keyword\">step</span> *= <span class=\"number\">2</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\tmerge(arr, <span class=\"number\">0</span>, <span class=\"keyword\">step</span> / <span class=\"number\">2</span> - <span class=\"number\">1</span>, <span class=\"built_in\">right</span>);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","title":"Latex公式语法","date":"2017-12-12T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\nKaTex用法如下：\n\n> KATEX: [https://katex.org/docs/supported.html](https://katex.org/docs/supported.html)\n\nLaTex用法如下：\n\n目录\n\n* 1.上标与下标\n* 2.分式\n* 3.根式\n* 4.求和与积分\n* 5.公式中的空格\n* 6.公式中的定界符\n* 7.矩阵\n* 8.排版数组\n* 9.数学模式重音符号\n* 10.希腊字母\n* 11.二元关系\n* 12.“大”运算符\n* 13.箭头\n* 14.定界符\n* 15.大定界符\n* 16.其他符号\n* 17.非数学符号\n* 18.AMS定界符\n* 19.AMS希腊和希伯来字母\n* 20.AMS二元关系\n* 21.AMS箭头\n* 22.AMS二元否定关系符和箭头\n* 23.AMS二元运算符\n* 24.AMS其他符号\n* 25.数学字母\n\n\n-------\n\n### 1.上标与下标\n\n上标命令是 ^{角标}，下标命令是 _{角标}。当角标是单个字符时可以不用花括号(在 LaTeX 中，花括号是用于分组，即花括号内部文本为一组)。\n\n```text\n$$x_1$$\n$$x_1^2$$\n$$x_{22}^{(n)}$$\n$${}^*\\!x^*$$\n```\n\n$$x_1$$\n$$x_1^2$$\n$$x_{22}^{(n)}$$\n$${}^*\\!x^*$$\n\n\n### 2.分式\n\n输入带有水平分数线的公式，可用命令：\\frac{分子}{分母}；\n\n```text\n$$\\frac{x+y}{2}$$\n$$\\frac{1}{1+\\frac{1}{2}}$$\n```\n\n$$\\frac{x+y}{2}$$\n$$\\frac{1}{1+\\frac{1}{2}}$$\n\n\n### 3.根式\n\n```text\n$$\\sqrt{2}<\\sqrt[3]{3}$$\n$$\\sqrt{1+\\sqrt[p]{1+a^2}}$$\n$$\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}$$\n```\n\n$$\\sqrt{2}<\\sqrt[3]{3}$$\n$$\\sqrt{1+\\sqrt[p]{1+a^2}}$$\n$$\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}$$\n\n\n### 4.求和与积分\n\n```text\n$$\\sum_{k=1}^{n}\\frac{1}{k}$$\n$$\\sum_{k=1}^n\\frac{1}{k}$$\n$$\\int_a^b f(x)dx$$\n$$\\int_a^b f(x)dx$$\n微分符直体：$$\\int_a^b f(x)\\mathrm{d}x$$\n```\n\n$$\\sum_{k=1}^{n}\\frac{1}{k}$$\n$$\\sum_{k=1}^n\\frac{1}{k}$$\n$$\\int_a^b f(x)dx$$\n$$\\int_a^b f(x)dx$$\n微分符直体：$$\\int_a^b f(x)\\mathrm{d}x$$\n\n\n### 5.公式中的空格\n\n```text\n紧贴 $$a\\!b$$\n没有空格 $$ab$$\n小空格 $$a\\,b$$\n中等空格 $$a\\;b$$\n大空格 $$a\\ b$$\nquad空格 $$a\\quad b$$\n两个quad空格 $$a\\qquad b$$\n```\n\n紧贴 $$a\\!b$$\n没有空格 $$ab$$\n小空格 $$a\\,b$$\n中等空格 $$a\\;b$$\n大空格 $$a\\ b$$\nquad空格 $$a\\quad b$$\n两个quad空格 $$a\\qquad b$$\n\n\n### 6.公式中的定界符\n\n这里所谓的定界符是指包围或分割公式的一些符号\n\n```text\n$$(    %($$\n$$)    %)$$\n$$[    %[$$\n$$]    %]$$\n$$\\{    %{$$\n$$\\}    %}$$\n$$|    %|$$\n$$\\|    %||$$\n```\n\n$$(    %($$\n$$)    %)$$\n$$[    %[$$\n$$]    %]$$\n$$\\{    %{$$\n$$\\}    %}$$\n$$|    %|$$\n$$\\|    %||$$\n\n在上述这些定界符之前冠以 \\left（修饰左定界符）或 \\right（修饰右定界符），可以得到自适应缩放的定界符，它们会根据定界符所包围的公式大小自适应缩放。\n\n```text\n$$\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)$$\n```\n\n$$\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)$$\n\n\n### 7.矩阵\n\n对于少于 10 列的矩阵，可使用 matrix，pmatrix，bmatrix，Bmatrix，vmatrix 和 Vmatrix 等环境。\n\n```text\n$$\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}$$\n$$\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}$$\n$$\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}$$\n$$\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}$$\n$$\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}$$\n$$\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}$$\n```\n\n$$\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}$$\n$$\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}$$\n$$\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}$$\n$$\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}$$\n$$\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}$$\n$$\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}$$\n\n\n### 8.排版数组\n\n```text\n$$\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n$$\n```\n\n$$\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n$$\n\n\\mathbf大写控制符，\\\\表示换行，{ccc}表示列样式。array 环境也可以用来排版这样的表达式，表达式中使用一个“.” 作为其隐藏的\\right 定界符\n\n```text\n$$\ny = \\left\\{ \\begin{array}{ll}\na & \\textrm{if $d>c$}\\\\\nb+x & \\textrm{in the morning}\\\\\nl & \\textrm{all day long}\n\\end{array} \\right.\n$$\n```\n\n你也可以在array 环境中画线，如分隔矩阵中元素。\n\n```text\n$$\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)\n$$\n```\n\n$$\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)\n$$\n\n\n### 9.数学模式重音符号\n\n![数学模式重音符号](/posts_res/2017-12-12-latex/9.png)\n\n\n### 10.希腊字母\n\n![希腊字母](/posts_res/2017-12-12-latex/10.png)\n\n\n### 11.二元关系\n\n可以在下列符号的相应命令前加上 \\not 命令，而得到其否定形式。\n\n![二元关系-1](/posts_res/2017-12-12-latex/11-1.png)\n\n![二元关系-2](/posts_res/2017-12-12-latex/11-2.png)\n\n\n### 12.“大”运算符\n\n![大运算符](/posts_res/2017-12-12-latex/12.png)\n\n\n### 13.箭头\n\n![箭头](/posts_res/2017-12-12-latex/13.png)\n\n\n### 14.定界符\n\n![定界符](/posts_res/2017-12-12-latex/14.png)\n\n\n### 15.大定界符\n\n![大定界符](/posts_res/2017-12-12-latex/15.png)\n\n\n### 16.其他符号\n\n![其他符号](/posts_res/2017-12-12-latex/16.png)\n\n\n### 17.非数学符号\n\n![非数学符号](/posts_res/2017-12-12-latex/17.png)\n\n\n### 18.AMS定界符\n\n![AMS定界符](/posts_res/2017-12-12-latex/18.png)\n\n\n### 19.AMS希腊和希伯来字母\n\n![AMS希腊和希伯来字母](/posts_res/2017-12-12-latex/19.png)\n\n\n### 20.AMS二元关系\n\n![AMS二元关系](/posts_res/2017-12-12-latex/20.png)\n\n\n### 21.AMS箭头\n\n![AMS箭头](/posts_res/2017-12-12-latex/21.png)\n\n\n### 22.AMS二元否定关系符和箭头\n\n![AMS二元否定关系符和箭头](/posts_res/2017-12-12-latex/22.png)\n\n\n### 23.AMS二元运算符\n\n![AMS二元运算符](/posts_res/2017-12-12-latex/23.png)\n\n\n### 24.AMS其他符号\n\n![AMS其他符号](/posts_res/2017-12-12-latex/24.png)\n\n\n### 25.数学字母\n\n![数学字母](/posts_res/2017-12-12-latex/25.png)\n\n\n--------\n\n> [LATEX数学公式基本语法](https://www.cnblogs.com/houkai/p/3399646.html)\n","source":"_posts/2017-12-12-latex.md","raw":"---\nlayout: post\ntitle: Latex公式语法\ndate: 2017-12-12 12:10 +0800\ncategories: 基础知识\ntags:\n- Latex\nmathjax: true\ncopyright: true\n---\n\nKaTex用法如下：\n\n> KATEX: [https://katex.org/docs/supported.html](https://katex.org/docs/supported.html)\n\nLaTex用法如下：\n\n目录\n\n* 1.上标与下标\n* 2.分式\n* 3.根式\n* 4.求和与积分\n* 5.公式中的空格\n* 6.公式中的定界符\n* 7.矩阵\n* 8.排版数组\n* 9.数学模式重音符号\n* 10.希腊字母\n* 11.二元关系\n* 12.“大”运算符\n* 13.箭头\n* 14.定界符\n* 15.大定界符\n* 16.其他符号\n* 17.非数学符号\n* 18.AMS定界符\n* 19.AMS希腊和希伯来字母\n* 20.AMS二元关系\n* 21.AMS箭头\n* 22.AMS二元否定关系符和箭头\n* 23.AMS二元运算符\n* 24.AMS其他符号\n* 25.数学字母\n\n\n-------\n\n### 1.上标与下标\n\n上标命令是 ^{角标}，下标命令是 _{角标}。当角标是单个字符时可以不用花括号(在 LaTeX 中，花括号是用于分组，即花括号内部文本为一组)。\n\n```text\n$$x_1$$\n$$x_1^2$$\n$$x_{22}^{(n)}$$\n$${}^*\\!x^*$$\n```\n\n$$x_1$$\n$$x_1^2$$\n$$x_{22}^{(n)}$$\n$${}^*\\!x^*$$\n\n\n### 2.分式\n\n输入带有水平分数线的公式，可用命令：\\frac{分子}{分母}；\n\n```text\n$$\\frac{x+y}{2}$$\n$$\\frac{1}{1+\\frac{1}{2}}$$\n```\n\n$$\\frac{x+y}{2}$$\n$$\\frac{1}{1+\\frac{1}{2}}$$\n\n\n### 3.根式\n\n```text\n$$\\sqrt{2}<\\sqrt[3]{3}$$\n$$\\sqrt{1+\\sqrt[p]{1+a^2}}$$\n$$\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}$$\n```\n\n$$\\sqrt{2}<\\sqrt[3]{3}$$\n$$\\sqrt{1+\\sqrt[p]{1+a^2}}$$\n$$\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}$$\n\n\n### 4.求和与积分\n\n```text\n$$\\sum_{k=1}^{n}\\frac{1}{k}$$\n$$\\sum_{k=1}^n\\frac{1}{k}$$\n$$\\int_a^b f(x)dx$$\n$$\\int_a^b f(x)dx$$\n微分符直体：$$\\int_a^b f(x)\\mathrm{d}x$$\n```\n\n$$\\sum_{k=1}^{n}\\frac{1}{k}$$\n$$\\sum_{k=1}^n\\frac{1}{k}$$\n$$\\int_a^b f(x)dx$$\n$$\\int_a^b f(x)dx$$\n微分符直体：$$\\int_a^b f(x)\\mathrm{d}x$$\n\n\n### 5.公式中的空格\n\n```text\n紧贴 $$a\\!b$$\n没有空格 $$ab$$\n小空格 $$a\\,b$$\n中等空格 $$a\\;b$$\n大空格 $$a\\ b$$\nquad空格 $$a\\quad b$$\n两个quad空格 $$a\\qquad b$$\n```\n\n紧贴 $$a\\!b$$\n没有空格 $$ab$$\n小空格 $$a\\,b$$\n中等空格 $$a\\;b$$\n大空格 $$a\\ b$$\nquad空格 $$a\\quad b$$\n两个quad空格 $$a\\qquad b$$\n\n\n### 6.公式中的定界符\n\n这里所谓的定界符是指包围或分割公式的一些符号\n\n```text\n$$(    %($$\n$$)    %)$$\n$$[    %[$$\n$$]    %]$$\n$$\\{    %{$$\n$$\\}    %}$$\n$$|    %|$$\n$$\\|    %||$$\n```\n\n$$(    %($$\n$$)    %)$$\n$$[    %[$$\n$$]    %]$$\n$$\\{    %{$$\n$$\\}    %}$$\n$$|    %|$$\n$$\\|    %||$$\n\n在上述这些定界符之前冠以 \\left（修饰左定界符）或 \\right（修饰右定界符），可以得到自适应缩放的定界符，它们会根据定界符所包围的公式大小自适应缩放。\n\n```text\n$$\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)$$\n```\n\n$$\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)$$\n\n\n### 7.矩阵\n\n对于少于 10 列的矩阵，可使用 matrix，pmatrix，bmatrix，Bmatrix，vmatrix 和 Vmatrix 等环境。\n\n```text\n$$\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}$$\n$$\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}$$\n$$\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}$$\n$$\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}$$\n$$\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}$$\n$$\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}$$\n```\n\n$$\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}$$\n$$\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}$$\n$$\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}$$\n$$\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}$$\n$$\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}$$\n$$\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}$$\n\n\n### 8.排版数组\n\n```text\n$$\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n$$\n```\n\n$$\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)\n$$\n\n\\mathbf大写控制符，\\\\表示换行，{ccc}表示列样式。array 环境也可以用来排版这样的表达式，表达式中使用一个“.” 作为其隐藏的\\right 定界符\n\n```text\n$$\ny = \\left\\{ \\begin{array}{ll}\na & \\textrm{if $d>c$}\\\\\nb+x & \\textrm{in the morning}\\\\\nl & \\textrm{all day long}\n\\end{array} \\right.\n$$\n```\n\n你也可以在array 环境中画线，如分隔矩阵中元素。\n\n```text\n$$\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)\n$$\n```\n\n$$\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)\n$$\n\n\n### 9.数学模式重音符号\n\n![数学模式重音符号](/posts_res/2017-12-12-latex/9.png)\n\n\n### 10.希腊字母\n\n![希腊字母](/posts_res/2017-12-12-latex/10.png)\n\n\n### 11.二元关系\n\n可以在下列符号的相应命令前加上 \\not 命令，而得到其否定形式。\n\n![二元关系-1](/posts_res/2017-12-12-latex/11-1.png)\n\n![二元关系-2](/posts_res/2017-12-12-latex/11-2.png)\n\n\n### 12.“大”运算符\n\n![大运算符](/posts_res/2017-12-12-latex/12.png)\n\n\n### 13.箭头\n\n![箭头](/posts_res/2017-12-12-latex/13.png)\n\n\n### 14.定界符\n\n![定界符](/posts_res/2017-12-12-latex/14.png)\n\n\n### 15.大定界符\n\n![大定界符](/posts_res/2017-12-12-latex/15.png)\n\n\n### 16.其他符号\n\n![其他符号](/posts_res/2017-12-12-latex/16.png)\n\n\n### 17.非数学符号\n\n![非数学符号](/posts_res/2017-12-12-latex/17.png)\n\n\n### 18.AMS定界符\n\n![AMS定界符](/posts_res/2017-12-12-latex/18.png)\n\n\n### 19.AMS希腊和希伯来字母\n\n![AMS希腊和希伯来字母](/posts_res/2017-12-12-latex/19.png)\n\n\n### 20.AMS二元关系\n\n![AMS二元关系](/posts_res/2017-12-12-latex/20.png)\n\n\n### 21.AMS箭头\n\n![AMS箭头](/posts_res/2017-12-12-latex/21.png)\n\n\n### 22.AMS二元否定关系符和箭头\n\n![AMS二元否定关系符和箭头](/posts_res/2017-12-12-latex/22.png)\n\n\n### 23.AMS二元运算符\n\n![AMS二元运算符](/posts_res/2017-12-12-latex/23.png)\n\n\n### 24.AMS其他符号\n\n![AMS其他符号](/posts_res/2017-12-12-latex/24.png)\n\n\n### 25.数学字母\n\n![数学字母](/posts_res/2017-12-12-latex/25.png)\n\n\n--------\n\n> [LATEX数学公式基本语法](https://www.cnblogs.com/houkai/p/3399646.html)\n","slug":"latex","published":1,"updated":"2019-08-17T09:28:45.540Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnun00082qwpmd6soted","content":"<p>KaTex用法如下：</p><blockquote>\n<p>KATEX: <a href=\"https://katex.org/docs/supported.html\" target=\"_blank\" rel=\"noopener\">https://katex.org/docs/supported.html</a></p>\n</blockquote><p>LaTex用法如下：</p><p>目录</p><ul>\n<li>1.上标与下标</li>\n<li>2.分式</li>\n<li>3.根式</li>\n<li>4.求和与积分</li>\n<li>5.公式中的空格</li>\n<li>6.公式中的定界符</li>\n<li>7.矩阵</li>\n<li>8.排版数组</li>\n<li>9.数学模式重音符号</li>\n<li>10.希腊字母</li>\n<li>11.二元关系</li>\n<li>12.“大”运算符</li>\n<li>13.箭头</li>\n<li>14.定界符</li>\n<li>15.大定界符</li>\n<li>16.其他符号</li>\n<li>17.非数学符号</li>\n<li>18.AMS定界符</li>\n<li>19.AMS希腊和希伯来字母</li>\n<li>20.AMS二元关系</li>\n<li>21.AMS箭头</li>\n<li>22.AMS二元否定关系符和箭头</li>\n<li>23.AMS二元运算符</li>\n<li>24.AMS其他符号</li>\n<li>25.数学字母</li>\n</ul><hr><h3 id=\"1-上标与下标\"><a href=\"#1-上标与下标\" class=\"headerlink\" title=\"1.上标与下标\"></a>1.上标与下标</h3><a id=\"more\"></a>\n\n\n\n\n\n<p>上标命令是 ^{角标}，下标命令是 _{角标}。当角标是单个字符时可以不用花括号(在 LaTeX 中，花括号是用于分组，即花括号内部文本为一组)。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$x_1$$</span><br><span class=\"line\">$$x_1^2$$</span><br><span class=\"line\">$$x_&#123;22&#125;^&#123;(n)&#125;$$</span><br><span class=\"line\">$$&#123;&#125;^*\\!x^*$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">x_1</script><script type=\"math/tex; mode=display\">x_1^2</script><script type=\"math/tex; mode=display\">x_{22}^{(n)}</script><script type=\"math/tex; mode=display\">{}^*\\!x^*</script><h3 id=\"2-分式\"><a href=\"#2-分式\" class=\"headerlink\" title=\"2.分式\"></a>2.分式</h3><p>输入带有水平分数线的公式，可用命令：\\frac{分子}{分母}；</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;x+y&#125;&#123;2&#125;$$</span><br><span class=\"line\">$$\\frac&#123;1&#125;&#123;1+\\frac&#123;1&#125;&#123;2&#125;&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\frac{x+y}{2}</script><script type=\"math/tex; mode=display\">\\frac{1}{1+\\frac{1}{2}}</script><h3 id=\"3-根式\"><a href=\"#3-根式\" class=\"headerlink\" title=\"3.根式\"></a>3.根式</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\sqrt&#123;2&#125;&lt;\\sqrt[3]&#123;3&#125;$$</span><br><span class=\"line\">$$\\sqrt&#123;1+\\sqrt[p]&#123;1+a^2&#125;&#125;$$</span><br><span class=\"line\">$$\\sqrt&#123;1+\\sqrt[^p\\!]&#123;1+a^2&#125;&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\sqrt{2}<\\sqrt[3]{3}</script><script type=\"math/tex; mode=display\">\\sqrt{1+\\sqrt[p]{1+a^2}}</script><script type=\"math/tex; mode=display\">\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}</script><h3 id=\"4-求和与积分\"><a href=\"#4-求和与积分\" class=\"headerlink\" title=\"4.求和与积分\"></a>4.求和与积分</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\sum_&#123;k=1&#125;^&#123;n&#125;\\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class=\"line\">$$\\sum_&#123;k=1&#125;^n\\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class=\"line\">$$\\int_a^b f(x)dx$$</span><br><span class=\"line\">$$\\int_a^b f(x)dx$$</span><br><span class=\"line\">微分符直体：$$\\int_a^b f(x)\\mathrm&#123;d&#125;x$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\sum_{k=1}^{n}\\frac{1}{k}</script><script type=\"math/tex; mode=display\">\\sum_{k=1}^n\\frac{1}{k}</script><script type=\"math/tex; mode=display\">\\int_a^b f(x)dx</script><script type=\"math/tex; mode=display\">\\int_a^b f(x)dx</script><p>微分符直体：<script type=\"math/tex\">\\int_a^b f(x)\\mathrm{d}x</script></p>\n<h3 id=\"5-公式中的空格\"><a href=\"#5-公式中的空格\" class=\"headerlink\" title=\"5.公式中的空格\"></a>5.公式中的空格</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">紧贴 $$a\\!b$$</span><br><span class=\"line\">没有空格 $$ab$$</span><br><span class=\"line\">小空格 $$a\\,b$$</span><br><span class=\"line\">中等空格 $$a\\;b$$</span><br><span class=\"line\">大空格 $$a\\ b$$</span><br><span class=\"line\">quad空格 $$a\\quad b$$</span><br><span class=\"line\">两个quad空格 $$a\\qquad b$$</span><br></pre></td></tr></table></figure>\n<p>紧贴 <script type=\"math/tex\">a\\!b</script>\n没有空格 <script type=\"math/tex\">ab</script>\n小空格 <script type=\"math/tex\">a\\,b</script>\n中等空格 <script type=\"math/tex\">a\\;b</script>\n大空格 <script type=\"math/tex\">a\\ b</script>\nquad空格 <script type=\"math/tex\">a\\quad b</script>\n两个quad空格 <script type=\"math/tex\">a\\qquad b</script></p>\n<h3 id=\"6-公式中的定界符\"><a href=\"#6-公式中的定界符\" class=\"headerlink\" title=\"6.公式中的定界符\"></a>6.公式中的定界符</h3><p>这里所谓的定界符是指包围或分割公式的一些符号</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$(    %($$</span><br><span class=\"line\">$$)    %)$$</span><br><span class=\"line\">$$[    %[$$</span><br><span class=\"line\">$$]    %]$$</span><br><span class=\"line\">$$\\&#123;    %&#123;$$</span><br><span class=\"line\">$$\\&#125;    %&#125;$$</span><br><span class=\"line\">$$|    %|$$</span><br><span class=\"line\">$$\\|    %||$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">(    %(</script><script type=\"math/tex; mode=display\">)    %)</script><script type=\"math/tex; mode=display\">[    %[</script><script type=\"math/tex; mode=display\">]    %]</script><script type=\"math/tex; mode=display\">\\{    %{</script><script type=\"math/tex; mode=display\">\\}    %}</script><script type=\"math/tex; mode=display\">|    %|</script><script type=\"math/tex; mode=display\">\\|    %||</script><p>在上述这些定界符之前冠以 \\left（修饰左定界符）或 \\right（修饰右定界符），可以得到自适应缩放的定界符，它们会根据定界符所包围的公式大小自适应缩放。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\left(\\sum_&#123;k=\\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\\frac&#123;1&#125;&#123;k&#125;\\right)$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)</script><h3 id=\"7-矩阵\"><a href=\"#7-矩阵\" class=\"headerlink\" title=\"7.矩阵\"></a>7.矩阵</h3><p>对于少于 10 列的矩阵，可使用 matrix，pmatrix，bmatrix，Bmatrix，vmatrix 和 Vmatrix 等环境。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;matrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;pmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;bmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;Bmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;vmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;Vmatrix&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}</script><script type=\"math/tex; mode=display\">\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}</script><script type=\"math/tex; mode=display\">\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}</script><script type=\"math/tex; mode=display\">\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}</script><script type=\"math/tex; mode=display\">\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}</script><h3 id=\"8-排版数组\"><a href=\"#8-排版数组\" class=\"headerlink\" title=\"8.排版数组\"></a>8.排版数组</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">\\mathbf&#123;X&#125; =</span><br><span class=\"line\">\\left( \\begin&#123;array&#125;&#123;ccc&#125;</span><br><span class=\"line\">x_&#123;11&#125; &amp; x_&#123;12&#125; &amp; \\ldots \\\\</span><br><span class=\"line\">x_&#123;21&#125; &amp; x_&#123;22&#125; &amp; \\ldots \\\\</span><br><span class=\"line\">\\vdots &amp; \\vdots &amp; \\ddots</span><br><span class=\"line\">\\end&#123;array&#125; \\right)</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)</script><p>\\mathbf大写控制符，\\表示换行，{ccc}表示列样式。array 环境也可以用来排版这样的表达式，表达式中使用一个“.” 作为其隐藏的\\right 定界符</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">y = \\left\\&#123; \\begin&#123;array&#125;&#123;ll&#125;</span><br><span class=\"line\">a &amp; \\textrm&#123;if $d&gt;c$&#125;\\\\</span><br><span class=\"line\">b+x &amp; \\textrm&#123;in the morning&#125;\\\\</span><br><span class=\"line\">l &amp; \\textrm&#123;all day long&#125;</span><br><span class=\"line\">\\end&#123;array&#125; \\right.</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<p>你也可以在array 环境中画线，如分隔矩阵中元素。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">\\left(\\begin&#123;array&#125;&#123;c|c&#125;</span><br><span class=\"line\">1 &amp; 2 \\\\</span><br><span class=\"line\">\\hline</span><br><span class=\"line\">3 &amp; 4</span><br><span class=\"line\">\\end&#123;array&#125;\\right)</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)</script><h3 id=\"9-数学模式重音符号\"><a href=\"#9-数学模式重音符号\" class=\"headerlink\" title=\"9.数学模式重音符号\"></a>9.数学模式重音符号</h3><p><img src=\"/posts_res/2017-12-12-latex/9.png\" alt=\"数学模式重音符号\"></p>\n<h3 id=\"10-希腊字母\"><a href=\"#10-希腊字母\" class=\"headerlink\" title=\"10.希腊字母\"></a>10.希腊字母</h3><p><img src=\"/posts_res/2017-12-12-latex/10.png\" alt=\"希腊字母\"></p>\n<h3 id=\"11-二元关系\"><a href=\"#11-二元关系\" class=\"headerlink\" title=\"11.二元关系\"></a>11.二元关系</h3><p>可以在下列符号的相应命令前加上 \\not 命令，而得到其否定形式。</p>\n<p><img src=\"/posts_res/2017-12-12-latex/11-1.png\" alt=\"二元关系-1\"></p>\n<p><img src=\"/posts_res/2017-12-12-latex/11-2.png\" alt=\"二元关系-2\"></p>\n<h3 id=\"12-“大”运算符\"><a href=\"#12-“大”运算符\" class=\"headerlink\" title=\"12.“大”运算符\"></a>12.“大”运算符</h3><p><img src=\"/posts_res/2017-12-12-latex/12.png\" alt=\"大运算符\"></p>\n<h3 id=\"13-箭头\"><a href=\"#13-箭头\" class=\"headerlink\" title=\"13.箭头\"></a>13.箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/13.png\" alt=\"箭头\"></p>\n<h3 id=\"14-定界符\"><a href=\"#14-定界符\" class=\"headerlink\" title=\"14.定界符\"></a>14.定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/14.png\" alt=\"定界符\"></p>\n<h3 id=\"15-大定界符\"><a href=\"#15-大定界符\" class=\"headerlink\" title=\"15.大定界符\"></a>15.大定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/15.png\" alt=\"大定界符\"></p>\n<h3 id=\"16-其他符号\"><a href=\"#16-其他符号\" class=\"headerlink\" title=\"16.其他符号\"></a>16.其他符号</h3><p><img src=\"/posts_res/2017-12-12-latex/16.png\" alt=\"其他符号\"></p>\n<h3 id=\"17-非数学符号\"><a href=\"#17-非数学符号\" class=\"headerlink\" title=\"17.非数学符号\"></a>17.非数学符号</h3><p><img src=\"/posts_res/2017-12-12-latex/17.png\" alt=\"非数学符号\"></p>\n<h3 id=\"18-AMS定界符\"><a href=\"#18-AMS定界符\" class=\"headerlink\" title=\"18.AMS定界符\"></a>18.AMS定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/18.png\" alt=\"AMS定界符\"></p>\n<h3 id=\"19-AMS希腊和希伯来字母\"><a href=\"#19-AMS希腊和希伯来字母\" class=\"headerlink\" title=\"19.AMS希腊和希伯来字母\"></a>19.AMS希腊和希伯来字母</h3><p><img src=\"/posts_res/2017-12-12-latex/19.png\" alt=\"AMS希腊和希伯来字母\"></p>\n<h3 id=\"20-AMS二元关系\"><a href=\"#20-AMS二元关系\" class=\"headerlink\" title=\"20.AMS二元关系\"></a>20.AMS二元关系</h3><p><img src=\"/posts_res/2017-12-12-latex/20.png\" alt=\"AMS二元关系\"></p>\n<h3 id=\"21-AMS箭头\"><a href=\"#21-AMS箭头\" class=\"headerlink\" title=\"21.AMS箭头\"></a>21.AMS箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/21.png\" alt=\"AMS箭头\"></p>\n<h3 id=\"22-AMS二元否定关系符和箭头\"><a href=\"#22-AMS二元否定关系符和箭头\" class=\"headerlink\" title=\"22.AMS二元否定关系符和箭头\"></a>22.AMS二元否定关系符和箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/22.png\" alt=\"AMS二元否定关系符和箭头\"></p>\n<h3 id=\"23-AMS二元运算符\"><a href=\"#23-AMS二元运算符\" class=\"headerlink\" title=\"23.AMS二元运算符\"></a>23.AMS二元运算符</h3><p><img src=\"/posts_res/2017-12-12-latex/23.png\" alt=\"AMS二元运算符\"></p>\n<h3 id=\"24-AMS其他符号\"><a href=\"#24-AMS其他符号\" class=\"headerlink\" title=\"24.AMS其他符号\"></a>24.AMS其他符号</h3><p><img src=\"/posts_res/2017-12-12-latex/24.png\" alt=\"AMS其他符号\"></p>\n<h3 id=\"25-数学字母\"><a href=\"#25-数学字母\" class=\"headerlink\" title=\"25.数学字母\"></a>25.数学字母</h3><p><img src=\"/posts_res/2017-12-12-latex/25.png\" alt=\"数学字母\"></p>\n<hr>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/houkai/p/3399646.html\" target=\"_blank\" rel=\"noopener\">LATEX数学公式基本语法</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>KaTex用法如下：</p><blockquote>\n<p>KATEX: <a href=\"https://katex.org/docs/supported.html\" target=\"_blank\" rel=\"noopener\">https://katex.org/docs/supported.html</a></p>\n</blockquote><p>LaTex用法如下：</p><p>目录</p><ul>\n<li>1.上标与下标</li>\n<li>2.分式</li>\n<li>3.根式</li>\n<li>4.求和与积分</li>\n<li>5.公式中的空格</li>\n<li>6.公式中的定界符</li>\n<li>7.矩阵</li>\n<li>8.排版数组</li>\n<li>9.数学模式重音符号</li>\n<li>10.希腊字母</li>\n<li>11.二元关系</li>\n<li>12.“大”运算符</li>\n<li>13.箭头</li>\n<li>14.定界符</li>\n<li>15.大定界符</li>\n<li>16.其他符号</li>\n<li>17.非数学符号</li>\n<li>18.AMS定界符</li>\n<li>19.AMS希腊和希伯来字母</li>\n<li>20.AMS二元关系</li>\n<li>21.AMS箭头</li>\n<li>22.AMS二元否定关系符和箭头</li>\n<li>23.AMS二元运算符</li>\n<li>24.AMS其他符号</li>\n<li>25.数学字母</li>\n</ul><hr><h3 id=\"1-上标与下标\"><a href=\"#1-上标与下标\" class=\"headerlink\" title=\"1.上标与下标\"></a>1.上标与下标</h3>","more":"\n\n\n\n\n\n<p>上标命令是 ^{角标}，下标命令是 _{角标}。当角标是单个字符时可以不用花括号(在 LaTeX 中，花括号是用于分组，即花括号内部文本为一组)。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$x_1$$</span><br><span class=\"line\">$$x_1^2$$</span><br><span class=\"line\">$$x_&#123;22&#125;^&#123;(n)&#125;$$</span><br><span class=\"line\">$$&#123;&#125;^*\\!x^*$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">x_1</script><script type=\"math/tex; mode=display\">x_1^2</script><script type=\"math/tex; mode=display\">x_{22}^{(n)}</script><script type=\"math/tex; mode=display\">{}^*\\!x^*</script><h3 id=\"2-分式\"><a href=\"#2-分式\" class=\"headerlink\" title=\"2.分式\"></a>2.分式</h3><p>输入带有水平分数线的公式，可用命令：\\frac{分子}{分母}；</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;x+y&#125;&#123;2&#125;$$</span><br><span class=\"line\">$$\\frac&#123;1&#125;&#123;1+\\frac&#123;1&#125;&#123;2&#125;&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\frac{x+y}{2}</script><script type=\"math/tex; mode=display\">\\frac{1}{1+\\frac{1}{2}}</script><h3 id=\"3-根式\"><a href=\"#3-根式\" class=\"headerlink\" title=\"3.根式\"></a>3.根式</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\sqrt&#123;2&#125;&lt;\\sqrt[3]&#123;3&#125;$$</span><br><span class=\"line\">$$\\sqrt&#123;1+\\sqrt[p]&#123;1+a^2&#125;&#125;$$</span><br><span class=\"line\">$$\\sqrt&#123;1+\\sqrt[^p\\!]&#123;1+a^2&#125;&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\sqrt{2}<\\sqrt[3]{3}</script><script type=\"math/tex; mode=display\">\\sqrt{1+\\sqrt[p]{1+a^2}}</script><script type=\"math/tex; mode=display\">\\sqrt{1+\\sqrt[^p\\!]{1+a^2}}</script><h3 id=\"4-求和与积分\"><a href=\"#4-求和与积分\" class=\"headerlink\" title=\"4.求和与积分\"></a>4.求和与积分</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\sum_&#123;k=1&#125;^&#123;n&#125;\\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class=\"line\">$$\\sum_&#123;k=1&#125;^n\\frac&#123;1&#125;&#123;k&#125;$$</span><br><span class=\"line\">$$\\int_a^b f(x)dx$$</span><br><span class=\"line\">$$\\int_a^b f(x)dx$$</span><br><span class=\"line\">微分符直体：$$\\int_a^b f(x)\\mathrm&#123;d&#125;x$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\sum_{k=1}^{n}\\frac{1}{k}</script><script type=\"math/tex; mode=display\">\\sum_{k=1}^n\\frac{1}{k}</script><script type=\"math/tex; mode=display\">\\int_a^b f(x)dx</script><script type=\"math/tex; mode=display\">\\int_a^b f(x)dx</script><p>微分符直体：<script type=\"math/tex\">\\int_a^b f(x)\\mathrm{d}x</script></p>\n<h3 id=\"5-公式中的空格\"><a href=\"#5-公式中的空格\" class=\"headerlink\" title=\"5.公式中的空格\"></a>5.公式中的空格</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">紧贴 $$a\\!b$$</span><br><span class=\"line\">没有空格 $$ab$$</span><br><span class=\"line\">小空格 $$a\\,b$$</span><br><span class=\"line\">中等空格 $$a\\;b$$</span><br><span class=\"line\">大空格 $$a\\ b$$</span><br><span class=\"line\">quad空格 $$a\\quad b$$</span><br><span class=\"line\">两个quad空格 $$a\\qquad b$$</span><br></pre></td></tr></table></figure>\n<p>紧贴 <script type=\"math/tex\">a\\!b</script>\n没有空格 <script type=\"math/tex\">ab</script>\n小空格 <script type=\"math/tex\">a\\,b</script>\n中等空格 <script type=\"math/tex\">a\\;b</script>\n大空格 <script type=\"math/tex\">a\\ b</script>\nquad空格 <script type=\"math/tex\">a\\quad b</script>\n两个quad空格 <script type=\"math/tex\">a\\qquad b</script></p>\n<h3 id=\"6-公式中的定界符\"><a href=\"#6-公式中的定界符\" class=\"headerlink\" title=\"6.公式中的定界符\"></a>6.公式中的定界符</h3><p>这里所谓的定界符是指包围或分割公式的一些符号</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$(    %($$</span><br><span class=\"line\">$$)    %)$$</span><br><span class=\"line\">$$[    %[$$</span><br><span class=\"line\">$$]    %]$$</span><br><span class=\"line\">$$\\&#123;    %&#123;$$</span><br><span class=\"line\">$$\\&#125;    %&#125;$$</span><br><span class=\"line\">$$|    %|$$</span><br><span class=\"line\">$$\\|    %||$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">(    %(</script><script type=\"math/tex; mode=display\">)    %)</script><script type=\"math/tex; mode=display\">[    %[</script><script type=\"math/tex; mode=display\">]    %]</script><script type=\"math/tex; mode=display\">\\{    %{</script><script type=\"math/tex; mode=display\">\\}    %}</script><script type=\"math/tex; mode=display\">|    %|</script><script type=\"math/tex; mode=display\">\\|    %||</script><p>在上述这些定界符之前冠以 \\left（修饰左定界符）或 \\right（修饰右定界符），可以得到自适应缩放的定界符，它们会根据定界符所包围的公式大小自适应缩放。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\left(\\sum_&#123;k=\\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\\frac&#123;1&#125;&#123;k&#125;\\right)$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\left(\\sum_{k=\\frac{1}{2}}^{N^2}\\frac{1}{k}\\right)</script><h3 id=\"7-矩阵\"><a href=\"#7-矩阵\" class=\"headerlink\" title=\"7.矩阵\"></a>7.矩阵</h3><p>对于少于 10 列的矩阵，可使用 matrix，pmatrix，bmatrix，Bmatrix，vmatrix 和 Vmatrix 等环境。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;matrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;pmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;bmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;Bmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;vmatrix&#125;$$</span><br><span class=\"line\">$$\\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\\end&#123;Vmatrix&#125;$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\\begin{matrix}1 & 2\\\\3 &4\\end{matrix}</script><script type=\"math/tex; mode=display\">\\begin{pmatrix}1 & 2\\\\3 &4\\end{pmatrix}</script><script type=\"math/tex; mode=display\">\\begin{bmatrix}1 & 2\\\\3 &4\\end{bmatrix}</script><script type=\"math/tex; mode=display\">\\begin{Bmatrix}1 & 2\\\\3 &4\\end{Bmatrix}</script><script type=\"math/tex; mode=display\">\\begin{vmatrix}1 & 2\\\\3 &4\\end{vmatrix}</script><script type=\"math/tex; mode=display\">\\begin{Vmatrix}1 & 2\\\\3 &4\\end{Vmatrix}</script><h3 id=\"8-排版数组\"><a href=\"#8-排版数组\" class=\"headerlink\" title=\"8.排版数组\"></a>8.排版数组</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">\\mathbf&#123;X&#125; =</span><br><span class=\"line\">\\left( \\begin&#123;array&#125;&#123;ccc&#125;</span><br><span class=\"line\">x_&#123;11&#125; &amp; x_&#123;12&#125; &amp; \\ldots \\\\</span><br><span class=\"line\">x_&#123;21&#125; &amp; x_&#123;22&#125; &amp; \\ldots \\\\</span><br><span class=\"line\">\\vdots &amp; \\vdots &amp; \\ddots</span><br><span class=\"line\">\\end&#123;array&#125; \\right)</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\mathbf{X} =\n\\left( \\begin{array}{ccc}\nx_{11} & x_{12} & \\ldots \\\\\nx_{21} & x_{22} & \\ldots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{array} \\right)</script><p>\\mathbf大写控制符，\\表示换行，{ccc}表示列样式。array 环境也可以用来排版这样的表达式，表达式中使用一个“.” 作为其隐藏的\\right 定界符</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">y = \\left\\&#123; \\begin&#123;array&#125;&#123;ll&#125;</span><br><span class=\"line\">a &amp; \\textrm&#123;if $d&gt;c$&#125;\\\\</span><br><span class=\"line\">b+x &amp; \\textrm&#123;in the morning&#125;\\\\</span><br><span class=\"line\">l &amp; \\textrm&#123;all day long&#125;</span><br><span class=\"line\">\\end&#123;array&#125; \\right.</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<p>你也可以在array 环境中画线，如分隔矩阵中元素。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$</span><br><span class=\"line\">\\left(\\begin&#123;array&#125;&#123;c|c&#125;</span><br><span class=\"line\">1 &amp; 2 \\\\</span><br><span class=\"line\">\\hline</span><br><span class=\"line\">3 &amp; 4</span><br><span class=\"line\">\\end&#123;array&#125;\\right)</span><br><span class=\"line\">$$</span><br></pre></td></tr></table></figure>\n<script type=\"math/tex; mode=display\">\n\\left(\\begin{array}{c|c}\n1 & 2 \\\\\n\\hline\n3 & 4\n\\end{array}\\right)</script><h3 id=\"9-数学模式重音符号\"><a href=\"#9-数学模式重音符号\" class=\"headerlink\" title=\"9.数学模式重音符号\"></a>9.数学模式重音符号</h3><p><img src=\"/posts_res/2017-12-12-latex/9.png\" alt=\"数学模式重音符号\"></p>\n<h3 id=\"10-希腊字母\"><a href=\"#10-希腊字母\" class=\"headerlink\" title=\"10.希腊字母\"></a>10.希腊字母</h3><p><img src=\"/posts_res/2017-12-12-latex/10.png\" alt=\"希腊字母\"></p>\n<h3 id=\"11-二元关系\"><a href=\"#11-二元关系\" class=\"headerlink\" title=\"11.二元关系\"></a>11.二元关系</h3><p>可以在下列符号的相应命令前加上 \\not 命令，而得到其否定形式。</p>\n<p><img src=\"/posts_res/2017-12-12-latex/11-1.png\" alt=\"二元关系-1\"></p>\n<p><img src=\"/posts_res/2017-12-12-latex/11-2.png\" alt=\"二元关系-2\"></p>\n<h3 id=\"12-“大”运算符\"><a href=\"#12-“大”运算符\" class=\"headerlink\" title=\"12.“大”运算符\"></a>12.“大”运算符</h3><p><img src=\"/posts_res/2017-12-12-latex/12.png\" alt=\"大运算符\"></p>\n<h3 id=\"13-箭头\"><a href=\"#13-箭头\" class=\"headerlink\" title=\"13.箭头\"></a>13.箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/13.png\" alt=\"箭头\"></p>\n<h3 id=\"14-定界符\"><a href=\"#14-定界符\" class=\"headerlink\" title=\"14.定界符\"></a>14.定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/14.png\" alt=\"定界符\"></p>\n<h3 id=\"15-大定界符\"><a href=\"#15-大定界符\" class=\"headerlink\" title=\"15.大定界符\"></a>15.大定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/15.png\" alt=\"大定界符\"></p>\n<h3 id=\"16-其他符号\"><a href=\"#16-其他符号\" class=\"headerlink\" title=\"16.其他符号\"></a>16.其他符号</h3><p><img src=\"/posts_res/2017-12-12-latex/16.png\" alt=\"其他符号\"></p>\n<h3 id=\"17-非数学符号\"><a href=\"#17-非数学符号\" class=\"headerlink\" title=\"17.非数学符号\"></a>17.非数学符号</h3><p><img src=\"/posts_res/2017-12-12-latex/17.png\" alt=\"非数学符号\"></p>\n<h3 id=\"18-AMS定界符\"><a href=\"#18-AMS定界符\" class=\"headerlink\" title=\"18.AMS定界符\"></a>18.AMS定界符</h3><p><img src=\"/posts_res/2017-12-12-latex/18.png\" alt=\"AMS定界符\"></p>\n<h3 id=\"19-AMS希腊和希伯来字母\"><a href=\"#19-AMS希腊和希伯来字母\" class=\"headerlink\" title=\"19.AMS希腊和希伯来字母\"></a>19.AMS希腊和希伯来字母</h3><p><img src=\"/posts_res/2017-12-12-latex/19.png\" alt=\"AMS希腊和希伯来字母\"></p>\n<h3 id=\"20-AMS二元关系\"><a href=\"#20-AMS二元关系\" class=\"headerlink\" title=\"20.AMS二元关系\"></a>20.AMS二元关系</h3><p><img src=\"/posts_res/2017-12-12-latex/20.png\" alt=\"AMS二元关系\"></p>\n<h3 id=\"21-AMS箭头\"><a href=\"#21-AMS箭头\" class=\"headerlink\" title=\"21.AMS箭头\"></a>21.AMS箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/21.png\" alt=\"AMS箭头\"></p>\n<h3 id=\"22-AMS二元否定关系符和箭头\"><a href=\"#22-AMS二元否定关系符和箭头\" class=\"headerlink\" title=\"22.AMS二元否定关系符和箭头\"></a>22.AMS二元否定关系符和箭头</h3><p><img src=\"/posts_res/2017-12-12-latex/22.png\" alt=\"AMS二元否定关系符和箭头\"></p>\n<h3 id=\"23-AMS二元运算符\"><a href=\"#23-AMS二元运算符\" class=\"headerlink\" title=\"23.AMS二元运算符\"></a>23.AMS二元运算符</h3><p><img src=\"/posts_res/2017-12-12-latex/23.png\" alt=\"AMS二元运算符\"></p>\n<h3 id=\"24-AMS其他符号\"><a href=\"#24-AMS其他符号\" class=\"headerlink\" title=\"24.AMS其他符号\"></a>24.AMS其他符号</h3><p><img src=\"/posts_res/2017-12-12-latex/24.png\" alt=\"AMS其他符号\"></p>\n<h3 id=\"25-数学字母\"><a href=\"#25-数学字母\" class=\"headerlink\" title=\"25.数学字母\"></a>25.数学字母</h3><p><img src=\"/posts_res/2017-12-12-latex/25.png\" alt=\"数学字母\"></p>\n<hr>\n<blockquote>\n<p><a href=\"https://www.cnblogs.com/houkai/p/3399646.html\" target=\"_blank\" rel=\"noopener\">LATEX数学公式基本语法</a></p>\n</blockquote>\n"},{"layout":"post","title":"二分查找 - BinarySearch","date":"2017-12-23T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n### 递归\n\n```cpp\nint binarysearch_recursion(vector<int> arr, int start, int end, int key)\n{\n\tint mid = (end - start) / 2 + start;\n\tif (arr[mid] == key)\n\t\treturn mid;\n\tif (start >= end)\n\t\treturn -1;\n\telse if (key > arr[mid])\n\t\treturn binarysearch_recursion(arr, mid + 1, end, key);\n\telse if (key < arr[mid])\n\t\treturn binarysearch_recursion(arr, start, mid - 1, key);\n\treturn -1;\n}\n```\n\n\n-----------\n\n### 非递归\n\n```cpp\nint binarysearch_nonrecursion(vector<int> arr, int key)\n{\n\tint mid, start = 0, end = arr.size() - 1;\n\twhile (start <= end)\n\t{\n\t\tmid = (end - start) / 2 + start;\n\t\tif (key < arr[mid])\n\t\t\tend = mid - 1;\n\t\telse if (key > arr[mid])\n\t\t\tstart = mid + 1;\n\t\telse\n\t\t\treturn mid;\n\t}\n\treturn -1;\n}\n```\n\n\n----------\n\n### 二分排序\n\n```cpp\n//O(nlogn)\nvoid binarysort(vector<int> &arr)\n{\n\tint start, end, tmp = 0, mid, j;\n\tfor (int i = 1; i < arr.size(); i++)\n\t{\n\t\tstart = 0; \n\t\tend = i - 1; \n\t\ttmp = arr[i];\n\t\twhile (start <= end)\n\t\t{\n\t\t\tmid = (start + end) / 2;\n\t\t\tif (tmp < arr[mid])\n\t\t\t\tend = mid - 1;\n\t\t\telse\n\t\t\t\tstart = mid + 1;\n\t\t}\n\t\tfor (j = i - 1; j >= start; j--)\n\t\t{\n\t\t\tarr[j + 1] = arr[j];\n\t\t}\n\t\tarr[start] = tmp;\n\t}\n}\n```\n","source":"_posts/2017-12-23-binarysearch.md","raw":"---\nlayout: post\ntitle: 二分查找 - BinarySearch\ndate: 2017-12-23 21:10 +0800\ncategories: LeetCode\ntags:\n- 搜索\nmathjax: true\ncopyright: true\n---\n\n\n### 递归\n\n```cpp\nint binarysearch_recursion(vector<int> arr, int start, int end, int key)\n{\n\tint mid = (end - start) / 2 + start;\n\tif (arr[mid] == key)\n\t\treturn mid;\n\tif (start >= end)\n\t\treturn -1;\n\telse if (key > arr[mid])\n\t\treturn binarysearch_recursion(arr, mid + 1, end, key);\n\telse if (key < arr[mid])\n\t\treturn binarysearch_recursion(arr, start, mid - 1, key);\n\treturn -1;\n}\n```\n\n\n-----------\n\n### 非递归\n\n```cpp\nint binarysearch_nonrecursion(vector<int> arr, int key)\n{\n\tint mid, start = 0, end = arr.size() - 1;\n\twhile (start <= end)\n\t{\n\t\tmid = (end - start) / 2 + start;\n\t\tif (key < arr[mid])\n\t\t\tend = mid - 1;\n\t\telse if (key > arr[mid])\n\t\t\tstart = mid + 1;\n\t\telse\n\t\t\treturn mid;\n\t}\n\treturn -1;\n}\n```\n\n\n----------\n\n### 二分排序\n\n```cpp\n//O(nlogn)\nvoid binarysort(vector<int> &arr)\n{\n\tint start, end, tmp = 0, mid, j;\n\tfor (int i = 1; i < arr.size(); i++)\n\t{\n\t\tstart = 0; \n\t\tend = i - 1; \n\t\ttmp = arr[i];\n\t\twhile (start <= end)\n\t\t{\n\t\t\tmid = (start + end) / 2;\n\t\t\tif (tmp < arr[mid])\n\t\t\t\tend = mid - 1;\n\t\t\telse\n\t\t\t\tstart = mid + 1;\n\t\t}\n\t\tfor (j = i - 1; j >= start; j--)\n\t\t{\n\t\t\tarr[j + 1] = arr[j];\n\t\t}\n\t\tarr[start] = tmp;\n\t}\n}\n```\n","slug":"binarysearch","published":1,"updated":"2019-08-17T09:30:33.060Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnup00092qwpgbueci40","content":"<h3 id=\"递归\"><a href=\"#递归\" class=\"headerlink\" title=\"递归\"></a>递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">binarysearch_recursion</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end, <span class=\"keyword\">int</span> key)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid = (end - start) / <span class=\"number\">2</span> + start;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (arr[mid] == key)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> mid;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (start &gt;= end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &gt; arr[mid])</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> binarysearch_recursion(arr, mid + <span class=\"number\">1</span>, end, key);</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &lt; arr[mid])</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> binarysearch_recursion(arr, start, mid - <span class=\"number\">1</span>, key);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure><a id=\"more\"></a>\n<hr>\n<h3 id=\"非递归\"><a href=\"#非递归\" class=\"headerlink\" title=\"非递归\"></a>非递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">binarysearch_nonrecursion</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> key)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid, start = <span class=\"number\">0</span>, end = arr.size() - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (start &lt;= end)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tmid = (end - start) / <span class=\"number\">2</span> + start;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (key &lt; arr[mid])</span><br><span class=\"line\">\t\t\tend = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &gt; arr[mid])</span><br><span class=\"line\">\t\t\tstart = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> mid;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"二分排序\"><a href=\"#二分排序\" class=\"headerlink\" title=\"二分排序\"></a>二分排序</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//O(nlogn)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">binarysort</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;arr)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> start, end, tmp = <span class=\"number\">0</span>, mid, j;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; arr.size(); i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tstart = <span class=\"number\">0</span>; </span><br><span class=\"line\">\t\tend = i - <span class=\"number\">1</span>; </span><br><span class=\"line\">\t\ttmp = arr[i];</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (start &lt;= end)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmid = (start + end) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (tmp &lt; arr[mid])</span><br><span class=\"line\">\t\t\t\tend = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\t\tstart = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (j = i - <span class=\"number\">1</span>; j &gt;= start; j--)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tarr[j + <span class=\"number\">1</span>] = arr[j];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tarr[start] = tmp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h3 id=\"递归\"><a href=\"#递归\" class=\"headerlink\" title=\"递归\"></a>递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">binarysearch_recursion</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> start, <span class=\"keyword\">int</span> end, <span class=\"keyword\">int</span> key)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid = (end - start) / <span class=\"number\">2</span> + start;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (arr[mid] == key)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> mid;</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (start &gt;= end)</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &gt; arr[mid])</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> binarysearch_recursion(arr, mid + <span class=\"number\">1</span>, end, key);</span><br><span class=\"line\">\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &lt; arr[mid])</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> binarysearch_recursion(arr, start, mid - <span class=\"number\">1</span>, key);</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"\n<hr>\n<h3 id=\"非递归\"><a href=\"#非递归\" class=\"headerlink\" title=\"非递归\"></a>非递归</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">binarysearch_nonrecursion</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; arr, <span class=\"keyword\">int</span> key)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> mid, start = <span class=\"number\">0</span>, end = arr.size() - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (start &lt;= end)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tmid = (end - start) / <span class=\"number\">2</span> + start;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (key &lt; arr[mid])</span><br><span class=\"line\">\t\t\tend = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (key &gt; arr[mid])</span><br><span class=\"line\">\t\t\tstart = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> mid;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"二分排序\"><a href=\"#二分排序\" class=\"headerlink\" title=\"二分排序\"></a>二分排序</h3><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//O(nlogn)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">binarysort</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; &amp;arr)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> start, end, tmp = <span class=\"number\">0</span>, mid, j;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; arr.size(); i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tstart = <span class=\"number\">0</span>; </span><br><span class=\"line\">\t\tend = i - <span class=\"number\">1</span>; </span><br><span class=\"line\">\t\ttmp = arr[i];</span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (start &lt;= end)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmid = (start + end) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">if</span> (tmp &lt; arr[mid])</span><br><span class=\"line\">\t\t\t\tend = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t\t<span class=\"keyword\">else</span></span><br><span class=\"line\">\t\t\t\tstart = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">for</span> (j = i - <span class=\"number\">1</span>; j &gt;= start; j--)</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tarr[j + <span class=\"number\">1</span>] = arr[j];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\tarr[start] = tmp;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","title":"朴素贝叶斯 Naive Bayes","date":"2018-03-08T02:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>朴素贝叶斯 - Naive Bayes</center>\n\n#### 目录\n* 基本思想\n* 算法\n* 后验概率最大化的意义\n* 特点\n* 代码实现\n\n\n------\n\n### 1. 基本思想\n\n朴素贝叶斯法通过训练数据集学习联合概率分布\\\\( P(X,Y) \\\\)。具体地，学习以下先验概率分布及条件概率分布。\n\n先验概率分布：\n\n\\\\[P(Y=c_k),\\quad k=1,2,...,K\\\\]\n\n条件概率分布: \n\\\\[P(X=x | Y=c\\_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\\_k),\\quad k=1,2,...,K\\\\]\n\n于是通过贝叶斯公式\\\\(P(X,Y) = P(X | Y)\\cdot P(Y)\\\\)，\n可以学习到联合概率分布\\\\(P(X,Y)\\\\)。\n\n**但是，条件概率分布\\\\(P(X=x \\| Y=c\\_k)\\\\)有指数级数量的参数，其估计实际是不可行的。事实上，假设\\\\(x^{(j)}\\\\) 可取值有\\\\(S\\_j\\\\)个，\\\\(j=1,2,...,n\\\\)，\\\\(Y\\\\)可取值有\\\\(K\\\\)个，那么参数个数为$K\\prod_{j=1}^nS\\_j$。**\n\n朴素贝叶斯法对条件概率分布作了条件独立性的假设，即假设特征之间相互独立，公式表达如下：\n\\\\[P(X=x | Y=c\\_k) \\quad = \\quad P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\\_k) \\quad = \\quad \\prod\\_{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c\\_k)\\\\]\n\n\n-----------\n\n### 2. 算法\n\n输入：\n\n训练数据\\\\( T={((x_1, y_1)),...,(x\\_N,y\\_N)} \\\\)，其中\\\\( x\\_i=(x\\_i^{(1)},x\\_i^{(2)},...,x\\_i^{(n)})^T \\\\),\\\\( x\\_i^{(j)} \\\\)是第\\\\( i \\\\)个样本的第\\\\( j \\\\)个特征，\\\\( x\\_i^{(j)}\\in {a\\_{j1},a\\_{j2},...,a\\_{jS\\_j}} \\\\), \\\\( a\\_{jl} \\\\)是第\\\\( j \\\\)个特征可能取的第\\\\( l \\\\)个值，\\\\( j=1,2,...,n \\quad l=1,2,...,S\\_j, \\quad y\\_i \\in {c\\_1, c\\_2, ..., c\\_k} \\\\); 实例\\\\( x \\\\)\n\n输出：\n\n实例\\\\( x \\\\)的分类\n\n（1）计算先验概率及条件概率\n\n\\\\[P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c\\_k)}{N}, \\quad k=1,2,...,K\\\\]\n\n\\\\[P(X^{(j)}=a_{jl} \\| Y=c_k)=\\frac{\\sum_{i=1}^N I(x_i^{(j)}, y_i=c_k)}{\\sum_{i=1}^N I(y_i=c\\_k)}\\\\]\n\n\\\\[j=1,2,...,n; \\quad l=1,2,...,S_j; \\quad k=1,2,...,K\\\\]\n\n（2）对于给定的实例\\\\( x=(x^{(1)}, x^{(2)},.., x^{(n)})^T \\\\)，计算\n\n\\\\[P(Y=c_k) \\prod_{j=1}^n P(X^{(j)}=x^{(j)} \\| Y=c_k), \\quad k=1,2,...,K\\\\]\n\n（3）确定实例\\\\( x \\\\)的类\n\\\\[ y = \\mathop{\\arg\\max}\\_{c\\_k} P(Y=c\\_k) \\prod\\_{j=1}^n P(X^{(j)} \\| Y=c\\_k)\\\\]\n\n\n------\n\n### 3. 后验概率最大化的意义\n\n朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：\n\\\\[ L(Y,f(X)) = \\begin{cases}\n1, & Y \\neq f(X) \\\\\\\n0, & Y = f(X)\n\\end{cases} \\\\]\n\n式中\\\\( f(X) \\\\)是决策函数，这时期望风险函数为\n\\\\[ R\\_{exp}(f)=E[L(Y,f(X)] \\\\]\n\n期望是对联合分布 \\\\( P(X,Y) \\\\)取的，由此取条件期望\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nR_{exp}(f)\n& = \\iint_{D_{xy}}L(y,f(x)) \\cdot P(x,y) dxdy \\\\\\\n& = \\int_{D_x} \\int_{D_y} L(y, f(x)) \\cdot P(y\\|x) \\cdot P(x) dxdy \\\\\\\n& = \\int_{D_x} [\\int_{D_y} L(x,f(x)) \\cdot P(y\\|x) dy] P(x) dx \\\\\\\n& = E\\_X \\sum\\_{k=1}^K [L(c\\_k, f(X)]P(c\\_k|X)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n为了使期望风险最小化，只需要对 \\\\( X=x \\\\)逐个极小化，由此得到：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} \\sum_{k=1}^K L(c_k,y)P(c_k|X=x) \\\\\\\n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} \\sum_{k=1}^K P(y \\neq c_k| X=x) \\\\\\\n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} (1-P(y=c_k|X=x) \\\\\\\n& = \\mathop{\\arg\\max}\\_{y\\in \\mathbb{y}} P(y=c_k|X=x)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n这样，根据期望奉献最小化准则就得到了后验概率最大化准则：\n\\\\[ \nf(x)=\\mathop{\\arg\\max}\\_{y=c_k} P(c_k|X=x) \n\\\\]\n\n即朴素贝叶斯法采用的原理。\n\n\n------\n\n### 4. 特点\n\n* 优点：源于古典数学理论，有稳定的分类效率；小数据好，可增量训练；对缺失数据不敏感，算法简单。\n* 缺点：独立性假设强，实际中根据属性间的关系强弱，效果不同；须知先验概率，但先验有时取决于假设；对数据的输入形式敏感。\n\n\n------\n\n### 5. 代码实现\n\n[Naive Bayes 代码](/posts_res/2018-03-08-native-bayes/native_bayes.py)\n    \n    # coding:utf-8\n    \n    import numpy as np\n    import time\n    from datetime import timedelta\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    \n    \n    def time_consume(s_t):\n        diff = time.time()-s_t\n        return timedelta(seconds=int(diff))\n    \n    \n    class NaiveBayes(object):\n        def __init__(self, class_num=-1, features_dim=-1):\n            self.class_num = class_num\n            self.features_dim = features_dim\n    \n        def fit(self, X, y):\n            self.nlen = X.shape[0]\n            # shape meaning: class_num\n            prior_prob = np.zeros(shape=self.class_num)\n            # shape meaning: class_num, feature_dim, feature_value\n            conditional_prob = np.zeros(shape=(self.class_num, self.features_dim, 2))\n    \n            for i in range(self.nlen):\n                prior_prob[y[i]] += 1.0\n                for j in range(self.features_dim):\n                    conditional_prob[y[i]][j][X[i][j]] += 1.0\n    \n            for i in range(self.class_num):\n                for j in range(self.features_dim):\n                    p_0 = conditional_prob[i][j][0]\n                    p_1 = conditional_prob[i][j][1]\n                    prob_0 = p_0 / (p_0 + p_1)\n                    prob_1 = p_1 / (p_0 + p_1)\n                    conditional_prob[i][j][0] = prob_0\n                    conditional_prob[i][j][1] = prob_1\n    \n            self.prior_prob = prior_prob # 没有除self.nlen，对结果无影响\n            self.conditional_prob = conditional_prob\n    \n        def __calculate_prob__(self, sample, label):\n            prob = int(self.prior_prob[label])\n            for i in range(self.features_dim):\n                prob *= self.conditional_prob[label][i][sample[i]]\n            return prob\n    \n        def predict(self, X):\n            if self.class_num == -1:\n                raise ValueError(\"Please fit first.\")\n            y_pred = np.zeros(shape=X.shape[0])\n            for i in range(X.shape[0]):\n                label = 0\n                prob = self.__calculate_prob__(X[i], 0)\n                for j in range(1, self.class_num):\n                    this_prob = self.__calculate_prob__(X[i], j)\n                    if prob < this_prob:\n                        prob = this_prob\n                        label = j\n                y_pred[i] = label\n            return y_pred\n    \n        def accuracy(self, y_true, y_pred):\n            return accuracy_score(y_true, y_pred)\n    \n    \n    def load_data():\n        \"\"\"normalize value of data to {0, 1}\"\"\"\n        data, target = load_breast_cancer(return_X_y=True)\n        nlen = data.shape[0]\n        ndim = data.shape[1]\n        X = np.zeros(shape=data.shape, dtype=np.int32)\n        for i in range(ndim):\n            mean = np.mean(data[:, i])\n            for j in range(nlen):\n                if data[j][i] > mean:\n                    X[j][i] = 1\n        return X, target\n    \n    \n    if __name__ == '__main__':\n        start_time = time.time()\n    \n        data, target = load_data()\n        train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=2048, shuffle=True)\n    \n        ml = NaiveBayes(class_num=2, features_dim=30)\n        ml.fit(train_x, train_y)\n        y_pred = ml.predict(test_x)\n        accuracy = ml.accuracy(test_y, y_pred)\n        print(\"Accuracy is \", accuracy)\n        print(\"Timeusage: %s\" % (time_consume(start_time)))\n\n\n结果如下：\n\n    Accuracy is  0.947368421053\n    Timeusage: 0:00:00\n\n\n------\n\n### 6. 参考\n\n> 1. 李航 - 《统计学习方法》\n> 2. [刘建平Pinard's Blog](http://www.cnblogs.com/pinard/p/6069267.html)\n> 3. [scikit-learn](http://scikit-learn.org/)\n> 4. [WenDesi's Github](https://github.com/WenDesi/lihang_book_algorithm)\n","source":"_posts/2018-03-08-naive-bayes.md","raw":"---\nlayout: post\ntitle: 朴素贝叶斯 Naive Bayes\ndate: 2018-03-08 10:10 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>朴素贝叶斯 - Naive Bayes</center>\n\n#### 目录\n* 基本思想\n* 算法\n* 后验概率最大化的意义\n* 特点\n* 代码实现\n\n\n------\n\n### 1. 基本思想\n\n朴素贝叶斯法通过训练数据集学习联合概率分布\\\\( P(X,Y) \\\\)。具体地，学习以下先验概率分布及条件概率分布。\n\n先验概率分布：\n\n\\\\[P(Y=c_k),\\quad k=1,2,...,K\\\\]\n\n条件概率分布: \n\\\\[P(X=x | Y=c\\_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\\_k),\\quad k=1,2,...,K\\\\]\n\n于是通过贝叶斯公式\\\\(P(X,Y) = P(X | Y)\\cdot P(Y)\\\\)，\n可以学习到联合概率分布\\\\(P(X,Y)\\\\)。\n\n**但是，条件概率分布\\\\(P(X=x \\| Y=c\\_k)\\\\)有指数级数量的参数，其估计实际是不可行的。事实上，假设\\\\(x^{(j)}\\\\) 可取值有\\\\(S\\_j\\\\)个，\\\\(j=1,2,...,n\\\\)，\\\\(Y\\\\)可取值有\\\\(K\\\\)个，那么参数个数为$K\\prod_{j=1}^nS\\_j$。**\n\n朴素贝叶斯法对条件概率分布作了条件独立性的假设，即假设特征之间相互独立，公式表达如下：\n\\\\[P(X=x | Y=c\\_k) \\quad = \\quad P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)} | Y=c\\_k) \\quad = \\quad \\prod\\_{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c\\_k)\\\\]\n\n\n-----------\n\n### 2. 算法\n\n输入：\n\n训练数据\\\\( T={((x_1, y_1)),...,(x\\_N,y\\_N)} \\\\)，其中\\\\( x\\_i=(x\\_i^{(1)},x\\_i^{(2)},...,x\\_i^{(n)})^T \\\\),\\\\( x\\_i^{(j)} \\\\)是第\\\\( i \\\\)个样本的第\\\\( j \\\\)个特征，\\\\( x\\_i^{(j)}\\in {a\\_{j1},a\\_{j2},...,a\\_{jS\\_j}} \\\\), \\\\( a\\_{jl} \\\\)是第\\\\( j \\\\)个特征可能取的第\\\\( l \\\\)个值，\\\\( j=1,2,...,n \\quad l=1,2,...,S\\_j, \\quad y\\_i \\in {c\\_1, c\\_2, ..., c\\_k} \\\\); 实例\\\\( x \\\\)\n\n输出：\n\n实例\\\\( x \\\\)的分类\n\n（1）计算先验概率及条件概率\n\n\\\\[P(Y=c_k) = \\frac{\\sum_{i=1}^N I(y_i=c\\_k)}{N}, \\quad k=1,2,...,K\\\\]\n\n\\\\[P(X^{(j)}=a_{jl} \\| Y=c_k)=\\frac{\\sum_{i=1}^N I(x_i^{(j)}, y_i=c_k)}{\\sum_{i=1}^N I(y_i=c\\_k)}\\\\]\n\n\\\\[j=1,2,...,n; \\quad l=1,2,...,S_j; \\quad k=1,2,...,K\\\\]\n\n（2）对于给定的实例\\\\( x=(x^{(1)}, x^{(2)},.., x^{(n)})^T \\\\)，计算\n\n\\\\[P(Y=c_k) \\prod_{j=1}^n P(X^{(j)}=x^{(j)} \\| Y=c_k), \\quad k=1,2,...,K\\\\]\n\n（3）确定实例\\\\( x \\\\)的类\n\\\\[ y = \\mathop{\\arg\\max}\\_{c\\_k} P(Y=c\\_k) \\prod\\_{j=1}^n P(X^{(j)} \\| Y=c\\_k)\\\\]\n\n\n------\n\n### 3. 后验概率最大化的意义\n\n朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：\n\\\\[ L(Y,f(X)) = \\begin{cases}\n1, & Y \\neq f(X) \\\\\\\n0, & Y = f(X)\n\\end{cases} \\\\]\n\n式中\\\\( f(X) \\\\)是决策函数，这时期望风险函数为\n\\\\[ R\\_{exp}(f)=E[L(Y,f(X)] \\\\]\n\n期望是对联合分布 \\\\( P(X,Y) \\\\)取的，由此取条件期望\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nR_{exp}(f)\n& = \\iint_{D_{xy}}L(y,f(x)) \\cdot P(x,y) dxdy \\\\\\\n& = \\int_{D_x} \\int_{D_y} L(y, f(x)) \\cdot P(y\\|x) \\cdot P(x) dxdy \\\\\\\n& = \\int_{D_x} [\\int_{D_y} L(x,f(x)) \\cdot P(y\\|x) dy] P(x) dx \\\\\\\n& = E\\_X \\sum\\_{k=1}^K [L(c\\_k, f(X)]P(c\\_k|X)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n为了使期望风险最小化，只需要对 \\\\( X=x \\\\)逐个极小化，由此得到：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} \\sum_{k=1}^K L(c_k,y)P(c_k|X=x) \\\\\\\n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} \\sum_{k=1}^K P(y \\neq c_k| X=x) \\\\\\\n& = \\mathop{\\arg\\min}\\_{y\\in \\mathbb{y}} (1-P(y=c_k|X=x) \\\\\\\n& = \\mathop{\\arg\\max}\\_{y\\in \\mathbb{y}} P(y=c_k|X=x)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n这样，根据期望奉献最小化准则就得到了后验概率最大化准则：\n\\\\[ \nf(x)=\\mathop{\\arg\\max}\\_{y=c_k} P(c_k|X=x) \n\\\\]\n\n即朴素贝叶斯法采用的原理。\n\n\n------\n\n### 4. 特点\n\n* 优点：源于古典数学理论，有稳定的分类效率；小数据好，可增量训练；对缺失数据不敏感，算法简单。\n* 缺点：独立性假设强，实际中根据属性间的关系强弱，效果不同；须知先验概率，但先验有时取决于假设；对数据的输入形式敏感。\n\n\n------\n\n### 5. 代码实现\n\n[Naive Bayes 代码](/posts_res/2018-03-08-native-bayes/native_bayes.py)\n    \n    # coding:utf-8\n    \n    import numpy as np\n    import time\n    from datetime import timedelta\n    from sklearn.datasets import load_breast_cancer\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    \n    \n    def time_consume(s_t):\n        diff = time.time()-s_t\n        return timedelta(seconds=int(diff))\n    \n    \n    class NaiveBayes(object):\n        def __init__(self, class_num=-1, features_dim=-1):\n            self.class_num = class_num\n            self.features_dim = features_dim\n    \n        def fit(self, X, y):\n            self.nlen = X.shape[0]\n            # shape meaning: class_num\n            prior_prob = np.zeros(shape=self.class_num)\n            # shape meaning: class_num, feature_dim, feature_value\n            conditional_prob = np.zeros(shape=(self.class_num, self.features_dim, 2))\n    \n            for i in range(self.nlen):\n                prior_prob[y[i]] += 1.0\n                for j in range(self.features_dim):\n                    conditional_prob[y[i]][j][X[i][j]] += 1.0\n    \n            for i in range(self.class_num):\n                for j in range(self.features_dim):\n                    p_0 = conditional_prob[i][j][0]\n                    p_1 = conditional_prob[i][j][1]\n                    prob_0 = p_0 / (p_0 + p_1)\n                    prob_1 = p_1 / (p_0 + p_1)\n                    conditional_prob[i][j][0] = prob_0\n                    conditional_prob[i][j][1] = prob_1\n    \n            self.prior_prob = prior_prob # 没有除self.nlen，对结果无影响\n            self.conditional_prob = conditional_prob\n    \n        def __calculate_prob__(self, sample, label):\n            prob = int(self.prior_prob[label])\n            for i in range(self.features_dim):\n                prob *= self.conditional_prob[label][i][sample[i]]\n            return prob\n    \n        def predict(self, X):\n            if self.class_num == -1:\n                raise ValueError(\"Please fit first.\")\n            y_pred = np.zeros(shape=X.shape[0])\n            for i in range(X.shape[0]):\n                label = 0\n                prob = self.__calculate_prob__(X[i], 0)\n                for j in range(1, self.class_num):\n                    this_prob = self.__calculate_prob__(X[i], j)\n                    if prob < this_prob:\n                        prob = this_prob\n                        label = j\n                y_pred[i] = label\n            return y_pred\n    \n        def accuracy(self, y_true, y_pred):\n            return accuracy_score(y_true, y_pred)\n    \n    \n    def load_data():\n        \"\"\"normalize value of data to {0, 1}\"\"\"\n        data, target = load_breast_cancer(return_X_y=True)\n        nlen = data.shape[0]\n        ndim = data.shape[1]\n        X = np.zeros(shape=data.shape, dtype=np.int32)\n        for i in range(ndim):\n            mean = np.mean(data[:, i])\n            for j in range(nlen):\n                if data[j][i] > mean:\n                    X[j][i] = 1\n        return X, target\n    \n    \n    if __name__ == '__main__':\n        start_time = time.time()\n    \n        data, target = load_data()\n        train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=2048, shuffle=True)\n    \n        ml = NaiveBayes(class_num=2, features_dim=30)\n        ml.fit(train_x, train_y)\n        y_pred = ml.predict(test_x)\n        accuracy = ml.accuracy(test_y, y_pred)\n        print(\"Accuracy is \", accuracy)\n        print(\"Timeusage: %s\" % (time_consume(start_time)))\n\n\n结果如下：\n\n    Accuracy is  0.947368421053\n    Timeusage: 0:00:00\n\n\n------\n\n### 6. 参考\n\n> 1. 李航 - 《统计学习方法》\n> 2. [刘建平Pinard's Blog](http://www.cnblogs.com/pinard/p/6069267.html)\n> 3. [scikit-learn](http://scikit-learn.org/)\n> 4. [WenDesi's Github](https://github.com/WenDesi/lihang_book_algorithm)\n","slug":"naive-bayes","published":1,"updated":"2019-08-17T09:31:32.460Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnus000e2qwphdae82vz","content":"<h2 id=\"朴素贝叶斯-Naive-Bayes\"><a href=\"#朴素贝叶斯-Naive-Bayes\" class=\"headerlink\" title=\"朴素贝叶斯 - Naive Bayes\"></a><center>朴素贝叶斯 - Naive Bayes</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>基本思想</li>\n<li>算法</li>\n<li>后验概率最大化的意义</li>\n<li>特点</li>\n<li>代码实现</li>\n</ul><hr><h3 id=\"1-基本思想\"><a href=\"#1-基本思想\" class=\"headerlink\" title=\"1. 基本思想\"></a>1. 基本思想</h3><p>朴素贝叶斯法通过训练数据集学习联合概率分布\\( P(X,Y) \\)。具体地，学习以下先验概率分布及条件概率分布。</p><p>先验概率分布：</p><p>\\[P(Y=c_k),\\quad k=1,2,…,K\\]</p><p>条件概率分布: \n\\[P(X=x | Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k),\\quad k=1,2,…,K\\]</p><p>于是通过贝叶斯公式\\(P(X,Y) = P(X | Y)\\cdot P(Y)\\)，\n可以学习到联合概率分布\\(P(X,Y)\\)。</p><p><strong>但是，条件概率分布\\(P(X=x | Y=c_k)\\)有指数级数量的参数，其估计实际是不可行的。事实上，假设\\(x^{(j)}\\) 可取值有\\(S_j\\)个，\\(j=1,2,…,n\\)，\\(Y\\)可取值有\\(K\\)个，那么参数个数为$K\\prod_{j=1}^nS_j$。</strong></p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<p>朴素贝叶斯法对条件概率分布作了条件独立性的假设，即假设特征之间相互独立，公式表达如下：\n\\[P(X=x | Y=c_k) \\quad = \\quad P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k) \\quad = \\quad \\prod_{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c_k)\\]</p>\n<hr>\n<h3 id=\"2-算法\"><a href=\"#2-算法\" class=\"headerlink\" title=\"2. 算法\"></a>2. 算法</h3><p>输入：</p>\n<p>训练数据\\( T={((x<em>1, y_1)),…,(x_N,y_N)} \\)，其中\\( x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T \\),\\( x_i^{(j)} \\)是第\\( i \\)个样本的第\\( j \\)个特征，\\( x_i^{(j)}\\in {a\\</em>{j1},a_{j2},…,a_{jS_j}} \\), \\( a_{jl} \\)是第\\( j \\)个特征可能取的第\\( l \\)个值，\\( j=1,2,…,n \\quad l=1,2,…,S_j, \\quad y_i \\in {c_1, c_2, …, c_k} \\); 实例\\( x \\)</p>\n<p>输出：</p>\n<p>实例\\( x \\)的分类</p>\n<p>（1）计算先验概率及条件概率</p>\n<p>\\[P(Y=c<em>k) = \\frac{\\sum</em>{i=1}^N I(y_i=c_k)}{N}, \\quad k=1,2,…,K\\]</p>\n<p>\\[P(X^{(j)}=a<em>{jl} | Y=c_k)=\\frac{\\sum</em>{i=1}^N I(x<em>i^{(j)}, y_i=c_k)}{\\sum</em>{i=1}^N I(y_i=c_k)}\\]</p>\n<p>\\[j=1,2,…,n; \\quad l=1,2,…,S_j; \\quad k=1,2,…,K\\]</p>\n<p>（2）对于给定的实例\\( x=(x^{(1)}, x^{(2)},.., x^{(n)})^T \\)，计算</p>\n<p>\\[P(Y=c<em>k) \\prod</em>{j=1}^n P(X^{(j)}=x^{(j)} | Y=c_k), \\quad k=1,2,…,K\\]</p>\n<p>（3）确定实例\\( x \\)的类\n\\[ y = \\mathop{\\arg\\max}_{c_k} P(Y=c_k) \\prod_{j=1}^n P(X^{(j)} | Y=c_k)\\]</p>\n<hr>\n<h3 id=\"3-后验概率最大化的意义\"><a href=\"#3-后验概率最大化的意义\" class=\"headerlink\" title=\"3. 后验概率最大化的意义\"></a>3. 后验概率最大化的意义</h3><p>朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：\n\\[ L(Y,f(X)) = \\begin{cases}\n1, &amp; Y \\neq f(X) \\\\\n0, &amp; Y = f(X)\n\\end{cases} \\]</p>\n<p>式中\\( f(X) \\)是决策函数，这时期望风险函数为\n\\[ R_{exp}(f)=E[L(Y,f(X)] \\]</p>\n<p>期望是对联合分布 \\( P(X,Y) \\)取的，由此取条件期望\n\\[\n\\begin{equation}\n\\begin{aligned}\nR<em>{exp}(f)\n&amp; = \\iint</em>{D<em>{xy}}L(y,f(x)) \\cdot P(x,y) dxdy \\\\\n&amp; = \\int</em>{D<em>x} \\int</em>{D<em>y} L(y, f(x)) \\cdot P(y|x) \\cdot P(x) dxdy \\\\\n&amp; = \\int</em>{D<em>x} [\\int</em>{D<em>y} L(x,f(x)) \\cdot P(y|x) dy] P(x) dx \\\\\n&amp; = E_X \\sum\\</em>{k=1}^K [L(c_k, f(X)]P(c_k|X)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>为了使期望风险最小化，只需要对 \\( X=x \\)逐个极小化，由此得到：\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = \\mathop{\\arg\\min}_{y\\in \\mathbb{y}} \\sum<em>{k=1}^K L(c_k,y)P(c_k|X=x) \\\\\n&amp; = \\mathop{\\arg\\min}\\</em>{y\\in \\mathbb{y}} \\sum<em>{k=1}^K P(y \\neq c_k| X=x) \\\\\n&amp; = \\mathop{\\arg\\min}\\</em>{y\\in \\mathbb{y}} (1-P(y=c<em>k|X=x) \\\\\n&amp; = \\mathop{\\arg\\max}\\</em>{y\\in \\mathbb{y}} P(y=c_k|X=x)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>这样，根据期望奉献最小化准则就得到了后验概率最大化准则：\n\\[ \nf(x)=\\mathop{\\arg\\max}_{y=c_k} P(c_k|X=x) \n\\]</p>\n<p>即朴素贝叶斯法采用的原理。</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><ul>\n<li>优点：源于古典数学理论，有稳定的分类效率；小数据好，可增量训练；对缺失数据不敏感，算法简单。</li>\n<li>缺点：独立性假设强，实际中根据属性间的关系强弱，效果不同；须知先验概率，但先验有时取决于假设；对数据的输入形式敏感。</li>\n</ul>\n<hr>\n<h3 id=\"5-代码实现\"><a href=\"#5-代码实现\" class=\"headerlink\" title=\"5. 代码实现\"></a>5. 代码实现</h3><p><a href=\"/posts_res/2018-03-08-native-bayes/native_bayes.py\">Naive Bayes 代码</a></p>\n<pre><code># coding:utf-8\n\nimport numpy as np\nimport time\nfrom datetime import timedelta\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef time_consume(s_t):\n    diff = time.time()-s_t\n    return timedelta(seconds=int(diff))\n\n\nclass NaiveBayes(object):\n    def __init__(self, class_num=-1, features_dim=-1):\n        self.class_num = class_num\n        self.features_dim = features_dim\n\n    def fit(self, X, y):\n        self.nlen = X.shape[0]\n        # shape meaning: class_num\n        prior_prob = np.zeros(shape=self.class_num)\n        # shape meaning: class_num, feature_dim, feature_value\n        conditional_prob = np.zeros(shape=(self.class_num, self.features_dim, 2))\n\n        for i in range(self.nlen):\n            prior_prob[y[i]] += 1.0\n            for j in range(self.features_dim):\n                conditional_prob[y[i]][j][X[i][j]] += 1.0\n\n        for i in range(self.class_num):\n            for j in range(self.features_dim):\n                p_0 = conditional_prob[i][j][0]\n                p_1 = conditional_prob[i][j][1]\n                prob_0 = p_0 / (p_0 + p_1)\n                prob_1 = p_1 / (p_0 + p_1)\n                conditional_prob[i][j][0] = prob_0\n                conditional_prob[i][j][1] = prob_1\n\n        self.prior_prob = prior_prob # 没有除self.nlen，对结果无影响\n        self.conditional_prob = conditional_prob\n\n    def __calculate_prob__(self, sample, label):\n        prob = int(self.prior_prob[label])\n        for i in range(self.features_dim):\n            prob *= self.conditional_prob[label][i][sample[i]]\n        return prob\n\n    def predict(self, X):\n        if self.class_num == -1:\n            raise ValueError(&quot;Please fit first.&quot;)\n        y_pred = np.zeros(shape=X.shape[0])\n        for i in range(X.shape[0]):\n            label = 0\n            prob = self.__calculate_prob__(X[i], 0)\n            for j in range(1, self.class_num):\n                this_prob = self.__calculate_prob__(X[i], j)\n                if prob &lt; this_prob:\n                    prob = this_prob\n                    label = j\n            y_pred[i] = label\n        return y_pred\n\n    def accuracy(self, y_true, y_pred):\n        return accuracy_score(y_true, y_pred)\n\n\ndef load_data():\n    &quot;&quot;&quot;normalize value of data to {0, 1}&quot;&quot;&quot;\n    data, target = load_breast_cancer(return_X_y=True)\n    nlen = data.shape[0]\n    ndim = data.shape[1]\n    X = np.zeros(shape=data.shape, dtype=np.int32)\n    for i in range(ndim):\n        mean = np.mean(data[:, i])\n        for j in range(nlen):\n            if data[j][i] &gt; mean:\n                X[j][i] = 1\n    return X, target\n\n\nif __name__ == &#39;__main__&#39;:\n    start_time = time.time()\n\n    data, target = load_data()\n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=2048, shuffle=True)\n\n    ml = NaiveBayes(class_num=2, features_dim=30)\n    ml.fit(train_x, train_y)\n    y_pred = ml.predict(test_x)\n    accuracy = ml.accuracy(test_y, y_pred)\n    print(&quot;Accuracy is &quot;, accuracy)\n    print(&quot;Timeusage: %s&quot; % (time_consume(start_time)))\n</code></pre><p>结果如下：</p>\n<pre><code>Accuracy is  0.947368421053\nTimeusage: 0:00:00\n</code></pre><hr>\n<h3 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6. 参考\"></a>6. 参考</h3><blockquote>\n<ol>\n<li>李航 - 《统计学习方法》</li>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard’s Blog</a></li>\n<li><a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">scikit-learn</a></li>\n<li><a href=\"https://github.com/WenDesi/lihang_book_algorithm\" target=\"_blank\" rel=\"noopener\">WenDesi’s Github</a></li>\n</ol>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"朴素贝叶斯-Naive-Bayes\"><a href=\"#朴素贝叶斯-Naive-Bayes\" class=\"headerlink\" title=\"朴素贝叶斯 - Naive Bayes\"></a><center>朴素贝叶斯 - Naive Bayes</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>基本思想</li>\n<li>算法</li>\n<li>后验概率最大化的意义</li>\n<li>特点</li>\n<li>代码实现</li>\n</ul><hr><h3 id=\"1-基本思想\"><a href=\"#1-基本思想\" class=\"headerlink\" title=\"1. 基本思想\"></a>1. 基本思想</h3><p>朴素贝叶斯法通过训练数据集学习联合概率分布\\( P(X,Y) \\)。具体地，学习以下先验概率分布及条件概率分布。</p><p>先验概率分布：</p><p>\\[P(Y=c_k),\\quad k=1,2,…,K\\]</p><p>条件概率分布: \n\\[P(X=x | Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k),\\quad k=1,2,…,K\\]</p><p>于是通过贝叶斯公式\\(P(X,Y) = P(X | Y)\\cdot P(Y)\\)，\n可以学习到联合概率分布\\(P(X,Y)\\)。</p><p><strong>但是，条件概率分布\\(P(X=x | Y=c_k)\\)有指数级数量的参数，其估计实际是不可行的。事实上，假设\\(x^{(j)}\\) 可取值有\\(S_j\\)个，\\(j=1,2,…,n\\)，\\(Y\\)可取值有\\(K\\)个，那么参数个数为$K\\prod_{j=1}^nS_j$。</strong></p>","more":"\n\n\n\n\n\n\n\n<p>朴素贝叶斯法对条件概率分布作了条件独立性的假设，即假设特征之间相互独立，公式表达如下：\n\\[P(X=x | Y=c_k) \\quad = \\quad P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)} | Y=c_k) \\quad = \\quad \\prod_{j=1}^{n}P(X^{(j)}=x^{(j)} | Y=c_k)\\]</p>\n<hr>\n<h3 id=\"2-算法\"><a href=\"#2-算法\" class=\"headerlink\" title=\"2. 算法\"></a>2. 算法</h3><p>输入：</p>\n<p>训练数据\\( T={((x<em>1, y_1)),…,(x_N,y_N)} \\)，其中\\( x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T \\),\\( x_i^{(j)} \\)是第\\( i \\)个样本的第\\( j \\)个特征，\\( x_i^{(j)}\\in {a\\</em>{j1},a_{j2},…,a_{jS_j}} \\), \\( a_{jl} \\)是第\\( j \\)个特征可能取的第\\( l \\)个值，\\( j=1,2,…,n \\quad l=1,2,…,S_j, \\quad y_i \\in {c_1, c_2, …, c_k} \\); 实例\\( x \\)</p>\n<p>输出：</p>\n<p>实例\\( x \\)的分类</p>\n<p>（1）计算先验概率及条件概率</p>\n<p>\\[P(Y=c<em>k) = \\frac{\\sum</em>{i=1}^N I(y_i=c_k)}{N}, \\quad k=1,2,…,K\\]</p>\n<p>\\[P(X^{(j)}=a<em>{jl} | Y=c_k)=\\frac{\\sum</em>{i=1}^N I(x<em>i^{(j)}, y_i=c_k)}{\\sum</em>{i=1}^N I(y_i=c_k)}\\]</p>\n<p>\\[j=1,2,…,n; \\quad l=1,2,…,S_j; \\quad k=1,2,…,K\\]</p>\n<p>（2）对于给定的实例\\( x=(x^{(1)}, x^{(2)},.., x^{(n)})^T \\)，计算</p>\n<p>\\[P(Y=c<em>k) \\prod</em>{j=1}^n P(X^{(j)}=x^{(j)} | Y=c_k), \\quad k=1,2,…,K\\]</p>\n<p>（3）确定实例\\( x \\)的类\n\\[ y = \\mathop{\\arg\\max}_{c_k} P(Y=c_k) \\prod_{j=1}^n P(X^{(j)} | Y=c_k)\\]</p>\n<hr>\n<h3 id=\"3-后验概率最大化的意义\"><a href=\"#3-后验概率最大化的意义\" class=\"headerlink\" title=\"3. 后验概率最大化的意义\"></a>3. 后验概率最大化的意义</h3><p>朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：\n\\[ L(Y,f(X)) = \\begin{cases}\n1, &amp; Y \\neq f(X) \\\\\n0, &amp; Y = f(X)\n\\end{cases} \\]</p>\n<p>式中\\( f(X) \\)是决策函数，这时期望风险函数为\n\\[ R_{exp}(f)=E[L(Y,f(X)] \\]</p>\n<p>期望是对联合分布 \\( P(X,Y) \\)取的，由此取条件期望\n\\[\n\\begin{equation}\n\\begin{aligned}\nR<em>{exp}(f)\n&amp; = \\iint</em>{D<em>{xy}}L(y,f(x)) \\cdot P(x,y) dxdy \\\\\n&amp; = \\int</em>{D<em>x} \\int</em>{D<em>y} L(y, f(x)) \\cdot P(y|x) \\cdot P(x) dxdy \\\\\n&amp; = \\int</em>{D<em>x} [\\int</em>{D<em>y} L(x,f(x)) \\cdot P(y|x) dy] P(x) dx \\\\\n&amp; = E_X \\sum\\</em>{k=1}^K [L(c_k, f(X)]P(c_k|X)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>为了使期望风险最小化，只需要对 \\( X=x \\)逐个极小化，由此得到：\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = \\mathop{\\arg\\min}_{y\\in \\mathbb{y}} \\sum<em>{k=1}^K L(c_k,y)P(c_k|X=x) \\\\\n&amp; = \\mathop{\\arg\\min}\\</em>{y\\in \\mathbb{y}} \\sum<em>{k=1}^K P(y \\neq c_k| X=x) \\\\\n&amp; = \\mathop{\\arg\\min}\\</em>{y\\in \\mathbb{y}} (1-P(y=c<em>k|X=x) \\\\\n&amp; = \\mathop{\\arg\\max}\\</em>{y\\in \\mathbb{y}} P(y=c_k|X=x)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>这样，根据期望奉献最小化准则就得到了后验概率最大化准则：\n\\[ \nf(x)=\\mathop{\\arg\\max}_{y=c_k} P(c_k|X=x) \n\\]</p>\n<p>即朴素贝叶斯法采用的原理。</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><ul>\n<li>优点：源于古典数学理论，有稳定的分类效率；小数据好，可增量训练；对缺失数据不敏感，算法简单。</li>\n<li>缺点：独立性假设强，实际中根据属性间的关系强弱，效果不同；须知先验概率，但先验有时取决于假设；对数据的输入形式敏感。</li>\n</ul>\n<hr>\n<h3 id=\"5-代码实现\"><a href=\"#5-代码实现\" class=\"headerlink\" title=\"5. 代码实现\"></a>5. 代码实现</h3><p><a href=\"/posts_res/2018-03-08-native-bayes/native_bayes.py\">Naive Bayes 代码</a></p>\n<pre><code># coding:utf-8\n\nimport numpy as np\nimport time\nfrom datetime import timedelta\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\ndef time_consume(s_t):\n    diff = time.time()-s_t\n    return timedelta(seconds=int(diff))\n\n\nclass NaiveBayes(object):\n    def __init__(self, class_num=-1, features_dim=-1):\n        self.class_num = class_num\n        self.features_dim = features_dim\n\n    def fit(self, X, y):\n        self.nlen = X.shape[0]\n        # shape meaning: class_num\n        prior_prob = np.zeros(shape=self.class_num)\n        # shape meaning: class_num, feature_dim, feature_value\n        conditional_prob = np.zeros(shape=(self.class_num, self.features_dim, 2))\n\n        for i in range(self.nlen):\n            prior_prob[y[i]] += 1.0\n            for j in range(self.features_dim):\n                conditional_prob[y[i]][j][X[i][j]] += 1.0\n\n        for i in range(self.class_num):\n            for j in range(self.features_dim):\n                p_0 = conditional_prob[i][j][0]\n                p_1 = conditional_prob[i][j][1]\n                prob_0 = p_0 / (p_0 + p_1)\n                prob_1 = p_1 / (p_0 + p_1)\n                conditional_prob[i][j][0] = prob_0\n                conditional_prob[i][j][1] = prob_1\n\n        self.prior_prob = prior_prob # 没有除self.nlen，对结果无影响\n        self.conditional_prob = conditional_prob\n\n    def __calculate_prob__(self, sample, label):\n        prob = int(self.prior_prob[label])\n        for i in range(self.features_dim):\n            prob *= self.conditional_prob[label][i][sample[i]]\n        return prob\n\n    def predict(self, X):\n        if self.class_num == -1:\n            raise ValueError(&quot;Please fit first.&quot;)\n        y_pred = np.zeros(shape=X.shape[0])\n        for i in range(X.shape[0]):\n            label = 0\n            prob = self.__calculate_prob__(X[i], 0)\n            for j in range(1, self.class_num):\n                this_prob = self.__calculate_prob__(X[i], j)\n                if prob &lt; this_prob:\n                    prob = this_prob\n                    label = j\n            y_pred[i] = label\n        return y_pred\n\n    def accuracy(self, y_true, y_pred):\n        return accuracy_score(y_true, y_pred)\n\n\ndef load_data():\n    &quot;&quot;&quot;normalize value of data to {0, 1}&quot;&quot;&quot;\n    data, target = load_breast_cancer(return_X_y=True)\n    nlen = data.shape[0]\n    ndim = data.shape[1]\n    X = np.zeros(shape=data.shape, dtype=np.int32)\n    for i in range(ndim):\n        mean = np.mean(data[:, i])\n        for j in range(nlen):\n            if data[j][i] &gt; mean:\n                X[j][i] = 1\n    return X, target\n\n\nif __name__ == &#39;__main__&#39;:\n    start_time = time.time()\n\n    data, target = load_data()\n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=2048, shuffle=True)\n\n    ml = NaiveBayes(class_num=2, features_dim=30)\n    ml.fit(train_x, train_y)\n    y_pred = ml.predict(test_x)\n    accuracy = ml.accuracy(test_y, y_pred)\n    print(&quot;Accuracy is &quot;, accuracy)\n    print(&quot;Timeusage: %s&quot; % (time_consume(start_time)))\n</code></pre><p>结果如下：</p>\n<pre><code>Accuracy is  0.947368421053\nTimeusage: 0:00:00\n</code></pre><hr>\n<h3 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6. 参考\"></a>6. 参考</h3><blockquote>\n<ol>\n<li>李航 - 《统计学习方法》</li>\n<li><a href=\"http://www.cnblogs.com/pinard/p/6069267.html\" target=\"_blank\" rel=\"noopener\">刘建平Pinard’s Blog</a></li>\n<li><a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">scikit-learn</a></li>\n<li><a href=\"https://github.com/WenDesi/lihang_book_algorithm\" target=\"_blank\" rel=\"noopener\">WenDesi’s Github</a></li>\n</ol>\n</blockquote>\n"},{"layout":"post","title":"2017开放学术精准画像大赛","date":"2018-03-01T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n**队伍名：ICRC@HITSZ，获决赛第三名(3/412)**，[大赛主页](https://biendata.com/competition/scholar/)\n\n![results](/posts_res/2018-03-01-2017aca/results.png)\n\n**本文是对比赛的思路等的整理，包含后期参考其他队的思路**\n\n\n目录\n\n* Task 1 学者画像信息抽取\n* Task 2 学者兴趣标签预测\n* Task 3 学者未来影响力预测\n* 评测方案\n\n\n------------\n\n### Task 1 学者画像信息抽取\n\n#### Task 1 赛题\n\n学者画像信息具体包括学者的主页地址、性别、职位等。随着互联网越来越普及，与学者相关的网页的数量和内容的丰富度和复杂度都大大增加，其中包含了学者的大量冗余信息，通过整合互联网上多种来源的学者数据，采用合适的机器学习模型，获得学者的精准信息是一项潜在有效的学者画像技术。\n\n给定学者姓名、学者所属机构，以及谷歌搜索关键字“学者姓名＋机构”返回的第一页搜索结果的缓存页面的URL（静态页面，一般包含10条搜索结果），同时允许参赛者访问其中每条搜索结果的网页链接以及该网页内的链接，要求抽取学者的个人主页URL、学者头像URL、邮箱地址、性别、职称/职位列表，以及学者当前所属机构所在国家。\n\n示例:\n\n```text\n输入：\nscholar name: Jiawei Han\norganization name: UIUC\nsearch results page: http://xxx.xxx/xxx.html\n\n输出：\nhomepage: http://hanj.cs.illinois.edu/ (注释: 从搜索结果中找)\npic: http://hanj.cs.illinois.edu/images/hanj_tour.jpg (注释: 从个人主页中找)\nemail: hanj[at]cs.uiuc.edu (注释: email的格式跟学者个人主页中保持一致)\ngender: m (注释: m 代表 male, f 代表 female)\nposition: Abel Bliss Professor (注释: 需要跟个人主页中的格式保持一致)\nlocation: USA (注释: 需要跟个人主页中的格式保持一致)\n```\n\n<br>\n\n#### Task 1 思路\n\n**个人主页URL**\n\n对于个人主页搜索结果的每条记录提取特征后进行是否是主页的分类，其中概率最高的被判断为个人主页。\n\n特征如下：\n1. 网站在搜索结果中的排名；-> 1\n2. 学者所在机构是否为大学/学校；-> 1\n3. 搜索结果标题/摘要以及学者所在机构的文本长度 -> 3\n4. url中是否包含给定关键词(社交类linkedin……、论文类spring……、正向词homepage……)，对每个部分做平均值为特征； -> 35\n5. 搜索结果摘要中是否包含mail，address； -> 2\n6. 搜索结果的标题/摘要中的学者姓名得分。这里将学者姓名分词为n份，如果其中m份在文本中出现，那么姓名的得分就是 m/n，同时增加完整名字在标题摘要中出现的得分； -> 1\n\n```text\n一级负向词\n'facebook', 'linkedin', 'twitter', 'youtube', 'instagram', 'quora', 'blots', \n'dl.acm', 'arxiv', 'ieeexplore', 'springer', 'webofknowledge', 'library', \n'researchgate', 'books.google', 'netprofile', 'cnki', 'ted.com', 'antpedia', \n'academictree', 'phdtree', 'shguang.net', 'airbnb', 'bloomberg', 'pinterest', \n'weibo', 'hmetmer', 'peekyou', 'mitbbs', 'schneier.com', 'directory.ubc.ca', \n'citation', 'videolectures', 'slideshare', 'brokercheck.finra.org',\n\n二级负向词\n'pdf', 'news', 'report', 'scholarmate', 'journals', 'ratemyprofessors',\n'doc', 'conference', 'wiki', 'scholar', 'book', 'publication', 'github', 'crunchbase',\n'group', 'scholarmate', 'google', 'stories', 'story', 'relationshipscience',\n\n正向词\n'~', 'edu', 'home', 'faculty'\n```\n\n由于正负样本不均衡(1:9)，所以首先尝试通过**过采样**的方式进行处理。具体如下：\n\n1. 重复增加正样本的数量，尝试通过XGBoost模型及SVM模型进行分类，最终在训练集中的效果有提升，但在测试集上变化不大；\n2. (未实现)尝试通过[SMOTE算法](/2018/06/smote)进行过采样。\n\n鉴于第一种方式的效果并不明显，分析由于样本特征比较少(43维)，导致出现了过拟合的现象出现。\n\n接下来尝试通过**负采样**的方式进行解决，具体如下：\n\n1. 因为负样本数量多，进行随机选择，构造新的训练数据集；\n2. 通过XGBoost模型及SVM模型进行分类，效果明显得到提升。\n\n其中XGBoost效果比SVM效果稍微好一点。\n1. XGBoost可以自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线性化需要基于核函数，要求学习之前就要定好核函数，然而确定核函数并不是一件容易的事；\n2. 多分类器隐含正则项：XGBoost自带正则化项（包含叶结点数量等），SVM通常需要增加L2正则\n3. 真实世界的数据具有特征多样性的特点，SVM本质是一个几何模型，需要人为去定义样例之间的核函数或者相似性；\n4. 多种可调的参数，效率、灵活、易用方面，XGBoost都有比较好的交互使用。\n\n最终我们在个人主页上的测试集取得了0.74的准确率，考虑到数据噪声以及搜索结果中多个正确主页的存在，真实的准确率应该接近0.85。\n\n\n**性别**\n\n主要通过学者的名字进行判断。通过网上的开源姓名性别数据，训练了一个朴素贝叶斯分类器，其中用到的特征是名字的后2位到后7位字母(每一位为一维特征)，以及名字长度，名字分词长度。如果学者名字长度不足的，用空格填充。\n\n结果上能取得0.9的准确率。\n\n\n**邮箱**\n\n主要通过正则表达式进行识别。根据[RFC2822](https://tools.ietf.org/html/rfc2822)中的邮箱标准，并结合训练集中邮箱的格式，我们写了一个很复杂的正则表达式，另外针对仍然无法获取到邮箱的，我们捕获网页中的mailto的链接来确定该学者的邮箱。\n\n这样仍然有一部分的图片邮件或者通过加密技术加密的邮箱无法获得，针对图片邮箱，我们通过捕获图片的名称，判断是否和邮件相关，相关即取该图片的url。\n对于加密邮箱，暂未想到比较合适的方法，而且这种邮箱数量比较少，所以对最终结果应该不大。\n\n\n**照片**\n\n首先利用正则表达式+xpath等手段，获取到网页中的所有图片url及图片的名称，对于图片名称是学者名字或名字一部分的图片，直接判断该图片为学者的照片；\n若没有获得结果，使用开源工具[face_recognition](https://github.com/ageitgey/face_recognition)对网页的图片进行人脸识别，当该网页中的人脸数量少于阈值4的时候，选择第一个人脸照片为该学者的照片，对人脸数量大于阈值且没有获得邮箱的，认为对学者的个人主页选择错误，并将该学者的信息留空。\n\n\n**职称**\n\n通过统计训练集中的职称，并做了一些加工之后，使用正则表达式对网页的内容进行匹配。\n\n\n**国家**\n\n主要根据获取的邮箱、以及网页中出现的地址、电话、传真等地点指向性词进行判断；另外学者的名字有时也具有一定的指向性。\n\n当无法获得有效信息时，将该信息留空。\n\n\n#### 其他\n\n因为该任务需要联网获取数据，且学者的网站服务器良莠不齐，所以实现了一个多进程并行的爬虫。主要用到的Python库如下：\n1. requests\n2. selenium\n3. urllib.request\n4. lxml\n5. multiprocessing\n6. bs4\n\n\n-------\n\n### Task 2 学者兴趣标签预测\n\n#### Task 2 赛题\n\n研究兴趣是学者画像的重要组成部分，其不仅是学者本身的研究心得或研究拓展方向的集中体现，也能从中窥视不同背景的学者对研究领域热点或学科研究趋势的关注度、敏感度的集体反映。与学者画像信息抽取类似，通过整合互联网上的大规模多源信息，可以对学者的研究兴趣进行判断。\n\n给定学者已发表的论文信息和合著关系网络，参赛者需为每位学者标注5个最合适的兴趣标签。\n\n示例:\n\n```text\n输入：\nname: Jiawei Han\n\n输出：\nresearch interests: data mining, database, information networks, knowledge discovery,  machine learning\n```\n\n<br>\n\n#### Task 2 思路\n\n本任务主要用到论文信息中的title以及期刊，首先将论文title分别以作者和标签为单位收集在一起并打上标签\n\n主要是基于词袋模型，以训练集作者为单位收集的标题文本，分词去停用次后使用[卡方检验](/2018/03/chisquaredtest)进一步筛选特征，按卡方值筛选了前10%的特征词作为特征，\n以标签为单位收集所有相关作者的标题文本，筛选了50%的特征词作为特征。\n\n确定文档特征词后，采用词的tf-idf作为文档向量相应位置的权重。\n\n考虑到文本分类大多都是线性可分的最终采用sklean中的SGDClassifier，分别对上述构建的两个数据集（以作者为单位收集的文档和以标签为单位收集的文档）训练了两个模型；尝试过Stacking，sklearn中的BaggingClassifier， GridSearchCV 5 folds来寻找最优参数；\n\n因为本任务是多标签预测，所以我们根据训练的模型为每个作者输出概率最大的n个标签以及概率(以作者为单位的模型输出24个标签做候选标签集合，以标签为单位的模型输出23个标签做候选标签集合)\n训练两个模型(以作者为单位和以标签为单位)从不同的角度提取的特征，是一个相互补充，相互修正的作用；将两个模型的候选标签集合取并集，遍历候选标签集合，评价的量为每个标签两个模型预测的概率的乘积，\n期间统计作者发过的论文，记录title中出现该标签的频数(限定该作者在合著名单里为2作或者3作以内)，最后在每个标签评价量上加频数 * 评价量。最终排序取top5个标签为当前作者的标签。\n\n\n--------\n\n### Task 3 学者未来影响力预测\n\n#### Task 3 赛题\n\n学术影响力用来衡量学者在专业理论及技术方面的影响，常用的评价指标有论文被引量，期刊影响因子、作者H指数等，其中论文被引量是一个重要而直观的指标。本任务的目的是基于学者当前的相关学术数据预测其未来某段时间内的总论文被引量。\n\n给定学者截止到2013年底的全部论文数据（包括论文详细引用关系，详见数据描述部分），参赛者需预测学者截止至2017年6月的总被引用数.\n\n示例:\n\n```\n输入：\nname: Jiawei Han\n\n输出：\ncitation number: 126147\n```\n\n<br>\n\n#### Task 3 思路\n\n**预处理**\n\npapers.txt包含了学者截止2016年所有论文的数据，其中需要预测的学者隐去了2013年后所发表的论文。所以任务三只用到2013(包含2013)年前的论文数据，\n对paper.txt中的学者进行统计引用数, 去除实际引用数小于统计引用数训练集中的学者。\n\n统计发现，学者论文引用数为0的，约占整个训练集的24%，故尝试首先通过二分类，判断学者是否具有论文引用量，之后对具有论文引用量的学者进行回归预测(两次使用同样的特征)。\n\n**特征**\n\n1. 学者距今0,1,2,3,4,5,6,7,8,9年的文章数被引用数(cited)；\n2. 学者文章被引用数的平均值，最小值，最大值；\n3. 学者距今0,1,2,3,4,5,6,7,8,9,10,20,30,40,50年的引用数(reference)；\n4. 学者学术年龄、学者发表的会议期刊的数量；\n5. 学者的hindex以及近两年的变化值；\n6. 学者的共同作者每四年中hindex的最小值,最大值,平均值,和；\n7. 会议期刊文章总数，hindex，引用他人文章的最小值，最大值，平均值以及近两年的变化值；\n8. 会议期刊被引用数的中最小值，最大值，平均值，和；\n9. 会议期刊按被引用数排序的特征；\n10. 以共同作者关系建边图的pagerank值，以及使用共同作者次数加权的weighted_pagerank(作者合作的越多，引用数越接近)；\n11. 使用每篇论文title建模[LDA模型](/2018/07/ldatopic)求出每篇论文困惑度(perplexity)和主题(topic)权值，和每个topic的被引用数，得到每个作者论文按主题计算的引用数和排序结果；\n12. 通过有label的训练集中作者的真实被引用数估计每篇论文的权重，然后使用每篇文章的权重来预测未知作者的权重weight1(作者的权重由其论文权重决定)；\n13. 根据训练集的label和论文被引用数关系迭代训练得出每篇论文的权值，从而每个作者得到权重weight2(比起weight1使用了paper.txt统计的论文被引用数)；\n\n**模型**\n\n* 分类模型：XGBClassifier(训练集上统计分类准确率为92.3%)\n* 回归模型：XGBoost中XGBRegressor, sklearn中的RandomForestRegressor, ExtraTreesRegressor, SVR, LinearRegressor，并对结果进行Stacking集成\n\n**其他trick**\n\n学者的论文被引用数的跨度很大，有0，有10W+，所以在预测的时候对标签使用了log进行平滑；\n\n\n------------\n\n### 评测方案\n\n#### Task 1\n\n任务1中，每个学者共有 k (在该项任务中 k = 6)项个人画像数据需要预测。这 k 个中除了职称/职位外都通过完全匹配的方式进行评测，完全匹配上得1分，否则得0分；而职称/职位由于是一个集合，所以通过Jaccard index (即两个集合交集的元素个数除以两个集合并集的元素个数) 来进行计算提取出来的集合跟标注答案给出的集合的相似度 (介于0 ~ 1之间)。一个学者的画像数据的预测值的最终得分为这 k 个的平均分。一个参赛选手在该任务上的最终得分为对各个学者的画像数据的预测得分的平均分。即：\n\n$$\nscore1 = \\frac{1}{kN} \\sum_{i=1}^N \\sum_{j=1}^k s_i\n$$\n\n其中$ s_i $表示 k 项画像数据中第 i 项的得分，取值0 ~ 1。\n\n<br>\n\n#### Task 2\n\n任务2的得分score2为参赛队伍计算生成的学者兴趣与给定的学者兴趣完全相同的比例，即：\n\n$$\nscore2 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\| T_i \\bigcap T_i^{\\ast} \\|}{\\| T_i^{\\ast} \\|}\n$$\n\n其中，N为任务2的评测集样本个数，$T_i$为计算生成的用户i 的兴趣集合,$T_i^{\\ast}$为给定的用户i 的兴趣集合。\n\n<br>\n\n#### Task 3\n\n任务3的得分score3用参赛队伍预测的学者被引数与给定的学者真实被引数之间的相对误差来计算来计算\n\n$$\nscore3 = 1 - \\frac{1}{N} \\sum_{i=1}^N \n\\begin{cases} \n0, \\qquad \\qquad \\quad v_i=0, v_i^{\\ast}=0 \\\\\n\\| v_i - v_i^{\\ast} \\| / max(v_i, v_i^{\\ast}), \\quad otherwise\n\\end{cases}\n$$\n\n其中，N 为任务3的评测集样本个数，$v_i$为用户i 的预测被引数，$v_i^{\\ast}$为用户i 的真实被引数。\n\n<br>\n\n#### 最终得分\n\n$$\nscore = score1 + score2 + score3\n$$\n\n\n","source":"_posts/2018-03-01-2017aca.md","raw":"---\nlayout: post\ntitle: 2017开放学术精准画像大赛\ndate: 2018-03-01 12:10 +0800\ncategories: 个人小结\ntags:\n- 比赛\nmathjax: true\ncopyright: true\n---\n\n**队伍名：ICRC@HITSZ，获决赛第三名(3/412)**，[大赛主页](https://biendata.com/competition/scholar/)\n\n![results](/posts_res/2018-03-01-2017aca/results.png)\n\n**本文是对比赛的思路等的整理，包含后期参考其他队的思路**\n\n\n目录\n\n* Task 1 学者画像信息抽取\n* Task 2 学者兴趣标签预测\n* Task 3 学者未来影响力预测\n* 评测方案\n\n\n------------\n\n### Task 1 学者画像信息抽取\n\n#### Task 1 赛题\n\n学者画像信息具体包括学者的主页地址、性别、职位等。随着互联网越来越普及，与学者相关的网页的数量和内容的丰富度和复杂度都大大增加，其中包含了学者的大量冗余信息，通过整合互联网上多种来源的学者数据，采用合适的机器学习模型，获得学者的精准信息是一项潜在有效的学者画像技术。\n\n给定学者姓名、学者所属机构，以及谷歌搜索关键字“学者姓名＋机构”返回的第一页搜索结果的缓存页面的URL（静态页面，一般包含10条搜索结果），同时允许参赛者访问其中每条搜索结果的网页链接以及该网页内的链接，要求抽取学者的个人主页URL、学者头像URL、邮箱地址、性别、职称/职位列表，以及学者当前所属机构所在国家。\n\n示例:\n\n```text\n输入：\nscholar name: Jiawei Han\norganization name: UIUC\nsearch results page: http://xxx.xxx/xxx.html\n\n输出：\nhomepage: http://hanj.cs.illinois.edu/ (注释: 从搜索结果中找)\npic: http://hanj.cs.illinois.edu/images/hanj_tour.jpg (注释: 从个人主页中找)\nemail: hanj[at]cs.uiuc.edu (注释: email的格式跟学者个人主页中保持一致)\ngender: m (注释: m 代表 male, f 代表 female)\nposition: Abel Bliss Professor (注释: 需要跟个人主页中的格式保持一致)\nlocation: USA (注释: 需要跟个人主页中的格式保持一致)\n```\n\n<br>\n\n#### Task 1 思路\n\n**个人主页URL**\n\n对于个人主页搜索结果的每条记录提取特征后进行是否是主页的分类，其中概率最高的被判断为个人主页。\n\n特征如下：\n1. 网站在搜索结果中的排名；-> 1\n2. 学者所在机构是否为大学/学校；-> 1\n3. 搜索结果标题/摘要以及学者所在机构的文本长度 -> 3\n4. url中是否包含给定关键词(社交类linkedin……、论文类spring……、正向词homepage……)，对每个部分做平均值为特征； -> 35\n5. 搜索结果摘要中是否包含mail，address； -> 2\n6. 搜索结果的标题/摘要中的学者姓名得分。这里将学者姓名分词为n份，如果其中m份在文本中出现，那么姓名的得分就是 m/n，同时增加完整名字在标题摘要中出现的得分； -> 1\n\n```text\n一级负向词\n'facebook', 'linkedin', 'twitter', 'youtube', 'instagram', 'quora', 'blots', \n'dl.acm', 'arxiv', 'ieeexplore', 'springer', 'webofknowledge', 'library', \n'researchgate', 'books.google', 'netprofile', 'cnki', 'ted.com', 'antpedia', \n'academictree', 'phdtree', 'shguang.net', 'airbnb', 'bloomberg', 'pinterest', \n'weibo', 'hmetmer', 'peekyou', 'mitbbs', 'schneier.com', 'directory.ubc.ca', \n'citation', 'videolectures', 'slideshare', 'brokercheck.finra.org',\n\n二级负向词\n'pdf', 'news', 'report', 'scholarmate', 'journals', 'ratemyprofessors',\n'doc', 'conference', 'wiki', 'scholar', 'book', 'publication', 'github', 'crunchbase',\n'group', 'scholarmate', 'google', 'stories', 'story', 'relationshipscience',\n\n正向词\n'~', 'edu', 'home', 'faculty'\n```\n\n由于正负样本不均衡(1:9)，所以首先尝试通过**过采样**的方式进行处理。具体如下：\n\n1. 重复增加正样本的数量，尝试通过XGBoost模型及SVM模型进行分类，最终在训练集中的效果有提升，但在测试集上变化不大；\n2. (未实现)尝试通过[SMOTE算法](/2018/06/smote)进行过采样。\n\n鉴于第一种方式的效果并不明显，分析由于样本特征比较少(43维)，导致出现了过拟合的现象出现。\n\n接下来尝试通过**负采样**的方式进行解决，具体如下：\n\n1. 因为负样本数量多，进行随机选择，构造新的训练数据集；\n2. 通过XGBoost模型及SVM模型进行分类，效果明显得到提升。\n\n其中XGBoost效果比SVM效果稍微好一点。\n1. XGBoost可以自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线性化需要基于核函数，要求学习之前就要定好核函数，然而确定核函数并不是一件容易的事；\n2. 多分类器隐含正则项：XGBoost自带正则化项（包含叶结点数量等），SVM通常需要增加L2正则\n3. 真实世界的数据具有特征多样性的特点，SVM本质是一个几何模型，需要人为去定义样例之间的核函数或者相似性；\n4. 多种可调的参数，效率、灵活、易用方面，XGBoost都有比较好的交互使用。\n\n最终我们在个人主页上的测试集取得了0.74的准确率，考虑到数据噪声以及搜索结果中多个正确主页的存在，真实的准确率应该接近0.85。\n\n\n**性别**\n\n主要通过学者的名字进行判断。通过网上的开源姓名性别数据，训练了一个朴素贝叶斯分类器，其中用到的特征是名字的后2位到后7位字母(每一位为一维特征)，以及名字长度，名字分词长度。如果学者名字长度不足的，用空格填充。\n\n结果上能取得0.9的准确率。\n\n\n**邮箱**\n\n主要通过正则表达式进行识别。根据[RFC2822](https://tools.ietf.org/html/rfc2822)中的邮箱标准，并结合训练集中邮箱的格式，我们写了一个很复杂的正则表达式，另外针对仍然无法获取到邮箱的，我们捕获网页中的mailto的链接来确定该学者的邮箱。\n\n这样仍然有一部分的图片邮件或者通过加密技术加密的邮箱无法获得，针对图片邮箱，我们通过捕获图片的名称，判断是否和邮件相关，相关即取该图片的url。\n对于加密邮箱，暂未想到比较合适的方法，而且这种邮箱数量比较少，所以对最终结果应该不大。\n\n\n**照片**\n\n首先利用正则表达式+xpath等手段，获取到网页中的所有图片url及图片的名称，对于图片名称是学者名字或名字一部分的图片，直接判断该图片为学者的照片；\n若没有获得结果，使用开源工具[face_recognition](https://github.com/ageitgey/face_recognition)对网页的图片进行人脸识别，当该网页中的人脸数量少于阈值4的时候，选择第一个人脸照片为该学者的照片，对人脸数量大于阈值且没有获得邮箱的，认为对学者的个人主页选择错误，并将该学者的信息留空。\n\n\n**职称**\n\n通过统计训练集中的职称，并做了一些加工之后，使用正则表达式对网页的内容进行匹配。\n\n\n**国家**\n\n主要根据获取的邮箱、以及网页中出现的地址、电话、传真等地点指向性词进行判断；另外学者的名字有时也具有一定的指向性。\n\n当无法获得有效信息时，将该信息留空。\n\n\n#### 其他\n\n因为该任务需要联网获取数据，且学者的网站服务器良莠不齐，所以实现了一个多进程并行的爬虫。主要用到的Python库如下：\n1. requests\n2. selenium\n3. urllib.request\n4. lxml\n5. multiprocessing\n6. bs4\n\n\n-------\n\n### Task 2 学者兴趣标签预测\n\n#### Task 2 赛题\n\n研究兴趣是学者画像的重要组成部分，其不仅是学者本身的研究心得或研究拓展方向的集中体现，也能从中窥视不同背景的学者对研究领域热点或学科研究趋势的关注度、敏感度的集体反映。与学者画像信息抽取类似，通过整合互联网上的大规模多源信息，可以对学者的研究兴趣进行判断。\n\n给定学者已发表的论文信息和合著关系网络，参赛者需为每位学者标注5个最合适的兴趣标签。\n\n示例:\n\n```text\n输入：\nname: Jiawei Han\n\n输出：\nresearch interests: data mining, database, information networks, knowledge discovery,  machine learning\n```\n\n<br>\n\n#### Task 2 思路\n\n本任务主要用到论文信息中的title以及期刊，首先将论文title分别以作者和标签为单位收集在一起并打上标签\n\n主要是基于词袋模型，以训练集作者为单位收集的标题文本，分词去停用次后使用[卡方检验](/2018/03/chisquaredtest)进一步筛选特征，按卡方值筛选了前10%的特征词作为特征，\n以标签为单位收集所有相关作者的标题文本，筛选了50%的特征词作为特征。\n\n确定文档特征词后，采用词的tf-idf作为文档向量相应位置的权重。\n\n考虑到文本分类大多都是线性可分的最终采用sklean中的SGDClassifier，分别对上述构建的两个数据集（以作者为单位收集的文档和以标签为单位收集的文档）训练了两个模型；尝试过Stacking，sklearn中的BaggingClassifier， GridSearchCV 5 folds来寻找最优参数；\n\n因为本任务是多标签预测，所以我们根据训练的模型为每个作者输出概率最大的n个标签以及概率(以作者为单位的模型输出24个标签做候选标签集合，以标签为单位的模型输出23个标签做候选标签集合)\n训练两个模型(以作者为单位和以标签为单位)从不同的角度提取的特征，是一个相互补充，相互修正的作用；将两个模型的候选标签集合取并集，遍历候选标签集合，评价的量为每个标签两个模型预测的概率的乘积，\n期间统计作者发过的论文，记录title中出现该标签的频数(限定该作者在合著名单里为2作或者3作以内)，最后在每个标签评价量上加频数 * 评价量。最终排序取top5个标签为当前作者的标签。\n\n\n--------\n\n### Task 3 学者未来影响力预测\n\n#### Task 3 赛题\n\n学术影响力用来衡量学者在专业理论及技术方面的影响，常用的评价指标有论文被引量，期刊影响因子、作者H指数等，其中论文被引量是一个重要而直观的指标。本任务的目的是基于学者当前的相关学术数据预测其未来某段时间内的总论文被引量。\n\n给定学者截止到2013年底的全部论文数据（包括论文详细引用关系，详见数据描述部分），参赛者需预测学者截止至2017年6月的总被引用数.\n\n示例:\n\n```\n输入：\nname: Jiawei Han\n\n输出：\ncitation number: 126147\n```\n\n<br>\n\n#### Task 3 思路\n\n**预处理**\n\npapers.txt包含了学者截止2016年所有论文的数据，其中需要预测的学者隐去了2013年后所发表的论文。所以任务三只用到2013(包含2013)年前的论文数据，\n对paper.txt中的学者进行统计引用数, 去除实际引用数小于统计引用数训练集中的学者。\n\n统计发现，学者论文引用数为0的，约占整个训练集的24%，故尝试首先通过二分类，判断学者是否具有论文引用量，之后对具有论文引用量的学者进行回归预测(两次使用同样的特征)。\n\n**特征**\n\n1. 学者距今0,1,2,3,4,5,6,7,8,9年的文章数被引用数(cited)；\n2. 学者文章被引用数的平均值，最小值，最大值；\n3. 学者距今0,1,2,3,4,5,6,7,8,9,10,20,30,40,50年的引用数(reference)；\n4. 学者学术年龄、学者发表的会议期刊的数量；\n5. 学者的hindex以及近两年的变化值；\n6. 学者的共同作者每四年中hindex的最小值,最大值,平均值,和；\n7. 会议期刊文章总数，hindex，引用他人文章的最小值，最大值，平均值以及近两年的变化值；\n8. 会议期刊被引用数的中最小值，最大值，平均值，和；\n9. 会议期刊按被引用数排序的特征；\n10. 以共同作者关系建边图的pagerank值，以及使用共同作者次数加权的weighted_pagerank(作者合作的越多，引用数越接近)；\n11. 使用每篇论文title建模[LDA模型](/2018/07/ldatopic)求出每篇论文困惑度(perplexity)和主题(topic)权值，和每个topic的被引用数，得到每个作者论文按主题计算的引用数和排序结果；\n12. 通过有label的训练集中作者的真实被引用数估计每篇论文的权重，然后使用每篇文章的权重来预测未知作者的权重weight1(作者的权重由其论文权重决定)；\n13. 根据训练集的label和论文被引用数关系迭代训练得出每篇论文的权值，从而每个作者得到权重weight2(比起weight1使用了paper.txt统计的论文被引用数)；\n\n**模型**\n\n* 分类模型：XGBClassifier(训练集上统计分类准确率为92.3%)\n* 回归模型：XGBoost中XGBRegressor, sklearn中的RandomForestRegressor, ExtraTreesRegressor, SVR, LinearRegressor，并对结果进行Stacking集成\n\n**其他trick**\n\n学者的论文被引用数的跨度很大，有0，有10W+，所以在预测的时候对标签使用了log进行平滑；\n\n\n------------\n\n### 评测方案\n\n#### Task 1\n\n任务1中，每个学者共有 k (在该项任务中 k = 6)项个人画像数据需要预测。这 k 个中除了职称/职位外都通过完全匹配的方式进行评测，完全匹配上得1分，否则得0分；而职称/职位由于是一个集合，所以通过Jaccard index (即两个集合交集的元素个数除以两个集合并集的元素个数) 来进行计算提取出来的集合跟标注答案给出的集合的相似度 (介于0 ~ 1之间)。一个学者的画像数据的预测值的最终得分为这 k 个的平均分。一个参赛选手在该任务上的最终得分为对各个学者的画像数据的预测得分的平均分。即：\n\n$$\nscore1 = \\frac{1}{kN} \\sum_{i=1}^N \\sum_{j=1}^k s_i\n$$\n\n其中$ s_i $表示 k 项画像数据中第 i 项的得分，取值0 ~ 1。\n\n<br>\n\n#### Task 2\n\n任务2的得分score2为参赛队伍计算生成的学者兴趣与给定的学者兴趣完全相同的比例，即：\n\n$$\nscore2 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\| T_i \\bigcap T_i^{\\ast} \\|}{\\| T_i^{\\ast} \\|}\n$$\n\n其中，N为任务2的评测集样本个数，$T_i$为计算生成的用户i 的兴趣集合,$T_i^{\\ast}$为给定的用户i 的兴趣集合。\n\n<br>\n\n#### Task 3\n\n任务3的得分score3用参赛队伍预测的学者被引数与给定的学者真实被引数之间的相对误差来计算来计算\n\n$$\nscore3 = 1 - \\frac{1}{N} \\sum_{i=1}^N \n\\begin{cases} \n0, \\qquad \\qquad \\quad v_i=0, v_i^{\\ast}=0 \\\\\n\\| v_i - v_i^{\\ast} \\| / max(v_i, v_i^{\\ast}), \\quad otherwise\n\\end{cases}\n$$\n\n其中，N 为任务3的评测集样本个数，$v_i$为用户i 的预测被引数，$v_i^{\\ast}$为用户i 的真实被引数。\n\n<br>\n\n#### 最终得分\n\n$$\nscore = score1 + score2 + score3\n$$\n\n\n","slug":"2017aca","published":1,"updated":"2019-08-17T09:30:56.237Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnuv000f2qwpmo86fq7c","content":"<p><strong>队伍名：ICRC@HITSZ，获决赛第三名(3/412)</strong>，<a href=\"https://biendata.com/competition/scholar/\" target=\"_blank\" rel=\"noopener\">大赛主页</a></p><p><img src=\"/posts_res/2018-03-01-2017aca/results.png\" alt=\"results\"></p><p><strong>本文是对比赛的思路等的整理，包含后期参考其他队的思路</strong></p><p>目录</p><ul>\n<li>Task 1 学者画像信息抽取</li>\n<li>Task 2 学者兴趣标签预测</li>\n<li>Task 3 学者未来影响力预测</li>\n<li>评测方案</li>\n</ul><hr><h3 id=\"Task-1-学者画像信息抽取\"><a href=\"#Task-1-学者画像信息抽取\" class=\"headerlink\" title=\"Task 1 学者画像信息抽取\"></a>Task 1 学者画像信息抽取</h3><h4 id=\"Task-1-赛题\"><a href=\"#Task-1-赛题\" class=\"headerlink\" title=\"Task 1 赛题\"></a>Task 1 赛题</h4><p>学者画像信息具体包括学者的主页地址、性别、职位等。随着互联网越来越普及，与学者相关的网页的数量和内容的丰富度和复杂度都大大增加，其中包含了学者的大量冗余信息，通过整合互联网上多种来源的学者数据，采用合适的机器学习模型，获得学者的精准信息是一项潜在有效的学者画像技术。</p><p>给定学者姓名、学者所属机构，以及谷歌搜索关键字“学者姓名＋机构”返回的第一页搜索结果的缓存页面的URL（静态页面，一般包含10条搜索结果），同时允许参赛者访问其中每条搜索结果的网页链接以及该网页内的链接，要求抽取学者的个人主页URL、学者头像URL、邮箱地址、性别、职称/职位列表，以及学者当前所属机构所在国家。</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<p>示例:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\">scholar name: Jiawei Han</span><br><span class=\"line\">organization name: UIUC</span><br><span class=\"line\">search results page: http://xxx.xxx/xxx.html</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">homepage: http://hanj.cs.illinois.edu/ (注释: 从搜索结果中找)</span><br><span class=\"line\">pic: http://hanj.cs.illinois.edu/images/hanj_tour.jpg (注释: 从个人主页中找)</span><br><span class=\"line\">email: hanj[at]cs.uiuc.edu (注释: email的格式跟学者个人主页中保持一致)</span><br><span class=\"line\">gender: m (注释: m 代表 male, f 代表 female)</span><br><span class=\"line\">position: Abel Bliss Professor (注释: 需要跟个人主页中的格式保持一致)</span><br><span class=\"line\">location: USA (注释: 需要跟个人主页中的格式保持一致)</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-1-思路\"><a href=\"#Task-1-思路\" class=\"headerlink\" title=\"Task 1 思路\"></a>Task 1 思路</h4><p><strong>个人主页URL</strong></p>\n<p>对于个人主页搜索结果的每条记录提取特征后进行是否是主页的分类，其中概率最高的被判断为个人主页。</p>\n<p>特征如下：</p>\n<ol>\n<li>网站在搜索结果中的排名；-&gt; 1</li>\n<li>学者所在机构是否为大学/学校；-&gt; 1</li>\n<li>搜索结果标题/摘要以及学者所在机构的文本长度 -&gt; 3</li>\n<li>url中是否包含给定关键词(社交类linkedin……、论文类spring……、正向词homepage……)，对每个部分做平均值为特征； -&gt; 35</li>\n<li>搜索结果摘要中是否包含mail，address； -&gt; 2</li>\n<li>搜索结果的标题/摘要中的学者姓名得分。这里将学者姓名分词为n份，如果其中m份在文本中出现，那么姓名的得分就是 m/n，同时增加完整名字在标题摘要中出现的得分； -&gt; 1</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">一级负向词</span><br><span class=\"line\">&apos;facebook&apos;, &apos;linkedin&apos;, &apos;twitter&apos;, &apos;youtube&apos;, &apos;instagram&apos;, &apos;quora&apos;, &apos;blots&apos;, </span><br><span class=\"line\">&apos;dl.acm&apos;, &apos;arxiv&apos;, &apos;ieeexplore&apos;, &apos;springer&apos;, &apos;webofknowledge&apos;, &apos;library&apos;, </span><br><span class=\"line\">&apos;researchgate&apos;, &apos;books.google&apos;, &apos;netprofile&apos;, &apos;cnki&apos;, &apos;ted.com&apos;, &apos;antpedia&apos;, </span><br><span class=\"line\">&apos;academictree&apos;, &apos;phdtree&apos;, &apos;shguang.net&apos;, &apos;airbnb&apos;, &apos;bloomberg&apos;, &apos;pinterest&apos;, </span><br><span class=\"line\">&apos;weibo&apos;, &apos;hmetmer&apos;, &apos;peekyou&apos;, &apos;mitbbs&apos;, &apos;schneier.com&apos;, &apos;directory.ubc.ca&apos;, </span><br><span class=\"line\">&apos;citation&apos;, &apos;videolectures&apos;, &apos;slideshare&apos;, &apos;brokercheck.finra.org&apos;,</span><br><span class=\"line\"></span><br><span class=\"line\">二级负向词</span><br><span class=\"line\">&apos;pdf&apos;, &apos;news&apos;, &apos;report&apos;, &apos;scholarmate&apos;, &apos;journals&apos;, &apos;ratemyprofessors&apos;,</span><br><span class=\"line\">&apos;doc&apos;, &apos;conference&apos;, &apos;wiki&apos;, &apos;scholar&apos;, &apos;book&apos;, &apos;publication&apos;, &apos;github&apos;, &apos;crunchbase&apos;,</span><br><span class=\"line\">&apos;group&apos;, &apos;scholarmate&apos;, &apos;google&apos;, &apos;stories&apos;, &apos;story&apos;, &apos;relationshipscience&apos;,</span><br><span class=\"line\"></span><br><span class=\"line\">正向词</span><br><span class=\"line\">&apos;~&apos;, &apos;edu&apos;, &apos;home&apos;, &apos;faculty&apos;</span><br></pre></td></tr></table></figure>\n<p>由于正负样本不均衡(1:9)，所以首先尝试通过<strong>过采样</strong>的方式进行处理。具体如下：</p>\n<ol>\n<li>重复增加正样本的数量，尝试通过XGBoost模型及SVM模型进行分类，最终在训练集中的效果有提升，但在测试集上变化不大；</li>\n<li>(未实现)尝试通过<a href=\"/2018/06/smote\">SMOTE算法</a>进行过采样。</li>\n</ol>\n<p>鉴于第一种方式的效果并不明显，分析由于样本特征比较少(43维)，导致出现了过拟合的现象出现。</p>\n<p>接下来尝试通过<strong>负采样</strong>的方式进行解决，具体如下：</p>\n<ol>\n<li>因为负样本数量多，进行随机选择，构造新的训练数据集；</li>\n<li>通过XGBoost模型及SVM模型进行分类，效果明显得到提升。</li>\n</ol>\n<p>其中XGBoost效果比SVM效果稍微好一点。</p>\n<ol>\n<li>XGBoost可以自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线性化需要基于核函数，要求学习之前就要定好核函数，然而确定核函数并不是一件容易的事；</li>\n<li>多分类器隐含正则项：XGBoost自带正则化项（包含叶结点数量等），SVM通常需要增加L2正则</li>\n<li>真实世界的数据具有特征多样性的特点，SVM本质是一个几何模型，需要人为去定义样例之间的核函数或者相似性；</li>\n<li>多种可调的参数，效率、灵活、易用方面，XGBoost都有比较好的交互使用。</li>\n</ol>\n<p>最终我们在个人主页上的测试集取得了0.74的准确率，考虑到数据噪声以及搜索结果中多个正确主页的存在，真实的准确率应该接近0.85。</p>\n<p><strong>性别</strong></p>\n<p>主要通过学者的名字进行判断。通过网上的开源姓名性别数据，训练了一个朴素贝叶斯分类器，其中用到的特征是名字的后2位到后7位字母(每一位为一维特征)，以及名字长度，名字分词长度。如果学者名字长度不足的，用空格填充。</p>\n<p>结果上能取得0.9的准确率。</p>\n<p><strong>邮箱</strong></p>\n<p>主要通过正则表达式进行识别。根据<a href=\"https://tools.ietf.org/html/rfc2822\" target=\"_blank\" rel=\"noopener\">RFC2822</a>中的邮箱标准，并结合训练集中邮箱的格式，我们写了一个很复杂的正则表达式，另外针对仍然无法获取到邮箱的，我们捕获网页中的mailto的链接来确定该学者的邮箱。</p>\n<p>这样仍然有一部分的图片邮件或者通过加密技术加密的邮箱无法获得，针对图片邮箱，我们通过捕获图片的名称，判断是否和邮件相关，相关即取该图片的url。\n对于加密邮箱，暂未想到比较合适的方法，而且这种邮箱数量比较少，所以对最终结果应该不大。</p>\n<p><strong>照片</strong></p>\n<p>首先利用正则表达式+xpath等手段，获取到网页中的所有图片url及图片的名称，对于图片名称是学者名字或名字一部分的图片，直接判断该图片为学者的照片；\n若没有获得结果，使用开源工具<a href=\"https://github.com/ageitgey/face_recognition\" target=\"_blank\" rel=\"noopener\">face_recognition</a>对网页的图片进行人脸识别，当该网页中的人脸数量少于阈值4的时候，选择第一个人脸照片为该学者的照片，对人脸数量大于阈值且没有获得邮箱的，认为对学者的个人主页选择错误，并将该学者的信息留空。</p>\n<p><strong>职称</strong></p>\n<p>通过统计训练集中的职称，并做了一些加工之后，使用正则表达式对网页的内容进行匹配。</p>\n<p><strong>国家</strong></p>\n<p>主要根据获取的邮箱、以及网页中出现的地址、电话、传真等地点指向性词进行判断；另外学者的名字有时也具有一定的指向性。</p>\n<p>当无法获得有效信息时，将该信息留空。</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><p>因为该任务需要联网获取数据，且学者的网站服务器良莠不齐，所以实现了一个多进程并行的爬虫。主要用到的Python库如下：</p>\n<ol>\n<li>requests</li>\n<li>selenium</li>\n<li>urllib.request</li>\n<li>lxml</li>\n<li>multiprocessing</li>\n<li>bs4</li>\n</ol>\n<hr>\n<h3 id=\"Task-2-学者兴趣标签预测\"><a href=\"#Task-2-学者兴趣标签预测\" class=\"headerlink\" title=\"Task 2 学者兴趣标签预测\"></a>Task 2 学者兴趣标签预测</h3><h4 id=\"Task-2-赛题\"><a href=\"#Task-2-赛题\" class=\"headerlink\" title=\"Task 2 赛题\"></a>Task 2 赛题</h4><p>研究兴趣是学者画像的重要组成部分，其不仅是学者本身的研究心得或研究拓展方向的集中体现，也能从中窥视不同背景的学者对研究领域热点或学科研究趋势的关注度、敏感度的集体反映。与学者画像信息抽取类似，通过整合互联网上的大规模多源信息，可以对学者的研究兴趣进行判断。</p>\n<p>给定学者已发表的论文信息和合著关系网络，参赛者需为每位学者标注5个最合适的兴趣标签。</p>\n<p>示例:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\">name: Jiawei Han</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">research interests: data mining, database, information networks, knowledge discovery,  machine learning</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-2-思路\"><a href=\"#Task-2-思路\" class=\"headerlink\" title=\"Task 2 思路\"></a>Task 2 思路</h4><p>本任务主要用到论文信息中的title以及期刊，首先将论文title分别以作者和标签为单位收集在一起并打上标签</p>\n<p>主要是基于词袋模型，以训练集作者为单位收集的标题文本，分词去停用次后使用<a href=\"/2018/03/chisquaredtest\">卡方检验</a>进一步筛选特征，按卡方值筛选了前10%的特征词作为特征，\n以标签为单位收集所有相关作者的标题文本，筛选了50%的特征词作为特征。</p>\n<p>确定文档特征词后，采用词的tf-idf作为文档向量相应位置的权重。</p>\n<p>考虑到文本分类大多都是线性可分的最终采用sklean中的SGDClassifier，分别对上述构建的两个数据集（以作者为单位收集的文档和以标签为单位收集的文档）训练了两个模型；尝试过Stacking，sklearn中的BaggingClassifier， GridSearchCV 5 folds来寻找最优参数；</p>\n<p>因为本任务是多标签预测，所以我们根据训练的模型为每个作者输出概率最大的n个标签以及概率(以作者为单位的模型输出24个标签做候选标签集合，以标签为单位的模型输出23个标签做候选标签集合)\n训练两个模型(以作者为单位和以标签为单位)从不同的角度提取的特征，是一个相互补充，相互修正的作用；将两个模型的候选标签集合取并集，遍历候选标签集合，评价的量为每个标签两个模型预测的概率的乘积，\n期间统计作者发过的论文，记录title中出现该标签的频数(限定该作者在合著名单里为2作或者3作以内)，最后在每个标签评价量上加频数 * 评价量。最终排序取top5个标签为当前作者的标签。</p>\n<hr>\n<h3 id=\"Task-3-学者未来影响力预测\"><a href=\"#Task-3-学者未来影响力预测\" class=\"headerlink\" title=\"Task 3 学者未来影响力预测\"></a>Task 3 学者未来影响力预测</h3><h4 id=\"Task-3-赛题\"><a href=\"#Task-3-赛题\" class=\"headerlink\" title=\"Task 3 赛题\"></a>Task 3 赛题</h4><p>学术影响力用来衡量学者在专业理论及技术方面的影响，常用的评价指标有论文被引量，期刊影响因子、作者H指数等，其中论文被引量是一个重要而直观的指标。本任务的目的是基于学者当前的相关学术数据预测其未来某段时间内的总论文被引量。</p>\n<p>给定学者截止到2013年底的全部论文数据（包括论文详细引用关系，详见数据描述部分），参赛者需预测学者截止至2017年6月的总被引用数.</p>\n<p>示例:</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\"><span class=\"built_in\">name</span>: Jiawei Han</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">citation <span class=\"built_in\">number</span>: <span class=\"number\">126147</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-3-思路\"><a href=\"#Task-3-思路\" class=\"headerlink\" title=\"Task 3 思路\"></a>Task 3 思路</h4><p><strong>预处理</strong></p>\n<p>papers.txt包含了学者截止2016年所有论文的数据，其中需要预测的学者隐去了2013年后所发表的论文。所以任务三只用到2013(包含2013)年前的论文数据，\n对paper.txt中的学者进行统计引用数, 去除实际引用数小于统计引用数训练集中的学者。</p>\n<p>统计发现，学者论文引用数为0的，约占整个训练集的24%，故尝试首先通过二分类，判断学者是否具有论文引用量，之后对具有论文引用量的学者进行回归预测(两次使用同样的特征)。</p>\n<p><strong>特征</strong></p>\n<ol>\n<li>学者距今0,1,2,3,4,5,6,7,8,9年的文章数被引用数(cited)；</li>\n<li>学者文章被引用数的平均值，最小值，最大值；</li>\n<li>学者距今0,1,2,3,4,5,6,7,8,9,10,20,30,40,50年的引用数(reference)；</li>\n<li>学者学术年龄、学者发表的会议期刊的数量；</li>\n<li>学者的hindex以及近两年的变化值；</li>\n<li>学者的共同作者每四年中hindex的最小值,最大值,平均值,和；</li>\n<li>会议期刊文章总数，hindex，引用他人文章的最小值，最大值，平均值以及近两年的变化值；</li>\n<li>会议期刊被引用数的中最小值，最大值，平均值，和；</li>\n<li>会议期刊按被引用数排序的特征；</li>\n<li>以共同作者关系建边图的pagerank值，以及使用共同作者次数加权的weighted_pagerank(作者合作的越多，引用数越接近)；</li>\n<li>使用每篇论文title建模<a href=\"/2018/07/ldatopic\">LDA模型</a>求出每篇论文困惑度(perplexity)和主题(topic)权值，和每个topic的被引用数，得到每个作者论文按主题计算的引用数和排序结果；</li>\n<li>通过有label的训练集中作者的真实被引用数估计每篇论文的权重，然后使用每篇文章的权重来预测未知作者的权重weight1(作者的权重由其论文权重决定)；</li>\n<li>根据训练集的label和论文被引用数关系迭代训练得出每篇论文的权值，从而每个作者得到权重weight2(比起weight1使用了paper.txt统计的论文被引用数)；</li>\n</ol>\n<p><strong>模型</strong></p>\n<ul>\n<li>分类模型：XGBClassifier(训练集上统计分类准确率为92.3%)</li>\n<li>回归模型：XGBoost中XGBRegressor, sklearn中的RandomForestRegressor, ExtraTreesRegressor, SVR, LinearRegressor，并对结果进行Stacking集成</li>\n</ul>\n<p><strong>其他trick</strong></p>\n<p>学者的论文被引用数的跨度很大，有0，有10W+，所以在预测的时候对标签使用了log进行平滑；</p>\n<hr>\n<h3 id=\"评测方案\"><a href=\"#评测方案\" class=\"headerlink\" title=\"评测方案\"></a>评测方案</h3><h4 id=\"Task-1\"><a href=\"#Task-1\" class=\"headerlink\" title=\"Task 1\"></a>Task 1</h4><p>任务1中，每个学者共有 k (在该项任务中 k = 6)项个人画像数据需要预测。这 k 个中除了职称/职位外都通过完全匹配的方式进行评测，完全匹配上得1分，否则得0分；而职称/职位由于是一个集合，所以通过Jaccard index (即两个集合交集的元素个数除以两个集合并集的元素个数) 来进行计算提取出来的集合跟标注答案给出的集合的相似度 (介于0 ~ 1之间)。一个学者的画像数据的预测值的最终得分为这 k 个的平均分。一个参赛选手在该任务上的最终得分为对各个学者的画像数据的预测得分的平均分。即：</p>\n<script type=\"math/tex; mode=display\">\nscore1 = \\frac{1}{kN} \\sum_{i=1}^N \\sum_{j=1}^k s_i</script><p>其中$ s_i $表示 k 项画像数据中第 i 项的得分，取值0 ~ 1。</p>\n<p><br></p>\n<h4 id=\"Task-2\"><a href=\"#Task-2\" class=\"headerlink\" title=\"Task 2\"></a>Task 2</h4><p>任务2的得分score2为参赛队伍计算生成的学者兴趣与给定的学者兴趣完全相同的比例，即：</p>\n<script type=\"math/tex; mode=display\">\nscore2 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\| T_i \\bigcap T_i^{\\ast} \\|}{\\| T_i^{\\ast} \\|}</script><p>其中，N为任务2的评测集样本个数，$T_i$为计算生成的用户i 的兴趣集合,$T_i^{\\ast}$为给定的用户i 的兴趣集合。</p>\n<p><br></p>\n<h4 id=\"Task-3\"><a href=\"#Task-3\" class=\"headerlink\" title=\"Task 3\"></a>Task 3</h4><p>任务3的得分score3用参赛队伍预测的学者被引数与给定的学者真实被引数之间的相对误差来计算来计算</p>\n<script type=\"math/tex; mode=display\">\nscore3 = 1 - \\frac{1}{N} \\sum_{i=1}^N \n\\begin{cases} \n0, \\qquad \\qquad \\quad v_i=0, v_i^{\\ast}=0 \\\\\n\\| v_i - v_i^{\\ast} \\| / max(v_i, v_i^{\\ast}), \\quad otherwise\n\\end{cases}</script><p>其中，N 为任务3的评测集样本个数，$v_i$为用户i 的预测被引数，$v_i^{\\ast}$为用户i 的真实被引数。</p>\n<p><br></p>\n<h4 id=\"最终得分\"><a href=\"#最终得分\" class=\"headerlink\" title=\"最终得分\"></a>最终得分</h4><script type=\"math/tex; mode=display\">\nscore = score1 + score2 + score3</script>","site":{"data":{}},"excerpt":"<p><strong>队伍名：ICRC@HITSZ，获决赛第三名(3/412)</strong>，<a href=\"https://biendata.com/competition/scholar/\" target=\"_blank\" rel=\"noopener\">大赛主页</a></p><p><img src=\"/posts_res/2018-03-01-2017aca/results.png\" alt=\"results\"></p><p><strong>本文是对比赛的思路等的整理，包含后期参考其他队的思路</strong></p><p>目录</p><ul>\n<li>Task 1 学者画像信息抽取</li>\n<li>Task 2 学者兴趣标签预测</li>\n<li>Task 3 学者未来影响力预测</li>\n<li>评测方案</li>\n</ul><hr><h3 id=\"Task-1-学者画像信息抽取\"><a href=\"#Task-1-学者画像信息抽取\" class=\"headerlink\" title=\"Task 1 学者画像信息抽取\"></a>Task 1 学者画像信息抽取</h3><h4 id=\"Task-1-赛题\"><a href=\"#Task-1-赛题\" class=\"headerlink\" title=\"Task 1 赛题\"></a>Task 1 赛题</h4><p>学者画像信息具体包括学者的主页地址、性别、职位等。随着互联网越来越普及，与学者相关的网页的数量和内容的丰富度和复杂度都大大增加，其中包含了学者的大量冗余信息，通过整合互联网上多种来源的学者数据，采用合适的机器学习模型，获得学者的精准信息是一项潜在有效的学者画像技术。</p><p>给定学者姓名、学者所属机构，以及谷歌搜索关键字“学者姓名＋机构”返回的第一页搜索结果的缓存页面的URL（静态页面，一般包含10条搜索结果），同时允许参赛者访问其中每条搜索结果的网页链接以及该网页内的链接，要求抽取学者的个人主页URL、学者头像URL、邮箱地址、性别、职称/职位列表，以及学者当前所属机构所在国家。</p>","more":"\n\n\n\n\n\n\n\n<p>示例:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\">scholar name: Jiawei Han</span><br><span class=\"line\">organization name: UIUC</span><br><span class=\"line\">search results page: http://xxx.xxx/xxx.html</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">homepage: http://hanj.cs.illinois.edu/ (注释: 从搜索结果中找)</span><br><span class=\"line\">pic: http://hanj.cs.illinois.edu/images/hanj_tour.jpg (注释: 从个人主页中找)</span><br><span class=\"line\">email: hanj[at]cs.uiuc.edu (注释: email的格式跟学者个人主页中保持一致)</span><br><span class=\"line\">gender: m (注释: m 代表 male, f 代表 female)</span><br><span class=\"line\">position: Abel Bliss Professor (注释: 需要跟个人主页中的格式保持一致)</span><br><span class=\"line\">location: USA (注释: 需要跟个人主页中的格式保持一致)</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-1-思路\"><a href=\"#Task-1-思路\" class=\"headerlink\" title=\"Task 1 思路\"></a>Task 1 思路</h4><p><strong>个人主页URL</strong></p>\n<p>对于个人主页搜索结果的每条记录提取特征后进行是否是主页的分类，其中概率最高的被判断为个人主页。</p>\n<p>特征如下：</p>\n<ol>\n<li>网站在搜索结果中的排名；-&gt; 1</li>\n<li>学者所在机构是否为大学/学校；-&gt; 1</li>\n<li>搜索结果标题/摘要以及学者所在机构的文本长度 -&gt; 3</li>\n<li>url中是否包含给定关键词(社交类linkedin……、论文类spring……、正向词homepage……)，对每个部分做平均值为特征； -&gt; 35</li>\n<li>搜索结果摘要中是否包含mail，address； -&gt; 2</li>\n<li>搜索结果的标题/摘要中的学者姓名得分。这里将学者姓名分词为n份，如果其中m份在文本中出现，那么姓名的得分就是 m/n，同时增加完整名字在标题摘要中出现的得分； -&gt; 1</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">一级负向词</span><br><span class=\"line\">&apos;facebook&apos;, &apos;linkedin&apos;, &apos;twitter&apos;, &apos;youtube&apos;, &apos;instagram&apos;, &apos;quora&apos;, &apos;blots&apos;, </span><br><span class=\"line\">&apos;dl.acm&apos;, &apos;arxiv&apos;, &apos;ieeexplore&apos;, &apos;springer&apos;, &apos;webofknowledge&apos;, &apos;library&apos;, </span><br><span class=\"line\">&apos;researchgate&apos;, &apos;books.google&apos;, &apos;netprofile&apos;, &apos;cnki&apos;, &apos;ted.com&apos;, &apos;antpedia&apos;, </span><br><span class=\"line\">&apos;academictree&apos;, &apos;phdtree&apos;, &apos;shguang.net&apos;, &apos;airbnb&apos;, &apos;bloomberg&apos;, &apos;pinterest&apos;, </span><br><span class=\"line\">&apos;weibo&apos;, &apos;hmetmer&apos;, &apos;peekyou&apos;, &apos;mitbbs&apos;, &apos;schneier.com&apos;, &apos;directory.ubc.ca&apos;, </span><br><span class=\"line\">&apos;citation&apos;, &apos;videolectures&apos;, &apos;slideshare&apos;, &apos;brokercheck.finra.org&apos;,</span><br><span class=\"line\"></span><br><span class=\"line\">二级负向词</span><br><span class=\"line\">&apos;pdf&apos;, &apos;news&apos;, &apos;report&apos;, &apos;scholarmate&apos;, &apos;journals&apos;, &apos;ratemyprofessors&apos;,</span><br><span class=\"line\">&apos;doc&apos;, &apos;conference&apos;, &apos;wiki&apos;, &apos;scholar&apos;, &apos;book&apos;, &apos;publication&apos;, &apos;github&apos;, &apos;crunchbase&apos;,</span><br><span class=\"line\">&apos;group&apos;, &apos;scholarmate&apos;, &apos;google&apos;, &apos;stories&apos;, &apos;story&apos;, &apos;relationshipscience&apos;,</span><br><span class=\"line\"></span><br><span class=\"line\">正向词</span><br><span class=\"line\">&apos;~&apos;, &apos;edu&apos;, &apos;home&apos;, &apos;faculty&apos;</span><br></pre></td></tr></table></figure>\n<p>由于正负样本不均衡(1:9)，所以首先尝试通过<strong>过采样</strong>的方式进行处理。具体如下：</p>\n<ol>\n<li>重复增加正样本的数量，尝试通过XGBoost模型及SVM模型进行分类，最终在训练集中的效果有提升，但在测试集上变化不大；</li>\n<li>(未实现)尝试通过<a href=\"/2018/06/smote\">SMOTE算法</a>进行过采样。</li>\n</ol>\n<p>鉴于第一种方式的效果并不明显，分析由于样本特征比较少(43维)，导致出现了过拟合的现象出现。</p>\n<p>接下来尝试通过<strong>负采样</strong>的方式进行解决，具体如下：</p>\n<ol>\n<li>因为负样本数量多，进行随机选择，构造新的训练数据集；</li>\n<li>通过XGBoost模型及SVM模型进行分类，效果明显得到提升。</li>\n</ol>\n<p>其中XGBoost效果比SVM效果稍微好一点。</p>\n<ol>\n<li>XGBoost可以自适应非线性：随着决策树的生长，能够产生高度非线性的模型，而SVM等线性模型的非线性化需要基于核函数，要求学习之前就要定好核函数，然而确定核函数并不是一件容易的事；</li>\n<li>多分类器隐含正则项：XGBoost自带正则化项（包含叶结点数量等），SVM通常需要增加L2正则</li>\n<li>真实世界的数据具有特征多样性的特点，SVM本质是一个几何模型，需要人为去定义样例之间的核函数或者相似性；</li>\n<li>多种可调的参数，效率、灵活、易用方面，XGBoost都有比较好的交互使用。</li>\n</ol>\n<p>最终我们在个人主页上的测试集取得了0.74的准确率，考虑到数据噪声以及搜索结果中多个正确主页的存在，真实的准确率应该接近0.85。</p>\n<p><strong>性别</strong></p>\n<p>主要通过学者的名字进行判断。通过网上的开源姓名性别数据，训练了一个朴素贝叶斯分类器，其中用到的特征是名字的后2位到后7位字母(每一位为一维特征)，以及名字长度，名字分词长度。如果学者名字长度不足的，用空格填充。</p>\n<p>结果上能取得0.9的准确率。</p>\n<p><strong>邮箱</strong></p>\n<p>主要通过正则表达式进行识别。根据<a href=\"https://tools.ietf.org/html/rfc2822\" target=\"_blank\" rel=\"noopener\">RFC2822</a>中的邮箱标准，并结合训练集中邮箱的格式，我们写了一个很复杂的正则表达式，另外针对仍然无法获取到邮箱的，我们捕获网页中的mailto的链接来确定该学者的邮箱。</p>\n<p>这样仍然有一部分的图片邮件或者通过加密技术加密的邮箱无法获得，针对图片邮箱，我们通过捕获图片的名称，判断是否和邮件相关，相关即取该图片的url。\n对于加密邮箱，暂未想到比较合适的方法，而且这种邮箱数量比较少，所以对最终结果应该不大。</p>\n<p><strong>照片</strong></p>\n<p>首先利用正则表达式+xpath等手段，获取到网页中的所有图片url及图片的名称，对于图片名称是学者名字或名字一部分的图片，直接判断该图片为学者的照片；\n若没有获得结果，使用开源工具<a href=\"https://github.com/ageitgey/face_recognition\" target=\"_blank\" rel=\"noopener\">face_recognition</a>对网页的图片进行人脸识别，当该网页中的人脸数量少于阈值4的时候，选择第一个人脸照片为该学者的照片，对人脸数量大于阈值且没有获得邮箱的，认为对学者的个人主页选择错误，并将该学者的信息留空。</p>\n<p><strong>职称</strong></p>\n<p>通过统计训练集中的职称，并做了一些加工之后，使用正则表达式对网页的内容进行匹配。</p>\n<p><strong>国家</strong></p>\n<p>主要根据获取的邮箱、以及网页中出现的地址、电话、传真等地点指向性词进行判断；另外学者的名字有时也具有一定的指向性。</p>\n<p>当无法获得有效信息时，将该信息留空。</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><p>因为该任务需要联网获取数据，且学者的网站服务器良莠不齐，所以实现了一个多进程并行的爬虫。主要用到的Python库如下：</p>\n<ol>\n<li>requests</li>\n<li>selenium</li>\n<li>urllib.request</li>\n<li>lxml</li>\n<li>multiprocessing</li>\n<li>bs4</li>\n</ol>\n<hr>\n<h3 id=\"Task-2-学者兴趣标签预测\"><a href=\"#Task-2-学者兴趣标签预测\" class=\"headerlink\" title=\"Task 2 学者兴趣标签预测\"></a>Task 2 学者兴趣标签预测</h3><h4 id=\"Task-2-赛题\"><a href=\"#Task-2-赛题\" class=\"headerlink\" title=\"Task 2 赛题\"></a>Task 2 赛题</h4><p>研究兴趣是学者画像的重要组成部分，其不仅是学者本身的研究心得或研究拓展方向的集中体现，也能从中窥视不同背景的学者对研究领域热点或学科研究趋势的关注度、敏感度的集体反映。与学者画像信息抽取类似，通过整合互联网上的大规模多源信息，可以对学者的研究兴趣进行判断。</p>\n<p>给定学者已发表的论文信息和合著关系网络，参赛者需为每位学者标注5个最合适的兴趣标签。</p>\n<p>示例:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\">name: Jiawei Han</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">research interests: data mining, database, information networks, knowledge discovery,  machine learning</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-2-思路\"><a href=\"#Task-2-思路\" class=\"headerlink\" title=\"Task 2 思路\"></a>Task 2 思路</h4><p>本任务主要用到论文信息中的title以及期刊，首先将论文title分别以作者和标签为单位收集在一起并打上标签</p>\n<p>主要是基于词袋模型，以训练集作者为单位收集的标题文本，分词去停用次后使用<a href=\"/2018/03/chisquaredtest\">卡方检验</a>进一步筛选特征，按卡方值筛选了前10%的特征词作为特征，\n以标签为单位收集所有相关作者的标题文本，筛选了50%的特征词作为特征。</p>\n<p>确定文档特征词后，采用词的tf-idf作为文档向量相应位置的权重。</p>\n<p>考虑到文本分类大多都是线性可分的最终采用sklean中的SGDClassifier，分别对上述构建的两个数据集（以作者为单位收集的文档和以标签为单位收集的文档）训练了两个模型；尝试过Stacking，sklearn中的BaggingClassifier， GridSearchCV 5 folds来寻找最优参数；</p>\n<p>因为本任务是多标签预测，所以我们根据训练的模型为每个作者输出概率最大的n个标签以及概率(以作者为单位的模型输出24个标签做候选标签集合，以标签为单位的模型输出23个标签做候选标签集合)\n训练两个模型(以作者为单位和以标签为单位)从不同的角度提取的特征，是一个相互补充，相互修正的作用；将两个模型的候选标签集合取并集，遍历候选标签集合，评价的量为每个标签两个模型预测的概率的乘积，\n期间统计作者发过的论文，记录title中出现该标签的频数(限定该作者在合著名单里为2作或者3作以内)，最后在每个标签评价量上加频数 * 评价量。最终排序取top5个标签为当前作者的标签。</p>\n<hr>\n<h3 id=\"Task-3-学者未来影响力预测\"><a href=\"#Task-3-学者未来影响力预测\" class=\"headerlink\" title=\"Task 3 学者未来影响力预测\"></a>Task 3 学者未来影响力预测</h3><h4 id=\"Task-3-赛题\"><a href=\"#Task-3-赛题\" class=\"headerlink\" title=\"Task 3 赛题\"></a>Task 3 赛题</h4><p>学术影响力用来衡量学者在专业理论及技术方面的影响，常用的评价指标有论文被引量，期刊影响因子、作者H指数等，其中论文被引量是一个重要而直观的指标。本任务的目的是基于学者当前的相关学术数据预测其未来某段时间内的总论文被引量。</p>\n<p>给定学者截止到2013年底的全部论文数据（包括论文详细引用关系，详见数据描述部分），参赛者需预测学者截止至2017年6月的总被引用数.</p>\n<p>示例:</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入：</span><br><span class=\"line\"><span class=\"built_in\">name</span>: Jiawei Han</span><br><span class=\"line\"></span><br><span class=\"line\">输出：</span><br><span class=\"line\">citation <span class=\"built_in\">number</span>: <span class=\"number\">126147</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<h4 id=\"Task-3-思路\"><a href=\"#Task-3-思路\" class=\"headerlink\" title=\"Task 3 思路\"></a>Task 3 思路</h4><p><strong>预处理</strong></p>\n<p>papers.txt包含了学者截止2016年所有论文的数据，其中需要预测的学者隐去了2013年后所发表的论文。所以任务三只用到2013(包含2013)年前的论文数据，\n对paper.txt中的学者进行统计引用数, 去除实际引用数小于统计引用数训练集中的学者。</p>\n<p>统计发现，学者论文引用数为0的，约占整个训练集的24%，故尝试首先通过二分类，判断学者是否具有论文引用量，之后对具有论文引用量的学者进行回归预测(两次使用同样的特征)。</p>\n<p><strong>特征</strong></p>\n<ol>\n<li>学者距今0,1,2,3,4,5,6,7,8,9年的文章数被引用数(cited)；</li>\n<li>学者文章被引用数的平均值，最小值，最大值；</li>\n<li>学者距今0,1,2,3,4,5,6,7,8,9,10,20,30,40,50年的引用数(reference)；</li>\n<li>学者学术年龄、学者发表的会议期刊的数量；</li>\n<li>学者的hindex以及近两年的变化值；</li>\n<li>学者的共同作者每四年中hindex的最小值,最大值,平均值,和；</li>\n<li>会议期刊文章总数，hindex，引用他人文章的最小值，最大值，平均值以及近两年的变化值；</li>\n<li>会议期刊被引用数的中最小值，最大值，平均值，和；</li>\n<li>会议期刊按被引用数排序的特征；</li>\n<li>以共同作者关系建边图的pagerank值，以及使用共同作者次数加权的weighted_pagerank(作者合作的越多，引用数越接近)；</li>\n<li>使用每篇论文title建模<a href=\"/2018/07/ldatopic\">LDA模型</a>求出每篇论文困惑度(perplexity)和主题(topic)权值，和每个topic的被引用数，得到每个作者论文按主题计算的引用数和排序结果；</li>\n<li>通过有label的训练集中作者的真实被引用数估计每篇论文的权重，然后使用每篇文章的权重来预测未知作者的权重weight1(作者的权重由其论文权重决定)；</li>\n<li>根据训练集的label和论文被引用数关系迭代训练得出每篇论文的权值，从而每个作者得到权重weight2(比起weight1使用了paper.txt统计的论文被引用数)；</li>\n</ol>\n<p><strong>模型</strong></p>\n<ul>\n<li>分类模型：XGBClassifier(训练集上统计分类准确率为92.3%)</li>\n<li>回归模型：XGBoost中XGBRegressor, sklearn中的RandomForestRegressor, ExtraTreesRegressor, SVR, LinearRegressor，并对结果进行Stacking集成</li>\n</ul>\n<p><strong>其他trick</strong></p>\n<p>学者的论文被引用数的跨度很大，有0，有10W+，所以在预测的时候对标签使用了log进行平滑；</p>\n<hr>\n<h3 id=\"评测方案\"><a href=\"#评测方案\" class=\"headerlink\" title=\"评测方案\"></a>评测方案</h3><h4 id=\"Task-1\"><a href=\"#Task-1\" class=\"headerlink\" title=\"Task 1\"></a>Task 1</h4><p>任务1中，每个学者共有 k (在该项任务中 k = 6)项个人画像数据需要预测。这 k 个中除了职称/职位外都通过完全匹配的方式进行评测，完全匹配上得1分，否则得0分；而职称/职位由于是一个集合，所以通过Jaccard index (即两个集合交集的元素个数除以两个集合并集的元素个数) 来进行计算提取出来的集合跟标注答案给出的集合的相似度 (介于0 ~ 1之间)。一个学者的画像数据的预测值的最终得分为这 k 个的平均分。一个参赛选手在该任务上的最终得分为对各个学者的画像数据的预测得分的平均分。即：</p>\n<script type=\"math/tex; mode=display\">\nscore1 = \\frac{1}{kN} \\sum_{i=1}^N \\sum_{j=1}^k s_i</script><p>其中$ s_i $表示 k 项画像数据中第 i 项的得分，取值0 ~ 1。</p>\n<p><br></p>\n<h4 id=\"Task-2\"><a href=\"#Task-2\" class=\"headerlink\" title=\"Task 2\"></a>Task 2</h4><p>任务2的得分score2为参赛队伍计算生成的学者兴趣与给定的学者兴趣完全相同的比例，即：</p>\n<script type=\"math/tex; mode=display\">\nscore2 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\| T_i \\bigcap T_i^{\\ast} \\|}{\\| T_i^{\\ast} \\|}</script><p>其中，N为任务2的评测集样本个数，$T_i$为计算生成的用户i 的兴趣集合,$T_i^{\\ast}$为给定的用户i 的兴趣集合。</p>\n<p><br></p>\n<h4 id=\"Task-3\"><a href=\"#Task-3\" class=\"headerlink\" title=\"Task 3\"></a>Task 3</h4><p>任务3的得分score3用参赛队伍预测的学者被引数与给定的学者真实被引数之间的相对误差来计算来计算</p>\n<script type=\"math/tex; mode=display\">\nscore3 = 1 - \\frac{1}{N} \\sum_{i=1}^N \n\\begin{cases} \n0, \\qquad \\qquad \\quad v_i=0, v_i^{\\ast}=0 \\\\\n\\| v_i - v_i^{\\ast} \\| / max(v_i, v_i^{\\ast}), \\quad otherwise\n\\end{cases}</script><p>其中，N 为任务3的评测集样本个数，$v_i$为用户i 的预测被引数，$v_i^{\\ast}$为用户i 的真实被引数。</p>\n<p><br></p>\n<h4 id=\"最终得分\"><a href=\"#最终得分\" class=\"headerlink\" title=\"最终得分\"></a>最终得分</h4><script type=\"math/tex; mode=display\">\nscore = score1 + score2 + score3</script>"},{"layout":"post","title":"逻辑斯谛回归 Logistic Regression","date":"2018-03-11T03:38:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>逻辑斯谛回归 - Logistic Regression</center>\n\n#### 目录\n* 逻辑斯谛分布\n* 逻辑斯谛回归模型\n* 逻辑斯谛模型的参数估计\n* 逻辑斯谛模型的特点\n* 代码实现\n\n``逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。``\n\n\n-----\n\n### 1. 逻辑斯谛分布\n设 \\\\( X \\\\) 是连续随机变量，\\\\( X \\\\) 服从逻辑斯谛分布是指 \\\\( X \\\\) 具有下列分布函数和密度函数：\n\n\\\\[\nF(x)=P(X\\le{x})=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n\\\\]\n\n\\\\[\nf(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^{2}}\n\\\\]\n\n密度函数与分布函数图像如下,其中分布函数以\\\\( (\\mu,\\frac{1}{2}) \\\\)中心对称。\n\n![fenbu](/posts_res/2018-03-11-logistic-regression/1-1.png)\n\n\n-----\n\n### 2. 逻辑斯谛回归模型\n\n\\\\[ \nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} \n\\\\]\n\n\\\\[ \nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)} \n\\\\]\n\n\n-----\n\n### 3. 逻辑斯谛模型的参数估计\n\n设\n\n\\\\[\nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)}=\\pi(x)\n\\\\]\n\n\\\\[\nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)}=1-\\pi(x)\n\\\\]\n\n似然函数(N为样本数量)：\n\n\\\\[\nl(w)=\\prod_{i=1}^{N}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n\\\\]\n\n对数似然：\n\n\\\\[\nL(w)=logl(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]\n\\\\]\n\n对\\\\( L(w) \\\\)求极值\n\n<center>\n<img src=\"/posts_res/2018-03-11-logistic-regression/1-4.gif\">\n</center>\n\n其中\\\\( x\\_i \\\\)表示第\\\\( i \\\\)个训练样例；\\\\(x\\_{ij} \\\\)表示第\\\\( i \\\\)个训练样例的第\\\\( j \\\\)个维度。\n\n\\\\( N \\\\)表示样本数量；\n\\\\( D \\\\)表示样本维度。\n\n\n-------\n\n### 4. 逻辑斯谛模型的特点\n\n* 优点： \n\n形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。\n\n模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。\n\n训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。\n\n资源占用小，尤其是内存。因为只需要存储各个维度的特征值。\n\n方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。\n\n\n* 缺点：\n\n准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。\n\n很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。\n\n处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。\n\n逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。\n\n\n------\n\n### 5. 代码实现\n\n使用sklearn中的load\\_breast\\_cancer()作为二分类数据集。\n\n[Binary Logistic Regression 代码](/posts_res/2018-03-11-logistic-regression/logistic_regression.py)\n    \n    # coding:utf-8\n    import time\n    import numpy as np\n    from datetime import timedelta\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import load_breast_cancer\n    \n    def time_consume(s_t):\n        diff = time.time()-s_t\n        return timedelta(seconds=int(diff))\n    \n    class BinaryLR(object):\n        def __init__(self):\n            self.weight = None\n            self.learning_rate = 0.001\n            self.max_iteration = 3000\n    \n        def predict_single_sample(self, feature):\n            feature = list(feature)\n            feature.append(1.0)\n            wx = np.sum(np.matmul(self.weight, feature))\n            exp_wx = np.exp(wx)\n            if exp_wx/(1+exp_wx) > 0.5:\n                return 1\n            else:\n                return 0\n    \n        def fit(self, X, y):\n            self.nlen = X.shape[0]\n            self.ndim = X.shape[1]\n            self.weight = np.zeros(shape=self.ndim+1) # add bias to weight\n    \n            correct = 0\n            exec_times = 0\n            while exec_times < self.max_iteration:\n                index = np.random.randint(0, self.nlen)\n                feature = list(X[index])\n                label = self.predict_single_sample(feature)\n    \n                if label == y[index]:\n                    correct += 1\n                    if correct > self.max_iteration:\n                        break\n                    continue\n    \n                exec_times += 1\n                correct = 0\n    \n                feature.append(1.0)\n                wx = np.sum(np.matmul(self.weight, feature))\n                exp_wx = np.exp(wx)\n    \n                # update weight\n                for i in range(self.weight.shape[0]):\n                    self.weight[i] -= self.learning_rate * (label - exp_wx / (1.0 + exp_wx)) * feature[i]\n    \n                if exec_times % 100 == 0:\n                    print(\"Times:%d TrainAcc:%.4f Timeusage:%s\" % (exec_times, self.accuracy(train_y, self.predict(train_x)), time_consume(start_time)))\n    \n        def predict(self, X):\n            if self.weight is None:\n                raise ValueError(\"Please train model first.\")\n    \n            labels = list()\n            for i in range(X.shape[0]):\n                d = X[i]\n                labels.append(self.predict_single_sample(d))\n            return labels\n    \n        def accuracy(self, y_true, y_pred):\n            return accuracy_score(y_true, y_pred)\n    \n    def load_dataset():\n        data = load_breast_cancer()\n        return data.data, data.target\n    \n    \n    if __name__ == '__main__':\n        start_time = time.time()\n    \n        data, target = load_dataset()\n        print(\"Data Shape:\", data.shape, target.shape)\n    \n        train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=1024, shuffle=True)\n    \n        ml = BinaryLR()\n        ml.fit(train_x, train_y)\n        y_pred = ml.predict(test_x)\n        accuracy = ml.accuracy(test_y, y_pred)\n        print(\"Accuracy is \", accuracy)\n        print(\"Timeusage: %.2f s\" % (time.time()-start_time))\n\n\n运行结果如下：（不同的划分会产生不同的结果，且差异比较大）\n\n    Times:100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1000 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2000 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:3000 TrainAcc:0.8518 Timeusage:0:00:00\n    Accuracy is  0.877192982456\n    Timeusage: 0.71 s\n\n\n-----------\n\n### 面试常问\n\n* 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？\n\n损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。\n将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。\n$$ \\theta_j = \\theta_j - (y_i - h_{\\theta}(x_i)) \\cdot x_{ij} $$\n更新速度只和$ x_{ij}，y_i $相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。\n\n为什么不选平方损失函数的呢？\n其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和 sigmod 函数本身的梯度是很相关的。\nsigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。\n\n* 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？\n\n先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。\n\n但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。\n\n如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。\n\n* 为什么我们还是会在训练的过程当中将高度相关的特征去掉？\n\n去掉高度相关的特征会让模型的可解释性更好\n\n可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。\n其次是特征多了，本身就会增大训练的时间。\n\n\n-----------\n\n### 参考：\n\n> 1. 李航 - 《统计学习方法》\n> 2. 周志华 - 《机器学习》\n> 3. [scikit-learn](http://scikit-learn.org/)\n> 4. [WenDesi's Github](https://github.com/WenDesi/lihang_book_algorithm)\n","source":"_posts/2018-03-11-logistic-regression.md","raw":"---\nlayout: post\ntitle: 逻辑斯谛回归 Logistic Regression\ndate: 2018-03-11 11:38 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>逻辑斯谛回归 - Logistic Regression</center>\n\n#### 目录\n* 逻辑斯谛分布\n* 逻辑斯谛回归模型\n* 逻辑斯谛模型的参数估计\n* 逻辑斯谛模型的特点\n* 代码实现\n\n``逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。``\n\n\n-----\n\n### 1. 逻辑斯谛分布\n设 \\\\( X \\\\) 是连续随机变量，\\\\( X \\\\) 服从逻辑斯谛分布是指 \\\\( X \\\\) 具有下列分布函数和密度函数：\n\n\\\\[\nF(x)=P(X\\le{x})=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n\\\\]\n\n\\\\[\nf(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^{2}}\n\\\\]\n\n密度函数与分布函数图像如下,其中分布函数以\\\\( (\\mu,\\frac{1}{2}) \\\\)中心对称。\n\n![fenbu](/posts_res/2018-03-11-logistic-regression/1-1.png)\n\n\n-----\n\n### 2. 逻辑斯谛回归模型\n\n\\\\[ \nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} \n\\\\]\n\n\\\\[ \nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)} \n\\\\]\n\n\n-----\n\n### 3. 逻辑斯谛模型的参数估计\n\n设\n\n\\\\[\nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)}=\\pi(x)\n\\\\]\n\n\\\\[\nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)}=1-\\pi(x)\n\\\\]\n\n似然函数(N为样本数量)：\n\n\\\\[\nl(w)=\\prod_{i=1}^{N}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n\\\\]\n\n对数似然：\n\n\\\\[\nL(w)=logl(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]\n\\\\]\n\n对\\\\( L(w) \\\\)求极值\n\n<center>\n<img src=\"/posts_res/2018-03-11-logistic-regression/1-4.gif\">\n</center>\n\n其中\\\\( x\\_i \\\\)表示第\\\\( i \\\\)个训练样例；\\\\(x\\_{ij} \\\\)表示第\\\\( i \\\\)个训练样例的第\\\\( j \\\\)个维度。\n\n\\\\( N \\\\)表示样本数量；\n\\\\( D \\\\)表示样本维度。\n\n\n-------\n\n### 4. 逻辑斯谛模型的特点\n\n* 优点： \n\n形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。\n\n模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。\n\n训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。\n\n资源占用小，尤其是内存。因为只需要存储各个维度的特征值。\n\n方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。\n\n\n* 缺点：\n\n准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。\n\n很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。\n\n处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。\n\n逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。\n\n\n------\n\n### 5. 代码实现\n\n使用sklearn中的load\\_breast\\_cancer()作为二分类数据集。\n\n[Binary Logistic Regression 代码](/posts_res/2018-03-11-logistic-regression/logistic_regression.py)\n    \n    # coding:utf-8\n    import time\n    import numpy as np\n    from datetime import timedelta\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import load_breast_cancer\n    \n    def time_consume(s_t):\n        diff = time.time()-s_t\n        return timedelta(seconds=int(diff))\n    \n    class BinaryLR(object):\n        def __init__(self):\n            self.weight = None\n            self.learning_rate = 0.001\n            self.max_iteration = 3000\n    \n        def predict_single_sample(self, feature):\n            feature = list(feature)\n            feature.append(1.0)\n            wx = np.sum(np.matmul(self.weight, feature))\n            exp_wx = np.exp(wx)\n            if exp_wx/(1+exp_wx) > 0.5:\n                return 1\n            else:\n                return 0\n    \n        def fit(self, X, y):\n            self.nlen = X.shape[0]\n            self.ndim = X.shape[1]\n            self.weight = np.zeros(shape=self.ndim+1) # add bias to weight\n    \n            correct = 0\n            exec_times = 0\n            while exec_times < self.max_iteration:\n                index = np.random.randint(0, self.nlen)\n                feature = list(X[index])\n                label = self.predict_single_sample(feature)\n    \n                if label == y[index]:\n                    correct += 1\n                    if correct > self.max_iteration:\n                        break\n                    continue\n    \n                exec_times += 1\n                correct = 0\n    \n                feature.append(1.0)\n                wx = np.sum(np.matmul(self.weight, feature))\n                exp_wx = np.exp(wx)\n    \n                # update weight\n                for i in range(self.weight.shape[0]):\n                    self.weight[i] -= self.learning_rate * (label - exp_wx / (1.0 + exp_wx)) * feature[i]\n    \n                if exec_times % 100 == 0:\n                    print(\"Times:%d TrainAcc:%.4f Timeusage:%s\" % (exec_times, self.accuracy(train_y, self.predict(train_x)), time_consume(start_time)))\n    \n        def predict(self, X):\n            if self.weight is None:\n                raise ValueError(\"Please train model first.\")\n    \n            labels = list()\n            for i in range(X.shape[0]):\n                d = X[i]\n                labels.append(self.predict_single_sample(d))\n            return labels\n    \n        def accuracy(self, y_true, y_pred):\n            return accuracy_score(y_true, y_pred)\n    \n    def load_dataset():\n        data = load_breast_cancer()\n        return data.data, data.target\n    \n    \n    if __name__ == '__main__':\n        start_time = time.time()\n    \n        data, target = load_dataset()\n        print(\"Data Shape:\", data.shape, target.shape)\n    \n        train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=1024, shuffle=True)\n    \n        ml = BinaryLR()\n        ml.fit(train_x, train_y)\n        y_pred = ml.predict(test_x)\n        accuracy = ml.accuracy(test_y, y_pred)\n        print(\"Accuracy is \", accuracy)\n        print(\"Timeusage: %.2f s\" % (time.time()-start_time))\n\n\n运行结果如下：（不同的划分会产生不同的结果，且差异比较大）\n\n    Times:100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1000 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:1900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2000 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2100 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2200 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2300 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2400 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2500 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2600 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2700 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2800 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:2900 TrainAcc:0.8518 Timeusage:0:00:00\n    Times:3000 TrainAcc:0.8518 Timeusage:0:00:00\n    Accuracy is  0.877192982456\n    Timeusage: 0.71 s\n\n\n-----------\n\n### 面试常问\n\n* 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？\n\n损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。\n将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。\n$$ \\theta_j = \\theta_j - (y_i - h_{\\theta}(x_i)) \\cdot x_{ij} $$\n更新速度只和$ x_{ij}，y_i $相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。\n\n为什么不选平方损失函数的呢？\n其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和 sigmod 函数本身的梯度是很相关的。\nsigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。\n\n* 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？\n\n先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。\n\n但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。\n\n如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。\n\n* 为什么我们还是会在训练的过程当中将高度相关的特征去掉？\n\n去掉高度相关的特征会让模型的可解释性更好\n\n可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。\n其次是特征多了，本身就会增大训练的时间。\n\n\n-----------\n\n### 参考：\n\n> 1. 李航 - 《统计学习方法》\n> 2. 周志华 - 《机器学习》\n> 3. [scikit-learn](http://scikit-learn.org/)\n> 4. [WenDesi's Github](https://github.com/WenDesi/lihang_book_algorithm)\n","slug":"logistic-regression","published":1,"updated":"2019-08-17T09:31:43.902Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnuy000j2qwpofqxlysm","content":"<h2 id=\"逻辑斯谛回归-Logistic-Regression\"><a href=\"#逻辑斯谛回归-Logistic-Regression\" class=\"headerlink\" title=\"逻辑斯谛回归 - Logistic Regression\"></a><center>逻辑斯谛回归 - Logistic Regression</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>逻辑斯谛分布</li>\n<li>逻辑斯谛回归模型</li>\n<li>逻辑斯谛模型的参数估计</li>\n<li>逻辑斯谛模型的特点</li>\n<li>代码实现</li>\n</ul><p><code>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</code></p><hr><h3 id=\"1-逻辑斯谛分布\"><a href=\"#1-逻辑斯谛分布\" class=\"headerlink\" title=\"1. 逻辑斯谛分布\"></a>1. 逻辑斯谛分布</h3><p>设 \\( X \\) 是连续随机变量，\\( X \\) 服从逻辑斯谛分布是指 \\( X \\) 具有下列分布函数和密度函数：</p><p>\\[\nF(x)=P(X\\le{x})=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n\\]</p><p>\\[\nf(x)=F’(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^{2}}\n\\]</p><a id=\"more\"></a>\n\n\n\n\n\n<p>密度函数与分布函数图像如下,其中分布函数以\\( (\\mu,\\frac{1}{2}) \\)中心对称。</p>\n<p><img src=\"/posts_res/2018-03-11-logistic-regression/1-1.png\" alt=\"fenbu\"></p>\n<hr>\n<h3 id=\"2-逻辑斯谛回归模型\"><a href=\"#2-逻辑斯谛回归模型\" class=\"headerlink\" title=\"2. 逻辑斯谛回归模型\"></a>2. 逻辑斯谛回归模型</h3><p>\\[ \nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} \n\\]</p>\n<p>\\[ \nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)} \n\\]</p>\n<hr>\n<h3 id=\"3-逻辑斯谛模型的参数估计\"><a href=\"#3-逻辑斯谛模型的参数估计\" class=\"headerlink\" title=\"3. 逻辑斯谛模型的参数估计\"></a>3. 逻辑斯谛模型的参数估计</h3><p>设</p>\n<p>\\[\nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)}=\\pi(x)\n\\]</p>\n<p>\\[\nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)}=1-\\pi(x)\n\\]</p>\n<p>似然函数(N为样本数量)：</p>\n<p>\\[\nl(w)=\\prod_{i=1}^{N}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n\\]</p>\n<p>对数似然：</p>\n<p>\\[\nL(w)=logl(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]\n\\]</p>\n<p>对\\( L(w) \\)求极值</p>\n<center>\n<img src=\"/posts_res/2018-03-11-logistic-regression/1-4.gif\">\n</center>\n\n<p>其中\\( x_i \\)表示第\\( i \\)个训练样例；\\(x_{ij} \\)表示第\\( i \\)个训练样例的第\\( j \\)个维度。</p>\n<p>\\( N \\)表示样本数量；\n\\( D \\)表示样本维度。</p>\n<hr>\n<h3 id=\"4-逻辑斯谛模型的特点\"><a href=\"#4-逻辑斯谛模型的特点\" class=\"headerlink\" title=\"4. 逻辑斯谛模型的特点\"></a>4. 逻辑斯谛模型的特点</h3><ul>\n<li>优点： </li>\n</ul>\n<p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</p>\n<p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</p>\n<p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</p>\n<p>资源占用小，尤其是内存。因为只需要存储各个维度的特征值。</p>\n<p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p>\n<ul>\n<li>缺点：</li>\n</ul>\n<p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</p>\n<p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</p>\n<p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。</p>\n<p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p>\n<hr>\n<h3 id=\"5-代码实现\"><a href=\"#5-代码实现\" class=\"headerlink\" title=\"5. 代码实现\"></a>5. 代码实现</h3><p>使用sklearn中的load_breast_cancer()作为二分类数据集。</p>\n<p><a href=\"/posts_res/2018-03-11-logistic-regression/logistic_regression.py\">Binary Logistic Regression 代码</a></p>\n<pre><code># coding:utf-8\nimport time\nimport numpy as np\nfrom datetime import timedelta\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\ndef time_consume(s_t):\n    diff = time.time()-s_t\n    return timedelta(seconds=int(diff))\n\nclass BinaryLR(object):\n    def __init__(self):\n        self.weight = None\n        self.learning_rate = 0.001\n        self.max_iteration = 3000\n\n    def predict_single_sample(self, feature):\n        feature = list(feature)\n        feature.append(1.0)\n        wx = np.sum(np.matmul(self.weight, feature))\n        exp_wx = np.exp(wx)\n        if exp_wx/(1+exp_wx) &gt; 0.5:\n            return 1\n        else:\n            return 0\n\n    def fit(self, X, y):\n        self.nlen = X.shape[0]\n        self.ndim = X.shape[1]\n        self.weight = np.zeros(shape=self.ndim+1) # add bias to weight\n\n        correct = 0\n        exec_times = 0\n        while exec_times &lt; self.max_iteration:\n            index = np.random.randint(0, self.nlen)\n            feature = list(X[index])\n            label = self.predict_single_sample(feature)\n\n            if label == y[index]:\n                correct += 1\n                if correct &gt; self.max_iteration:\n                    break\n                continue\n\n            exec_times += 1\n            correct = 0\n\n            feature.append(1.0)\n            wx = np.sum(np.matmul(self.weight, feature))\n            exp_wx = np.exp(wx)\n\n            # update weight\n            for i in range(self.weight.shape[0]):\n                self.weight[i] -= self.learning_rate * (label - exp_wx / (1.0 + exp_wx)) * feature[i]\n\n            if exec_times % 100 == 0:\n                print(&quot;Times:%d TrainAcc:%.4f Timeusage:%s&quot; % (exec_times, self.accuracy(train_y, self.predict(train_x)), time_consume(start_time)))\n\n    def predict(self, X):\n        if self.weight is None:\n            raise ValueError(&quot;Please train model first.&quot;)\n\n        labels = list()\n        for i in range(X.shape[0]):\n            d = X[i]\n            labels.append(self.predict_single_sample(d))\n        return labels\n\n    def accuracy(self, y_true, y_pred):\n        return accuracy_score(y_true, y_pred)\n\ndef load_dataset():\n    data = load_breast_cancer()\n    return data.data, data.target\n\n\nif __name__ == &#39;__main__&#39;:\n    start_time = time.time()\n\n    data, target = load_dataset()\n    print(&quot;Data Shape:&quot;, data.shape, target.shape)\n\n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=1024, shuffle=True)\n\n    ml = BinaryLR()\n    ml.fit(train_x, train_y)\n    y_pred = ml.predict(test_x)\n    accuracy = ml.accuracy(test_y, y_pred)\n    print(&quot;Accuracy is &quot;, accuracy)\n    print(&quot;Timeusage: %.2f s&quot; % (time.time()-start_time))\n</code></pre><p>运行结果如下：（不同的划分会产生不同的结果，且差异比较大）</p>\n<pre><code>Times:100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1000 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2000 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:3000 TrainAcc:0.8518 Timeusage:0:00:00\nAccuracy is  0.877192982456\nTimeusage: 0.71 s\n</code></pre><hr>\n<h3 id=\"面试常问\"><a href=\"#面试常问\" class=\"headerlink\" title=\"面试常问\"></a>面试常问</h3><ul>\n<li>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</li>\n</ul>\n<p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。\n将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</p>\n<script type=\"math/tex; mode=display\">\\theta_j = \\theta_j - (y_i - h_{\\theta}(x_i)) \\cdot x_{ij}</script><p>更新速度只和$ x_{ij}，y_i $相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</p>\n<p>为什么不选平方损失函数的呢？\n其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和 sigmod 函数本身的梯度是很相关的。\nsigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>\n<ul>\n<li>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</li>\n</ul>\n<p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p>\n<p>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。</p>\n<p>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p>\n<ul>\n<li>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</li>\n</ul>\n<p>去掉高度相关的特征会让模型的可解释性更好</p>\n<p>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。\n其次是特征多了，本身就会增大训练的时间。</p>\n<hr>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><blockquote>\n<ol>\n<li>李航 - 《统计学习方法》</li>\n<li>周志华 - 《机器学习》</li>\n<li><a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">scikit-learn</a></li>\n<li><a href=\"https://github.com/WenDesi/lihang_book_algorithm\" target=\"_blank\" rel=\"noopener\">WenDesi’s Github</a></li>\n</ol>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"逻辑斯谛回归-Logistic-Regression\"><a href=\"#逻辑斯谛回归-Logistic-Regression\" class=\"headerlink\" title=\"逻辑斯谛回归 - Logistic Regression\"></a><center>逻辑斯谛回归 - Logistic Regression</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>逻辑斯谛分布</li>\n<li>逻辑斯谛回归模型</li>\n<li>逻辑斯谛模型的参数估计</li>\n<li>逻辑斯谛模型的特点</li>\n<li>代码实现</li>\n</ul><p><code>逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</code></p><hr><h3 id=\"1-逻辑斯谛分布\"><a href=\"#1-逻辑斯谛分布\" class=\"headerlink\" title=\"1. 逻辑斯谛分布\"></a>1. 逻辑斯谛分布</h3><p>设 \\( X \\) 是连续随机变量，\\( X \\) 服从逻辑斯谛分布是指 \\( X \\) 具有下列分布函数和密度函数：</p><p>\\[\nF(x)=P(X\\le{x})=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n\\]</p><p>\\[\nf(x)=F’(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^{2}}\n\\]</p>","more":"\n\n\n\n\n\n<p>密度函数与分布函数图像如下,其中分布函数以\\( (\\mu,\\frac{1}{2}) \\)中心对称。</p>\n<p><img src=\"/posts_res/2018-03-11-logistic-regression/1-1.png\" alt=\"fenbu\"></p>\n<hr>\n<h3 id=\"2-逻辑斯谛回归模型\"><a href=\"#2-逻辑斯谛回归模型\" class=\"headerlink\" title=\"2. 逻辑斯谛回归模型\"></a>2. 逻辑斯谛回归模型</h3><p>\\[ \nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} \n\\]</p>\n<p>\\[ \nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)} \n\\]</p>\n<hr>\n<h3 id=\"3-逻辑斯谛模型的参数估计\"><a href=\"#3-逻辑斯谛模型的参数估计\" class=\"headerlink\" title=\"3. 逻辑斯谛模型的参数估计\"></a>3. 逻辑斯谛模型的参数估计</h3><p>设</p>\n<p>\\[\nP(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)}=\\pi(x)\n\\]</p>\n<p>\\[\nP(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)}=1-\\pi(x)\n\\]</p>\n<p>似然函数(N为样本数量)：</p>\n<p>\\[\nl(w)=\\prod_{i=1}^{N}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\n\\]</p>\n<p>对数似然：</p>\n<p>\\[\nL(w)=logl(w)=\\sum_{i=1}^N[y_ilog\\pi(x_i)+(1-y_i)log(1-\\pi(x_i))]\n\\]</p>\n<p>对\\( L(w) \\)求极值</p>\n<center>\n<img src=\"/posts_res/2018-03-11-logistic-regression/1-4.gif\">\n</center>\n\n<p>其中\\( x_i \\)表示第\\( i \\)个训练样例；\\(x_{ij} \\)表示第\\( i \\)个训练样例的第\\( j \\)个维度。</p>\n<p>\\( N \\)表示样本数量；\n\\( D \\)表示样本维度。</p>\n<hr>\n<h3 id=\"4-逻辑斯谛模型的特点\"><a href=\"#4-逻辑斯谛模型的特点\" class=\"headerlink\" title=\"4. 逻辑斯谛模型的特点\"></a>4. 逻辑斯谛模型的特点</h3><ul>\n<li>优点： </li>\n</ul>\n<p>形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。</p>\n<p>模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。</p>\n<p>训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。</p>\n<p>资源占用小，尤其是内存。因为只需要存储各个维度的特征值。</p>\n<p>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。</p>\n<ul>\n<li>缺点：</li>\n</ul>\n<p>准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。</p>\n<p>很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。</p>\n<p>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题。</p>\n<p>逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。</p>\n<hr>\n<h3 id=\"5-代码实现\"><a href=\"#5-代码实现\" class=\"headerlink\" title=\"5. 代码实现\"></a>5. 代码实现</h3><p>使用sklearn中的load_breast_cancer()作为二分类数据集。</p>\n<p><a href=\"/posts_res/2018-03-11-logistic-regression/logistic_regression.py\">Binary Logistic Regression 代码</a></p>\n<pre><code># coding:utf-8\nimport time\nimport numpy as np\nfrom datetime import timedelta\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\ndef time_consume(s_t):\n    diff = time.time()-s_t\n    return timedelta(seconds=int(diff))\n\nclass BinaryLR(object):\n    def __init__(self):\n        self.weight = None\n        self.learning_rate = 0.001\n        self.max_iteration = 3000\n\n    def predict_single_sample(self, feature):\n        feature = list(feature)\n        feature.append(1.0)\n        wx = np.sum(np.matmul(self.weight, feature))\n        exp_wx = np.exp(wx)\n        if exp_wx/(1+exp_wx) &gt; 0.5:\n            return 1\n        else:\n            return 0\n\n    def fit(self, X, y):\n        self.nlen = X.shape[0]\n        self.ndim = X.shape[1]\n        self.weight = np.zeros(shape=self.ndim+1) # add bias to weight\n\n        correct = 0\n        exec_times = 0\n        while exec_times &lt; self.max_iteration:\n            index = np.random.randint(0, self.nlen)\n            feature = list(X[index])\n            label = self.predict_single_sample(feature)\n\n            if label == y[index]:\n                correct += 1\n                if correct &gt; self.max_iteration:\n                    break\n                continue\n\n            exec_times += 1\n            correct = 0\n\n            feature.append(1.0)\n            wx = np.sum(np.matmul(self.weight, feature))\n            exp_wx = np.exp(wx)\n\n            # update weight\n            for i in range(self.weight.shape[0]):\n                self.weight[i] -= self.learning_rate * (label - exp_wx / (1.0 + exp_wx)) * feature[i]\n\n            if exec_times % 100 == 0:\n                print(&quot;Times:%d TrainAcc:%.4f Timeusage:%s&quot; % (exec_times, self.accuracy(train_y, self.predict(train_x)), time_consume(start_time)))\n\n    def predict(self, X):\n        if self.weight is None:\n            raise ValueError(&quot;Please train model first.&quot;)\n\n        labels = list()\n        for i in range(X.shape[0]):\n            d = X[i]\n            labels.append(self.predict_single_sample(d))\n        return labels\n\n    def accuracy(self, y_true, y_pred):\n        return accuracy_score(y_true, y_pred)\n\ndef load_dataset():\n    data = load_breast_cancer()\n    return data.data, data.target\n\n\nif __name__ == &#39;__main__&#39;:\n    start_time = time.time()\n\n    data, target = load_dataset()\n    print(&quot;Data Shape:&quot;, data.shape, target.shape)\n\n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=1024, shuffle=True)\n\n    ml = BinaryLR()\n    ml.fit(train_x, train_y)\n    y_pred = ml.predict(test_x)\n    accuracy = ml.accuracy(test_y, y_pred)\n    print(&quot;Accuracy is &quot;, accuracy)\n    print(&quot;Timeusage: %.2f s&quot; % (time.time()-start_time))\n</code></pre><p>运行结果如下：（不同的划分会产生不同的结果，且差异比较大）</p>\n<pre><code>Times:100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1000 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:1900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2000 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2100 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2200 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2300 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2400 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2500 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2600 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2700 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2800 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:2900 TrainAcc:0.8518 Timeusage:0:00:00\nTimes:3000 TrainAcc:0.8518 Timeusage:0:00:00\nAccuracy is  0.877192982456\nTimeusage: 0.71 s\n</code></pre><hr>\n<h3 id=\"面试常问\"><a href=\"#面试常问\" class=\"headerlink\" title=\"面试常问\"></a>面试常问</h3><ul>\n<li>逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？</li>\n</ul>\n<p>损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。\n将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</p>\n<script type=\"math/tex; mode=display\">\\theta_j = \\theta_j - (y_i - h_{\\theta}(x_i)) \\cdot x_{ij}</script><p>更新速度只和$ x_{ij}，y_i $相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。</p>\n<p>为什么不选平方损失函数的呢？\n其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和 sigmod 函数本身的梯度是很相关的。\nsigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>\n<ul>\n<li>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</li>\n</ul>\n<p>先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。</p>\n<p>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。</p>\n<p>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p>\n<ul>\n<li>为什么我们还是会在训练的过程当中将高度相关的特征去掉？</li>\n</ul>\n<p>去掉高度相关的特征会让模型的可解释性更好</p>\n<p>可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。\n其次是特征多了，本身就会增大训练的时间。</p>\n<hr>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><blockquote>\n<ol>\n<li>李航 - 《统计学习方法》</li>\n<li>周志华 - 《机器学习》</li>\n<li><a href=\"http://scikit-learn.org/\" target=\"_blank\" rel=\"noopener\">scikit-learn</a></li>\n<li><a href=\"https://github.com/WenDesi/lihang_book_algorithm\" target=\"_blank\" rel=\"noopener\">WenDesi’s Github</a></li>\n</ol>\n</blockquote>\n"},{"layout":"post","title":"泰勒级数","date":"2018-03-20T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 泰勒级数 - Taylor Series </center>\n\n### 1. 通项公式\n\n\\\\[\nf(x) = \\sum\\_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f^{(1)}(a)(x-a)^1 + \\frac{1}{2!}f^{(2)}(a)(x-a)^2 + \\frac{1}{3!}f^{(3)}(a)(x-a)^3+...\n\\\\]\n\n------\n\n### 2. 特殊函数的泰勒展开\n\n\\\\[\ne^x = 1 + x + \\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+...\n\\\\]\n\n\\\\[\n\\frac{1}{1-x} = 1+x+x^2+x^3+...\n\\\\]\n\n\n\n\n\n\n\n\n","source":"_posts/2018-03-20-taylorseries.md","raw":"---\nlayout: post\ntitle: 泰勒级数\ndate: 2018-03-20 12:10 +0800\ncategories: 基础知识\ntags:\n- 数学\nmathjax: true\ncopyright: true\n---\n\n## <center> 泰勒级数 - Taylor Series </center>\n\n### 1. 通项公式\n\n\\\\[\nf(x) = \\sum\\_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f^{(1)}(a)(x-a)^1 + \\frac{1}{2!}f^{(2)}(a)(x-a)^2 + \\frac{1}{3!}f^{(3)}(a)(x-a)^3+...\n\\\\]\n\n------\n\n### 2. 特殊函数的泰勒展开\n\n\\\\[\ne^x = 1 + x + \\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+...\n\\\\]\n\n\\\\[\n\\frac{1}{1-x} = 1+x+x^2+x^3+...\n\\\\]\n\n\n\n\n\n\n\n\n","slug":"taylorseries","published":1,"updated":"2019-08-17T09:32:08.302Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnv1000l2qwpgqaviodr","content":"<h2 id=\"泰勒级数-Taylor-Series\"><a href=\"#泰勒级数-Taylor-Series\" class=\"headerlink\" title=\" 泰勒级数 - Taylor Series \"></a><center> 泰勒级数 - Taylor Series </center></h2><h3 id=\"1-通项公式\"><a href=\"#1-通项公式\" class=\"headerlink\" title=\"1. 通项公式\"></a>1. 通项公式</h3><p>\\[\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f^{(1)}(a)(x-a)^1 + \\frac{1}{2!}f^{(2)}(a)(x-a)^2 + \\frac{1}{3!}f^{(3)}(a)(x-a)^3+…\n\\]</p>\n<hr>\n<h3 id=\"2-特殊函数的泰勒展开\"><a href=\"#2-特殊函数的泰勒展开\" class=\"headerlink\" title=\"2. 特殊函数的泰勒展开\"></a>2. 特殊函数的泰勒展开</h3><p>\\[\ne^x = 1 + x + \\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+…\n\\]</p>\n<p>\\[\n\\frac{1}{1-x} = 1+x+x^2+x^3+…\n\\]</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"泰勒级数-Taylor-Series\"><a href=\"#泰勒级数-Taylor-Series\" class=\"headerlink\" title=\" 泰勒级数 - Taylor Series \"></a><center> 泰勒级数 - Taylor Series </center></h2><h3 id=\"1-通项公式\"><a href=\"#1-通项公式\" class=\"headerlink\" title=\"1. 通项公式\"></a>1. 通项公式</h3><p>\\[\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^n = f(a) + f^{(1)}(a)(x-a)^1 + \\frac{1}{2!}f^{(2)}(a)(x-a)^2 + \\frac{1}{3!}f^{(3)}(a)(x-a)^3+…\n\\]</p>\n<hr>\n<h3 id=\"2-特殊函数的泰勒展开\"><a href=\"#2-特殊函数的泰勒展开\" class=\"headerlink\" title=\"2. 特殊函数的泰勒展开\"></a>2. 特殊函数的泰勒展开</h3><p>\\[\ne^x = 1 + x + \\frac{x^2}{2!}+\\frac{x^3}{3!}+\\frac{x^4}{4!}+…\n\\]</p>\n<p>\\[\n\\frac{1}{1-x} = 1+x+x^2+x^3+…\n\\]</p>\n"},{"layout":"post","title":"卡方检验","date":"2018-03-21T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 卡方检验 - Chi-Squared Test </center>\n\n目录\n* 定义\n* 统计机器学习中的应用\n* 举例\n* 特点\n\n\n--------\n\n### 1. 定义\n\n卡方检验（Chi-Squared Test）是一种统计量的分布在零假设成立时近似服从卡方分布（\\\\(\\chi^2\\\\)分布）的假设检验。在没有其他的限定条件或说明时，卡方检验一般指代的是皮尔森卡方检验。在卡方检验的一般运用中，研究人员将观察量的值划分成若干互斥的分类，并且使用一套理论（或零假设）尝试去说明观察量的值落入不同分类的概率分布的模型。而卡方检验的目的就在于去衡量这个假设对观察结果所反映的程度。\n\n\n-------\n\n### 2. 统计机器学习中的应用\n\n卡方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。\n具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（指“如果两者确实独立”的情况下应该有的值）的偏差程度，\n如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；\n如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。\n\n那么用什么来衡量偏差程度呢？假设理论值为\\\\(E\\\\)（数学期望），实际值为\\\\(x\\\\)，如果仅仅使用所有样本的观察值与理论值的差值\\\\(x-E\\\\)之和\n\\\\[\n\\sum\\_{i=1}^n (x\\_i - E)\n\\\\]\n来衡量，单个的观察值还好说，当有多个观察值\\\\(x\\_1,x\\_2,x\\_3\\\\)的时候，很可能\\\\(x\\_1-E，x\\_2-E，x\\_3-E\\\\)的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！\n此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用\n\\\\[\n\\sum\\_{i=1}^n (x\\_i - E)^2\n\\\\]\n这时又引来了新的问题，对于\\\\(500\\\\)的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。\n因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断。改进的式子如下：\n\\\\[\n\\sum\\_{i=1}^n \\frac{(x\\_i - E)^2}{E}\n\\\\]\n上面这个式子已经相当好了。实际上这个式子就是卡方检验使用的差值衡量公式。\n当提供了数个样本的观察值\\\\(x\\_1, x\\_2, ..., x\\_i, ..., x\\_n\\\\)之后，代入到\\\\(式(3)\\\\)中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。\n\n### 3. 举例\n\n在文本分类问题的特征选择阶段，我们主要关心一个词word（一个随机变量）与一个类别Ci（另一个随机变量）之间是否相互独立？\n如果独立，就可以说词word对类别Ci完全没有表征作用，即我们根本无法根据word出现与否来判断一篇文档是否属于Ci这个分类。\n但与最普通的卡方检验不同，我们不需要设定阈值，因为很难说词word和类别Ci关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最相关的即可。\n\n我们越倾向于认为原假设的反面情况是正确的，一般都使用”词word与类别Ci不相关“来做原假设。选择的过程也变成了为每个词计算它与类别Ci的卡方值，从大到小排个序（此时开方值越大越相关），取前k个就可以。\n\n比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有四个观察值可以使用：\n\n1. 包含“篮球”且属于“体育”类别的文档数，命名为A\n2. 包含“篮球”但不属于“体育”类别的文档数，命名为B\n3. 不包含“篮球”但却属于“体育”类别的文档数，命名为C\n4. 既不包含“篮球”也不属于“体育”类别的文档数，命名为D\n\n![table](/posts_res/2018-03-21-chisquaredtest/3-1.png)\n\n下面我们需要根据上面数据估算上面的ABCD四个值，根据估算值计算卡方值\n\n对于A的估计值（上文说的是理论值）：\n\\\\[\nA+B+C+D=N\n\\\\]\nA+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。\n\n好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。\n这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近\n\n\\\\[\n\\frac{A+B}{N}\n\\\\]\n\n（因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中，\n应该有\\\\(E\\_{11} = (A+C)\\frac{A+B}{N}\\\\)篇包含“篮球”这个词（数量乘以概率），实际上是“体育类”中有A篇包含篮球这个词。\n\n此时使用\\\\( 式(3) \\\\)对这种情况的差值就可以计算出来：\n\\\\[\nD\\_{11} = \\frac{(A-E\\_{11})^2}{E\\_{11}}\n\\\\]\n\n同样，我们还可以计算剩下三种情况的差值\\\\(D\\_{12},D\\_{21},D\\_{22}\\\\)。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的卡方值\n\\\\[\n\\chi^2(篮球，体育) = D\\_{11}+D\\_{12}+D\\_{21}+D\\_{22} = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\\\]\n因此，计算卡方值的一般公式的形式为：\n\\\\[\n\\chi^2(t，c) = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\\\]\n\n接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的卡方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。\n\n\n-----------\n\n### 4. 特点\n\n针对英文纯文本的实验结果表明：\n作为特征选择方法时，卡方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；\n文档频率方法的性能同前两者大体相当，术语强度方法性能一般；\n互信息方法的性能最差。\n\n但卡方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词word，却不管word在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。\n甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。\n这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。\n\n\n---------\n\n### 参考\n\n> [卡方检验](https://zh.wikipedia.org/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)\n\n> [特征选择之Chi卡方检验](https://www.jianshu.com/p/b670b2a23187)\n","source":"_posts/2018-03-21-chisquaredtest.md","raw":"---\nlayout: post\ntitle: 卡方检验\ndate: 2018-03-21 12:10 +0800\ncategories: 基础知识\ntags:\n- 特征选择\nmathjax: true\ncopyright: true\n---\n\n## <center> 卡方检验 - Chi-Squared Test </center>\n\n目录\n* 定义\n* 统计机器学习中的应用\n* 举例\n* 特点\n\n\n--------\n\n### 1. 定义\n\n卡方检验（Chi-Squared Test）是一种统计量的分布在零假设成立时近似服从卡方分布（\\\\(\\chi^2\\\\)分布）的假设检验。在没有其他的限定条件或说明时，卡方检验一般指代的是皮尔森卡方检验。在卡方检验的一般运用中，研究人员将观察量的值划分成若干互斥的分类，并且使用一套理论（或零假设）尝试去说明观察量的值落入不同分类的概率分布的模型。而卡方检验的目的就在于去衡量这个假设对观察结果所反映的程度。\n\n\n-------\n\n### 2. 统计机器学习中的应用\n\n卡方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。\n具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（指“如果两者确实独立”的情况下应该有的值）的偏差程度，\n如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；\n如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。\n\n那么用什么来衡量偏差程度呢？假设理论值为\\\\(E\\\\)（数学期望），实际值为\\\\(x\\\\)，如果仅仅使用所有样本的观察值与理论值的差值\\\\(x-E\\\\)之和\n\\\\[\n\\sum\\_{i=1}^n (x\\_i - E)\n\\\\]\n来衡量，单个的观察值还好说，当有多个观察值\\\\(x\\_1,x\\_2,x\\_3\\\\)的时候，很可能\\\\(x\\_1-E，x\\_2-E，x\\_3-E\\\\)的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！\n此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用\n\\\\[\n\\sum\\_{i=1}^n (x\\_i - E)^2\n\\\\]\n这时又引来了新的问题，对于\\\\(500\\\\)的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。\n因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断。改进的式子如下：\n\\\\[\n\\sum\\_{i=1}^n \\frac{(x\\_i - E)^2}{E}\n\\\\]\n上面这个式子已经相当好了。实际上这个式子就是卡方检验使用的差值衡量公式。\n当提供了数个样本的观察值\\\\(x\\_1, x\\_2, ..., x\\_i, ..., x\\_n\\\\)之后，代入到\\\\(式(3)\\\\)中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。\n\n### 3. 举例\n\n在文本分类问题的特征选择阶段，我们主要关心一个词word（一个随机变量）与一个类别Ci（另一个随机变量）之间是否相互独立？\n如果独立，就可以说词word对类别Ci完全没有表征作用，即我们根本无法根据word出现与否来判断一篇文档是否属于Ci这个分类。\n但与最普通的卡方检验不同，我们不需要设定阈值，因为很难说词word和类别Ci关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最相关的即可。\n\n我们越倾向于认为原假设的反面情况是正确的，一般都使用”词word与类别Ci不相关“来做原假设。选择的过程也变成了为每个词计算它与类别Ci的卡方值，从大到小排个序（此时开方值越大越相关），取前k个就可以。\n\n比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有四个观察值可以使用：\n\n1. 包含“篮球”且属于“体育”类别的文档数，命名为A\n2. 包含“篮球”但不属于“体育”类别的文档数，命名为B\n3. 不包含“篮球”但却属于“体育”类别的文档数，命名为C\n4. 既不包含“篮球”也不属于“体育”类别的文档数，命名为D\n\n![table](/posts_res/2018-03-21-chisquaredtest/3-1.png)\n\n下面我们需要根据上面数据估算上面的ABCD四个值，根据估算值计算卡方值\n\n对于A的估计值（上文说的是理论值）：\n\\\\[\nA+B+C+D=N\n\\\\]\nA+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。\n\n好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。\n这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近\n\n\\\\[\n\\frac{A+B}{N}\n\\\\]\n\n（因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中，\n应该有\\\\(E\\_{11} = (A+C)\\frac{A+B}{N}\\\\)篇包含“篮球”这个词（数量乘以概率），实际上是“体育类”中有A篇包含篮球这个词。\n\n此时使用\\\\( 式(3) \\\\)对这种情况的差值就可以计算出来：\n\\\\[\nD\\_{11} = \\frac{(A-E\\_{11})^2}{E\\_{11}}\n\\\\]\n\n同样，我们还可以计算剩下三种情况的差值\\\\(D\\_{12},D\\_{21},D\\_{22}\\\\)。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的卡方值\n\\\\[\n\\chi^2(篮球，体育) = D\\_{11}+D\\_{12}+D\\_{21}+D\\_{22} = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\\\]\n因此，计算卡方值的一般公式的形式为：\n\\\\[\n\\chi^2(t，c) = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\\\]\n\n接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的卡方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。\n\n\n-----------\n\n### 4. 特点\n\n针对英文纯文本的实验结果表明：\n作为特征选择方法时，卡方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；\n文档频率方法的性能同前两者大体相当，术语强度方法性能一般；\n互信息方法的性能最差。\n\n但卡方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词word，却不管word在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。\n甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。\n这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。\n\n\n---------\n\n### 参考\n\n> [卡方检验](https://zh.wikipedia.org/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)\n\n> [特征选择之Chi卡方检验](https://www.jianshu.com/p/b670b2a23187)\n","slug":"chisquaredtest","published":1,"updated":"2019-08-17T09:32:39.997Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnv3000p2qwp0n6g7lpp","content":"<h2 id=\"卡方检验-Chi-Squared-Test\"><a href=\"#卡方检验-Chi-Squared-Test\" class=\"headerlink\" title=\" 卡方检验 - Chi-Squared Test \"></a><center> 卡方检验 - Chi-Squared Test </center></h2><p>目录</p><ul>\n<li>定义</li>\n<li>统计机器学习中的应用</li>\n<li>举例</li>\n<li>特点</li>\n</ul><hr><h3 id=\"1-定义\"><a href=\"#1-定义\" class=\"headerlink\" title=\"1. 定义\"></a>1. 定义</h3><p>卡方检验（Chi-Squared Test）是一种统计量的分布在零假设成立时近似服从卡方分布（\\(\\chi^2\\)分布）的假设检验。在没有其他的限定条件或说明时，卡方检验一般指代的是皮尔森卡方检验。在卡方检验的一般运用中，研究人员将观察量的值划分成若干互斥的分类，并且使用一套理论（或零假设）尝试去说明观察量的值落入不同分类的概率分布的模型。而卡方检验的目的就在于去衡量这个假设对观察结果所反映的程度。</p><hr><h3 id=\"2-统计机器学习中的应用\"><a href=\"#2-统计机器学习中的应用\" class=\"headerlink\" title=\"2. 统计机器学习中的应用\"></a>2. 统计机器学习中的应用</h3><p>卡方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。\n具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（指“如果两者确实独立”的情况下应该有的值）的偏差程度，\n如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；\n如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。</p><a id=\"more\"></a>\n\n\n\n\n\n<p>那么用什么来衡量偏差程度呢？假设理论值为\\(E\\)（数学期望），实际值为\\(x\\)，如果仅仅使用所有样本的观察值与理论值的差值\\(x-E\\)之和\n\\[\n\\sum_{i=1}^n (x_i - E)\n\\]\n来衡量，单个的观察值还好说，当有多个观察值\\(x_1,x_2,x_3\\)的时候，很可能\\(x_1-E，x_2-E，x_3-E\\)的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！\n此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用\n\\[\n\\sum_{i=1}^n (x_i - E)^2\n\\]\n这时又引来了新的问题，对于\\(500\\)的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。\n因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断。改进的式子如下：\n\\[\n\\sum_{i=1}^n \\frac{(x_i - E)^2}{E}\n\\]\n上面这个式子已经相当好了。实际上这个式子就是卡方检验使用的差值衡量公式。\n当提供了数个样本的观察值\\(x_1, x_2, …, x_i, …, x_n\\)之后，代入到\\(式(3)\\)中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。</p>\n<h3 id=\"3-举例\"><a href=\"#3-举例\" class=\"headerlink\" title=\"3. 举例\"></a>3. 举例</h3><p>在文本分类问题的特征选择阶段，我们主要关心一个词word（一个随机变量）与一个类别Ci（另一个随机变量）之间是否相互独立？\n如果独立，就可以说词word对类别Ci完全没有表征作用，即我们根本无法根据word出现与否来判断一篇文档是否属于Ci这个分类。\n但与最普通的卡方检验不同，我们不需要设定阈值，因为很难说词word和类别Ci关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最相关的即可。</p>\n<p>我们越倾向于认为原假设的反面情况是正确的，一般都使用”词word与类别Ci不相关“来做原假设。选择的过程也变成了为每个词计算它与类别Ci的卡方值，从大到小排个序（此时开方值越大越相关），取前k个就可以。</p>\n<p>比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有四个观察值可以使用：</p>\n<ol>\n<li>包含“篮球”且属于“体育”类别的文档数，命名为A</li>\n<li>包含“篮球”但不属于“体育”类别的文档数，命名为B</li>\n<li>不包含“篮球”但却属于“体育”类别的文档数，命名为C</li>\n<li>既不包含“篮球”也不属于“体育”类别的文档数，命名为D</li>\n</ol>\n<p><img src=\"/posts_res/2018-03-21-chisquaredtest/3-1.png\" alt=\"table\"></p>\n<p>下面我们需要根据上面数据估算上面的ABCD四个值，根据估算值计算卡方值</p>\n<p>对于A的估计值（上文说的是理论值）：\n\\[\nA+B+C+D=N\n\\]\nA+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。</p>\n<p>好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。\n这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近</p>\n<p>\\[\n\\frac{A+B}{N}\n\\]</p>\n<p>（因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中，\n应该有\\(E_{11} = (A+C)\\frac{A+B}{N}\\)篇包含“篮球”这个词（数量乘以概率），实际上是“体育类”中有A篇包含篮球这个词。</p>\n<p>此时使用\\( 式(3) \\)对这种情况的差值就可以计算出来：\n\\[\nD_{11} = \\frac{(A-E_{11})^2}{E_{11}}\n\\]</p>\n<p>同样，我们还可以计算剩下三种情况的差值\\(D_{12},D_{21},D_{22}\\)。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的卡方值\n\\[\n\\chi^2(篮球，体育) = D_{11}+D_{12}+D_{21}+D_{22} = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\]\n因此，计算卡方值的一般公式的形式为：\n\\[\n\\chi^2(t，c) = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\]</p>\n<p>接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的卡方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><p>针对英文纯文本的实验结果表明：\n作为特征选择方法时，卡方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；\n文档频率方法的性能同前两者大体相当，术语强度方法性能一般；\n互信息方法的性能最差。</p>\n<p>但卡方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词word，却不管word在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。\n甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。\n这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C\" target=\"_blank\" rel=\"noopener\">卡方检验</a></p>\n<p><a href=\"https://www.jianshu.com/p/b670b2a23187\" target=\"_blank\" rel=\"noopener\">特征选择之Chi卡方检验</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"卡方检验-Chi-Squared-Test\"><a href=\"#卡方检验-Chi-Squared-Test\" class=\"headerlink\" title=\" 卡方检验 - Chi-Squared Test \"></a><center> 卡方检验 - Chi-Squared Test </center></h2><p>目录</p><ul>\n<li>定义</li>\n<li>统计机器学习中的应用</li>\n<li>举例</li>\n<li>特点</li>\n</ul><hr><h3 id=\"1-定义\"><a href=\"#1-定义\" class=\"headerlink\" title=\"1. 定义\"></a>1. 定义</h3><p>卡方检验（Chi-Squared Test）是一种统计量的分布在零假设成立时近似服从卡方分布（\\(\\chi^2\\)分布）的假设检验。在没有其他的限定条件或说明时，卡方检验一般指代的是皮尔森卡方检验。在卡方检验的一般运用中，研究人员将观察量的值划分成若干互斥的分类，并且使用一套理论（或零假设）尝试去说明观察量的值落入不同分类的概率分布的模型。而卡方检验的目的就在于去衡量这个假设对观察结果所反映的程度。</p><hr><h3 id=\"2-统计机器学习中的应用\"><a href=\"#2-统计机器学习中的应用\" class=\"headerlink\" title=\"2. 统计机器学习中的应用\"></a>2. 统计机器学习中的应用</h3><p>卡方检验最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。\n具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（指“如果两者确实独立”的情况下应该有的值）的偏差程度，\n如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；\n如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。</p>","more":"\n\n\n\n\n\n<p>那么用什么来衡量偏差程度呢？假设理论值为\\(E\\)（数学期望），实际值为\\(x\\)，如果仅仅使用所有样本的观察值与理论值的差值\\(x-E\\)之和\n\\[\n\\sum_{i=1}^n (x_i - E)\n\\]\n来衡量，单个的观察值还好说，当有多个观察值\\(x_1,x_2,x_3\\)的时候，很可能\\(x_1-E，x_2-E，x_3-E\\)的值有正有负，因而互相抵消，使得最终的结果看上好像偏差为0，但实际上每个都有偏差，而且都还不小！\n此时很直接的想法便是使用方差代替均值，这样就解决了正负抵消的问题，即使用\n\\[\n\\sum_{i=1}^n (x_i - E)^2\n\\]\n这时又引来了新的问题，对于\\(500\\)的均值来说，相差5其实是很小的（相差1%），而对20的均值来说，5相当于25%的差异，这是使用方差也无法体现的。\n因此应该考虑改进上面的式子，让均值的大小不影响我们对差异程度的判断。改进的式子如下：\n\\[\n\\sum_{i=1}^n \\frac{(x_i - E)^2}{E}\n\\]\n上面这个式子已经相当好了。实际上这个式子就是卡方检验使用的差值衡量公式。\n当提供了数个样本的观察值\\(x_1, x_2, …, x_i, …, x_n\\)之后，代入到\\(式(3)\\)中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。</p>\n<h3 id=\"3-举例\"><a href=\"#3-举例\" class=\"headerlink\" title=\"3. 举例\"></a>3. 举例</h3><p>在文本分类问题的特征选择阶段，我们主要关心一个词word（一个随机变量）与一个类别Ci（另一个随机变量）之间是否相互独立？\n如果独立，就可以说词word对类别Ci完全没有表征作用，即我们根本无法根据word出现与否来判断一篇文档是否属于Ci这个分类。\n但与最普通的卡方检验不同，我们不需要设定阈值，因为很难说词word和类别Ci关联到什么程度才算是有表征作用，我们只想借用这个方法来选出一些最相关的即可。</p>\n<p>我们越倾向于认为原假设的反面情况是正确的，一般都使用”词word与类别Ci不相关“来做原假设。选择的过程也变成了为每个词计算它与类别Ci的卡方值，从大到小排个序（此时开方值越大越相关），取前k个就可以。</p>\n<p>比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有四个观察值可以使用：</p>\n<ol>\n<li>包含“篮球”且属于“体育”类别的文档数，命名为A</li>\n<li>包含“篮球”但不属于“体育”类别的文档数，命名为B</li>\n<li>不包含“篮球”但却属于“体育”类别的文档数，命名为C</li>\n<li>既不包含“篮球”也不属于“体育”类别的文档数，命名为D</li>\n</ol>\n<p><img src=\"/posts_res/2018-03-21-chisquaredtest/3-1.png\" alt=\"table\"></p>\n<p>下面我们需要根据上面数据估算上面的ABCD四个值，根据估算值计算卡方值</p>\n<p>对于A的估计值（上文说的是理论值）：\n\\[\nA+B+C+D=N\n\\]\nA+C的意思其实就是说“属于体育类的文章数量”，因此，它就等于M，同时，B+D就等于N-M。</p>\n<p>好，那么理论值是什么呢？以包含“篮球”且属于“体育”类别的文档数为例。如果原假设是成立的，即“篮球”和体育类文章没什么关联性，那么在所有的文章中，“篮球”这个词都应该是等概率出现，而不管文章是不是体育类的。\n这个概率具体是多少，我们并不知道，但他应该体现在观察结果中（就好比抛硬币的概率是二分之一，可以通过观察多次抛的结果来大致确定），因此我们可以说这个概率接近</p>\n<p>\\[\n\\frac{A+B}{N}\n\\]</p>\n<p>（因为A+B是包含“篮球”的文章数，除以总文档数就是“篮球”出现的概率，当然，这里认为在一篇文章中出现即可，而不管出现了几次）而属于体育类的文章数为A+C，在这些个文档中，\n应该有\\(E_{11} = (A+C)\\frac{A+B}{N}\\)篇包含“篮球”这个词（数量乘以概率），实际上是“体育类”中有A篇包含篮球这个词。</p>\n<p>此时使用\\( 式(3) \\)对这种情况的差值就可以计算出来：\n\\[\nD_{11} = \\frac{(A-E_{11})^2}{E_{11}}\n\\]</p>\n<p>同样，我们还可以计算剩下三种情况的差值\\(D_{12},D_{21},D_{22}\\)。有了所有观察值的差值，就可以计算“篮球”与“体育”类文章的卡方值\n\\[\n\\chi^2(篮球，体育) = D_{11}+D_{12}+D_{21}+D_{22} = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\]\n因此，计算卡方值的一般公式的形式为：\n\\[\n\\chi^2(t，c) = \\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}\n\\]</p>\n<p>接下来我们就可以计算其他词如“排球”，“产品”，“银行”等等与体育类别的卡方值，然后根据大小来排序，选择我们需要的最大的数个词汇作为特征项就可以了。</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><p>针对英文纯文本的实验结果表明：\n作为特征选择方法时，卡方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；\n文档频率方法的性能同前两者大体相当，术语强度方法性能一般；\n互信息方法的性能最差。</p>\n<p>但卡方检验也并非就十全十美了。回头想想A和B的值是怎么得出来的，它统计文档中是否出现词word，却不管word在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。\n甚至会出现有些情况，一个词在一类文章的每篇文档中都只出现了一次，其开方值却大过了在该类文章99%的文档中出现了10次的词，其实后面的词才是更具代表性的，但只因为它出现的文档数比前面的词少了“1”，特征选择的时候就可能筛掉后面的词而保留了前者。\n这就是开方检验著名的“低频词缺陷“。因此开方检验也经常同其他因素如词频综合考虑来扬长避短。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C\" target=\"_blank\" rel=\"noopener\">卡方检验</a></p>\n<p><a href=\"https://www.jianshu.com/p/b670b2a23187\" target=\"_blank\" rel=\"noopener\">特征选择之Chi卡方检验</a></p>\n</blockquote>\n"},{"layout":"post","title":"决策树 Decision Tree","date":"2018-03-18T12:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>决策树 - Decision Tree</center>\n\n#### 目录\n* ID3\n* C45\n* 决策树的剪枝\n* CART\n* 特点\n* 代码实现\n\n\n---------------\n\n决策树学习的3个步骤：\n* 特征选择\n* 决策树生成\n* 决策树修剪\n\n决策树的损失函数通常为正则化的极大似然函数。\n\n\n--------------\n\n### 1. ID3\n\nID3算法使用信息增益作为特征选择的标准，信息增益**越大越好**。\n\n#### 1.1 信息增益算法\n输入：\n\n训练数据集 \\\\( D \\\\) 和特征 \\\\( A \\\\);\n\n输出：\n\n特征\\\\(A\\\\)对训练数据集\\\\(D\\\\)的信息增益 \\\\( g(D,A) \\\\)\n\n（1）计算数据集 \\\\( D \\\\) 的经验熵 \\\\( H(D) \\\\)\n\\\\[\nH(D) = - \\sum\\_{k=1}^K \\frac{ | C\\_k |}{| D |}log\\_2 \\frac{| C_k |}{| D |}\n\\\\]\n\n（2）计算特征\\\\(A\\\\)对数据集\\\\(D\\\\)的经验条件熵\\\\( H(D | A) \\\\)\n\\\\[\nH(D | A) = \\sum\\_{i=1}^n \\frac{| D\\_i |}{| D |} H(D\\_i) = - \\sum\\_{i=1}^n \\frac{ | D\\_i | }{ | D | } \\sum\\_{k=1}^K \\frac{| D\\_{ik} |}{ | D\\_i |} log\\_2 \\frac{ | D\\_{ik} |}{| D\\_i |}\n\\\\]\n\n（3）计算信息增益\n\\\\[\ng(D,A) = H(D) - H(D | A)\n\\\\]\n\n其中\\\\(K\\\\)为类别的数量\\\\( \\lbrace C\\_1, C\\_2, ..., C\\_k \\rbrace \\\\)，\\\\(n\\\\)为特征\\\\(A\\\\)的取值数量\\\\( \\lbrace a\\_1, a\\_2, ..., a\\_n \\rbrace \\\\)。\n\n\n-----------\n\n### 2. C45\n\n信息增益作为标准时，存在偏向于取值数量较多的特征的问题，因此C45算法选择信息增益比作为划分的标准，信息增益比**越大越好**。\n\n#### 2.1 信息增益比(基于信息增益)\n定义为信息增益\\\\(g(D,A)\\\\)与关于特征\\\\(A\\\\)的值的熵\\\\(H\\_A(D)\\\\)之比，即：\n\\\\[\ng\\_R(D,A) = \\frac{g(D,A)}{H\\_A(D)}\n\\\\]\n\\\\[\nH\\_A(D) = - \\sum\\_{i=1}^n \\frac{ | D\\_i | } { | D | } log\\_2 \\frac{ | D\\_i | }{ | D |}\n\\\\]\n其中\\\\(n\\\\)为特征\\\\(A\\\\)取值的个数。\n\n\n-----------\n\n### 3. 决策树的剪枝\n\n设树\\\\( T \\\\)的叶结点个数为\\\\( | T | \\\\)，\\\\(t\\\\)是树\\\\(T\\\\)的叶结点，该叶结点有\\\\(N\\_t\\\\)个样本点，其中\\\\(k\\\\)类的样本点有\\\\(N\\_{tk}\\\\)个，\\\\(k=1,2,...,K\\\\)，\\\\(H\\_t(T)\\\\)为叶结点上的经验熵，则决策树的损失函数可以定义为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nC\\_{\\alpha}(T)\n& = \\sum\\_{t=1}^{ | T | } N\\_t H\\_t(T) + \\alpha | T | \\\\\\\n& = - \\sum\\_{t=1}^{ | T | } \\sum\\_{k=1}^K N\\_{tk} log \\frac{N\\_{tk}}{N\\_t} + \\alpha | T | \\\\\\\n& = C(T) + \\alpha | T |\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n通过 \\\\( \\alpha \\geq 0 \\\\)控制模型与训练数据拟合度和模型复杂度。\n\n#### 3.1 ID3&C45树的剪枝算法\n输入：生成算法产生的整个树 \\\\( T \\\\)，参数 \\\\( \\alpha \\\\)；\n\n输出：修剪后的子树\\\\( T\\_{\\alpha} \\\\)。\n\n（1）计算每个结点的经验熵, 经验熵计算公式；\n\\\\[\nH\\_t(T) = - \\sum\\_k \\frac{N\\_{tk}}{N\\_t} log \\frac{N\\_{tk}}{N\\_t}\n\\\\]\n\n（2）递归地从树的叶结点向上回缩；\n\n设一组叶结点回缩到其父结点之前与之后的整体树分别为\\\\(T\\_B\\\\)与\\\\(T\\_A\\\\)，其对应的损失函数值分别为\\\\(C\\_{\\alpha}(T\\_B)\\\\)与\\\\(C\\_{\\alpha}(T\\_A)\\\\)，如果\n\\\\[\nC\\_{\\alpha}(T\\_B) \\leq C\\_{\\alpha}(T\\_A)\n\\\\]\n则进行剪枝，即将父结点变为新的叶结点。\n\n（3）返回（2），直至不能继续为止，得到损失函数最小的子树\\\\(T\\_{\\alpha}\\\\)。\n\n\n-----------\n\n### 4. CART\n\nCART假设决策树是二叉树，内部结点特征取值为“是”和“否”，递归二分每个特征。\n\n* 回归数使用平方误差最小化准则，**越小越好**。\n* 分类数使用基尼指数最小化准则，**越小越好**。\n\n#### 4.1 回归树\n\n输入：训练数据集\\\\( D=\\lbrace (x\\_1, y\\_1), (x\\_2, y\\_2), ..., (x\\_N, y\\_N) \\rbrace \\\\), 并且\\\\(Y\\\\)是连续型变量；\n\n输出：回归树 \\\\( f(x) \\\\)\n\n在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n\n（1）选择最优切分变量\\\\(j\\\\)与切分点\\\\(s\\\\)，求解\n\\\\[\n\\mathop{\\min}\\_{j,s} [\\mathop{\\min}\\_{c\\_1} \\sum\\_{x\\_i \\in R\\_1(j,s)} (y\\_i - c\\_1)^2 + \\mathop{\\min}\\_{c\\_2} \\sum\\_{x\\_i \\in R\\_2(j,s)} (y\\_i - c\\_2)^2]\n\\\\]\n遍历\\\\(j\\\\)，对固定的切分变量\\\\(j\\\\)扫描切分点\\\\(s\\\\)，选择使得上式达到最小值的对\\\\((j,s)\\\\)。\n\n（2）用选定的对\\\\((j,s)\\\\)划分区域并决定相应的输出值(均值)：\n\\\\[\nR\\_1(j,s) = \\lbrace x | x^{(j)} \\leq s \\rbrace, \\quad R\\_2(j,s)=\\lbrace x | x^{(j)} > s \\rbrace \\\\\\\n\\hat{c}\\_m = \\frac{1}{N\\_m} \\sum\\_{x\\_i \\in R\\_m(j,s)} y\\_i, \\quad x \\in R\\_m, \\quad m=1,2\n\\\\]\n\n\n（3）继续对两个子区域调用步骤(1),(2)，直至满足停止条件；\n\n（4）将输入空间划分为\\\\(M\\\\)个区域\\\\( R\\_1, R\\_2, ..., R\\_M \\\\)，生成决策树：\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\hat{c}\\_m I(x \\in R\\_m)\n\\\\]\n\n#### 4.2 分类树\n\n在特征\\\\( A \\\\)的条件下，集合\\\\( D \\\\)的基尼指数定义为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGini(D, A)\n& = \\frac{ | D\\_1 | }{ | D | } Gini(D\\_1) + \\frac{ | D\\_2 | }{ | D | } Gini(D\\_2) \\\\\\\n& = \\frac{ | D\\_1 | }{ | D | } [1 - \\sum\\_{k=1}^K(\\frac{ | C\\_k | }{ | D\\_1 | })^2 ] + \\frac{ | D\\_2 | }{ | D | } [1 - \\sum\\_{k=1}^K(\\frac{ | C\\_k | }{ | D\\_2 | })^2 ]\n\\end{aligned}\n\\end{equation}\n\\\\]\n其中\\\\( C\\_k \\\\)是\\\\( D\\_i \\\\)中属于第\\\\(k\\\\)类的样本子集，\\\\(K\\\\)是类的个数。\n\n\n输入：训练集\\\\(D\\\\),停止计算的条件\n\n输出：CART决策树\n\n根据训练集，从跟结点开始递归地对每个结点进行一下操作，构建二叉决策树。\n\n（1）根据每一个特征以及每个特征的取值，计算相应二叉划分时的基尼指数；\n\n（2）在所有可能的特征及可能的特征值中，选择基尼指数**最小**的特征及相应特征值作为划分切分特征及切分点，并将训练集划分到两个子结点中；\n\n（3）对两个子节点递归调用(1),(2)，直至满足停止条件；\n\n（4）生成CART决策树。\n\n#### 4.3 CART的剪枝算法\n\n* 1. 剪枝，形成一个子树序列\n\n在剪枝过程中，计算子树的损失函数：\n\\\\[\nC\\_{\\alpha}(T) = C(T) + \\alpha | T |\n\\\\]\n其中，\\\\(T\\\\)为任意子树，\\\\(C(T)\\\\)表示训练数据的预测误差，\\\\(| T |\\\\)为子树的叶结点数目，\\\\( \\alpha \\geq 0 \\\\)为参数。\n\n具体地，从整体树\\\\(T\\_0\\\\)开始剪枝，对\\\\(T\\_0\\\\)的任意内部结点\\\\(t\\\\)，以\\\\(t\\\\)为单结点树的损失函数是\n\\\\[\nC\\_{\\alpha}(t) = C(t) + \\alpha\n\\\\]\n以\\\\(t\\\\)为根节点的子树\\\\(T\\_t\\\\)的损失函数是\n\\\\[\nC\\_{\\alpha}(T\\_t) = C(T\\_t) + \\alpha | T\\_t |\n\\\\]\n<center>\n<img src=\"/posts_res/2018-03-18-decision-tree/cart_cut.png\" />\n</center>\n\n当\\\\( \\alpha=0 \\\\)及\\\\(\\alpha\\\\)充分小时，有不等式\n\\\\[\nC\\_{\\alpha}(T\\_t) < C\\_{\\alpha}(t)\n\\\\]\n当\\\\( \\alpha \\\\)增大时，在某一\\\\( \\alpha \\\\)有\n\\\\[\nC\\_{\\alpha}(T\\_t) = C\\_{\\alpha} (t)\n\\\\]\n当\\\\(\\alpha\\\\)增大时，不等式反向。只要\\\\( \\alpha=\\frac{C(t)-C(T\\_t)}{| T\\_t | - 1} \\\\)，\\\\(T\\_t\\\\)与\\\\(t\\\\)具有相同的损失，但是\\\\(t\\\\)的结点数量更少，因此\\\\(t\\\\)更可取，所以剪枝。\n\n对\\\\(T\\_0\\\\)中每一个内部结点\\\\(t\\\\)，计算\n\\\\[\ng(t) = \\frac{C(t)-C(T\\_t)}{| T\\_t | - 1}\n\\\\]\n它表示间之后整体损失函数减少的程度，在\\\\(T\\_0\\\\)中减去\\\\(g(t)\\\\)最小的\\\\(T\\_t\\\\)，将得到的子树作为\\\\(T\\_1\\\\)，同时将最小的\\\\(g(t)\\\\)设为\\\\(\\alpha\\\\)， \\\\(T\\_1\\\\)为区间[\\\\(\\alpha\\_1,\\alpha\\_2\\\\)]的最优子树。\n\n**这个地方理解为：最小的g(t)是一个阈值，选择\\\\(\\alpha=\\mathop{\\min}\\lbrace g(t) \\rbrace \\Longleftrightarrow\\\\) [其他g(t)的情况是-剪枝比不剪枝的损失大，即式（16）不等号反向的情况]，所以在最小g(t)处剪枝**\n\n如此剪枝下去，直至得到根节点，在这个过程中，不断增加\\\\(\\alpha\\\\)的值，产生新的区间。\n\n* 2. 在剪枝得到的子树序列中通过交叉验证选取最优子树\n\n利用独立的验证数据集，测试子树序列\\\\( T\\_0,T\\_1,...,T\\_n \\\\)中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优决策树，在子树序列中，每棵子树\\\\( T\\_1,T\\_2,...,T\\_n \\\\)都对应着一个参数\\\\( \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n \\\\)，所以当最优子树\\\\(T\\_k\\\\)确定时，对应的\\\\(\\alpha\\_k\\\\)也确定了，即得到最优决策树\\\\(T\\_\\alpha\\\\)。\n\n-----------------\n\n输入：CART算法生成的决策树\\\\(T\\_0\\\\)；\n\n输出：最优决策树\\\\(T\\_\\alpha\\\\)。\n\n（1）设\\\\(k=0, \\quad T=T\\_0\\\\)\n\n（2）设\\\\(\\alpha=0\\\\)\n\n（3）自下而上地对各内部结点\\\\(t\\\\)计算\\\\(C(T\\_t)\\\\)，\\\\(| T\\_t | \\\\)以及\n\\\\[\ng(t)=\\frac{C(t)-C(T\\_t)}{| T\\_t | - 1}\n\\\\]\n\\\\[\n\\alpha = \\mathop{\\min}(\\alpha, g(t))\n\\\\]\n这里，\\\\(T\\_t\\\\)表示以\\\\(t\\\\)为根节点的子树，\\\\(C(T\\_t)\\\\)是对训练数据的预测误差，\\\\( | T\\_t | \\\\)是\\\\(T\\_t\\\\)的叶结点个数。\n\n（4）对\\\\(g(t)=\\alpha\\\\)的内部结点\\\\(t\\\\)进行剪枝，并对叶结点\\\\(t\\\\)以多数表决法决定其类，得到树\\\\(T\\\\)。\n\n（5）设\\\\( k=k+1, \\quad \\alpha\\_k=\\alpha, \\quad T\\_k=T \\\\)。\n\n（6）如果\\\\(T\\_k\\\\)不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令\\\\(T\\_k=T\\_n\\\\)。\n\n（7）采用交叉验证法在子树序列\\\\(T\\_0,T\\_1,...,T\\_n\\\\)中选择最优子树\\\\(T\\_\\alpha\\\\)。\n\n\n-------------\n\n### 5. 特点\n\n* 优点：可解释性；可处理多种数值类型；没有复杂的参数设置；运算快\n* 缺点：易过拟合；不适合高维数据；异常值敏感；泛化能力差\n\n**控制过拟合的方式**\n\n```\n（1）树的深度\n（2）一个结点被拆分出子结点所需要的包含最少的样本个数\n（3）最底层结点所需要好汉的最小样本个数\n（4）集成学习的方法(随机森林，Xgboost等)\n```\n\n**连续值如何划分？**\n\n```\nC45：基于阈值的信息增益(比)\nCART：最优切分变量和最优切分点\n```\n\n**缺失值如何处理？**\n\n```\n概率权重(Probability Weights)：C45、ID3\n替代法(Alternate/Surrogate Splits)：CART\n```\n\n**不完整数据如何处理？** [决策树是如何处理不完整数据的？-知乎](https://www.zhihu.com/question/34867991?sort=created)\n\n```\n（1）抛弃缺失值\n抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。\n\n（2）补充缺失值\n缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多,这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。\n\n（3）概率化缺失值\n对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,“F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和”,即F表示所给属性具有已知值样本的概率。\n```\n\n-------------\n\n### 6. 代码实现\n\n代码中的公式均指李航老师的《统计学习方法》中的公式。\n\n[Decision\\_Tree.py](/posts_res/2018-03-18-decision-tree/decision_tree.py)\n\n```python\n# coding:utf-8\nimport numpy as np\nimport pandas as pd\n\nclass DecisionTree(object):\n    def __init__(self, feature_names, threshold, principle=\"information gain\"):\n        self.feature_names = feature_names\n        self.threshold = threshold\n        self.principle = principle\n    # formula 5.7\n    def __calculate_entropy__(self, y):\n        datalen = len(y)\n        labelprob = {l: 0 for l in set(y)}\n        entropy = 0.0\n        for l in y:\n            labelprob[l] += 1\n        for l in labelprob.keys():\n            thisfrac = labelprob[l] / datalen\n            entropy -= thisfrac * np.log2(thisfrac)\n        return entropy\n    # formula 5.8\n    def __calculate_conditional_entropy__(self, X, y, axis):\n        datalen = len(y)\n        featureset = set([x[axis] for x in X])\n        sub_y = {f:list() for f in featureset}\n        for i in range(datalen):\n            sub_y[X[i][axis]].append(y[i])\n        conditional_entropy = 0.0\n        for key in sub_y.keys():\n            prob = len(sub_y[key]) / datalen\n            entropy = self.__calculate_entropy__(sub_y[key])\n            conditional_entropy += prob * entropy\n        return conditional_entropy\n    # formula 5.9\n    def calculate_information_gain(self, X, y, axis):\n        hd = self.__calculate_entropy__(y)\n        hda = self.__calculate_conditional_entropy__(X, y, axis)\n        gda = hd - hda\n        return gda\n\n    def __most_class__(self, y):\n        labelset = set(y)\n        labelcnt = {l:0 for l in labelset}\n        for y_i in y:\n            labelcnt[y_i] += 1\n        st = sorted(labelcnt.items(), key=lambda x: x[1], reverse=True)\n        return st[0][0]\n    # formula 5.10\n    def calculate_information_gain_ratio(self, X, y, axis):\n        gda = self.calculate_information_gain(X, y, axis)\n        had = self.__calculate_entropy__(X[:, axis])\n        grda = gda / had\n        return grda\n\n    def __split_dataset__(self, X, y, axis, value):\n        rstX = list()\n        rsty = list()\n        for i in range(len(X)):\n            if X[i][axis] == value:\n                tmpfeature = list(X[i][:axis])\n                tmpfeature.extend(list(X[i][axis+1:]))\n                rstX.append(tmpfeature)\n                rsty.append(y[i])\n        return np.asarray(rstX), np.asarray(rsty)\n\n    def __best_split_feature__(self, X, y, feature_names):\n        best_feature = -1\n        max_principle = -1.0\n        for feature_n in feature_names:\n            axis = feature_names.index(feature_n)\n            if self.principle == \"information gain\":\n                this_principle = self.calculate_information_gain(X, y, axis)\n            else:\n                this_principle = self.calculate_information_gain_ratio(X, y, axis)\n            print(\"%s\\t%f\\t%s\" % (feature_n, this_principle, self.principle))\n            if this_principle > max_principle:\n                best_feature = axis\n                max_principle = this_principle\n        print(\"-----\")\n        return best_feature, max_principle\n\n    def _fit(self, X, y, feature_names):\n        # 所有实例属于同一类\n        labelset = set(y)\n        if len(labelset) == 1:\n            return labelset.pop()\n        # 如果特征集为空集，置T为单结点树，实例最多的类作为该结点的类，并返回T\n        if len(feature_names) == 0:\n            return self.__most_class__(y)\n        # 计算准则,选择特征\n        best_feature, max_principle = self.__best_split_feature__(X, y, feature_names)\n        # 如果小于阈值，置T为单结点树，实例最多的类作为该结点的类，并返回T\n        if max_principle < self.threshold:\n            return self.__most_class__(y)\n\n        best_feature_label = feature_names[best_feature]\n        del feature_names[best_feature]\n        tree = {best_feature_label: {}}\n\n        bestfeature_values = set([x[best_feature] for x in X])\n        for value in bestfeature_values:\n            sub_X, sub_y = self.__split_dataset__(X, y, best_feature, value)\n            tree[best_feature_label][value] = self._fit(sub_X, sub_y, feature_names)\n        return tree\n\n    def fit(self, X, y):\n        feature_names = self.feature_names[:]\n        self.tree = self._fit(X, y, feature_names)\n\n    def _predict(self, tree, feature_names, x):\n        firstStr = list(tree.keys())[0]\n        secondDict = tree[firstStr]\n        featIndex = feature_names.index(firstStr)\n        key = x[featIndex]\n        valueOfFeat = secondDict[key]\n        if isinstance(valueOfFeat, dict):\n            classLabel = self._predict(valueOfFeat, feature_names, x)\n        else:\n            classLabel = valueOfFeat\n        return classLabel\n\n    def predict(self, X):\n        preds = list()\n        for x in X:\n            preds.append(self._predict(self.tree, self.feature_names, x))\n        return preds\n\n    def output_tree(self):\n        import treePlot     # cite: https://gitee.com/orayang_admin/ID3_decisiontree/tree/master\n        import importlib\n        importlib.reload(treePlot)\n        treePlot.createPlot(self.tree)\n\ndef load_data():\n    dt = pd.read_csv(\"./credit.csv\")    # from lihang - \"statistic learning method\" - page59, table 5.1\n    # dt = pd.read_csv(\"./titanic.csv\")\n    data = dt.values\n    feature_names = dt.columns[:-1] # delete label column\n    return data, list(feature_names)\n\ndef run_ID3():\n    data, feature_names = load_data()\n    print(\"ID3 Descision Tree ... \")\n    ml = DecisionTree(feature_names=feature_names, threshold=0, principle=\"information gain\")\n    ml.fit(data[:, :-1], data[:, -1])\n    test = [[\"mid\", \"yes\", \"no\", \"good\"]]\n    preds = ml.predict(test)\n    print(\"ID3 predict:\", preds)\n    ml.output_tree()\n\ndef run_C45():\n    data, feature_names = load_data()\n    print(\"C45 Descision Tree ... \")\n    ml = DecisionTree(feature_names=feature_names, threshold=0, principle=\"information gain ratio\")\n    ml.fit(data[:, :-1], data[:, -1])\n    test = [[\"mid\", \"yes\", \"no\", \"good\"]]\n    preds = ml.predict(test)\n    print(\"C45 predict:\", preds)\n    ml.output_tree()\n\nif __name__ == '__main__':\n    # run_ID3()\n    run_C45()\n```\n\n结果：\n\n    ID3 Descision Tree ... \n    age\t0.083007\tinformation gain\n    job\t0.323650\tinformation gain\n    house\t0.419973\tinformation gain\n    credit\t0.362990\tinformation gain\n    -----\n    age\t0.251629\tinformation gain\n    job\t0.918296\tinformation gain\n    credit\t0.473851\tinformation gain\n    -----\n    ID3 predict: ['yes']\n\n-----\n\n    C45 Descision Tree ... \n    age\t0.052372\tinformation gain ratio\n    job\t0.352447\tinformation gain ratio\n    house\t0.432538\tinformation gain ratio\n    credit\t0.231854\tinformation gain ratio\n    -----\n    age\t0.164411\tinformation gain ratio\n    job\t1.000000\tinformation gain ratio\n    credit\t0.340374\tinformation gain ratio\n    -----\n    C45 predict: ['yes']\n\n![result](/posts_res/2018-03-18-decision-tree/dt.png)\n\n\n-------------\n\n### 7. 参考\n\n> 李航 - 《统计学习方法》\n> \n> zergzzlun - [cart树怎么进行剪枝？](https://www.zhihu.com/question/22697086/answer/134841101)\n> \n> 巩固,张虹 - 决策树算法中属性缺失值的研究\n>\n> 周志华 - 《机器学习》\n>\n> [OraYang的博客](https://blog.csdn.net/u010665216/article/details/78173064)\n>\n","source":"_posts/2018-03-18-decision-tree.md","raw":"---\nlayout: post\ntitle: 决策树 Decision Tree\ndate: 2018-03-18 20:10 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n## <center>决策树 - Decision Tree</center>\n\n#### 目录\n* ID3\n* C45\n* 决策树的剪枝\n* CART\n* 特点\n* 代码实现\n\n\n---------------\n\n决策树学习的3个步骤：\n* 特征选择\n* 决策树生成\n* 决策树修剪\n\n决策树的损失函数通常为正则化的极大似然函数。\n\n\n--------------\n\n### 1. ID3\n\nID3算法使用信息增益作为特征选择的标准，信息增益**越大越好**。\n\n#### 1.1 信息增益算法\n输入：\n\n训练数据集 \\\\( D \\\\) 和特征 \\\\( A \\\\);\n\n输出：\n\n特征\\\\(A\\\\)对训练数据集\\\\(D\\\\)的信息增益 \\\\( g(D,A) \\\\)\n\n（1）计算数据集 \\\\( D \\\\) 的经验熵 \\\\( H(D) \\\\)\n\\\\[\nH(D) = - \\sum\\_{k=1}^K \\frac{ | C\\_k |}{| D |}log\\_2 \\frac{| C_k |}{| D |}\n\\\\]\n\n（2）计算特征\\\\(A\\\\)对数据集\\\\(D\\\\)的经验条件熵\\\\( H(D | A) \\\\)\n\\\\[\nH(D | A) = \\sum\\_{i=1}^n \\frac{| D\\_i |}{| D |} H(D\\_i) = - \\sum\\_{i=1}^n \\frac{ | D\\_i | }{ | D | } \\sum\\_{k=1}^K \\frac{| D\\_{ik} |}{ | D\\_i |} log\\_2 \\frac{ | D\\_{ik} |}{| D\\_i |}\n\\\\]\n\n（3）计算信息增益\n\\\\[\ng(D,A) = H(D) - H(D | A)\n\\\\]\n\n其中\\\\(K\\\\)为类别的数量\\\\( \\lbrace C\\_1, C\\_2, ..., C\\_k \\rbrace \\\\)，\\\\(n\\\\)为特征\\\\(A\\\\)的取值数量\\\\( \\lbrace a\\_1, a\\_2, ..., a\\_n \\rbrace \\\\)。\n\n\n-----------\n\n### 2. C45\n\n信息增益作为标准时，存在偏向于取值数量较多的特征的问题，因此C45算法选择信息增益比作为划分的标准，信息增益比**越大越好**。\n\n#### 2.1 信息增益比(基于信息增益)\n定义为信息增益\\\\(g(D,A)\\\\)与关于特征\\\\(A\\\\)的值的熵\\\\(H\\_A(D)\\\\)之比，即：\n\\\\[\ng\\_R(D,A) = \\frac{g(D,A)}{H\\_A(D)}\n\\\\]\n\\\\[\nH\\_A(D) = - \\sum\\_{i=1}^n \\frac{ | D\\_i | } { | D | } log\\_2 \\frac{ | D\\_i | }{ | D |}\n\\\\]\n其中\\\\(n\\\\)为特征\\\\(A\\\\)取值的个数。\n\n\n-----------\n\n### 3. 决策树的剪枝\n\n设树\\\\( T \\\\)的叶结点个数为\\\\( | T | \\\\)，\\\\(t\\\\)是树\\\\(T\\\\)的叶结点，该叶结点有\\\\(N\\_t\\\\)个样本点，其中\\\\(k\\\\)类的样本点有\\\\(N\\_{tk}\\\\)个，\\\\(k=1,2,...,K\\\\)，\\\\(H\\_t(T)\\\\)为叶结点上的经验熵，则决策树的损失函数可以定义为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nC\\_{\\alpha}(T)\n& = \\sum\\_{t=1}^{ | T | } N\\_t H\\_t(T) + \\alpha | T | \\\\\\\n& = - \\sum\\_{t=1}^{ | T | } \\sum\\_{k=1}^K N\\_{tk} log \\frac{N\\_{tk}}{N\\_t} + \\alpha | T | \\\\\\\n& = C(T) + \\alpha | T |\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n通过 \\\\( \\alpha \\geq 0 \\\\)控制模型与训练数据拟合度和模型复杂度。\n\n#### 3.1 ID3&C45树的剪枝算法\n输入：生成算法产生的整个树 \\\\( T \\\\)，参数 \\\\( \\alpha \\\\)；\n\n输出：修剪后的子树\\\\( T\\_{\\alpha} \\\\)。\n\n（1）计算每个结点的经验熵, 经验熵计算公式；\n\\\\[\nH\\_t(T) = - \\sum\\_k \\frac{N\\_{tk}}{N\\_t} log \\frac{N\\_{tk}}{N\\_t}\n\\\\]\n\n（2）递归地从树的叶结点向上回缩；\n\n设一组叶结点回缩到其父结点之前与之后的整体树分别为\\\\(T\\_B\\\\)与\\\\(T\\_A\\\\)，其对应的损失函数值分别为\\\\(C\\_{\\alpha}(T\\_B)\\\\)与\\\\(C\\_{\\alpha}(T\\_A)\\\\)，如果\n\\\\[\nC\\_{\\alpha}(T\\_B) \\leq C\\_{\\alpha}(T\\_A)\n\\\\]\n则进行剪枝，即将父结点变为新的叶结点。\n\n（3）返回（2），直至不能继续为止，得到损失函数最小的子树\\\\(T\\_{\\alpha}\\\\)。\n\n\n-----------\n\n### 4. CART\n\nCART假设决策树是二叉树，内部结点特征取值为“是”和“否”，递归二分每个特征。\n\n* 回归数使用平方误差最小化准则，**越小越好**。\n* 分类数使用基尼指数最小化准则，**越小越好**。\n\n#### 4.1 回归树\n\n输入：训练数据集\\\\( D=\\lbrace (x\\_1, y\\_1), (x\\_2, y\\_2), ..., (x\\_N, y\\_N) \\rbrace \\\\), 并且\\\\(Y\\\\)是连续型变量；\n\n输出：回归树 \\\\( f(x) \\\\)\n\n在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n\n（1）选择最优切分变量\\\\(j\\\\)与切分点\\\\(s\\\\)，求解\n\\\\[\n\\mathop{\\min}\\_{j,s} [\\mathop{\\min}\\_{c\\_1} \\sum\\_{x\\_i \\in R\\_1(j,s)} (y\\_i - c\\_1)^2 + \\mathop{\\min}\\_{c\\_2} \\sum\\_{x\\_i \\in R\\_2(j,s)} (y\\_i - c\\_2)^2]\n\\\\]\n遍历\\\\(j\\\\)，对固定的切分变量\\\\(j\\\\)扫描切分点\\\\(s\\\\)，选择使得上式达到最小值的对\\\\((j,s)\\\\)。\n\n（2）用选定的对\\\\((j,s)\\\\)划分区域并决定相应的输出值(均值)：\n\\\\[\nR\\_1(j,s) = \\lbrace x | x^{(j)} \\leq s \\rbrace, \\quad R\\_2(j,s)=\\lbrace x | x^{(j)} > s \\rbrace \\\\\\\n\\hat{c}\\_m = \\frac{1}{N\\_m} \\sum\\_{x\\_i \\in R\\_m(j,s)} y\\_i, \\quad x \\in R\\_m, \\quad m=1,2\n\\\\]\n\n\n（3）继续对两个子区域调用步骤(1),(2)，直至满足停止条件；\n\n（4）将输入空间划分为\\\\(M\\\\)个区域\\\\( R\\_1, R\\_2, ..., R\\_M \\\\)，生成决策树：\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\hat{c}\\_m I(x \\in R\\_m)\n\\\\]\n\n#### 4.2 分类树\n\n在特征\\\\( A \\\\)的条件下，集合\\\\( D \\\\)的基尼指数定义为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGini(D, A)\n& = \\frac{ | D\\_1 | }{ | D | } Gini(D\\_1) + \\frac{ | D\\_2 | }{ | D | } Gini(D\\_2) \\\\\\\n& = \\frac{ | D\\_1 | }{ | D | } [1 - \\sum\\_{k=1}^K(\\frac{ | C\\_k | }{ | D\\_1 | })^2 ] + \\frac{ | D\\_2 | }{ | D | } [1 - \\sum\\_{k=1}^K(\\frac{ | C\\_k | }{ | D\\_2 | })^2 ]\n\\end{aligned}\n\\end{equation}\n\\\\]\n其中\\\\( C\\_k \\\\)是\\\\( D\\_i \\\\)中属于第\\\\(k\\\\)类的样本子集，\\\\(K\\\\)是类的个数。\n\n\n输入：训练集\\\\(D\\\\),停止计算的条件\n\n输出：CART决策树\n\n根据训练集，从跟结点开始递归地对每个结点进行一下操作，构建二叉决策树。\n\n（1）根据每一个特征以及每个特征的取值，计算相应二叉划分时的基尼指数；\n\n（2）在所有可能的特征及可能的特征值中，选择基尼指数**最小**的特征及相应特征值作为划分切分特征及切分点，并将训练集划分到两个子结点中；\n\n（3）对两个子节点递归调用(1),(2)，直至满足停止条件；\n\n（4）生成CART决策树。\n\n#### 4.3 CART的剪枝算法\n\n* 1. 剪枝，形成一个子树序列\n\n在剪枝过程中，计算子树的损失函数：\n\\\\[\nC\\_{\\alpha}(T) = C(T) + \\alpha | T |\n\\\\]\n其中，\\\\(T\\\\)为任意子树，\\\\(C(T)\\\\)表示训练数据的预测误差，\\\\(| T |\\\\)为子树的叶结点数目，\\\\( \\alpha \\geq 0 \\\\)为参数。\n\n具体地，从整体树\\\\(T\\_0\\\\)开始剪枝，对\\\\(T\\_0\\\\)的任意内部结点\\\\(t\\\\)，以\\\\(t\\\\)为单结点树的损失函数是\n\\\\[\nC\\_{\\alpha}(t) = C(t) + \\alpha\n\\\\]\n以\\\\(t\\\\)为根节点的子树\\\\(T\\_t\\\\)的损失函数是\n\\\\[\nC\\_{\\alpha}(T\\_t) = C(T\\_t) + \\alpha | T\\_t |\n\\\\]\n<center>\n<img src=\"/posts_res/2018-03-18-decision-tree/cart_cut.png\" />\n</center>\n\n当\\\\( \\alpha=0 \\\\)及\\\\(\\alpha\\\\)充分小时，有不等式\n\\\\[\nC\\_{\\alpha}(T\\_t) < C\\_{\\alpha}(t)\n\\\\]\n当\\\\( \\alpha \\\\)增大时，在某一\\\\( \\alpha \\\\)有\n\\\\[\nC\\_{\\alpha}(T\\_t) = C\\_{\\alpha} (t)\n\\\\]\n当\\\\(\\alpha\\\\)增大时，不等式反向。只要\\\\( \\alpha=\\frac{C(t)-C(T\\_t)}{| T\\_t | - 1} \\\\)，\\\\(T\\_t\\\\)与\\\\(t\\\\)具有相同的损失，但是\\\\(t\\\\)的结点数量更少，因此\\\\(t\\\\)更可取，所以剪枝。\n\n对\\\\(T\\_0\\\\)中每一个内部结点\\\\(t\\\\)，计算\n\\\\[\ng(t) = \\frac{C(t)-C(T\\_t)}{| T\\_t | - 1}\n\\\\]\n它表示间之后整体损失函数减少的程度，在\\\\(T\\_0\\\\)中减去\\\\(g(t)\\\\)最小的\\\\(T\\_t\\\\)，将得到的子树作为\\\\(T\\_1\\\\)，同时将最小的\\\\(g(t)\\\\)设为\\\\(\\alpha\\\\)， \\\\(T\\_1\\\\)为区间[\\\\(\\alpha\\_1,\\alpha\\_2\\\\)]的最优子树。\n\n**这个地方理解为：最小的g(t)是一个阈值，选择\\\\(\\alpha=\\mathop{\\min}\\lbrace g(t) \\rbrace \\Longleftrightarrow\\\\) [其他g(t)的情况是-剪枝比不剪枝的损失大，即式（16）不等号反向的情况]，所以在最小g(t)处剪枝**\n\n如此剪枝下去，直至得到根节点，在这个过程中，不断增加\\\\(\\alpha\\\\)的值，产生新的区间。\n\n* 2. 在剪枝得到的子树序列中通过交叉验证选取最优子树\n\n利用独立的验证数据集，测试子树序列\\\\( T\\_0,T\\_1,...,T\\_n \\\\)中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优决策树，在子树序列中，每棵子树\\\\( T\\_1,T\\_2,...,T\\_n \\\\)都对应着一个参数\\\\( \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n \\\\)，所以当最优子树\\\\(T\\_k\\\\)确定时，对应的\\\\(\\alpha\\_k\\\\)也确定了，即得到最优决策树\\\\(T\\_\\alpha\\\\)。\n\n-----------------\n\n输入：CART算法生成的决策树\\\\(T\\_0\\\\)；\n\n输出：最优决策树\\\\(T\\_\\alpha\\\\)。\n\n（1）设\\\\(k=0, \\quad T=T\\_0\\\\)\n\n（2）设\\\\(\\alpha=0\\\\)\n\n（3）自下而上地对各内部结点\\\\(t\\\\)计算\\\\(C(T\\_t)\\\\)，\\\\(| T\\_t | \\\\)以及\n\\\\[\ng(t)=\\frac{C(t)-C(T\\_t)}{| T\\_t | - 1}\n\\\\]\n\\\\[\n\\alpha = \\mathop{\\min}(\\alpha, g(t))\n\\\\]\n这里，\\\\(T\\_t\\\\)表示以\\\\(t\\\\)为根节点的子树，\\\\(C(T\\_t)\\\\)是对训练数据的预测误差，\\\\( | T\\_t | \\\\)是\\\\(T\\_t\\\\)的叶结点个数。\n\n（4）对\\\\(g(t)=\\alpha\\\\)的内部结点\\\\(t\\\\)进行剪枝，并对叶结点\\\\(t\\\\)以多数表决法决定其类，得到树\\\\(T\\\\)。\n\n（5）设\\\\( k=k+1, \\quad \\alpha\\_k=\\alpha, \\quad T\\_k=T \\\\)。\n\n（6）如果\\\\(T\\_k\\\\)不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令\\\\(T\\_k=T\\_n\\\\)。\n\n（7）采用交叉验证法在子树序列\\\\(T\\_0,T\\_1,...,T\\_n\\\\)中选择最优子树\\\\(T\\_\\alpha\\\\)。\n\n\n-------------\n\n### 5. 特点\n\n* 优点：可解释性；可处理多种数值类型；没有复杂的参数设置；运算快\n* 缺点：易过拟合；不适合高维数据；异常值敏感；泛化能力差\n\n**控制过拟合的方式**\n\n```\n（1）树的深度\n（2）一个结点被拆分出子结点所需要的包含最少的样本个数\n（3）最底层结点所需要好汉的最小样本个数\n（4）集成学习的方法(随机森林，Xgboost等)\n```\n\n**连续值如何划分？**\n\n```\nC45：基于阈值的信息增益(比)\nCART：最优切分变量和最优切分点\n```\n\n**缺失值如何处理？**\n\n```\n概率权重(Probability Weights)：C45、ID3\n替代法(Alternate/Surrogate Splits)：CART\n```\n\n**不完整数据如何处理？** [决策树是如何处理不完整数据的？-知乎](https://www.zhihu.com/question/34867991?sort=created)\n\n```\n（1）抛弃缺失值\n抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。\n\n（2）补充缺失值\n缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多,这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。\n\n（3）概率化缺失值\n对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,“F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和”,即F表示所给属性具有已知值样本的概率。\n```\n\n-------------\n\n### 6. 代码实现\n\n代码中的公式均指李航老师的《统计学习方法》中的公式。\n\n[Decision\\_Tree.py](/posts_res/2018-03-18-decision-tree/decision_tree.py)\n\n```python\n# coding:utf-8\nimport numpy as np\nimport pandas as pd\n\nclass DecisionTree(object):\n    def __init__(self, feature_names, threshold, principle=\"information gain\"):\n        self.feature_names = feature_names\n        self.threshold = threshold\n        self.principle = principle\n    # formula 5.7\n    def __calculate_entropy__(self, y):\n        datalen = len(y)\n        labelprob = {l: 0 for l in set(y)}\n        entropy = 0.0\n        for l in y:\n            labelprob[l] += 1\n        for l in labelprob.keys():\n            thisfrac = labelprob[l] / datalen\n            entropy -= thisfrac * np.log2(thisfrac)\n        return entropy\n    # formula 5.8\n    def __calculate_conditional_entropy__(self, X, y, axis):\n        datalen = len(y)\n        featureset = set([x[axis] for x in X])\n        sub_y = {f:list() for f in featureset}\n        for i in range(datalen):\n            sub_y[X[i][axis]].append(y[i])\n        conditional_entropy = 0.0\n        for key in sub_y.keys():\n            prob = len(sub_y[key]) / datalen\n            entropy = self.__calculate_entropy__(sub_y[key])\n            conditional_entropy += prob * entropy\n        return conditional_entropy\n    # formula 5.9\n    def calculate_information_gain(self, X, y, axis):\n        hd = self.__calculate_entropy__(y)\n        hda = self.__calculate_conditional_entropy__(X, y, axis)\n        gda = hd - hda\n        return gda\n\n    def __most_class__(self, y):\n        labelset = set(y)\n        labelcnt = {l:0 for l in labelset}\n        for y_i in y:\n            labelcnt[y_i] += 1\n        st = sorted(labelcnt.items(), key=lambda x: x[1], reverse=True)\n        return st[0][0]\n    # formula 5.10\n    def calculate_information_gain_ratio(self, X, y, axis):\n        gda = self.calculate_information_gain(X, y, axis)\n        had = self.__calculate_entropy__(X[:, axis])\n        grda = gda / had\n        return grda\n\n    def __split_dataset__(self, X, y, axis, value):\n        rstX = list()\n        rsty = list()\n        for i in range(len(X)):\n            if X[i][axis] == value:\n                tmpfeature = list(X[i][:axis])\n                tmpfeature.extend(list(X[i][axis+1:]))\n                rstX.append(tmpfeature)\n                rsty.append(y[i])\n        return np.asarray(rstX), np.asarray(rsty)\n\n    def __best_split_feature__(self, X, y, feature_names):\n        best_feature = -1\n        max_principle = -1.0\n        for feature_n in feature_names:\n            axis = feature_names.index(feature_n)\n            if self.principle == \"information gain\":\n                this_principle = self.calculate_information_gain(X, y, axis)\n            else:\n                this_principle = self.calculate_information_gain_ratio(X, y, axis)\n            print(\"%s\\t%f\\t%s\" % (feature_n, this_principle, self.principle))\n            if this_principle > max_principle:\n                best_feature = axis\n                max_principle = this_principle\n        print(\"-----\")\n        return best_feature, max_principle\n\n    def _fit(self, X, y, feature_names):\n        # 所有实例属于同一类\n        labelset = set(y)\n        if len(labelset) == 1:\n            return labelset.pop()\n        # 如果特征集为空集，置T为单结点树，实例最多的类作为该结点的类，并返回T\n        if len(feature_names) == 0:\n            return self.__most_class__(y)\n        # 计算准则,选择特征\n        best_feature, max_principle = self.__best_split_feature__(X, y, feature_names)\n        # 如果小于阈值，置T为单结点树，实例最多的类作为该结点的类，并返回T\n        if max_principle < self.threshold:\n            return self.__most_class__(y)\n\n        best_feature_label = feature_names[best_feature]\n        del feature_names[best_feature]\n        tree = {best_feature_label: {}}\n\n        bestfeature_values = set([x[best_feature] for x in X])\n        for value in bestfeature_values:\n            sub_X, sub_y = self.__split_dataset__(X, y, best_feature, value)\n            tree[best_feature_label][value] = self._fit(sub_X, sub_y, feature_names)\n        return tree\n\n    def fit(self, X, y):\n        feature_names = self.feature_names[:]\n        self.tree = self._fit(X, y, feature_names)\n\n    def _predict(self, tree, feature_names, x):\n        firstStr = list(tree.keys())[0]\n        secondDict = tree[firstStr]\n        featIndex = feature_names.index(firstStr)\n        key = x[featIndex]\n        valueOfFeat = secondDict[key]\n        if isinstance(valueOfFeat, dict):\n            classLabel = self._predict(valueOfFeat, feature_names, x)\n        else:\n            classLabel = valueOfFeat\n        return classLabel\n\n    def predict(self, X):\n        preds = list()\n        for x in X:\n            preds.append(self._predict(self.tree, self.feature_names, x))\n        return preds\n\n    def output_tree(self):\n        import treePlot     # cite: https://gitee.com/orayang_admin/ID3_decisiontree/tree/master\n        import importlib\n        importlib.reload(treePlot)\n        treePlot.createPlot(self.tree)\n\ndef load_data():\n    dt = pd.read_csv(\"./credit.csv\")    # from lihang - \"statistic learning method\" - page59, table 5.1\n    # dt = pd.read_csv(\"./titanic.csv\")\n    data = dt.values\n    feature_names = dt.columns[:-1] # delete label column\n    return data, list(feature_names)\n\ndef run_ID3():\n    data, feature_names = load_data()\n    print(\"ID3 Descision Tree ... \")\n    ml = DecisionTree(feature_names=feature_names, threshold=0, principle=\"information gain\")\n    ml.fit(data[:, :-1], data[:, -1])\n    test = [[\"mid\", \"yes\", \"no\", \"good\"]]\n    preds = ml.predict(test)\n    print(\"ID3 predict:\", preds)\n    ml.output_tree()\n\ndef run_C45():\n    data, feature_names = load_data()\n    print(\"C45 Descision Tree ... \")\n    ml = DecisionTree(feature_names=feature_names, threshold=0, principle=\"information gain ratio\")\n    ml.fit(data[:, :-1], data[:, -1])\n    test = [[\"mid\", \"yes\", \"no\", \"good\"]]\n    preds = ml.predict(test)\n    print(\"C45 predict:\", preds)\n    ml.output_tree()\n\nif __name__ == '__main__':\n    # run_ID3()\n    run_C45()\n```\n\n结果：\n\n    ID3 Descision Tree ... \n    age\t0.083007\tinformation gain\n    job\t0.323650\tinformation gain\n    house\t0.419973\tinformation gain\n    credit\t0.362990\tinformation gain\n    -----\n    age\t0.251629\tinformation gain\n    job\t0.918296\tinformation gain\n    credit\t0.473851\tinformation gain\n    -----\n    ID3 predict: ['yes']\n\n-----\n\n    C45 Descision Tree ... \n    age\t0.052372\tinformation gain ratio\n    job\t0.352447\tinformation gain ratio\n    house\t0.432538\tinformation gain ratio\n    credit\t0.231854\tinformation gain ratio\n    -----\n    age\t0.164411\tinformation gain ratio\n    job\t1.000000\tinformation gain ratio\n    credit\t0.340374\tinformation gain ratio\n    -----\n    C45 predict: ['yes']\n\n![result](/posts_res/2018-03-18-decision-tree/dt.png)\n\n\n-------------\n\n### 7. 参考\n\n> 李航 - 《统计学习方法》\n> \n> zergzzlun - [cart树怎么进行剪枝？](https://www.zhihu.com/question/22697086/answer/134841101)\n> \n> 巩固,张虹 - 决策树算法中属性缺失值的研究\n>\n> 周志华 - 《机器学习》\n>\n> [OraYang的博客](https://blog.csdn.net/u010665216/article/details/78173064)\n>\n","slug":"decision-tree","published":1,"updated":"2019-08-17T09:31:51.718Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnv4000r2qwpsapnoanp","content":"<h2 id=\"决策树-Decision-Tree\"><a href=\"#决策树-Decision-Tree\" class=\"headerlink\" title=\"决策树 - Decision Tree\"></a><center>决策树 - Decision Tree</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>ID3</li>\n<li>C45</li>\n<li>决策树的剪枝</li>\n<li>CART</li>\n<li>特点</li>\n<li>代码实现</li>\n</ul><hr><p>决策树学习的3个步骤：</p><ul>\n<li>特征选择</li>\n<li>决策树生成</li>\n<li>决策树修剪</li>\n</ul><p>决策树的损失函数通常为正则化的极大似然函数。</p><hr><h3 id=\"1-ID3\"><a href=\"#1-ID3\" class=\"headerlink\" title=\"1. ID3\"></a>1. ID3</h3><p>ID3算法使用信息增益作为特征选择的标准，信息增益<strong>越大越好</strong>。</p><h4 id=\"1-1-信息增益算法\"><a href=\"#1-1-信息增益算法\" class=\"headerlink\" title=\"1.1 信息增益算法\"></a>1.1 信息增益算法</h4><p>输入：</p><p>训练数据集 \\( D \\) 和特征 \\( A \\);</p><p>输出：</p><p>特征\\(A\\)对训练数据集\\(D\\)的信息增益 \\( g(D,A) \\)</p><p>（1）计算数据集 \\( D \\) 的经验熵 \\( H(D) \\)\n\\[\nH(D) = - \\sum_{k=1}^K \\frac{ | C_k |}{| D |}log_2 \\frac{| C_k |}{| D |}\n\\]</p><a id=\"more\"></a><!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>（2）计算特征\\(A\\)对数据集\\(D\\)的经验条件熵\\( H(D | A) \\)\n\\[\nH(D | A) = \\sum_{i=1}^n \\frac{| D_i |}{| D |} H(D_i) = - \\sum_{i=1}^n \\frac{ | D_i | }{ | D | } \\sum_{k=1}^K \\frac{| D_{ik} |}{ | D_i |} log_2 \\frac{ | D_{ik} |}{| D_i |}\n\\]</p>\n<p>（3）计算信息增益\n\\[\ng(D,A) = H(D) - H(D | A)\n\\]</p>\n<p>其中\\(K\\)为类别的数量\\( \\lbrace C_1, C_2, …, C_k \\rbrace \\)，\\(n\\)为特征\\(A\\)的取值数量\\( \\lbrace a_1, a_2, …, a_n \\rbrace \\)。</p>\n<hr>\n<h3 id=\"2-C45\"><a href=\"#2-C45\" class=\"headerlink\" title=\"2. C45\"></a>2. C45</h3><p>信息增益作为标准时，存在偏向于取值数量较多的特征的问题，因此C45算法选择信息增益比作为划分的标准，信息增益比<strong>越大越好</strong>。</p>\n<h4 id=\"2-1-信息增益比-基于信息增益\"><a href=\"#2-1-信息增益比-基于信息增益\" class=\"headerlink\" title=\"2.1 信息增益比(基于信息增益)\"></a>2.1 信息增益比(基于信息增益)</h4><p>定义为信息增益\\(g(D,A)\\)与关于特征\\(A\\)的值的熵\\(H_A(D)\\)之比，即：\n\\[\ng_R(D,A) = \\frac{g(D,A)}{H_A(D)}\n\\]\n\\[\nH_A(D) = - \\sum_{i=1}^n \\frac{ | D_i | } { | D | } log_2 \\frac{ | D_i | }{ | D |}\n\\]\n其中\\(n\\)为特征\\(A\\)取值的个数。</p>\n<hr>\n<h3 id=\"3-决策树的剪枝\"><a href=\"#3-决策树的剪枝\" class=\"headerlink\" title=\"3. 决策树的剪枝\"></a>3. 决策树的剪枝</h3><p>设树\\( T \\)的叶结点个数为\\( | T | \\)，\\(t\\)是树\\(T\\)的叶结点，该叶结点有\\(N_t\\)个样本点，其中\\(k\\)类的样本点有\\(N_{tk}\\)个，\\(k=1,2,…,K\\)，\\(H_t(T)\\)为叶结点上的经验熵，则决策树的损失函数可以定义为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nC_{\\alpha}(T)\n&amp; = \\sum_{t=1}^{ | T | } N_t H_t(T) + \\alpha | T | \\\\\n&amp; = - \\sum_{t=1}^{ | T | } \\sum_{k=1}^K N_{tk} log \\frac{N_{tk}}{N_t} + \\alpha | T | \\\\\n&amp; = C(T) + \\alpha | T |\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>通过 \\( \\alpha \\geq 0 \\)控制模型与训练数据拟合度和模型复杂度。</p>\n<h4 id=\"3-1-ID3-amp-C45树的剪枝算法\"><a href=\"#3-1-ID3-amp-C45树的剪枝算法\" class=\"headerlink\" title=\"3.1 ID3&amp;C45树的剪枝算法\"></a>3.1 ID3&amp;C45树的剪枝算法</h4><p>输入：生成算法产生的整个树 \\( T \\)，参数 \\( \\alpha \\)；</p>\n<p>输出：修剪后的子树\\( T_{\\alpha} \\)。</p>\n<p>（1）计算每个结点的经验熵, 经验熵计算公式；\n\\[\nH_t(T) = - \\sum_k \\frac{N_{tk}}{N_t} log \\frac{N_{tk}}{N_t}\n\\]</p>\n<p>（2）递归地从树的叶结点向上回缩；</p>\n<p>设一组叶结点回缩到其父结点之前与之后的整体树分别为\\(T_B\\)与\\(T_A\\)，其对应的损失函数值分别为\\(C_{\\alpha}(T_B)\\)与\\(C_{\\alpha}(T_A)\\)，如果\n\\[\nC_{\\alpha}(T_B) \\leq C_{\\alpha}(T_A)\n\\]\n则进行剪枝，即将父结点变为新的叶结点。</p>\n<p>（3）返回（2），直至不能继续为止，得到损失函数最小的子树\\(T_{\\alpha}\\)。</p>\n<hr>\n<h3 id=\"4-CART\"><a href=\"#4-CART\" class=\"headerlink\" title=\"4. CART\"></a>4. CART</h3><p>CART假设决策树是二叉树，内部结点特征取值为“是”和“否”，递归二分每个特征。</p>\n<ul>\n<li>回归数使用平方误差最小化准则，<strong>越小越好</strong>。</li>\n<li>分类数使用基尼指数最小化准则，<strong>越小越好</strong>。</li>\n</ul>\n<h4 id=\"4-1-回归树\"><a href=\"#4-1-回归树\" class=\"headerlink\" title=\"4.1 回归树\"></a>4.1 回归树</h4><p>输入：训练数据集\\( D=\\lbrace (x_1, y_1), (x_2, y_2), …, (x_N, y_N) \\rbrace \\), 并且\\(Y\\)是连续型变量；</p>\n<p>输出：回归树 \\( f(x) \\)</p>\n<p>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<p>（1）选择最优切分变量\\(j\\)与切分点\\(s\\)，求解\n\\[\n\\mathop{\\min}_{j,s} [\\mathop{\\min}_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\mathop{\\min}_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2]\n\\]\n遍历\\(j\\)，对固定的切分变量\\(j\\)扫描切分点\\(s\\)，选择使得上式达到最小值的对\\((j,s)\\)。</p>\n<p>（2）用选定的对\\((j,s)\\)划分区域并决定相应的输出值(均值)：\n\\[\nR_1(j,s) = \\lbrace x | x^{(j)} \\leq s \\rbrace, \\quad R_2(j,s)=\\lbrace x | x^{(j)} &gt; s \\rbrace \\\\\n\\hat{c}_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m(j,s)} y_i, \\quad x \\in R_m, \\quad m=1,2\n\\]</p>\n<p>（3）继续对两个子区域调用步骤(1),(2)，直至满足停止条件；</p>\n<p>（4）将输入空间划分为\\(M\\)个区域\\( R_1, R_2, …, R_M \\)，生成决策树：\n\\[\nf(x) = \\sum_{m=1}^M \\hat{c}_m I(x \\in R_m)\n\\]</p>\n<h4 id=\"4-2-分类树\"><a href=\"#4-2-分类树\" class=\"headerlink\" title=\"4.2 分类树\"></a>4.2 分类树</h4><p>在特征\\( A \\)的条件下，集合\\( D \\)的基尼指数定义为\n\\[\n\\begin{equation}\n\\begin{aligned}\nGini(D, A)\n&amp; = \\frac{ | D_1 | }{ | D | } Gini(D_1) + \\frac{ | D_2 | }{ | D | } Gini(D_2) \\\\\n&amp; = \\frac{ | D_1 | }{ | D | } [1 - \\sum_{k=1}^K(\\frac{ | C_k | }{ | D_1 | })^2 ] + \\frac{ | D_2 | }{ | D | } [1 - \\sum_{k=1}^K(\\frac{ | C_k | }{ | D_2 | })^2 ]\n\\end{aligned}\n\\end{equation}\n\\]\n其中\\( C_k \\)是\\( D_i \\)中属于第\\(k\\)类的样本子集，\\(K\\)是类的个数。</p>\n<p>输入：训练集\\(D\\),停止计算的条件</p>\n<p>输出：CART决策树</p>\n<p>根据训练集，从跟结点开始递归地对每个结点进行一下操作，构建二叉决策树。</p>\n<p>（1）根据每一个特征以及每个特征的取值，计算相应二叉划分时的基尼指数；</p>\n<p>（2）在所有可能的特征及可能的特征值中，选择基尼指数<strong>最小</strong>的特征及相应特征值作为划分切分特征及切分点，并将训练集划分到两个子结点中；</p>\n<p>（3）对两个子节点递归调用(1),(2)，直至满足停止条件；</p>\n<p>（4）生成CART决策树。</p>\n<h4 id=\"4-3-CART的剪枝算法\"><a href=\"#4-3-CART的剪枝算法\" class=\"headerlink\" title=\"4.3 CART的剪枝算法\"></a>4.3 CART的剪枝算法</h4><ul>\n<li><ol>\n<li>剪枝，形成一个子树序列</li>\n</ol>\n</li>\n</ul>\n<p>在剪枝过程中，计算子树的损失函数：\n\\[\nC_{\\alpha}(T) = C(T) + \\alpha | T |\n\\]\n其中，\\(T\\)为任意子树，\\(C(T)\\)表示训练数据的预测误差，\\(| T |\\)为子树的叶结点数目，\\( \\alpha \\geq 0 \\)为参数。</p>\n<p>具体地，从整体树\\(T_0\\)开始剪枝，对\\(T_0\\)的任意内部结点\\(t\\)，以\\(t\\)为单结点树的损失函数是\n\\[\nC_{\\alpha}(t) = C(t) + \\alpha\n\\]\n以\\(t\\)为根节点的子树\\(T_t\\)的损失函数是\n\\[\nC_{\\alpha}(T_t) = C(T_t) + \\alpha | T_t |\n\\]</p>\n<center>\n<img src=\"/posts_res/2018-03-18-decision-tree/cart_cut.png\">\n</center>\n\n<p>当\\( \\alpha=0 \\)及\\(\\alpha\\)充分小时，有不等式\n\\[\nC_{\\alpha}(T_t) &lt; C_{\\alpha}(t)\n\\]\n当\\( \\alpha \\)增大时，在某一\\( \\alpha \\)有\n\\[\nC_{\\alpha}(T_t) = C_{\\alpha} (t)\n\\]\n当\\(\\alpha\\)增大时，不等式反向。只要\\( \\alpha=\\frac{C(t)-C(T_t)}{| T_t | - 1} \\)，\\(T_t\\)与\\(t\\)具有相同的损失，但是\\(t\\)的结点数量更少，因此\\(t\\)更可取，所以剪枝。</p>\n<p>对\\(T_0\\)中每一个内部结点\\(t\\)，计算\n\\[\ng(t) = \\frac{C(t)-C(T_t)}{| T_t | - 1}\n\\]\n它表示间之后整体损失函数减少的程度，在\\(T_0\\)中减去\\(g(t)\\)最小的\\(T_t\\)，将得到的子树作为\\(T_1\\)，同时将最小的\\(g(t)\\)设为\\(\\alpha\\)， \\(T_1\\)为区间[\\(\\alpha_1,\\alpha_2\\)]的最优子树。</p>\n<p><strong>这个地方理解为：最小的g(t)是一个阈值，选择\\(\\alpha=\\mathop{\\min}\\lbrace g(t) \\rbrace \\Longleftrightarrow\\) [其他g(t)的情况是-剪枝比不剪枝的损失大，即式（16）不等号反向的情况]，所以在最小g(t)处剪枝</strong></p>\n<p>如此剪枝下去，直至得到根节点，在这个过程中，不断增加\\(\\alpha\\)的值，产生新的区间。</p>\n<ul>\n<li><ol>\n<li>在剪枝得到的子树序列中通过交叉验证选取最优子树</li>\n</ol>\n</li>\n</ul>\n<p>利用独立的验证数据集，测试子树序列\\( T_0,T_1,…,T_n \\)中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优决策树，在子树序列中，每棵子树\\( T_1,T_2,…,T_n \\)都对应着一个参数\\( \\alpha_1, \\alpha_2, …, \\alpha_n \\)，所以当最优子树\\(T_k\\)确定时，对应的\\(\\alpha_k\\)也确定了，即得到最优决策树\\(T_\\alpha\\)。</p>\n<hr>\n<p>输入：CART算法生成的决策树\\(T_0\\)；</p>\n<p>输出：最优决策树\\(T_\\alpha\\)。</p>\n<p>（1）设\\(k=0, \\quad T=T_0\\)</p>\n<p>（2）设\\(\\alpha=0\\)</p>\n<p>（3）自下而上地对各内部结点\\(t\\)计算\\(C(T_t)\\)，\\(| T_t | \\)以及\n\\[\ng(t)=\\frac{C(t)-C(T_t)}{| T_t | - 1}\n\\]\n\\[\n\\alpha = \\mathop{\\min}(\\alpha, g(t))\n\\]\n这里，\\(T_t\\)表示以\\(t\\)为根节点的子树，\\(C(T_t)\\)是对训练数据的预测误差，\\( | T_t | \\)是\\(T_t\\)的叶结点个数。</p>\n<p>（4）对\\(g(t)=\\alpha\\)的内部结点\\(t\\)进行剪枝，并对叶结点\\(t\\)以多数表决法决定其类，得到树\\(T\\)。</p>\n<p>（5）设\\( k=k+1, \\quad \\alpha_k=\\alpha, \\quad T_k=T \\)。</p>\n<p>（6）如果\\(T_k\\)不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令\\(T_k=T_n\\)。</p>\n<p>（7）采用交叉验证法在子树序列\\(T_0,T_1,…,T_n\\)中选择最优子树\\(T_\\alpha\\)。</p>\n<hr>\n<h3 id=\"5-特点\"><a href=\"#5-特点\" class=\"headerlink\" title=\"5. 特点\"></a>5. 特点</h3><ul>\n<li>优点：可解释性；可处理多种数值类型；没有复杂的参数设置；运算快</li>\n<li>缺点：易过拟合；不适合高维数据；异常值敏感；泛化能力差</li>\n</ul>\n<p><strong>控制过拟合的方式</strong></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（<span class=\"number\">1</span>）树的深度</span><br><span class=\"line\">（<span class=\"number\">2</span>）一个结点被拆分出子结点所需要的包含最少的样本个数</span><br><span class=\"line\">（<span class=\"number\">3</span>）最底层结点所需要好汉的最小样本个数</span><br><span class=\"line\">（<span class=\"number\">4</span>）集成学习的方法(随机森林，Xgboost等)</span><br></pre></td></tr></table></figure>\n<p><strong>连续值如何划分？</strong></p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C<span class=\"number\">45</span>：基于阈值的信息增益<span class=\"comment\">(比)</span></span><br><span class=\"line\">CART：最优切分变量和最优切分点</span><br></pre></td></tr></table></figure>\n<p><strong>缺失值如何处理？</strong></p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">概率权重<span class=\"comment\">(Probability Weights)</span>：C<span class=\"number\">45</span>、ID<span class=\"number\">3</span></span><br><span class=\"line\">替代法<span class=\"comment\">(Alternate/Surrogate Splits)</span>：CART</span><br></pre></td></tr></table></figure>\n<p><strong>不完整数据如何处理？</strong> <a href=\"https://www.zhihu.com/question/34867991?sort=created\" target=\"_blank\" rel=\"noopener\">决策树是如何处理不完整数据的？-知乎</a></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（<span class=\"number\">1</span>）抛弃缺失值</span><br><span class=\"line\">抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。</span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">2</span>）补充缺失值</span><br><span class=\"line\">缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多,这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。</span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">3</span>）概率化缺失值</span><br><span class=\"line\">对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,“F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和”,即F表示所给属性具有已知值样本的概率。</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"6-代码实现\"><a href=\"#6-代码实现\" class=\"headerlink\" title=\"6. 代码实现\"></a>6. 代码实现</h3><p>代码中的公式均指李航老师的《统计学习方法》中的公式。</p>\n<p><a href=\"/posts_res/2018-03-18-decision-tree/decision_tree.py\">Decision_Tree.py</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding:utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecisionTree</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, feature_names, threshold, principle=<span class=\"string\">\"information gain\"</span>)</span>:</span></span><br><span class=\"line\">        self.feature_names = feature_names</span><br><span class=\"line\">        self.threshold = threshold</span><br><span class=\"line\">        self.principle = principle</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.7</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__calculate_entropy__</span><span class=\"params\">(self, y)</span>:</span></span><br><span class=\"line\">        datalen = len(y)</span><br><span class=\"line\">        labelprob = &#123;l: <span class=\"number\">0</span> <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> set(y)&#125;</span><br><span class=\"line\">        entropy = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">            labelprob[l] += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> labelprob.keys():</span><br><span class=\"line\">            thisfrac = labelprob[l] / datalen</span><br><span class=\"line\">            entropy -= thisfrac * np.log2(thisfrac)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> entropy</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.8</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__calculate_conditional_entropy__</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        datalen = len(y)</span><br><span class=\"line\">        featureset = set([x[axis] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X])</span><br><span class=\"line\">        sub_y = &#123;f:list() <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> featureset&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(datalen):</span><br><span class=\"line\">            sub_y[X[i][axis]].append(y[i])</span><br><span class=\"line\">        conditional_entropy = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> sub_y.keys():</span><br><span class=\"line\">            prob = len(sub_y[key]) / datalen</span><br><span class=\"line\">            entropy = self.__calculate_entropy__(sub_y[key])</span><br><span class=\"line\">            conditional_entropy += prob * entropy</span><br><span class=\"line\">        <span class=\"keyword\">return</span> conditional_entropy</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.9</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_information_gain</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        hd = self.__calculate_entropy__(y)</span><br><span class=\"line\">        hda = self.__calculate_conditional_entropy__(X, y, axis)</span><br><span class=\"line\">        gda = hd - hda</span><br><span class=\"line\">        <span class=\"keyword\">return</span> gda</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__most_class__</span><span class=\"params\">(self, y)</span>:</span></span><br><span class=\"line\">        labelset = set(y)</span><br><span class=\"line\">        labelcnt = &#123;l:<span class=\"number\">0</span> <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> labelset&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> y_i <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">            labelcnt[y_i] += <span class=\"number\">1</span></span><br><span class=\"line\">        st = sorted(labelcnt.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> st[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.10</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_information_gain_ratio</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        gda = self.calculate_information_gain(X, y, axis)</span><br><span class=\"line\">        had = self.__calculate_entropy__(X[:, axis])</span><br><span class=\"line\">        grda = gda / had</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grda</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__split_dataset__</span><span class=\"params\">(self, X, y, axis, value)</span>:</span></span><br><span class=\"line\">        rstX = list()</span><br><span class=\"line\">        rsty = list()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> X[i][axis] == value:</span><br><span class=\"line\">                tmpfeature = list(X[i][:axis])</span><br><span class=\"line\">                tmpfeature.extend(list(X[i][axis+<span class=\"number\">1</span>:]))</span><br><span class=\"line\">                rstX.append(tmpfeature)</span><br><span class=\"line\">                rsty.append(y[i])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.asarray(rstX), np.asarray(rsty)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__best_split_feature__</span><span class=\"params\">(self, X, y, feature_names)</span>:</span></span><br><span class=\"line\">        best_feature = <span class=\"number\">-1</span></span><br><span class=\"line\">        max_principle = <span class=\"number\">-1.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> feature_n <span class=\"keyword\">in</span> feature_names:</span><br><span class=\"line\">            axis = feature_names.index(feature_n)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.principle == <span class=\"string\">\"information gain\"</span>:</span><br><span class=\"line\">                this_principle = self.calculate_information_gain(X, y, axis)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                this_principle = self.calculate_information_gain_ratio(X, y, axis)</span><br><span class=\"line\">            print(<span class=\"string\">\"%s\\t%f\\t%s\"</span> % (feature_n, this_principle, self.principle))</span><br><span class=\"line\">            <span class=\"keyword\">if</span> this_principle &gt; max_principle:</span><br><span class=\"line\">                best_feature = axis</span><br><span class=\"line\">                max_principle = this_principle</span><br><span class=\"line\">        print(<span class=\"string\">\"-----\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> best_feature, max_principle</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_fit</span><span class=\"params\">(self, X, y, feature_names)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># 所有实例属于同一类</span></span><br><span class=\"line\">        labelset = set(y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(labelset) == <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> labelset.pop()</span><br><span class=\"line\">        <span class=\"comment\"># 如果特征集为空集，置T为单结点树，实例最多的类作为该结点的类，并返回T</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(feature_names) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.__most_class__(y)</span><br><span class=\"line\">        <span class=\"comment\"># 计算准则,选择特征</span></span><br><span class=\"line\">        best_feature, max_principle = self.__best_split_feature__(X, y, feature_names)</span><br><span class=\"line\">        <span class=\"comment\"># 如果小于阈值，置T为单结点树，实例最多的类作为该结点的类，并返回T</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> max_principle &lt; self.threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.__most_class__(y)</span><br><span class=\"line\"></span><br><span class=\"line\">        best_feature_label = feature_names[best_feature]</span><br><span class=\"line\">        <span class=\"keyword\">del</span> feature_names[best_feature]</span><br><span class=\"line\">        tree = &#123;best_feature_label: &#123;&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        bestfeature_values = set([x[best_feature] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> bestfeature_values:</span><br><span class=\"line\">            sub_X, sub_y = self.__split_dataset__(X, y, best_feature, value)</span><br><span class=\"line\">            tree[best_feature_label][value] = self._fit(sub_X, sub_y, feature_names)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span><span class=\"params\">(self, X, y)</span>:</span></span><br><span class=\"line\">        feature_names = self.feature_names[:]</span><br><span class=\"line\">        self.tree = self._fit(X, y, feature_names)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_predict</span><span class=\"params\">(self, tree, feature_names, x)</span>:</span></span><br><span class=\"line\">        firstStr = list(tree.keys())[<span class=\"number\">0</span>]</span><br><span class=\"line\">        secondDict = tree[firstStr]</span><br><span class=\"line\">        featIndex = feature_names.index(firstStr)</span><br><span class=\"line\">        key = x[featIndex]</span><br><span class=\"line\">        valueOfFeat = secondDict[key]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(valueOfFeat, dict):</span><br><span class=\"line\">            classLabel = self._predict(valueOfFeat, feature_names, x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            classLabel = valueOfFeat</span><br><span class=\"line\">        <span class=\"keyword\">return</span> classLabel</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        preds = list()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X:</span><br><span class=\"line\">            preds.append(self._predict(self.tree, self.feature_names, x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> preds</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">output_tree</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">import</span> treePlot     <span class=\"comment\"># cite: https://gitee.com/orayang_admin/ID3_decisiontree/tree/master</span></span><br><span class=\"line\">        <span class=\"keyword\">import</span> importlib</span><br><span class=\"line\">        importlib.reload(treePlot)</span><br><span class=\"line\">        treePlot.createPlot(self.tree)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    dt = pd.read_csv(<span class=\"string\">\"./credit.csv\"</span>)    <span class=\"comment\"># from lihang - \"statistic learning method\" - page59, table 5.1</span></span><br><span class=\"line\">    <span class=\"comment\"># dt = pd.read_csv(\"./titanic.csv\")</span></span><br><span class=\"line\">    data = dt.values</span><br><span class=\"line\">    feature_names = dt.columns[:<span class=\"number\">-1</span>] <span class=\"comment\"># delete label column</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> data, list(feature_names)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_ID3</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    data, feature_names = load_data()</span><br><span class=\"line\">    print(<span class=\"string\">\"ID3 Descision Tree ... \"</span>)</span><br><span class=\"line\">    ml = DecisionTree(feature_names=feature_names, threshold=<span class=\"number\">0</span>, principle=<span class=\"string\">\"information gain\"</span>)</span><br><span class=\"line\">    ml.fit(data[:, :<span class=\"number\">-1</span>], data[:, <span class=\"number\">-1</span>])</span><br><span class=\"line\">    test = [[<span class=\"string\">\"mid\"</span>, <span class=\"string\">\"yes\"</span>, <span class=\"string\">\"no\"</span>, <span class=\"string\">\"good\"</span>]]</span><br><span class=\"line\">    preds = ml.predict(test)</span><br><span class=\"line\">    print(<span class=\"string\">\"ID3 predict:\"</span>, preds)</span><br><span class=\"line\">    ml.output_tree()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_C45</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    data, feature_names = load_data()</span><br><span class=\"line\">    print(<span class=\"string\">\"C45 Descision Tree ... \"</span>)</span><br><span class=\"line\">    ml = DecisionTree(feature_names=feature_names, threshold=<span class=\"number\">0</span>, principle=<span class=\"string\">\"information gain ratio\"</span>)</span><br><span class=\"line\">    ml.fit(data[:, :<span class=\"number\">-1</span>], data[:, <span class=\"number\">-1</span>])</span><br><span class=\"line\">    test = [[<span class=\"string\">\"mid\"</span>, <span class=\"string\">\"yes\"</span>, <span class=\"string\">\"no\"</span>, <span class=\"string\">\"good\"</span>]]</span><br><span class=\"line\">    preds = ml.predict(test)</span><br><span class=\"line\">    print(<span class=\"string\">\"C45 predict:\"</span>, preds)</span><br><span class=\"line\">    ml.output_tree()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\"># run_ID3()</span></span><br><span class=\"line\">    run_C45()</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<pre><code>ID3 Descision Tree ... \nage    0.083007    information gain\njob    0.323650    information gain\nhouse    0.419973    information gain\ncredit    0.362990    information gain\n-----\nage    0.251629    information gain\njob    0.918296    information gain\ncredit    0.473851    information gain\n-----\nID3 predict: [&#39;yes&#39;]\n</code></pre><hr>\n<pre><code>C45 Descision Tree ... \nage    0.052372    information gain ratio\njob    0.352447    information gain ratio\nhouse    0.432538    information gain ratio\ncredit    0.231854    information gain ratio\n-----\nage    0.164411    information gain ratio\njob    1.000000    information gain ratio\ncredit    0.340374    information gain ratio\n-----\nC45 predict: [&#39;yes&#39;]\n</code></pre><p><img src=\"/posts_res/2018-03-18-decision-tree/dt.png\" alt=\"result\"></p>\n<hr>\n<h3 id=\"7-参考\"><a href=\"#7-参考\" class=\"headerlink\" title=\"7. 参考\"></a>7. 参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p>zergzzlun - <a href=\"https://www.zhihu.com/question/22697086/answer/134841101\" target=\"_blank\" rel=\"noopener\">cart树怎么进行剪枝？</a></p>\n<p>巩固,张虹 - 决策树算法中属性缺失值的研究</p>\n<p>周志华 - 《机器学习》</p>\n<p><a href=\"https://blog.csdn.net/u010665216/article/details/78173064\" target=\"_blank\" rel=\"noopener\">OraYang的博客</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"决策树-Decision-Tree\"><a href=\"#决策树-Decision-Tree\" class=\"headerlink\" title=\"决策树 - Decision Tree\"></a><center>决策树 - Decision Tree</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>ID3</li>\n<li>C45</li>\n<li>决策树的剪枝</li>\n<li>CART</li>\n<li>特点</li>\n<li>代码实现</li>\n</ul><hr><p>决策树学习的3个步骤：</p><ul>\n<li>特征选择</li>\n<li>决策树生成</li>\n<li>决策树修剪</li>\n</ul><p>决策树的损失函数通常为正则化的极大似然函数。</p><hr><h3 id=\"1-ID3\"><a href=\"#1-ID3\" class=\"headerlink\" title=\"1. ID3\"></a>1. ID3</h3><p>ID3算法使用信息增益作为特征选择的标准，信息增益<strong>越大越好</strong>。</p><h4 id=\"1-1-信息增益算法\"><a href=\"#1-1-信息增益算法\" class=\"headerlink\" title=\"1.1 信息增益算法\"></a>1.1 信息增益算法</h4><p>输入：</p><p>训练数据集 \\( D \\) 和特征 \\( A \\);</p><p>输出：</p><p>特征\\(A\\)对训练数据集\\(D\\)的信息增益 \\( g(D,A) \\)</p><p>（1）计算数据集 \\( D \\) 的经验熵 \\( H(D) \\)\n\\[\nH(D) = - \\sum_{k=1}^K \\frac{ | C_k |}{| D |}log_2 \\frac{| C_k |}{| D |}\n\\]</p>","more":"<!-- 如果该blog有其他图片代码文件，需在/posts_res/2018-01-01-template/存放 -->\n\n\n\n\n\n\n\n\n\n\n\n\n<p>（2）计算特征\\(A\\)对数据集\\(D\\)的经验条件熵\\( H(D | A) \\)\n\\[\nH(D | A) = \\sum_{i=1}^n \\frac{| D_i |}{| D |} H(D_i) = - \\sum_{i=1}^n \\frac{ | D_i | }{ | D | } \\sum_{k=1}^K \\frac{| D_{ik} |}{ | D_i |} log_2 \\frac{ | D_{ik} |}{| D_i |}\n\\]</p>\n<p>（3）计算信息增益\n\\[\ng(D,A) = H(D) - H(D | A)\n\\]</p>\n<p>其中\\(K\\)为类别的数量\\( \\lbrace C_1, C_2, …, C_k \\rbrace \\)，\\(n\\)为特征\\(A\\)的取值数量\\( \\lbrace a_1, a_2, …, a_n \\rbrace \\)。</p>\n<hr>\n<h3 id=\"2-C45\"><a href=\"#2-C45\" class=\"headerlink\" title=\"2. C45\"></a>2. C45</h3><p>信息增益作为标准时，存在偏向于取值数量较多的特征的问题，因此C45算法选择信息增益比作为划分的标准，信息增益比<strong>越大越好</strong>。</p>\n<h4 id=\"2-1-信息增益比-基于信息增益\"><a href=\"#2-1-信息增益比-基于信息增益\" class=\"headerlink\" title=\"2.1 信息增益比(基于信息增益)\"></a>2.1 信息增益比(基于信息增益)</h4><p>定义为信息增益\\(g(D,A)\\)与关于特征\\(A\\)的值的熵\\(H_A(D)\\)之比，即：\n\\[\ng_R(D,A) = \\frac{g(D,A)}{H_A(D)}\n\\]\n\\[\nH_A(D) = - \\sum_{i=1}^n \\frac{ | D_i | } { | D | } log_2 \\frac{ | D_i | }{ | D |}\n\\]\n其中\\(n\\)为特征\\(A\\)取值的个数。</p>\n<hr>\n<h3 id=\"3-决策树的剪枝\"><a href=\"#3-决策树的剪枝\" class=\"headerlink\" title=\"3. 决策树的剪枝\"></a>3. 决策树的剪枝</h3><p>设树\\( T \\)的叶结点个数为\\( | T | \\)，\\(t\\)是树\\(T\\)的叶结点，该叶结点有\\(N_t\\)个样本点，其中\\(k\\)类的样本点有\\(N_{tk}\\)个，\\(k=1,2,…,K\\)，\\(H_t(T)\\)为叶结点上的经验熵，则决策树的损失函数可以定义为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nC_{\\alpha}(T)\n&amp; = \\sum_{t=1}^{ | T | } N_t H_t(T) + \\alpha | T | \\\\\n&amp; = - \\sum_{t=1}^{ | T | } \\sum_{k=1}^K N_{tk} log \\frac{N_{tk}}{N_t} + \\alpha | T | \\\\\n&amp; = C(T) + \\alpha | T |\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>通过 \\( \\alpha \\geq 0 \\)控制模型与训练数据拟合度和模型复杂度。</p>\n<h4 id=\"3-1-ID3-amp-C45树的剪枝算法\"><a href=\"#3-1-ID3-amp-C45树的剪枝算法\" class=\"headerlink\" title=\"3.1 ID3&amp;C45树的剪枝算法\"></a>3.1 ID3&amp;C45树的剪枝算法</h4><p>输入：生成算法产生的整个树 \\( T \\)，参数 \\( \\alpha \\)；</p>\n<p>输出：修剪后的子树\\( T_{\\alpha} \\)。</p>\n<p>（1）计算每个结点的经验熵, 经验熵计算公式；\n\\[\nH_t(T) = - \\sum_k \\frac{N_{tk}}{N_t} log \\frac{N_{tk}}{N_t}\n\\]</p>\n<p>（2）递归地从树的叶结点向上回缩；</p>\n<p>设一组叶结点回缩到其父结点之前与之后的整体树分别为\\(T_B\\)与\\(T_A\\)，其对应的损失函数值分别为\\(C_{\\alpha}(T_B)\\)与\\(C_{\\alpha}(T_A)\\)，如果\n\\[\nC_{\\alpha}(T_B) \\leq C_{\\alpha}(T_A)\n\\]\n则进行剪枝，即将父结点变为新的叶结点。</p>\n<p>（3）返回（2），直至不能继续为止，得到损失函数最小的子树\\(T_{\\alpha}\\)。</p>\n<hr>\n<h3 id=\"4-CART\"><a href=\"#4-CART\" class=\"headerlink\" title=\"4. CART\"></a>4. CART</h3><p>CART假设决策树是二叉树，内部结点特征取值为“是”和“否”，递归二分每个特征。</p>\n<ul>\n<li>回归数使用平方误差最小化准则，<strong>越小越好</strong>。</li>\n<li>分类数使用基尼指数最小化准则，<strong>越小越好</strong>。</li>\n</ul>\n<h4 id=\"4-1-回归树\"><a href=\"#4-1-回归树\" class=\"headerlink\" title=\"4.1 回归树\"></a>4.1 回归树</h4><p>输入：训练数据集\\( D=\\lbrace (x_1, y_1), (x_2, y_2), …, (x_N, y_N) \\rbrace \\), 并且\\(Y\\)是连续型变量；</p>\n<p>输出：回归树 \\( f(x) \\)</p>\n<p>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>\n<p>（1）选择最优切分变量\\(j\\)与切分点\\(s\\)，求解\n\\[\n\\mathop{\\min}_{j,s} [\\mathop{\\min}_{c_1} \\sum_{x_i \\in R_1(j,s)} (y_i - c_1)^2 + \\mathop{\\min}_{c_2} \\sum_{x_i \\in R_2(j,s)} (y_i - c_2)^2]\n\\]\n遍历\\(j\\)，对固定的切分变量\\(j\\)扫描切分点\\(s\\)，选择使得上式达到最小值的对\\((j,s)\\)。</p>\n<p>（2）用选定的对\\((j,s)\\)划分区域并决定相应的输出值(均值)：\n\\[\nR_1(j,s) = \\lbrace x | x^{(j)} \\leq s \\rbrace, \\quad R_2(j,s)=\\lbrace x | x^{(j)} &gt; s \\rbrace \\\\\n\\hat{c}_m = \\frac{1}{N_m} \\sum_{x_i \\in R_m(j,s)} y_i, \\quad x \\in R_m, \\quad m=1,2\n\\]</p>\n<p>（3）继续对两个子区域调用步骤(1),(2)，直至满足停止条件；</p>\n<p>（4）将输入空间划分为\\(M\\)个区域\\( R_1, R_2, …, R_M \\)，生成决策树：\n\\[\nf(x) = \\sum_{m=1}^M \\hat{c}_m I(x \\in R_m)\n\\]</p>\n<h4 id=\"4-2-分类树\"><a href=\"#4-2-分类树\" class=\"headerlink\" title=\"4.2 分类树\"></a>4.2 分类树</h4><p>在特征\\( A \\)的条件下，集合\\( D \\)的基尼指数定义为\n\\[\n\\begin{equation}\n\\begin{aligned}\nGini(D, A)\n&amp; = \\frac{ | D_1 | }{ | D | } Gini(D_1) + \\frac{ | D_2 | }{ | D | } Gini(D_2) \\\\\n&amp; = \\frac{ | D_1 | }{ | D | } [1 - \\sum_{k=1}^K(\\frac{ | C_k | }{ | D_1 | })^2 ] + \\frac{ | D_2 | }{ | D | } [1 - \\sum_{k=1}^K(\\frac{ | C_k | }{ | D_2 | })^2 ]\n\\end{aligned}\n\\end{equation}\n\\]\n其中\\( C_k \\)是\\( D_i \\)中属于第\\(k\\)类的样本子集，\\(K\\)是类的个数。</p>\n<p>输入：训练集\\(D\\),停止计算的条件</p>\n<p>输出：CART决策树</p>\n<p>根据训练集，从跟结点开始递归地对每个结点进行一下操作，构建二叉决策树。</p>\n<p>（1）根据每一个特征以及每个特征的取值，计算相应二叉划分时的基尼指数；</p>\n<p>（2）在所有可能的特征及可能的特征值中，选择基尼指数<strong>最小</strong>的特征及相应特征值作为划分切分特征及切分点，并将训练集划分到两个子结点中；</p>\n<p>（3）对两个子节点递归调用(1),(2)，直至满足停止条件；</p>\n<p>（4）生成CART决策树。</p>\n<h4 id=\"4-3-CART的剪枝算法\"><a href=\"#4-3-CART的剪枝算法\" class=\"headerlink\" title=\"4.3 CART的剪枝算法\"></a>4.3 CART的剪枝算法</h4><ul>\n<li><ol>\n<li>剪枝，形成一个子树序列</li>\n</ol>\n</li>\n</ul>\n<p>在剪枝过程中，计算子树的损失函数：\n\\[\nC_{\\alpha}(T) = C(T) + \\alpha | T |\n\\]\n其中，\\(T\\)为任意子树，\\(C(T)\\)表示训练数据的预测误差，\\(| T |\\)为子树的叶结点数目，\\( \\alpha \\geq 0 \\)为参数。</p>\n<p>具体地，从整体树\\(T_0\\)开始剪枝，对\\(T_0\\)的任意内部结点\\(t\\)，以\\(t\\)为单结点树的损失函数是\n\\[\nC_{\\alpha}(t) = C(t) + \\alpha\n\\]\n以\\(t\\)为根节点的子树\\(T_t\\)的损失函数是\n\\[\nC_{\\alpha}(T_t) = C(T_t) + \\alpha | T_t |\n\\]</p>\n<center>\n<img src=\"/posts_res/2018-03-18-decision-tree/cart_cut.png\">\n</center>\n\n<p>当\\( \\alpha=0 \\)及\\(\\alpha\\)充分小时，有不等式\n\\[\nC_{\\alpha}(T_t) &lt; C_{\\alpha}(t)\n\\]\n当\\( \\alpha \\)增大时，在某一\\( \\alpha \\)有\n\\[\nC_{\\alpha}(T_t) = C_{\\alpha} (t)\n\\]\n当\\(\\alpha\\)增大时，不等式反向。只要\\( \\alpha=\\frac{C(t)-C(T_t)}{| T_t | - 1} \\)，\\(T_t\\)与\\(t\\)具有相同的损失，但是\\(t\\)的结点数量更少，因此\\(t\\)更可取，所以剪枝。</p>\n<p>对\\(T_0\\)中每一个内部结点\\(t\\)，计算\n\\[\ng(t) = \\frac{C(t)-C(T_t)}{| T_t | - 1}\n\\]\n它表示间之后整体损失函数减少的程度，在\\(T_0\\)中减去\\(g(t)\\)最小的\\(T_t\\)，将得到的子树作为\\(T_1\\)，同时将最小的\\(g(t)\\)设为\\(\\alpha\\)， \\(T_1\\)为区间[\\(\\alpha_1,\\alpha_2\\)]的最优子树。</p>\n<p><strong>这个地方理解为：最小的g(t)是一个阈值，选择\\(\\alpha=\\mathop{\\min}\\lbrace g(t) \\rbrace \\Longleftrightarrow\\) [其他g(t)的情况是-剪枝比不剪枝的损失大，即式（16）不等号反向的情况]，所以在最小g(t)处剪枝</strong></p>\n<p>如此剪枝下去，直至得到根节点，在这个过程中，不断增加\\(\\alpha\\)的值，产生新的区间。</p>\n<ul>\n<li><ol>\n<li>在剪枝得到的子树序列中通过交叉验证选取最优子树</li>\n</ol>\n</li>\n</ul>\n<p>利用独立的验证数据集，测试子树序列\\( T_0,T_1,…,T_n \\)中各棵子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优决策树，在子树序列中，每棵子树\\( T_1,T_2,…,T_n \\)都对应着一个参数\\( \\alpha_1, \\alpha_2, …, \\alpha_n \\)，所以当最优子树\\(T_k\\)确定时，对应的\\(\\alpha_k\\)也确定了，即得到最优决策树\\(T_\\alpha\\)。</p>\n<hr>\n<p>输入：CART算法生成的决策树\\(T_0\\)；</p>\n<p>输出：最优决策树\\(T_\\alpha\\)。</p>\n<p>（1）设\\(k=0, \\quad T=T_0\\)</p>\n<p>（2）设\\(\\alpha=0\\)</p>\n<p>（3）自下而上地对各内部结点\\(t\\)计算\\(C(T_t)\\)，\\(| T_t | \\)以及\n\\[\ng(t)=\\frac{C(t)-C(T_t)}{| T_t | - 1}\n\\]\n\\[\n\\alpha = \\mathop{\\min}(\\alpha, g(t))\n\\]\n这里，\\(T_t\\)表示以\\(t\\)为根节点的子树，\\(C(T_t)\\)是对训练数据的预测误差，\\( | T_t | \\)是\\(T_t\\)的叶结点个数。</p>\n<p>（4）对\\(g(t)=\\alpha\\)的内部结点\\(t\\)进行剪枝，并对叶结点\\(t\\)以多数表决法决定其类，得到树\\(T\\)。</p>\n<p>（5）设\\( k=k+1, \\quad \\alpha_k=\\alpha, \\quad T_k=T \\)。</p>\n<p>（6）如果\\(T_k\\)不是由根结点及两个叶结点构成的树，则回到步骤（3）；否则令\\(T_k=T_n\\)。</p>\n<p>（7）采用交叉验证法在子树序列\\(T_0,T_1,…,T_n\\)中选择最优子树\\(T_\\alpha\\)。</p>\n<hr>\n<h3 id=\"5-特点\"><a href=\"#5-特点\" class=\"headerlink\" title=\"5. 特点\"></a>5. 特点</h3><ul>\n<li>优点：可解释性；可处理多种数值类型；没有复杂的参数设置；运算快</li>\n<li>缺点：易过拟合；不适合高维数据；异常值敏感；泛化能力差</li>\n</ul>\n<p><strong>控制过拟合的方式</strong></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（<span class=\"number\">1</span>）树的深度</span><br><span class=\"line\">（<span class=\"number\">2</span>）一个结点被拆分出子结点所需要的包含最少的样本个数</span><br><span class=\"line\">（<span class=\"number\">3</span>）最底层结点所需要好汉的最小样本个数</span><br><span class=\"line\">（<span class=\"number\">4</span>）集成学习的方法(随机森林，Xgboost等)</span><br></pre></td></tr></table></figure>\n<p><strong>连续值如何划分？</strong></p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C<span class=\"number\">45</span>：基于阈值的信息增益<span class=\"comment\">(比)</span></span><br><span class=\"line\">CART：最优切分变量和最优切分点</span><br></pre></td></tr></table></figure>\n<p><strong>缺失值如何处理？</strong></p>\n<figure class=\"highlight gcode\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">概率权重<span class=\"comment\">(Probability Weights)</span>：C<span class=\"number\">45</span>、ID<span class=\"number\">3</span></span><br><span class=\"line\">替代法<span class=\"comment\">(Alternate/Surrogate Splits)</span>：CART</span><br></pre></td></tr></table></figure>\n<p><strong>不完整数据如何处理？</strong> <a href=\"https://www.zhihu.com/question/34867991?sort=created\" target=\"_blank\" rel=\"noopener\">决策树是如何处理不完整数据的？-知乎</a></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">（<span class=\"number\">1</span>）抛弃缺失值</span><br><span class=\"line\">抛弃极少量的缺失值的样本对决策树的创建影响不是太大。但是如果属性缺失值较多或是关键属性值缺失,创建的决策树将是不完全的,同时可能给用户造成知识上的大量错误信息,所以抛弃缺失值一般不采用。</span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">2</span>）补充缺失值</span><br><span class=\"line\">缺失值较少时按照我们上面的补充规则是可行的。但如果数据库的数据较大,缺失值较多,这样根据填充后的数据库创建的决策树可能和根据正确值创建的决策树有很大变化。</span><br><span class=\"line\"></span><br><span class=\"line\">（<span class=\"number\">3</span>）概率化缺失值</span><br><span class=\"line\">对缺失值的样本赋予该属性所有属性值的概率分布,即将缺失值按照其所在属性已知值的相对概率分布来创建决策树。用系数F进行合理的修正计算的信息量,“F=数据库中缺失值所在的属性值样本数量去掉缺失值样本数量/数据库中样本数量的总和”,即F表示所给属性具有已知值样本的概率。</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"6-代码实现\"><a href=\"#6-代码实现\" class=\"headerlink\" title=\"6. 代码实现\"></a>6. 代码实现</h3><p>代码中的公式均指李航老师的《统计学习方法》中的公式。</p>\n<p><a href=\"/posts_res/2018-03-18-decision-tree/decision_tree.py\">Decision_Tree.py</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding:utf-8</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DecisionTree</span><span class=\"params\">(object)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, feature_names, threshold, principle=<span class=\"string\">\"information gain\"</span>)</span>:</span></span><br><span class=\"line\">        self.feature_names = feature_names</span><br><span class=\"line\">        self.threshold = threshold</span><br><span class=\"line\">        self.principle = principle</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.7</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__calculate_entropy__</span><span class=\"params\">(self, y)</span>:</span></span><br><span class=\"line\">        datalen = len(y)</span><br><span class=\"line\">        labelprob = &#123;l: <span class=\"number\">0</span> <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> set(y)&#125;</span><br><span class=\"line\">        entropy = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">            labelprob[l] += <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> labelprob.keys():</span><br><span class=\"line\">            thisfrac = labelprob[l] / datalen</span><br><span class=\"line\">            entropy -= thisfrac * np.log2(thisfrac)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> entropy</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.8</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__calculate_conditional_entropy__</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        datalen = len(y)</span><br><span class=\"line\">        featureset = set([x[axis] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X])</span><br><span class=\"line\">        sub_y = &#123;f:list() <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> featureset&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(datalen):</span><br><span class=\"line\">            sub_y[X[i][axis]].append(y[i])</span><br><span class=\"line\">        conditional_entropy = <span class=\"number\">0.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> sub_y.keys():</span><br><span class=\"line\">            prob = len(sub_y[key]) / datalen</span><br><span class=\"line\">            entropy = self.__calculate_entropy__(sub_y[key])</span><br><span class=\"line\">            conditional_entropy += prob * entropy</span><br><span class=\"line\">        <span class=\"keyword\">return</span> conditional_entropy</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.9</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_information_gain</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        hd = self.__calculate_entropy__(y)</span><br><span class=\"line\">        hda = self.__calculate_conditional_entropy__(X, y, axis)</span><br><span class=\"line\">        gda = hd - hda</span><br><span class=\"line\">        <span class=\"keyword\">return</span> gda</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__most_class__</span><span class=\"params\">(self, y)</span>:</span></span><br><span class=\"line\">        labelset = set(y)</span><br><span class=\"line\">        labelcnt = &#123;l:<span class=\"number\">0</span> <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> labelset&#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> y_i <span class=\"keyword\">in</span> y:</span><br><span class=\"line\">            labelcnt[y_i] += <span class=\"number\">1</span></span><br><span class=\"line\">        st = sorted(labelcnt.items(), key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">1</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> st[<span class=\"number\">0</span>][<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"comment\"># formula 5.10</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">calculate_information_gain_ratio</span><span class=\"params\">(self, X, y, axis)</span>:</span></span><br><span class=\"line\">        gda = self.calculate_information_gain(X, y, axis)</span><br><span class=\"line\">        had = self.__calculate_entropy__(X[:, axis])</span><br><span class=\"line\">        grda = gda / had</span><br><span class=\"line\">        <span class=\"keyword\">return</span> grda</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__split_dataset__</span><span class=\"params\">(self, X, y, axis, value)</span>:</span></span><br><span class=\"line\">        rstX = list()</span><br><span class=\"line\">        rsty = list()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(X)):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> X[i][axis] == value:</span><br><span class=\"line\">                tmpfeature = list(X[i][:axis])</span><br><span class=\"line\">                tmpfeature.extend(list(X[i][axis+<span class=\"number\">1</span>:]))</span><br><span class=\"line\">                rstX.append(tmpfeature)</span><br><span class=\"line\">                rsty.append(y[i])</span><br><span class=\"line\">        <span class=\"keyword\">return</span> np.asarray(rstX), np.asarray(rsty)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__best_split_feature__</span><span class=\"params\">(self, X, y, feature_names)</span>:</span></span><br><span class=\"line\">        best_feature = <span class=\"number\">-1</span></span><br><span class=\"line\">        max_principle = <span class=\"number\">-1.0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> feature_n <span class=\"keyword\">in</span> feature_names:</span><br><span class=\"line\">            axis = feature_names.index(feature_n)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.principle == <span class=\"string\">\"information gain\"</span>:</span><br><span class=\"line\">                this_principle = self.calculate_information_gain(X, y, axis)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                this_principle = self.calculate_information_gain_ratio(X, y, axis)</span><br><span class=\"line\">            print(<span class=\"string\">\"%s\\t%f\\t%s\"</span> % (feature_n, this_principle, self.principle))</span><br><span class=\"line\">            <span class=\"keyword\">if</span> this_principle &gt; max_principle:</span><br><span class=\"line\">                best_feature = axis</span><br><span class=\"line\">                max_principle = this_principle</span><br><span class=\"line\">        print(<span class=\"string\">\"-----\"</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> best_feature, max_principle</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_fit</span><span class=\"params\">(self, X, y, feature_names)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># 所有实例属于同一类</span></span><br><span class=\"line\">        labelset = set(y)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(labelset) == <span class=\"number\">1</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> labelset.pop()</span><br><span class=\"line\">        <span class=\"comment\"># 如果特征集为空集，置T为单结点树，实例最多的类作为该结点的类，并返回T</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> len(feature_names) == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.__most_class__(y)</span><br><span class=\"line\">        <span class=\"comment\"># 计算准则,选择特征</span></span><br><span class=\"line\">        best_feature, max_principle = self.__best_split_feature__(X, y, feature_names)</span><br><span class=\"line\">        <span class=\"comment\"># 如果小于阈值，置T为单结点树，实例最多的类作为该结点的类，并返回T</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> max_principle &lt; self.threshold:</span><br><span class=\"line\">            <span class=\"keyword\">return</span> self.__most_class__(y)</span><br><span class=\"line\"></span><br><span class=\"line\">        best_feature_label = feature_names[best_feature]</span><br><span class=\"line\">        <span class=\"keyword\">del</span> feature_names[best_feature]</span><br><span class=\"line\">        tree = &#123;best_feature_label: &#123;&#125;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        bestfeature_values = set([x[best_feature] <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X])</span><br><span class=\"line\">        <span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> bestfeature_values:</span><br><span class=\"line\">            sub_X, sub_y = self.__split_dataset__(X, y, best_feature, value)</span><br><span class=\"line\">            tree[best_feature_label][value] = self._fit(sub_X, sub_y, feature_names)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> tree</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit</span><span class=\"params\">(self, X, y)</span>:</span></span><br><span class=\"line\">        feature_names = self.feature_names[:]</span><br><span class=\"line\">        self.tree = self._fit(X, y, feature_names)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_predict</span><span class=\"params\">(self, tree, feature_names, x)</span>:</span></span><br><span class=\"line\">        firstStr = list(tree.keys())[<span class=\"number\">0</span>]</span><br><span class=\"line\">        secondDict = tree[firstStr]</span><br><span class=\"line\">        featIndex = feature_names.index(firstStr)</span><br><span class=\"line\">        key = x[featIndex]</span><br><span class=\"line\">        valueOfFeat = secondDict[key]</span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(valueOfFeat, dict):</span><br><span class=\"line\">            classLabel = self._predict(valueOfFeat, feature_names, x)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            classLabel = valueOfFeat</span><br><span class=\"line\">        <span class=\"keyword\">return</span> classLabel</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X)</span>:</span></span><br><span class=\"line\">        preds = list()</span><br><span class=\"line\">        <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> X:</span><br><span class=\"line\">            preds.append(self._predict(self.tree, self.feature_names, x))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> preds</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">output_tree</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">import</span> treePlot     <span class=\"comment\"># cite: https://gitee.com/orayang_admin/ID3_decisiontree/tree/master</span></span><br><span class=\"line\">        <span class=\"keyword\">import</span> importlib</span><br><span class=\"line\">        importlib.reload(treePlot)</span><br><span class=\"line\">        treePlot.createPlot(self.tree)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    dt = pd.read_csv(<span class=\"string\">\"./credit.csv\"</span>)    <span class=\"comment\"># from lihang - \"statistic learning method\" - page59, table 5.1</span></span><br><span class=\"line\">    <span class=\"comment\"># dt = pd.read_csv(\"./titanic.csv\")</span></span><br><span class=\"line\">    data = dt.values</span><br><span class=\"line\">    feature_names = dt.columns[:<span class=\"number\">-1</span>] <span class=\"comment\"># delete label column</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> data, list(feature_names)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_ID3</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    data, feature_names = load_data()</span><br><span class=\"line\">    print(<span class=\"string\">\"ID3 Descision Tree ... \"</span>)</span><br><span class=\"line\">    ml = DecisionTree(feature_names=feature_names, threshold=<span class=\"number\">0</span>, principle=<span class=\"string\">\"information gain\"</span>)</span><br><span class=\"line\">    ml.fit(data[:, :<span class=\"number\">-1</span>], data[:, <span class=\"number\">-1</span>])</span><br><span class=\"line\">    test = [[<span class=\"string\">\"mid\"</span>, <span class=\"string\">\"yes\"</span>, <span class=\"string\">\"no\"</span>, <span class=\"string\">\"good\"</span>]]</span><br><span class=\"line\">    preds = ml.predict(test)</span><br><span class=\"line\">    print(<span class=\"string\">\"ID3 predict:\"</span>, preds)</span><br><span class=\"line\">    ml.output_tree()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">run_C45</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    data, feature_names = load_data()</span><br><span class=\"line\">    print(<span class=\"string\">\"C45 Descision Tree ... \"</span>)</span><br><span class=\"line\">    ml = DecisionTree(feature_names=feature_names, threshold=<span class=\"number\">0</span>, principle=<span class=\"string\">\"information gain ratio\"</span>)</span><br><span class=\"line\">    ml.fit(data[:, :<span class=\"number\">-1</span>], data[:, <span class=\"number\">-1</span>])</span><br><span class=\"line\">    test = [[<span class=\"string\">\"mid\"</span>, <span class=\"string\">\"yes\"</span>, <span class=\"string\">\"no\"</span>, <span class=\"string\">\"good\"</span>]]</span><br><span class=\"line\">    preds = ml.predict(test)</span><br><span class=\"line\">    print(<span class=\"string\">\"C45 predict:\"</span>, preds)</span><br><span class=\"line\">    ml.output_tree()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    <span class=\"comment\"># run_ID3()</span></span><br><span class=\"line\">    run_C45()</span><br></pre></td></tr></table></figure>\n<p>结果：</p>\n<pre><code>ID3 Descision Tree ... \nage    0.083007    information gain\njob    0.323650    information gain\nhouse    0.419973    information gain\ncredit    0.362990    information gain\n-----\nage    0.251629    information gain\njob    0.918296    information gain\ncredit    0.473851    information gain\n-----\nID3 predict: [&#39;yes&#39;]\n</code></pre><hr>\n<pre><code>C45 Descision Tree ... \nage    0.052372    information gain ratio\njob    0.352447    information gain ratio\nhouse    0.432538    information gain ratio\ncredit    0.231854    information gain ratio\n-----\nage    0.164411    information gain ratio\njob    1.000000    information gain ratio\ncredit    0.340374    information gain ratio\n-----\nC45 predict: [&#39;yes&#39;]\n</code></pre><p><img src=\"/posts_res/2018-03-18-decision-tree/dt.png\" alt=\"result\"></p>\n<hr>\n<h3 id=\"7-参考\"><a href=\"#7-参考\" class=\"headerlink\" title=\"7. 参考\"></a>7. 参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p>zergzzlun - <a href=\"https://www.zhihu.com/question/22697086/answer/134841101\" target=\"_blank\" rel=\"noopener\">cart树怎么进行剪枝？</a></p>\n<p>巩固,张虹 - 决策树算法中属性缺失值的研究</p>\n<p>周志华 - 《机器学习》</p>\n<p><a href=\"https://blog.csdn.net/u010665216/article/details/78173064\" target=\"_blank\" rel=\"noopener\">OraYang的博客</a></p>\n</blockquote>\n"},{"layout":"post","title":"机器学习中的正则化","date":"2018-03-24T15:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 机器学习中的正则化 - Regularization </center>\n\n目录\n* 先验知识\n* L0正则[范数]\n* L1正则[范数]\n* L2正则[范数]\n* L1和L2的区别\n\n--------\n\n### 1. 先验知识\n\n监督机器学习问题是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。\n所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化能力，而模型“简单”就是通过规则函数来实现的。\n另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。\n\n还有几种角度来看待规则化的。\n规则化符合奥卡姆剃刀(Occam's razor)原理：在所有可能选择的模型中，选择能够很好地解释已知数据并且简单的模型。\n从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。\n还有个说法，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。\n\n一般来说，监督学习可以看作最小化下面的目标函数：\n\\\\[\nw^{\\ast} = \\mathop{\\arg\\min}\\_w \\sum\\_i L(y\\_i, f(x\\_i; w)) + \\lambda \\Omega(w)\n\\\\]\n\n其中，第一项\\\\(L(yi,f(xi;w))\\\\)衡量我们的模型对第\\\\(i\\\\)个样本的预测值\\\\(f(x\\_i;w)\\\\)和真实的标签\\\\(y\\_i\\\\)之前的误差。\n因为模型是要拟合训练样本，所以要求这一项最小，也就是要求模型尽量的拟合训练数据。\n但正如上面说言，不仅要保证训练误差最小，更希望模型测试误差小，所以需要加上第二项，也就是对参数\\\\(w\\\\)的规则化函数\\\\(\\Omega(w)\\\\)去约束模型尽量的简单。\n\n对于第一项Loss函数，不同的loss函数，具有不同的拟合特性，具体问题具体分析:\n* 如果是Square loss，是最小二乘\n* 如果是Hinge Loss，是SVM\n* 如果是exp-Loss，是Boosting\n* 如果是log-Loss，是Logistic Regression\n* 等等\n\n规则化函数\\\\(\\Omega(w)\\\\)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。\n* 零范数\n* 一范数\n* 二范数\n* 迹范数\n* Frobenius范数(对应元素的平方和再开方) \\\\( \\sqrt{\\sum\\_{i=1}^m \\sum\\_{j=1}^n \\| a\\_{ij} \\| ^2} \\\\)\n* 核范数 \\\\( tr(\\sqrt{X^TX}) \\\\)\n* 等等\n\n------------\n\n### 2. L0正则[范数]\n\n定义： L0范数是指向量中非0的元素的个数。\n\n如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说就是让参数W是稀疏的。\n\n所以L0范数非常适合机器学习中稀疏编码，特征选择。但是由于L0范数很难优化求解（NP难问题），而**L1范数又是L0范数的最优凸近似[??]**，故通常用L1范数来代替。\n\n![l0l1](/posts_res/2018-03-24-regularization/2-1.jpg)\n\n其他可以参考Emmanuel Candes的工作。\n\n\n------------\n\n### 3. L1正则[范数]\n\n定义：L1范数是指向量中各个元素绝对值之和。\n\\\\[\n\\|\\| w \\|\\|\\_1 = \\sum\\_i \\| w\\_i \\|\n\\\\]\n\n**L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。**\n\n\n\n#### 3.1 参数稀疏的优势\n\n（1）特征选择\n\n大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。\n一般来说，\\\\(x\\_i\\\\)的大部分元素（也就是特征）都是和最终的输出\\\\(y\\_i\\\\)没有关系或者不提供任何信息的，在最小化目标函数的时候考虑\\\\(x\\_i\\\\)这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确\\\\(y\\_i\\\\)的预测。\n稀疏规则化算子的引入就是为了完成特征自动选择的任务，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。\n\n（2）可解释性\n\n另一个理由是，模型更容易解释。\n例如患某种病的概率是\\\\(y\\\\)，假设收集到的数据\\\\(x\\\\)是1000维的，也就是需要寻找这1000种因素到底是怎么影响患上这种病的概率的。\n假设我们这个是个回归模型：\\\\( y = w\\_1 x\\_1 + w\\_2 x\\_2 + ... + w\\_1000 x\\_1000 + b\\\\)（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。\n通过学习，如果最后学习到的\\\\(w^{\\ast}\\\\)就只有很少的非零元素，例如只有5个非零的\\\\(w\\_i\\\\)，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。\n也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个\\\\(w\\_i\\\\)都非0，医生面对这1000种因素就无从下手。\n\n\n------------\n\n### 4. L2正则[范数]\n\n定义：欧式距离\n\\\\[\n\\|\\| w \\|\\|\\_2 = \\sqrt{\\sum\\_i w\\_i^2}\n\\\\]\n\nL2正则对于改善过拟合具有强大的功效。\n让L2范数的规则项最小，可以使得\\\\(w\\\\)的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。\n\n```text\n为什么参数越小模型越简单？\n因为越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，\n而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大，反之，简单的模型，参数值较小。\n```\n\n**通过L2范数，可以实现对模型空间的限制，从而在一定程度上避免了过拟合**。\n\n说到可以避免过拟合，就要涉及到条件数（详见\n[条件数-维基百科](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%95%B0), \n[矩阵条件数-百度百科](https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E6%9D%A1%E4%BB%B6%E6%95%B0)）\n\n从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n优化有两大难题，一是：局部最小值，二是：病态(ill-condition)问题。前者找的是全局最小值，如果局部最小值太多，那优化算法就很容易陷入局部最小而不能自拔，这不是我们愿意看到的。\n病态对应着良置(well-condition)，假设我们有个方程组\\\\(AX=b\\\\)，我们需要求解\\\\(X\\\\)。如果\\\\(A\\\\)或者\\\\(b\\\\)稍微的改变，会使得\\\\(X\\\\)的解发生很大的改变，那么这个方程组系统就是病态的，\n反之就是良置的。\n\n下面看个例子：\n![example](/posts_res/2018-03-24-regularization/4-1.png)\n\n先看左边,第一行假设是\\\\(AX=b\\\\)，第二行稍微改变下\\\\(b\\\\)，得到的\\\\(x\\\\)和没改变前的差别很大，第三行稍微改变下系数矩阵\\\\(A\\\\)，可以看到结果的变化也很大。\n换句话来说，这个系统的解对系数矩阵\\\\(A\\\\)或者\\\\(b\\\\)太敏感了。又因为一般系数矩阵\\\\(A\\\\)和\\\\(b\\\\)是从实验数据里面估计得到的，所以是存在误差的，如果系统对这个误差是可以容忍的就还好，\n但系统对这个误差太敏感了，以至于解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是病态(ill-conditioned)的，不正常的，不稳定的，有问题的。右边那个就叫良置(well-condition)的系统。\n上面的条件数就是拿来衡量病态系统的可信度的。条件数衡量的是输入发生微小变化的时候，输出会发生多大的变化，也就是系统对微小变化的敏感度。条件数值小的就是良置的，大的就是病态的。\n\n对条件数来个总结：条件数是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的条件数在1附近，那么它就是良置的，\n如果远大于1，那么它就是病态的，如果一个系统是病态的，它的输出结果可信度就很低。\n\n回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：\n\\\\[\nw^{\\ast} = (X^TX)^{-1}X^Ty\n\\\\]\n然而，如果当样本\\\\(X\\\\)的数目比每个样本的维度还要小的时候，矩阵\\\\(X^TX\\\\)将会不是满秩的，也就是\\\\(X^TX\\\\)会变得不可逆，所以\\\\(w^{\\ast}\\\\)就没办法直接计算出来了。\n或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。\n\n但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：\n\\\\[\nw^{\\ast} = (X^TX+\\lambda I)^{-1} X^T y\n\\\\]\n要得到这个解，通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。\n考虑没有规则项的时候，也就是\\\\(\\lambda=0\\\\)的情况，如果矩阵\\\\(X^TX\\\\)的条件数很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善条件数。\n\n另外，如果使用迭代优化的算法，条件数太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成\\\\( \\lambda-strongly convex, (\\lambda\\\\)强凸)的了。\n\n关于强凸和普通凸的对比，看下图[其他详细数学内容自行查阅资料]:\n\n![strongconvex](/posts_res/2018-03-24-regularization/4-2.png)\n\n设最优解为\\\\(w^{\\ast}\\\\)指向的地方。如果我们的函数\\\\(f(w)\\\\)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算\\\\(w\\_t\\\\)和\\\\(w^{\\ast}\\\\)离的比较近的时候，\\\\(f(wt)\\\\)和\\\\(f(w^{\\ast})\\\\)的值差别还是挺大的，也就是会保证在最优解\\\\(w^{\\ast}\\\\)附近的时候，还存在较大的梯度值，这样才可以在比较少的迭代次数内达到\\\\(w^{\\ast}\\\\)。\n但对于右图，红色的函数\\\\(f(w)\\\\)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在\\\\(w\\_t\\\\)还离我们的最优点\\\\(w^{\\ast}\\\\)很远的时候，我们的近似梯度\\\\((f(w\\_t)-f(w^{\\ast}))/(w\\_t-w^{\\ast})\\\\)就已经非常小了，在\\\\(w\\_t\\\\)处的近似梯度\\\\(\\partial f / \\partial w \\\\)就更小了，这样通过梯度下降，我们得到的结果就是\\\\(w\\\\)的变化非常缓慢。\n如果要获得强凸性质怎么做？最简单的就是往里面加入一项\\\\( (\\alpha/2)* \\|\\| w \\|\\|^2 \\\\)。\n\n一言以蔽之：L2范数不但可以防止过拟合，还可以让优化求解变得稳定和快速。\n\n\n------------\n\n### 5. L1和L2的区别\n\n#### 5.1 下降速度\n\nL1和L2都是规则化的方式，将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。\n\n![l1l2](/posts_res/2018-03-24-regularization/5-1.png)\n\nL1又称Lasso，L2又称Ridge。\n\n\n#### 5.2 模型空间的限制\n\n 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：\n\\\\[\nL1(Lasso): \\mathop{\\min}\\_w \\frac{1}{n} \\|\\| y-w^T X \\|\\|^2, \\quad s.t. \\quad \\|\\| w \\|\\|\\_1 \\leq C \\\\\\\nL2(Ridge): \\mathop{\\min}\\_w \\frac{1}{n} \\|\\| y-w^T X \\|\\|^2, \\quad s.t. \\quad \\|\\| w \\|\\|\\_2 \\leq C \\\\\\\n\\\\]\n\n也就是说，我们将模型空间限制在\\\\(w\\\\)的一个L1-ball中。\n为了便于可视化，我们考虑两维的情况，在\\\\( (w\\_1, w\\_2) \\\\)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个norm ball。等高线与norm ball首次相交的地方就是最优解：\n\n![normball](/posts_res/2018-03-24-regularization/5-2.png)\n\n可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的等高线除非位置摆得非常好，大部分时候都会在角的地方相交。\n注意到在角的位置就会产生稀疏性，例如图中的相交点就有\\\\(w\\_1=0\\\\)，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。\n相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则能产生稀疏性，而L2正则不行的原因了。\n\n因此总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。\n\n\n-----------\n\n### 参考\n\n> [深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425)\n\n> [机器学习中的范数规则化](https://blog.csdn.net/zouxy09/article/details/24971995)\n\n> [机器学习中为什么越小的参数说明模型越简单?](https://www.zhihu.com/question/52993836)\n","source":"_posts/2018-03-24-regularization.md","raw":"---\nlayout: post\ntitle: 机器学习中的正则化\ndate: 2018-03-24 23:10 +0800\ncategories: 基础知识\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n## <center> 机器学习中的正则化 - Regularization </center>\n\n目录\n* 先验知识\n* L0正则[范数]\n* L1正则[范数]\n* L2正则[范数]\n* L1和L2的区别\n\n--------\n\n### 1. 先验知识\n\n监督机器学习问题是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。\n所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化能力，而模型“简单”就是通过规则函数来实现的。\n另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。\n\n还有几种角度来看待规则化的。\n规则化符合奥卡姆剃刀(Occam's razor)原理：在所有可能选择的模型中，选择能够很好地解释已知数据并且简单的模型。\n从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。\n还有个说法，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。\n\n一般来说，监督学习可以看作最小化下面的目标函数：\n\\\\[\nw^{\\ast} = \\mathop{\\arg\\min}\\_w \\sum\\_i L(y\\_i, f(x\\_i; w)) + \\lambda \\Omega(w)\n\\\\]\n\n其中，第一项\\\\(L(yi,f(xi;w))\\\\)衡量我们的模型对第\\\\(i\\\\)个样本的预测值\\\\(f(x\\_i;w)\\\\)和真实的标签\\\\(y\\_i\\\\)之前的误差。\n因为模型是要拟合训练样本，所以要求这一项最小，也就是要求模型尽量的拟合训练数据。\n但正如上面说言，不仅要保证训练误差最小，更希望模型测试误差小，所以需要加上第二项，也就是对参数\\\\(w\\\\)的规则化函数\\\\(\\Omega(w)\\\\)去约束模型尽量的简单。\n\n对于第一项Loss函数，不同的loss函数，具有不同的拟合特性，具体问题具体分析:\n* 如果是Square loss，是最小二乘\n* 如果是Hinge Loss，是SVM\n* 如果是exp-Loss，是Boosting\n* 如果是log-Loss，是Logistic Regression\n* 等等\n\n规则化函数\\\\(\\Omega(w)\\\\)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。\n* 零范数\n* 一范数\n* 二范数\n* 迹范数\n* Frobenius范数(对应元素的平方和再开方) \\\\( \\sqrt{\\sum\\_{i=1}^m \\sum\\_{j=1}^n \\| a\\_{ij} \\| ^2} \\\\)\n* 核范数 \\\\( tr(\\sqrt{X^TX}) \\\\)\n* 等等\n\n------------\n\n### 2. L0正则[范数]\n\n定义： L0范数是指向量中非0的元素的个数。\n\n如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说就是让参数W是稀疏的。\n\n所以L0范数非常适合机器学习中稀疏编码，特征选择。但是由于L0范数很难优化求解（NP难问题），而**L1范数又是L0范数的最优凸近似[??]**，故通常用L1范数来代替。\n\n![l0l1](/posts_res/2018-03-24-regularization/2-1.jpg)\n\n其他可以参考Emmanuel Candes的工作。\n\n\n------------\n\n### 3. L1正则[范数]\n\n定义：L1范数是指向量中各个元素绝对值之和。\n\\\\[\n\\|\\| w \\|\\|\\_1 = \\sum\\_i \\| w\\_i \\|\n\\\\]\n\n**L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。**\n\n\n\n#### 3.1 参数稀疏的优势\n\n（1）特征选择\n\n大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。\n一般来说，\\\\(x\\_i\\\\)的大部分元素（也就是特征）都是和最终的输出\\\\(y\\_i\\\\)没有关系或者不提供任何信息的，在最小化目标函数的时候考虑\\\\(x\\_i\\\\)这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确\\\\(y\\_i\\\\)的预测。\n稀疏规则化算子的引入就是为了完成特征自动选择的任务，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。\n\n（2）可解释性\n\n另一个理由是，模型更容易解释。\n例如患某种病的概率是\\\\(y\\\\)，假设收集到的数据\\\\(x\\\\)是1000维的，也就是需要寻找这1000种因素到底是怎么影响患上这种病的概率的。\n假设我们这个是个回归模型：\\\\( y = w\\_1 x\\_1 + w\\_2 x\\_2 + ... + w\\_1000 x\\_1000 + b\\\\)（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。\n通过学习，如果最后学习到的\\\\(w^{\\ast}\\\\)就只有很少的非零元素，例如只有5个非零的\\\\(w\\_i\\\\)，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。\n也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个\\\\(w\\_i\\\\)都非0，医生面对这1000种因素就无从下手。\n\n\n------------\n\n### 4. L2正则[范数]\n\n定义：欧式距离\n\\\\[\n\\|\\| w \\|\\|\\_2 = \\sqrt{\\sum\\_i w\\_i^2}\n\\\\]\n\nL2正则对于改善过拟合具有强大的功效。\n让L2范数的规则项最小，可以使得\\\\(w\\\\)的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。\n\n```text\n为什么参数越小模型越简单？\n因为越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，\n而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大，反之，简单的模型，参数值较小。\n```\n\n**通过L2范数，可以实现对模型空间的限制，从而在一定程度上避免了过拟合**。\n\n说到可以避免过拟合，就要涉及到条件数（详见\n[条件数-维基百科](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%95%B0), \n[矩阵条件数-百度百科](https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E6%9D%A1%E4%BB%B6%E6%95%B0)）\n\n从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n优化有两大难题，一是：局部最小值，二是：病态(ill-condition)问题。前者找的是全局最小值，如果局部最小值太多，那优化算法就很容易陷入局部最小而不能自拔，这不是我们愿意看到的。\n病态对应着良置(well-condition)，假设我们有个方程组\\\\(AX=b\\\\)，我们需要求解\\\\(X\\\\)。如果\\\\(A\\\\)或者\\\\(b\\\\)稍微的改变，会使得\\\\(X\\\\)的解发生很大的改变，那么这个方程组系统就是病态的，\n反之就是良置的。\n\n下面看个例子：\n![example](/posts_res/2018-03-24-regularization/4-1.png)\n\n先看左边,第一行假设是\\\\(AX=b\\\\)，第二行稍微改变下\\\\(b\\\\)，得到的\\\\(x\\\\)和没改变前的差别很大，第三行稍微改变下系数矩阵\\\\(A\\\\)，可以看到结果的变化也很大。\n换句话来说，这个系统的解对系数矩阵\\\\(A\\\\)或者\\\\(b\\\\)太敏感了。又因为一般系数矩阵\\\\(A\\\\)和\\\\(b\\\\)是从实验数据里面估计得到的，所以是存在误差的，如果系统对这个误差是可以容忍的就还好，\n但系统对这个误差太敏感了，以至于解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是病态(ill-conditioned)的，不正常的，不稳定的，有问题的。右边那个就叫良置(well-condition)的系统。\n上面的条件数就是拿来衡量病态系统的可信度的。条件数衡量的是输入发生微小变化的时候，输出会发生多大的变化，也就是系统对微小变化的敏感度。条件数值小的就是良置的，大的就是病态的。\n\n对条件数来个总结：条件数是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的条件数在1附近，那么它就是良置的，\n如果远大于1，那么它就是病态的，如果一个系统是病态的，它的输出结果可信度就很低。\n\n回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：\n\\\\[\nw^{\\ast} = (X^TX)^{-1}X^Ty\n\\\\]\n然而，如果当样本\\\\(X\\\\)的数目比每个样本的维度还要小的时候，矩阵\\\\(X^TX\\\\)将会不是满秩的，也就是\\\\(X^TX\\\\)会变得不可逆，所以\\\\(w^{\\ast}\\\\)就没办法直接计算出来了。\n或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。\n\n但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：\n\\\\[\nw^{\\ast} = (X^TX+\\lambda I)^{-1} X^T y\n\\\\]\n要得到这个解，通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。\n考虑没有规则项的时候，也就是\\\\(\\lambda=0\\\\)的情况，如果矩阵\\\\(X^TX\\\\)的条件数很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善条件数。\n\n另外，如果使用迭代优化的算法，条件数太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成\\\\( \\lambda-strongly convex, (\\lambda\\\\)强凸)的了。\n\n关于强凸和普通凸的对比，看下图[其他详细数学内容自行查阅资料]:\n\n![strongconvex](/posts_res/2018-03-24-regularization/4-2.png)\n\n设最优解为\\\\(w^{\\ast}\\\\)指向的地方。如果我们的函数\\\\(f(w)\\\\)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算\\\\(w\\_t\\\\)和\\\\(w^{\\ast}\\\\)离的比较近的时候，\\\\(f(wt)\\\\)和\\\\(f(w^{\\ast})\\\\)的值差别还是挺大的，也就是会保证在最优解\\\\(w^{\\ast}\\\\)附近的时候，还存在较大的梯度值，这样才可以在比较少的迭代次数内达到\\\\(w^{\\ast}\\\\)。\n但对于右图，红色的函数\\\\(f(w)\\\\)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在\\\\(w\\_t\\\\)还离我们的最优点\\\\(w^{\\ast}\\\\)很远的时候，我们的近似梯度\\\\((f(w\\_t)-f(w^{\\ast}))/(w\\_t-w^{\\ast})\\\\)就已经非常小了，在\\\\(w\\_t\\\\)处的近似梯度\\\\(\\partial f / \\partial w \\\\)就更小了，这样通过梯度下降，我们得到的结果就是\\\\(w\\\\)的变化非常缓慢。\n如果要获得强凸性质怎么做？最简单的就是往里面加入一项\\\\( (\\alpha/2)* \\|\\| w \\|\\|^2 \\\\)。\n\n一言以蔽之：L2范数不但可以防止过拟合，还可以让优化求解变得稳定和快速。\n\n\n------------\n\n### 5. L1和L2的区别\n\n#### 5.1 下降速度\n\nL1和L2都是规则化的方式，将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。\n\n![l1l2](/posts_res/2018-03-24-regularization/5-1.png)\n\nL1又称Lasso，L2又称Ridge。\n\n\n#### 5.2 模型空间的限制\n\n 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：\n\\\\[\nL1(Lasso): \\mathop{\\min}\\_w \\frac{1}{n} \\|\\| y-w^T X \\|\\|^2, \\quad s.t. \\quad \\|\\| w \\|\\|\\_1 \\leq C \\\\\\\nL2(Ridge): \\mathop{\\min}\\_w \\frac{1}{n} \\|\\| y-w^T X \\|\\|^2, \\quad s.t. \\quad \\|\\| w \\|\\|\\_2 \\leq C \\\\\\\n\\\\]\n\n也就是说，我们将模型空间限制在\\\\(w\\\\)的一个L1-ball中。\n为了便于可视化，我们考虑两维的情况，在\\\\( (w\\_1, w\\_2) \\\\)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个norm ball。等高线与norm ball首次相交的地方就是最优解：\n\n![normball](/posts_res/2018-03-24-regularization/5-2.png)\n\n可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的等高线除非位置摆得非常好，大部分时候都会在角的地方相交。\n注意到在角的位置就会产生稀疏性，例如图中的相交点就有\\\\(w\\_1=0\\\\)，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。\n相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则能产生稀疏性，而L2正则不行的原因了。\n\n因此总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。\n\n\n-----------\n\n### 参考\n\n> [深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425)\n\n> [机器学习中的范数规则化](https://blog.csdn.net/zouxy09/article/details/24971995)\n\n> [机器学习中为什么越小的参数说明模型越简单?](https://www.zhihu.com/question/52993836)\n","slug":"regularization","published":1,"updated":"2019-08-17T09:33:07.314Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnv6000v2qwplp6kuh1b","content":"<h2 id=\"机器学习中的正则化-Regularization\"><a href=\"#机器学习中的正则化-Regularization\" class=\"headerlink\" title=\" 机器学习中的正则化 - Regularization \"></a><center> 机器学习中的正则化 - Regularization </center></h2><p>目录</p><ul>\n<li>先验知识</li>\n<li>L0正则[范数]</li>\n<li>L1正则[范数]</li>\n<li>L2正则[范数]</li>\n<li>L1和L2的区别</li>\n</ul><hr><h3 id=\"1-先验知识\"><a href=\"#1-先验知识\" class=\"headerlink\" title=\"1. 先验知识\"></a>1. 先验知识</h3><p>监督机器学习问题是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。\n所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化能力，而模型“简单”就是通过规则函数来实现的。\n另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。</p><a id=\"more\"></a>\n\n\n\n<p>还有几种角度来看待规则化的。\n规则化符合奥卡姆剃刀(Occam’s razor)原理：在所有可能选择的模型中，选择能够很好地解释已知数据并且简单的模型。\n从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。\n还有个说法，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。</p>\n<p>一般来说，监督学习可以看作最小化下面的目标函数：\n\\[\nw^{\\ast} = \\mathop{\\arg\\min}_w \\sum_i L(y_i, f(x_i; w)) + \\lambda \\Omega(w)\n\\]</p>\n<p>其中，第一项\\(L(yi,f(xi;w))\\)衡量我们的模型对第\\(i\\)个样本的预测值\\(f(x_i;w)\\)和真实的标签\\(y_i\\)之前的误差。\n因为模型是要拟合训练样本，所以要求这一项最小，也就是要求模型尽量的拟合训练数据。\n但正如上面说言，不仅要保证训练误差最小，更希望模型测试误差小，所以需要加上第二项，也就是对参数\\(w\\)的规则化函数\\(\\Omega(w)\\)去约束模型尽量的简单。</p>\n<p>对于第一项Loss函数，不同的loss函数，具有不同的拟合特性，具体问题具体分析:</p>\n<ul>\n<li>如果是Square loss，是最小二乘</li>\n<li>如果是Hinge Loss，是SVM</li>\n<li>如果是exp-Loss，是Boosting</li>\n<li>如果是log-Loss，是Logistic Regression</li>\n<li>等等</li>\n</ul>\n<p>规则化函数\\(\\Omega(w)\\)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。</p>\n<ul>\n<li>零范数</li>\n<li>一范数</li>\n<li>二范数</li>\n<li>迹范数</li>\n<li>Frobenius范数(对应元素的平方和再开方) \\( \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n | a_{ij} | ^2} \\)</li>\n<li>核范数 \\( tr(\\sqrt{X^TX}) \\)</li>\n<li>等等</li>\n</ul>\n<hr>\n<h3 id=\"2-L0正则-范数\"><a href=\"#2-L0正则-范数\" class=\"headerlink\" title=\"2. L0正则[范数]\"></a>2. L0正则[范数]</h3><p>定义： L0范数是指向量中非0的元素的个数。</p>\n<p>如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说就是让参数W是稀疏的。</p>\n<p>所以L0范数非常适合机器学习中稀疏编码，特征选择。但是由于L0范数很难优化求解（NP难问题），而<strong>L1范数又是L0范数的最优凸近似[??]</strong>，故通常用L1范数来代替。</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/2-1.jpg\" alt=\"l0l1\"></p>\n<p>其他可以参考Emmanuel Candes的工作。</p>\n<hr>\n<h3 id=\"3-L1正则-范数\"><a href=\"#3-L1正则-范数\" class=\"headerlink\" title=\"3. L1正则[范数]\"></a>3. L1正则[范数]</h3><p>定义：L1范数是指向量中各个元素绝对值之和。\n\\[\n|| w ||_1 = \\sum_i | w_i |\n\\]</p>\n<p><strong>L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</strong></p>\n<h4 id=\"3-1-参数稀疏的优势\"><a href=\"#3-1-参数稀疏的优势\" class=\"headerlink\" title=\"3.1 参数稀疏的优势\"></a>3.1 参数稀疏的优势</h4><p>（1）特征选择</p>\n<p>大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。\n一般来说，\\(x_i\\)的大部分元素（也就是特征）都是和最终的输出\\(y_i\\)没有关系或者不提供任何信息的，在最小化目标函数的时候考虑\\(x_i\\)这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确\\(y_i\\)的预测。\n稀疏规则化算子的引入就是为了完成特征自动选择的任务，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p>\n<p>（2）可解释性</p>\n<p>另一个理由是，模型更容易解释。\n例如患某种病的概率是\\(y\\)，假设收集到的数据\\(x\\)是1000维的，也就是需要寻找这1000种因素到底是怎么影响患上这种病的概率的。\n假设我们这个是个回归模型：\\( y = w_1 x_1 + w_2 x_2 + … + w_1000 x_1000 + b\\)（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。\n通过学习，如果最后学习到的\\(w^{\\ast}\\)就只有很少的非零元素，例如只有5个非零的\\(w_i\\)，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。\n也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个\\(w_i\\)都非0，医生面对这1000种因素就无从下手。</p>\n<hr>\n<h3 id=\"4-L2正则-范数\"><a href=\"#4-L2正则-范数\" class=\"headerlink\" title=\"4. L2正则[范数]\"></a>4. L2正则[范数]</h3><p>定义：欧式距离\n\\[\n|| w ||_2 = \\sqrt{\\sum_i w_i^2}\n\\]</p>\n<p>L2正则对于改善过拟合具有强大的功效。\n让L2范数的规则项最小，可以使得\\(w\\)的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">为什么参数越小模型越简单？</span><br><span class=\"line\">因为越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，</span><br><span class=\"line\">而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大，反之，简单的模型，参数值较小。</span><br></pre></td></tr></table></figure>\n<p><strong>通过L2范数，可以实现对模型空间的限制，从而在一定程度上避免了过拟合</strong>。</p>\n<p>说到可以避免过拟合，就要涉及到条件数（详见\n<a href=\"https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%95%B0\" target=\"_blank\" rel=\"noopener\">条件数-维基百科</a>, \n<a href=\"https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E6%9D%A1%E4%BB%B6%E6%95%B0\" target=\"_blank\" rel=\"noopener\">矩阵条件数-百度百科</a>）</p>\n<p>从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n优化有两大难题，一是：局部最小值，二是：病态(ill-condition)问题。前者找的是全局最小值，如果局部最小值太多，那优化算法就很容易陷入局部最小而不能自拔，这不是我们愿意看到的。\n病态对应着良置(well-condition)，假设我们有个方程组\\(AX=b\\)，我们需要求解\\(X\\)。如果\\(A\\)或者\\(b\\)稍微的改变，会使得\\(X\\)的解发生很大的改变，那么这个方程组系统就是病态的，\n反之就是良置的。</p>\n<p>下面看个例子：\n<img src=\"/posts_res/2018-03-24-regularization/4-1.png\" alt=\"example\"></p>\n<p>先看左边,第一行假设是\\(AX=b\\)，第二行稍微改变下\\(b\\)，得到的\\(x\\)和没改变前的差别很大，第三行稍微改变下系数矩阵\\(A\\)，可以看到结果的变化也很大。\n换句话来说，这个系统的解对系数矩阵\\(A\\)或者\\(b\\)太敏感了。又因为一般系数矩阵\\(A\\)和\\(b\\)是从实验数据里面估计得到的，所以是存在误差的，如果系统对这个误差是可以容忍的就还好，\n但系统对这个误差太敏感了，以至于解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是病态(ill-conditioned)的，不正常的，不稳定的，有问题的。右边那个就叫良置(well-condition)的系统。\n上面的条件数就是拿来衡量病态系统的可信度的。条件数衡量的是输入发生微小变化的时候，输出会发生多大的变化，也就是系统对微小变化的敏感度。条件数值小的就是良置的，大的就是病态的。</p>\n<p>对条件数来个总结：条件数是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的条件数在1附近，那么它就是良置的，\n如果远大于1，那么它就是病态的，如果一个系统是病态的，它的输出结果可信度就很低。</p>\n<p>回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：\n\\[\nw^{\\ast} = (X^TX)^{-1}X^Ty\n\\]\n然而，如果当样本\\(X\\)的数目比每个样本的维度还要小的时候，矩阵\\(X^TX\\)将会不是满秩的，也就是\\(X^TX\\)会变得不可逆，所以\\(w^{\\ast}\\)就没办法直接计算出来了。\n或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</p>\n<p>但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：\n\\[\nw^{\\ast} = (X^TX+\\lambda I)^{-1} X^T y\n\\]\n要得到这个解，通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。\n考虑没有规则项的时候，也就是\\(\\lambda=0\\)的情况，如果矩阵\\(X^TX\\)的条件数很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善条件数。</p>\n<p>另外，如果使用迭代优化的算法，条件数太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成\\( \\lambda-strongly convex, (\\lambda\\)强凸)的了。</p>\n<p>关于强凸和普通凸的对比，看下图[其他详细数学内容自行查阅资料]:</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/4-2.png\" alt=\"strongconvex\"></p>\n<p>设最优解为\\(w^{\\ast}\\)指向的地方。如果我们的函数\\(f(w)\\)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算\\(w_t\\)和\\(w^{\\ast}\\)离的比较近的时候，\\(f(wt)\\)和\\(f(w^{\\ast})\\)的值差别还是挺大的，也就是会保证在最优解\\(w^{\\ast}\\)附近的时候，还存在较大的梯度值，这样才可以在比较少的迭代次数内达到\\(w^{\\ast}\\)。\n但对于右图，红色的函数\\(f(w)\\)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在\\(w_t\\)还离我们的最优点\\(w^{\\ast}\\)很远的时候，我们的近似梯度\\((f(w_t)-f(w^{\\ast}))/(w_t-w^{\\ast})\\)就已经非常小了，在\\(w_t\\)处的近似梯度\\(\\partial f / \\partial w \\)就更小了，这样通过梯度下降，我们得到的结果就是\\(w\\)的变化非常缓慢。\n如果要获得强凸性质怎么做？最简单的就是往里面加入一项\\( (\\alpha/2)* || w ||^2 \\)。</p>\n<p>一言以蔽之：L2范数不但可以防止过拟合，还可以让优化求解变得稳定和快速。</p>\n<hr>\n<h3 id=\"5-L1和L2的区别\"><a href=\"#5-L1和L2的区别\" class=\"headerlink\" title=\"5. L1和L2的区别\"></a>5. L1和L2的区别</h3><h4 id=\"5-1-下降速度\"><a href=\"#5-1-下降速度\" class=\"headerlink\" title=\"5.1 下降速度\"></a>5.1 下降速度</h4><p>L1和L2都是规则化的方式，将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/5-1.png\" alt=\"l1l2\"></p>\n<p>L1又称Lasso，L2又称Ridge。</p>\n<h4 id=\"5-2-模型空间的限制\"><a href=\"#5-2-模型空间的限制\" class=\"headerlink\" title=\"5.2 模型空间的限制\"></a>5.2 模型空间的限制</h4><p> 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：\n\\[\nL1(Lasso): \\mathop{\\min}_w \\frac{1}{n} || y-w^T X ||^2, \\quad s.t. \\quad || w ||_1 \\leq C \\\\\nL2(Ridge): \\mathop{\\min}_w \\frac{1}{n} || y-w^T X ||^2, \\quad s.t. \\quad || w ||_2 \\leq C \\\\\n\\]</p>\n<p>也就是说，我们将模型空间限制在\\(w\\)的一个L1-ball中。\n为了便于可视化，我们考虑两维的情况，在\\( (w_1, w_2) \\)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个norm ball。等高线与norm ball首次相交的地方就是最优解：</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/5-2.png\" alt=\"normball\"></p>\n<p>可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的等高线除非位置摆得非常好，大部分时候都会在角的地方相交。\n注意到在角的位置就会产生稀疏性，例如图中的相交点就有\\(w_1=0\\)，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。\n相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则能产生稀疏性，而L2正则不行的原因了。</p>\n<p>因此总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://zhuanlan.zhihu.com/p/29360425\" target=\"_blank\" rel=\"noopener\">深入理解L1、L2正则化</a></p>\n<p><a href=\"https://blog.csdn.net/zouxy09/article/details/24971995\" target=\"_blank\" rel=\"noopener\">机器学习中的范数规则化</a></p>\n<p><a href=\"https://www.zhihu.com/question/52993836\" target=\"_blank\" rel=\"noopener\">机器学习中为什么越小的参数说明模型越简单?</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"机器学习中的正则化-Regularization\"><a href=\"#机器学习中的正则化-Regularization\" class=\"headerlink\" title=\" 机器学习中的正则化 - Regularization \"></a><center> 机器学习中的正则化 - Regularization </center></h2><p>目录</p><ul>\n<li>先验知识</li>\n<li>L0正则[范数]</li>\n<li>L1正则[范数]</li>\n<li>L2正则[范数]</li>\n<li>L1和L2的区别</li>\n</ul><hr><h3 id=\"1-先验知识\"><a href=\"#1-先验知识\" class=\"headerlink\" title=\"1. 先验知识\"></a>1. 先验知识</h3><p>监督机器学习问题是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。\n因为参数太多，会导致我们的模型复杂度上升，容易过拟合。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。\n所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化能力，而模型“简单”就是通过规则函数来实现的。\n另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。</p>","more":"\n\n\n\n<p>还有几种角度来看待规则化的。\n规则化符合奥卡姆剃刀(Occam’s razor)原理：在所有可能选择的模型中，选择能够很好地解释已知数据并且简单的模型。\n从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。\n还有个说法，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。</p>\n<p>一般来说，监督学习可以看作最小化下面的目标函数：\n\\[\nw^{\\ast} = \\mathop{\\arg\\min}_w \\sum_i L(y_i, f(x_i; w)) + \\lambda \\Omega(w)\n\\]</p>\n<p>其中，第一项\\(L(yi,f(xi;w))\\)衡量我们的模型对第\\(i\\)个样本的预测值\\(f(x_i;w)\\)和真实的标签\\(y_i\\)之前的误差。\n因为模型是要拟合训练样本，所以要求这一项最小，也就是要求模型尽量的拟合训练数据。\n但正如上面说言，不仅要保证训练误差最小，更希望模型测试误差小，所以需要加上第二项，也就是对参数\\(w\\)的规则化函数\\(\\Omega(w)\\)去约束模型尽量的简单。</p>\n<p>对于第一项Loss函数，不同的loss函数，具有不同的拟合特性，具体问题具体分析:</p>\n<ul>\n<li>如果是Square loss，是最小二乘</li>\n<li>如果是Hinge Loss，是SVM</li>\n<li>如果是exp-Loss，是Boosting</li>\n<li>如果是log-Loss，是Logistic Regression</li>\n<li>等等</li>\n</ul>\n<p>规则化函数\\(\\Omega(w)\\)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。</p>\n<ul>\n<li>零范数</li>\n<li>一范数</li>\n<li>二范数</li>\n<li>迹范数</li>\n<li>Frobenius范数(对应元素的平方和再开方) \\( \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n | a_{ij} | ^2} \\)</li>\n<li>核范数 \\( tr(\\sqrt{X^TX}) \\)</li>\n<li>等等</li>\n</ul>\n<hr>\n<h3 id=\"2-L0正则-范数\"><a href=\"#2-L0正则-范数\" class=\"headerlink\" title=\"2. L0正则[范数]\"></a>2. L0正则[范数]</h3><p>定义： L0范数是指向量中非0的元素的个数。</p>\n<p>如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，换句话说就是让参数W是稀疏的。</p>\n<p>所以L0范数非常适合机器学习中稀疏编码，特征选择。但是由于L0范数很难优化求解（NP难问题），而<strong>L1范数又是L0范数的最优凸近似[??]</strong>，故通常用L1范数来代替。</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/2-1.jpg\" alt=\"l0l1\"></p>\n<p>其他可以参考Emmanuel Candes的工作。</p>\n<hr>\n<h3 id=\"3-L1正则-范数\"><a href=\"#3-L1正则-范数\" class=\"headerlink\" title=\"3. L1正则[范数]\"></a>3. L1正则[范数]</h3><p>定义：L1范数是指向量中各个元素绝对值之和。\n\\[\n|| w ||_1 = \\sum_i | w_i |\n\\]</p>\n<p><strong>L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</strong></p>\n<h4 id=\"3-1-参数稀疏的优势\"><a href=\"#3-1-参数稀疏的优势\" class=\"headerlink\" title=\"3.1 参数稀疏的优势\"></a>3.1 参数稀疏的优势</h4><p>（1）特征选择</p>\n<p>大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。\n一般来说，\\(x_i\\)的大部分元素（也就是特征）都是和最终的输出\\(y_i\\)没有关系或者不提供任何信息的，在最小化目标函数的时候考虑\\(x_i\\)这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确\\(y_i\\)的预测。\n稀疏规则化算子的引入就是为了完成特征自动选择的任务，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p>\n<p>（2）可解释性</p>\n<p>另一个理由是，模型更容易解释。\n例如患某种病的概率是\\(y\\)，假设收集到的数据\\(x\\)是1000维的，也就是需要寻找这1000种因素到底是怎么影响患上这种病的概率的。\n假设我们这个是个回归模型：\\( y = w_1 x_1 + w_2 x_2 + … + w_1000 x_1000 + b\\)（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。\n通过学习，如果最后学习到的\\(w^{\\ast}\\)就只有很少的非零元素，例如只有5个非零的\\(w_i\\)，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。\n也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个\\(w_i\\)都非0，医生面对这1000种因素就无从下手。</p>\n<hr>\n<h3 id=\"4-L2正则-范数\"><a href=\"#4-L2正则-范数\" class=\"headerlink\" title=\"4. L2正则[范数]\"></a>4. L2正则[范数]</h3><p>定义：欧式距离\n\\[\n|| w ||_2 = \\sqrt{\\sum_i w_i^2}\n\\]</p>\n<p>L2正则对于改善过拟合具有强大的功效。\n让L2范数的规则项最小，可以使得\\(w\\)的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">为什么参数越小模型越简单？</span><br><span class=\"line\">因为越是复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就很容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，</span><br><span class=\"line\">而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值比较大，反之，简单的模型，参数值较小。</span><br></pre></td></tr></table></figure>\n<p><strong>通过L2范数，可以实现对模型空间的限制，从而在一定程度上避免了过拟合</strong>。</p>\n<p>说到可以避免过拟合，就要涉及到条件数（详见\n<a href=\"https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E6%95%B0\" target=\"_blank\" rel=\"noopener\">条件数-维基百科</a>, \n<a href=\"https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E6%9D%A1%E4%BB%B6%E6%95%B0\" target=\"_blank\" rel=\"noopener\">矩阵条件数-百度百科</a>）</p>\n<p>从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n优化有两大难题，一是：局部最小值，二是：病态(ill-condition)问题。前者找的是全局最小值，如果局部最小值太多，那优化算法就很容易陷入局部最小而不能自拔，这不是我们愿意看到的。\n病态对应着良置(well-condition)，假设我们有个方程组\\(AX=b\\)，我们需要求解\\(X\\)。如果\\(A\\)或者\\(b\\)稍微的改变，会使得\\(X\\)的解发生很大的改变，那么这个方程组系统就是病态的，\n反之就是良置的。</p>\n<p>下面看个例子：\n<img src=\"/posts_res/2018-03-24-regularization/4-1.png\" alt=\"example\"></p>\n<p>先看左边,第一行假设是\\(AX=b\\)，第二行稍微改变下\\(b\\)，得到的\\(x\\)和没改变前的差别很大，第三行稍微改变下系数矩阵\\(A\\)，可以看到结果的变化也很大。\n换句话来说，这个系统的解对系数矩阵\\(A\\)或者\\(b\\)太敏感了。又因为一般系数矩阵\\(A\\)和\\(b\\)是从实验数据里面估计得到的，所以是存在误差的，如果系统对这个误差是可以容忍的就还好，\n但系统对这个误差太敏感了，以至于解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是病态(ill-conditioned)的，不正常的，不稳定的，有问题的。右边那个就叫良置(well-condition)的系统。\n上面的条件数就是拿来衡量病态系统的可信度的。条件数衡量的是输入发生微小变化的时候，输出会发生多大的变化，也就是系统对微小变化的敏感度。条件数值小的就是良置的，大的就是病态的。</p>\n<p>对条件数来个总结：条件数是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的条件数在1附近，那么它就是良置的，\n如果远大于1，那么它就是病态的，如果一个系统是病态的，它的输出结果可信度就很低。</p>\n<p>回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。\n因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：\n\\[\nw^{\\ast} = (X^TX)^{-1}X^Ty\n\\]\n然而，如果当样本\\(X\\)的数目比每个样本的维度还要小的时候，矩阵\\(X^TX\\)将会不是满秩的，也就是\\(X^TX\\)会变得不可逆，所以\\(w^{\\ast}\\)就没办法直接计算出来了。\n或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</p>\n<p>但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：\n\\[\nw^{\\ast} = (X^TX+\\lambda I)^{-1} X^T y\n\\]\n要得到这个解，通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。\n考虑没有规则项的时候，也就是\\(\\lambda=0\\)的情况，如果矩阵\\(X^TX\\)的条件数很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善条件数。</p>\n<p>另外，如果使用迭代优化的算法，条件数太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成\\( \\lambda-strongly convex, (\\lambda\\)强凸)的了。</p>\n<p>关于强凸和普通凸的对比，看下图[其他详细数学内容自行查阅资料]:</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/4-2.png\" alt=\"strongconvex\"></p>\n<p>设最优解为\\(w^{\\ast}\\)指向的地方。如果我们的函数\\(f(w)\\)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算\\(w_t\\)和\\(w^{\\ast}\\)离的比较近的时候，\\(f(wt)\\)和\\(f(w^{\\ast})\\)的值差别还是挺大的，也就是会保证在最优解\\(w^{\\ast}\\)附近的时候，还存在较大的梯度值，这样才可以在比较少的迭代次数内达到\\(w^{\\ast}\\)。\n但对于右图，红色的函数\\(f(w)\\)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在\\(w_t\\)还离我们的最优点\\(w^{\\ast}\\)很远的时候，我们的近似梯度\\((f(w_t)-f(w^{\\ast}))/(w_t-w^{\\ast})\\)就已经非常小了，在\\(w_t\\)处的近似梯度\\(\\partial f / \\partial w \\)就更小了，这样通过梯度下降，我们得到的结果就是\\(w\\)的变化非常缓慢。\n如果要获得强凸性质怎么做？最简单的就是往里面加入一项\\( (\\alpha/2)* || w ||^2 \\)。</p>\n<p>一言以蔽之：L2范数不但可以防止过拟合，还可以让优化求解变得稳定和快速。</p>\n<hr>\n<h3 id=\"5-L1和L2的区别\"><a href=\"#5-L1和L2的区别\" class=\"headerlink\" title=\"5. L1和L2的区别\"></a>5. L1和L2的区别</h3><h4 id=\"5-1-下降速度\"><a href=\"#5-1-下降速度\" class=\"headerlink\" title=\"5.1 下降速度\"></a>5.1 下降速度</h4><p>L1和L2都是规则化的方式，将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/5-1.png\" alt=\"l1l2\"></p>\n<p>L1又称Lasso，L2又称Ridge。</p>\n<h4 id=\"5-2-模型空间的限制\"><a href=\"#5-2-模型空间的限制\" class=\"headerlink\" title=\"5.2 模型空间的限制\"></a>5.2 模型空间的限制</h4><p> 实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：\n\\[\nL1(Lasso): \\mathop{\\min}_w \\frac{1}{n} || y-w^T X ||^2, \\quad s.t. \\quad || w ||_1 \\leq C \\\\\nL2(Ridge): \\mathop{\\min}_w \\frac{1}{n} || y-w^T X ||^2, \\quad s.t. \\quad || w ||_2 \\leq C \\\\\n\\]</p>\n<p>也就是说，我们将模型空间限制在\\(w\\)的一个L1-ball中。\n为了便于可视化，我们考虑两维的情况，在\\( (w_1, w_2) \\)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个norm ball。等高线与norm ball首次相交的地方就是最优解：</p>\n<p><img src=\"/posts_res/2018-03-24-regularization/5-2.png\" alt=\"normball\"></p>\n<p>可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的等高线除非位置摆得非常好，大部分时候都会在角的地方相交。\n注意到在角的位置就会产生稀疏性，例如图中的相交点就有\\(w_1=0\\)，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。\n相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则能产生稀疏性，而L2正则不行的原因了。</p>\n<p>因此总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://zhuanlan.zhihu.com/p/29360425\" target=\"_blank\" rel=\"noopener\">深入理解L1、L2正则化</a></p>\n<p><a href=\"https://blog.csdn.net/zouxy09/article/details/24971995\" target=\"_blank\" rel=\"noopener\">机器学习中的范数规则化</a></p>\n<p><a href=\"https://www.zhihu.com/question/52993836\" target=\"_blank\" rel=\"noopener\">机器学习中为什么越小的参数说明模型越简单?</a></p>\n</blockquote>\n"},{"layout":"post","title":"拉格朗日对偶性","date":"2018-03-27T12:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>拉格朗日对偶性 - Lagrange Duality</center>\n\n#### 目录\n* 原始问题\n* 对偶问题\n* 原始问题与对偶问题的关系\n\n\n--------------\n\n### 1. 原始问题\n\n假设\\\\( f(x),c\\_i(x),h\\_j(x) \\\\)是定义在\\\\(R^n\\\\)上的连续可微函数，考虑约束最优化问题\n\\\\[\n\\mathop{\\min}\\_{x \\in R^n} f(x)\n\\\\]\n\\\\[\ns.t. c\\_i(x) \\leq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\n\\quad \\quad h\\_j(x) = 0, \\quad j=1,2,...,k\n\\\\]\n\n称此约束最优化问题为原始最优化问题或原始问题。\n\n引入广义拉格朗日函数\n\\\\[\nL(x,\\alpha,\\beta) = f(x) + \\sum\\_{i=1}^k \\alpha\\_ic\\_i(x) + \\sum\\_{j=1}^l \\beta\\_j h\\_j(x)\n\\\\]\n这里，\\\\(x=(x^{(1)}, x^{(2)}, ..., x^{(n)})^T \\in R^n, \\alpha\\_i, \\beta\\_j\\\\)是拉格朗日乘子，\\\\( \\alpha\\_i \\geq 0 \\\\), **[不要求\\\\( \\beta\\_j \\geq 0 \\\\)，详细可以看[这里](https://www.cnblogs.com/ooon/p/5721119.html)]**。\n考虑\\\\(x\\\\)的函数：\n\\\\[\n\\theta\\_P(x) = \\mathop{\\max}\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0} L(x, \\alpha, \\beta)\n\\\\]\n\n这里下标\\\\(P\\\\)表示原始问题。\n\n当有约束条件不满足时，可以调整该约束条件的拉格朗日乘子，可以使得\\\\( \\theta\\_P(x)=+\\infty \\\\),只有当所有的约束条件全都满足时，则可知\\\\(\\theta\\_P(x) = f(x)\\\\)。\n所以，\n\\\\[\n\\theta\\_P(x) = \n\\begin{cases}\nf(x), &  x满足原始问题约束 \\\\\\\n+\\infty, & 其他\n\\end{cases}\n\\\\]\n\n所以，如果考虑极小化问题\n\\\\[\n\\mathop{\\min}\\_x \\theta\\_P(x) = \\mathop{\\min\\_x\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta)\n\\\\]\n该问题与原始最优化问题\\\\(式(1) \\sim 式(3)\\\\)等价，即它们具有相同解。问题\\\\( \\mathop{\\min}\\_x \\theta\\_P(x) = \\mathop{\\min\\_x\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta) \\\\)\n称为广义拉格朗日函数的极小极大问题，这样，原始最优化问题表示为广义拉格朗日函数的极小极大问题。\n\n定义原始问题的最优值：\n\\\\[\np^{\\ast} = \\mathop{\\min\\_x} \\theta\\_P(x)\n\\\\]\n称为原始问题的值。\n\n\n--------------\n\n### 2. 对偶问题\n\n定义\n\\\\[\n\\theta\\_D(\\alpha, \\beta) = \\mathop{\\min\\_x} L(x, \\alpha, \\beta)\n\\\\]\n再考虑极大化\n\\\\[\n\\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0}} \\theta\\_D(\\alpha, \\beta) = \\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\min\\_x} L(x,\\alpha, \\beta)\n\\\\]\n\n问题\\\\( \\mathop{\\max}\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\mathop{\\min\\_x} L(x,\\alpha, \\beta) \\\\)称为广义拉格朗日函数的极大极小问题。\n\n可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：\n\\\\[\n\\mathop{\\max\\_{\\alpha, \\beta}} \\theta\\_D(\\alpha, \\beta) = \\mathop{\\max\\_{\\alpha, \\beta} \\min\\_x} L(x, \\alpha, \\beta)\n\\\\]\n\\\\[\ns.t. \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,...,k\n\\\\]\n称为原始问题的对偶问题。\n\n定义对偶问题的最优解：\n\\\\[\nd^{\\ast} = \\mathop{\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} \\theta\\_D(\\alpha, \\beta)\n\\\\]\n称为对偶问题的值。\n\n\n--------------\n\n### 3. 原始问题与对偶问题的关系\n\n#### 3.1 定理 1\n\n若原始问题和对偶问题都有最优值，则\n\\\\[\nd^{\\ast} = \\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\min\\_x}L(x,\\alpha, \\beta) \\leq \\mathop{\\min\\_x \\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta) = p^{\\ast}\n\\\\]\n\n#### 3.2 定理 2\n\n考虑原始问题\\\\(式(1) \\sim 式(3)\\\\)和对偶问题\\\\(式(11) \\sim 式(12)\\\\)，\n假设函数\\\\(f(x)\\\\)和\\\\(c\\_i(x)\\\\)是**凸函数**，\\\\(h\\_j(x)\\\\)是仿射函数*[最高次数为1的多项式函数,常数项为零的仿射函数称为线性函数]*；\n并且假设不等式约束\\\\(c\\_i(x)\\\\)是严格可行的，即**存在**\\\\(x\\\\)，对所有\\\\(i\\\\)有\\\\(c\\_i(x)<0\\\\)，\n则存在\\\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\\\)，使\\\\(x^{\\ast}\\\\)是原始问题的解，\\\\( \\alpha^{\\ast}, \\beta^{\\ast} \\\\)是对偶问题的解，并且\n\\\\[\np^{\\ast} = d^{\\ast} = L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast})\n\\\\]\n\n#### 3.3 定理 3\n\n对原始问题\\\\(式(1) \\sim 式(3)\\\\)和对偶问题\\\\(式(11) \\sim 式(12)\\\\)，\n假设函数\\\\(f(x)\\\\)和\\\\(c\\_i(x)\\\\)是**凸函数**，\\\\(h\\_j(x)\\\\)是仿射函数，并且假设不等式约束\\\\(c\\_i(x)\\\\)是严格可行的，则\\\\( x^{\\ast} 和 \\alpha^{\\ast}, \\beta^{\\ast} \\\\)分别是原始问题和对偶问题的解的**充分必要条件**是\\\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\\\)满足下面的\\\\( Karush-Kuhn-Tucker(KKT) \\\\)条件：\n\\\\[\n\\nabla\\_x L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\\\n\\nabla\\_\\alpha L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\\\n\\nabla\\_\\beta L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast}c\\_i(x^{\\ast}) = 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\nc\\_i(x^{\\ast}) \\leq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast} \\geq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\nh\\_j(x^{\\ast}) = 0, \\quad i=1,2,...,l\n\\\\]\n\n特别指出，\\\\( 式(17) \\\\)称为\\\\(KKT\\\\)的对偶互补条件，由此条件可知：若\\\\( \\alpha^{\\ast} > 0 \\\\)，则\\\\( c\\_i(x^{\\ast})=0 \\\\)。\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n","source":"_posts/2018-03-27-lagrange.md","raw":"---\nlayout: post\ntitle: 拉格朗日对偶性\ndate: 2018-03-27 20:10 +0800\ncategories: 基础知识\ntags:\n- 数学\nmathjax: true\ncopyright: true\n---\n\n## <center>拉格朗日对偶性 - Lagrange Duality</center>\n\n#### 目录\n* 原始问题\n* 对偶问题\n* 原始问题与对偶问题的关系\n\n\n--------------\n\n### 1. 原始问题\n\n假设\\\\( f(x),c\\_i(x),h\\_j(x) \\\\)是定义在\\\\(R^n\\\\)上的连续可微函数，考虑约束最优化问题\n\\\\[\n\\mathop{\\min}\\_{x \\in R^n} f(x)\n\\\\]\n\\\\[\ns.t. c\\_i(x) \\leq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\n\\quad \\quad h\\_j(x) = 0, \\quad j=1,2,...,k\n\\\\]\n\n称此约束最优化问题为原始最优化问题或原始问题。\n\n引入广义拉格朗日函数\n\\\\[\nL(x,\\alpha,\\beta) = f(x) + \\sum\\_{i=1}^k \\alpha\\_ic\\_i(x) + \\sum\\_{j=1}^l \\beta\\_j h\\_j(x)\n\\\\]\n这里，\\\\(x=(x^{(1)}, x^{(2)}, ..., x^{(n)})^T \\in R^n, \\alpha\\_i, \\beta\\_j\\\\)是拉格朗日乘子，\\\\( \\alpha\\_i \\geq 0 \\\\), **[不要求\\\\( \\beta\\_j \\geq 0 \\\\)，详细可以看[这里](https://www.cnblogs.com/ooon/p/5721119.html)]**。\n考虑\\\\(x\\\\)的函数：\n\\\\[\n\\theta\\_P(x) = \\mathop{\\max}\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0} L(x, \\alpha, \\beta)\n\\\\]\n\n这里下标\\\\(P\\\\)表示原始问题。\n\n当有约束条件不满足时，可以调整该约束条件的拉格朗日乘子，可以使得\\\\( \\theta\\_P(x)=+\\infty \\\\),只有当所有的约束条件全都满足时，则可知\\\\(\\theta\\_P(x) = f(x)\\\\)。\n所以，\n\\\\[\n\\theta\\_P(x) = \n\\begin{cases}\nf(x), &  x满足原始问题约束 \\\\\\\n+\\infty, & 其他\n\\end{cases}\n\\\\]\n\n所以，如果考虑极小化问题\n\\\\[\n\\mathop{\\min}\\_x \\theta\\_P(x) = \\mathop{\\min\\_x\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta)\n\\\\]\n该问题与原始最优化问题\\\\(式(1) \\sim 式(3)\\\\)等价，即它们具有相同解。问题\\\\( \\mathop{\\min}\\_x \\theta\\_P(x) = \\mathop{\\min\\_x\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta) \\\\)\n称为广义拉格朗日函数的极小极大问题，这样，原始最优化问题表示为广义拉格朗日函数的极小极大问题。\n\n定义原始问题的最优值：\n\\\\[\np^{\\ast} = \\mathop{\\min\\_x} \\theta\\_P(x)\n\\\\]\n称为原始问题的值。\n\n\n--------------\n\n### 2. 对偶问题\n\n定义\n\\\\[\n\\theta\\_D(\\alpha, \\beta) = \\mathop{\\min\\_x} L(x, \\alpha, \\beta)\n\\\\]\n再考虑极大化\n\\\\[\n\\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0}} \\theta\\_D(\\alpha, \\beta) = \\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\min\\_x} L(x,\\alpha, \\beta)\n\\\\]\n\n问题\\\\( \\mathop{\\max}\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\mathop{\\min\\_x} L(x,\\alpha, \\beta) \\\\)称为广义拉格朗日函数的极大极小问题。\n\n可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：\n\\\\[\n\\mathop{\\max\\_{\\alpha, \\beta}} \\theta\\_D(\\alpha, \\beta) = \\mathop{\\max\\_{\\alpha, \\beta} \\min\\_x} L(x, \\alpha, \\beta)\n\\\\]\n\\\\[\ns.t. \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,...,k\n\\\\]\n称为原始问题的对偶问题。\n\n定义对偶问题的最优解：\n\\\\[\nd^{\\ast} = \\mathop{\\max\\_{\\alpha, \\beta: \\alpha\\_i \\geq 0}} \\theta\\_D(\\alpha, \\beta)\n\\\\]\n称为对偶问题的值。\n\n\n--------------\n\n### 3. 原始问题与对偶问题的关系\n\n#### 3.1 定理 1\n\n若原始问题和对偶问题都有最优值，则\n\\\\[\nd^{\\ast} = \\mathop{\\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0} \\min\\_x}L(x,\\alpha, \\beta) \\leq \\mathop{\\min\\_x \\max\\_{\\alpha,\\beta:\\alpha\\_i \\geq 0}} L(x, \\alpha, \\beta) = p^{\\ast}\n\\\\]\n\n#### 3.2 定理 2\n\n考虑原始问题\\\\(式(1) \\sim 式(3)\\\\)和对偶问题\\\\(式(11) \\sim 式(12)\\\\)，\n假设函数\\\\(f(x)\\\\)和\\\\(c\\_i(x)\\\\)是**凸函数**，\\\\(h\\_j(x)\\\\)是仿射函数*[最高次数为1的多项式函数,常数项为零的仿射函数称为线性函数]*；\n并且假设不等式约束\\\\(c\\_i(x)\\\\)是严格可行的，即**存在**\\\\(x\\\\)，对所有\\\\(i\\\\)有\\\\(c\\_i(x)<0\\\\)，\n则存在\\\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\\\)，使\\\\(x^{\\ast}\\\\)是原始问题的解，\\\\( \\alpha^{\\ast}, \\beta^{\\ast} \\\\)是对偶问题的解，并且\n\\\\[\np^{\\ast} = d^{\\ast} = L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast})\n\\\\]\n\n#### 3.3 定理 3\n\n对原始问题\\\\(式(1) \\sim 式(3)\\\\)和对偶问题\\\\(式(11) \\sim 式(12)\\\\)，\n假设函数\\\\(f(x)\\\\)和\\\\(c\\_i(x)\\\\)是**凸函数**，\\\\(h\\_j(x)\\\\)是仿射函数，并且假设不等式约束\\\\(c\\_i(x)\\\\)是严格可行的，则\\\\( x^{\\ast} 和 \\alpha^{\\ast}, \\beta^{\\ast} \\\\)分别是原始问题和对偶问题的解的**充分必要条件**是\\\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\\\)满足下面的\\\\( Karush-Kuhn-Tucker(KKT) \\\\)条件：\n\\\\[\n\\nabla\\_x L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\\\n\\nabla\\_\\alpha L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\\\n\\nabla\\_\\beta L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast}c\\_i(x^{\\ast}) = 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\nc\\_i(x^{\\ast}) \\leq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast} \\geq 0, \\quad i=1,2,...,k\n\\\\]\n\\\\[\nh\\_j(x^{\\ast}) = 0, \\quad i=1,2,...,l\n\\\\]\n\n特别指出，\\\\( 式(17) \\\\)称为\\\\(KKT\\\\)的对偶互补条件，由此条件可知：若\\\\( \\alpha^{\\ast} > 0 \\\\)，则\\\\( c\\_i(x^{\\ast})=0 \\\\)。\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n","slug":"lagrange","published":1,"updated":"2019-08-17T09:33:22.633Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnv8000y2qwph7603nvz","content":"<h2 id=\"拉格朗日对偶性-Lagrange-Duality\"><a href=\"#拉格朗日对偶性-Lagrange-Duality\" class=\"headerlink\" title=\"拉格朗日对偶性 - Lagrange Duality\"></a><center>拉格朗日对偶性 - Lagrange Duality</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>原始问题</li>\n<li>对偶问题</li>\n<li>原始问题与对偶问题的关系</li>\n</ul><hr><h3 id=\"1-原始问题\"><a href=\"#1-原始问题\" class=\"headerlink\" title=\"1. 原始问题\"></a>1. 原始问题</h3><p>假设\\( f(x),c_i(x),h_j(x) \\)是定义在\\(R^n\\)上的连续可微函数，考虑约束最优化问题\n\\[\n\\mathop{\\min}_{x \\in R^n} f(x)\n\\]\n\\[\ns.t. c_i(x) \\leq 0, \\quad i=1,2,…,k\n\\]\n\\[\n\\quad \\quad h_j(x) = 0, \\quad j=1,2,…,k\n\\]</p><p>称此约束最优化问题为原始最优化问题或原始问题。</p><p>引入广义拉格朗日函数\n\\[\nL(x,\\alpha,\\beta) = f(x) + \\sum_{i=1}^k \\alpha_ic_i(x) + \\sum_{j=1}^l \\beta_j h_j(x)\n\\]\n这里，\\(x=(x^{(1)}, x^{(2)}, …, x^{(n)})^T \\in R^n, \\alpha_i, \\beta_j\\)是拉格朗日乘子，\\( \\alpha_i \\geq 0 \\), <strong>[不要求\\( \\beta_j \\geq 0 \\)，详细可以看<a href=\"https://www.cnblogs.com/ooon/p/5721119.html\" target=\"_blank\" rel=\"noopener\">这里</a>]</strong>。\n考虑\\(x\\)的函数：\n\\[\n\\theta_P(x) = \\mathop{\\max}_{\\alpha, \\beta: \\alpha_i \\geq 0} L(x, \\alpha, \\beta)\n\\]</p><a id=\"more\"></a>\n\n\n\n\n<p>这里下标\\(P\\)表示原始问题。</p>\n<p>当有约束条件不满足时，可以调整该约束条件的拉格朗日乘子，可以使得\\( \\theta_P(x)=+\\infty \\),只有当所有的约束条件全都满足时，则可知\\(\\theta_P(x) = f(x)\\)。\n所以，\n\\[\n\\theta_P(x) = \n\\begin{cases}\nf(x), &amp;  x满足原始问题约束 \\\\\n+\\infty, &amp; 其他\n\\end{cases}\n\\]</p>\n<p>所以，如果考虑极小化问题\n\\[\n\\mathop{\\min}_x \\theta_P(x) = \\mathop{\\min_x\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} L(x, \\alpha, \\beta)\n\\]\n该问题与原始最优化问题\\(式(1) \\sim 式(3)\\)等价，即它们具有相同解。问题\\( \\mathop{\\min}_x \\theta_P(x) = \\mathop{\\min_x\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} L(x, \\alpha, \\beta) \\)\n称为广义拉格朗日函数的极小极大问题，这样，原始最优化问题表示为广义拉格朗日函数的极小极大问题。</p>\n<p>定义原始问题的最优值：\n\\[\np^{\\ast} = \\mathop{\\min_x} \\theta_P(x)\n\\]\n称为原始问题的值。</p>\n<hr>\n<h3 id=\"2-对偶问题\"><a href=\"#2-对偶问题\" class=\"headerlink\" title=\"2. 对偶问题\"></a>2. 对偶问题</h3><p>定义\n\\[\n\\theta_D(\\alpha, \\beta) = \\mathop{\\min_x} L(x, \\alpha, \\beta)\n\\]\n再考虑极大化\n\\[\n\\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0}} \\theta_D(\\alpha, \\beta) = \\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0} \\min_x} L(x,\\alpha, \\beta)\n\\]</p>\n<p>问题\\( \\mathop{\\max}_{\\alpha,\\beta:\\alpha_i \\geq 0} \\mathop{\\min_x} L(x,\\alpha, \\beta) \\)称为广义拉格朗日函数的极大极小问题。</p>\n<p>可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：\n\\[\n\\mathop{\\max_{\\alpha, \\beta}} \\theta_D(\\alpha, \\beta) = \\mathop{\\max_{\\alpha, \\beta} \\min_x} L(x, \\alpha, \\beta)\n\\]\n\\[\ns.t. \\quad \\alpha_i \\geq 0, \\quad i=1,2,…,k\n\\]\n称为原始问题的对偶问题。</p>\n<p>定义对偶问题的最优解：\n\\[\nd^{\\ast} = \\mathop{\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} \\theta_D(\\alpha, \\beta)\n\\]\n称为对偶问题的值。</p>\n<hr>\n<h3 id=\"3-原始问题与对偶问题的关系\"><a href=\"#3-原始问题与对偶问题的关系\" class=\"headerlink\" title=\"3. 原始问题与对偶问题的关系\"></a>3. 原始问题与对偶问题的关系</h3><h4 id=\"3-1-定理-1\"><a href=\"#3-1-定理-1\" class=\"headerlink\" title=\"3.1 定理 1\"></a>3.1 定理 1</h4><p>若原始问题和对偶问题都有最优值，则\n\\[\nd^{\\ast} = \\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0} \\min_x}L(x,\\alpha, \\beta) \\leq \\mathop{\\min_x \\max_{\\alpha,\\beta:\\alpha_i \\geq 0}} L(x, \\alpha, \\beta) = p^{\\ast}\n\\]</p>\n<h4 id=\"3-2-定理-2\"><a href=\"#3-2-定理-2\" class=\"headerlink\" title=\"3.2 定理 2\"></a>3.2 定理 2</h4><p>考虑原始问题\\(式(1) \\sim 式(3)\\)和对偶问题\\(式(11) \\sim 式(12)\\)，\n假设函数\\(f(x)\\)和\\(c_i(x)\\)是<strong>凸函数</strong>，\\(h_j(x)\\)是仿射函数<em>[最高次数为1的多项式函数,常数项为零的仿射函数称为线性函数]</em>；\n并且假设不等式约束\\(c_i(x)\\)是严格可行的，即<strong>存在</strong>\\(x\\)，对所有\\(i\\)有\\(c_i(x)&lt;0\\)，\n则存在\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\)，使\\(x^{\\ast}\\)是原始问题的解，\\( \\alpha^{\\ast}, \\beta^{\\ast} \\)是对偶问题的解，并且\n\\[\np^{\\ast} = d^{\\ast} = L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast})\n\\]</p>\n<h4 id=\"3-3-定理-3\"><a href=\"#3-3-定理-3\" class=\"headerlink\" title=\"3.3 定理 3\"></a>3.3 定理 3</h4><p>对原始问题\\(式(1) \\sim 式(3)\\)和对偶问题\\(式(11) \\sim 式(12)\\)，\n假设函数\\(f(x)\\)和\\(c_i(x)\\)是<strong>凸函数</strong>，\\(h_j(x)\\)是仿射函数，并且假设不等式约束\\(c_i(x)\\)是严格可行的，则\\( x^{\\ast} 和 \\alpha^{\\ast}, \\beta^{\\ast} \\)分别是原始问题和对偶问题的解的<strong>充分必要条件</strong>是\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\)满足下面的\\( Karush-Kuhn-Tucker(KKT) \\)条件：\n\\[\n\\nabla_x L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\n\\nabla_\\alpha L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\n\\nabla_\\beta L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0\n\\]\n\\[\n\\alpha_i^{\\ast}c_i(x^{\\ast}) = 0, \\quad i=1,2,…,k\n\\]\n\\[\nc_i(x^{\\ast}) \\leq 0, \\quad i=1,2,…,k\n\\]\n\\[\n\\alpha_i^{\\ast} \\geq 0, \\quad i=1,2,…,k\n\\]\n\\[\nh_j(x^{\\ast}) = 0, \\quad i=1,2,…,l\n\\]</p>\n<p>特别指出，\\( 式(17) \\)称为\\(KKT\\)的对偶互补条件，由此条件可知：若\\( \\alpha^{\\ast} &gt; 0 \\)，则\\( c_i(x^{\\ast})=0 \\)。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"拉格朗日对偶性-Lagrange-Duality\"><a href=\"#拉格朗日对偶性-Lagrange-Duality\" class=\"headerlink\" title=\"拉格朗日对偶性 - Lagrange Duality\"></a><center>拉格朗日对偶性 - Lagrange Duality</center></h2><h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>原始问题</li>\n<li>对偶问题</li>\n<li>原始问题与对偶问题的关系</li>\n</ul><hr><h3 id=\"1-原始问题\"><a href=\"#1-原始问题\" class=\"headerlink\" title=\"1. 原始问题\"></a>1. 原始问题</h3><p>假设\\( f(x),c_i(x),h_j(x) \\)是定义在\\(R^n\\)上的连续可微函数，考虑约束最优化问题\n\\[\n\\mathop{\\min}_{x \\in R^n} f(x)\n\\]\n\\[\ns.t. c_i(x) \\leq 0, \\quad i=1,2,…,k\n\\]\n\\[\n\\quad \\quad h_j(x) = 0, \\quad j=1,2,…,k\n\\]</p><p>称此约束最优化问题为原始最优化问题或原始问题。</p><p>引入广义拉格朗日函数\n\\[\nL(x,\\alpha,\\beta) = f(x) + \\sum_{i=1}^k \\alpha_ic_i(x) + \\sum_{j=1}^l \\beta_j h_j(x)\n\\]\n这里，\\(x=(x^{(1)}, x^{(2)}, …, x^{(n)})^T \\in R^n, \\alpha_i, \\beta_j\\)是拉格朗日乘子，\\( \\alpha_i \\geq 0 \\), <strong>[不要求\\( \\beta_j \\geq 0 \\)，详细可以看<a href=\"https://www.cnblogs.com/ooon/p/5721119.html\" target=\"_blank\" rel=\"noopener\">这里</a>]</strong>。\n考虑\\(x\\)的函数：\n\\[\n\\theta_P(x) = \\mathop{\\max}_{\\alpha, \\beta: \\alpha_i \\geq 0} L(x, \\alpha, \\beta)\n\\]</p>","more":"\n\n\n\n\n<p>这里下标\\(P\\)表示原始问题。</p>\n<p>当有约束条件不满足时，可以调整该约束条件的拉格朗日乘子，可以使得\\( \\theta_P(x)=+\\infty \\),只有当所有的约束条件全都满足时，则可知\\(\\theta_P(x) = f(x)\\)。\n所以，\n\\[\n\\theta_P(x) = \n\\begin{cases}\nf(x), &amp;  x满足原始问题约束 \\\\\n+\\infty, &amp; 其他\n\\end{cases}\n\\]</p>\n<p>所以，如果考虑极小化问题\n\\[\n\\mathop{\\min}_x \\theta_P(x) = \\mathop{\\min_x\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} L(x, \\alpha, \\beta)\n\\]\n该问题与原始最优化问题\\(式(1) \\sim 式(3)\\)等价，即它们具有相同解。问题\\( \\mathop{\\min}_x \\theta_P(x) = \\mathop{\\min_x\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} L(x, \\alpha, \\beta) \\)\n称为广义拉格朗日函数的极小极大问题，这样，原始最优化问题表示为广义拉格朗日函数的极小极大问题。</p>\n<p>定义原始问题的最优值：\n\\[\np^{\\ast} = \\mathop{\\min_x} \\theta_P(x)\n\\]\n称为原始问题的值。</p>\n<hr>\n<h3 id=\"2-对偶问题\"><a href=\"#2-对偶问题\" class=\"headerlink\" title=\"2. 对偶问题\"></a>2. 对偶问题</h3><p>定义\n\\[\n\\theta_D(\\alpha, \\beta) = \\mathop{\\min_x} L(x, \\alpha, \\beta)\n\\]\n再考虑极大化\n\\[\n\\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0}} \\theta_D(\\alpha, \\beta) = \\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0} \\min_x} L(x,\\alpha, \\beta)\n\\]</p>\n<p>问题\\( \\mathop{\\max}_{\\alpha,\\beta:\\alpha_i \\geq 0} \\mathop{\\min_x} L(x,\\alpha, \\beta) \\)称为广义拉格朗日函数的极大极小问题。</p>\n<p>可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：\n\\[\n\\mathop{\\max_{\\alpha, \\beta}} \\theta_D(\\alpha, \\beta) = \\mathop{\\max_{\\alpha, \\beta} \\min_x} L(x, \\alpha, \\beta)\n\\]\n\\[\ns.t. \\quad \\alpha_i \\geq 0, \\quad i=1,2,…,k\n\\]\n称为原始问题的对偶问题。</p>\n<p>定义对偶问题的最优解：\n\\[\nd^{\\ast} = \\mathop{\\max_{\\alpha, \\beta: \\alpha_i \\geq 0}} \\theta_D(\\alpha, \\beta)\n\\]\n称为对偶问题的值。</p>\n<hr>\n<h3 id=\"3-原始问题与对偶问题的关系\"><a href=\"#3-原始问题与对偶问题的关系\" class=\"headerlink\" title=\"3. 原始问题与对偶问题的关系\"></a>3. 原始问题与对偶问题的关系</h3><h4 id=\"3-1-定理-1\"><a href=\"#3-1-定理-1\" class=\"headerlink\" title=\"3.1 定理 1\"></a>3.1 定理 1</h4><p>若原始问题和对偶问题都有最优值，则\n\\[\nd^{\\ast} = \\mathop{\\max_{\\alpha,\\beta:\\alpha_i \\geq 0} \\min_x}L(x,\\alpha, \\beta) \\leq \\mathop{\\min_x \\max_{\\alpha,\\beta:\\alpha_i \\geq 0}} L(x, \\alpha, \\beta) = p^{\\ast}\n\\]</p>\n<h4 id=\"3-2-定理-2\"><a href=\"#3-2-定理-2\" class=\"headerlink\" title=\"3.2 定理 2\"></a>3.2 定理 2</h4><p>考虑原始问题\\(式(1) \\sim 式(3)\\)和对偶问题\\(式(11) \\sim 式(12)\\)，\n假设函数\\(f(x)\\)和\\(c_i(x)\\)是<strong>凸函数</strong>，\\(h_j(x)\\)是仿射函数<em>[最高次数为1的多项式函数,常数项为零的仿射函数称为线性函数]</em>；\n并且假设不等式约束\\(c_i(x)\\)是严格可行的，即<strong>存在</strong>\\(x\\)，对所有\\(i\\)有\\(c_i(x)&lt;0\\)，\n则存在\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\)，使\\(x^{\\ast}\\)是原始问题的解，\\( \\alpha^{\\ast}, \\beta^{\\ast} \\)是对偶问题的解，并且\n\\[\np^{\\ast} = d^{\\ast} = L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast})\n\\]</p>\n<h4 id=\"3-3-定理-3\"><a href=\"#3-3-定理-3\" class=\"headerlink\" title=\"3.3 定理 3\"></a>3.3 定理 3</h4><p>对原始问题\\(式(1) \\sim 式(3)\\)和对偶问题\\(式(11) \\sim 式(12)\\)，\n假设函数\\(f(x)\\)和\\(c_i(x)\\)是<strong>凸函数</strong>，\\(h_j(x)\\)是仿射函数，并且假设不等式约束\\(c_i(x)\\)是严格可行的，则\\( x^{\\ast} 和 \\alpha^{\\ast}, \\beta^{\\ast} \\)分别是原始问题和对偶问题的解的<strong>充分必要条件</strong>是\\( x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast} \\)满足下面的\\( Karush-Kuhn-Tucker(KKT) \\)条件：\n\\[\n\\nabla_x L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\n\\nabla_\\alpha L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0 \\\\\n\\nabla_\\beta L(x^{\\ast}, \\alpha^{\\ast}, \\beta^{\\ast}) = 0\n\\]\n\\[\n\\alpha_i^{\\ast}c_i(x^{\\ast}) = 0, \\quad i=1,2,…,k\n\\]\n\\[\nc_i(x^{\\ast}) \\leq 0, \\quad i=1,2,…,k\n\\]\n\\[\n\\alpha_i^{\\ast} \\geq 0, \\quad i=1,2,…,k\n\\]\n\\[\nh_j(x^{\\ast}) = 0, \\quad i=1,2,…,l\n\\]</p>\n<p>特别指出，\\( 式(17) \\)称为\\(KKT\\)的对偶互补条件，由此条件可知：若\\( \\alpha^{\\ast} &gt; 0 \\)，则\\( c_i(x^{\\ast})=0 \\)。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n</blockquote>\n"},{"layout":"post","title":"Support Vector Machines - Part 1","date":"2018-03-28T14:11:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>支持向量机 - SVM（Support Vector Machines）Part 1</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 基本概念\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n* 特点\n\n\n--------------\n\n### 1. 基本概念\n\n定义函数间隔(用\\\\(\\hat{\\gamma}\\\\)表示)为：\n\\\\[\n\\hat{\\gamma} = y(w x + b) = yf(x)\n\\\\]\n超平面\\\\((w,b)\\\\)关于训练数据集\\\\(T\\\\)中所有样本点\\\\( (x\\_i, y\\_i) \\\\)的函数间隔最小值，即为超平面\\\\( (w,b) \\\\)关于训练数据集\\\\(T\\\\)的函数间隔：\n\\\\[\n\\hat{\\gamma} = min \\hat{\\gamma\\_i}, \\quad i=1,2,...,n\n\\\\]\n其中\\\\(n\\\\)表示样本点数量。**[问题：将\\\\(w\\\\)和\\\\(b\\\\)成比例改变为\\\\(\\lambda w\\\\)和\\\\(\\lambda b\\\\)，函数间隔的值也会变化为原来的\\\\(\\lambda\\\\)倍]**，因此自然地引出几何间隔的概念。\n\n-----------\n\n定义几何间隔(用\\\\(\\gamma\\\\)表示)为：\n\\\\[\n\\gamma = \\frac{y(wx+b)}{\\|\\|w\\|\\|} = \\frac{yf(x)}{\\|\\|w\\|\\|} \\\\\\\n\\\\]\n其中\\\\(\\|\\|w\\|\\|\\\\)为\\\\(w\\\\)的\\\\(L\\_2\\\\)范数。 \n\n由上可知，\n\\\\[\n\\gamma = \\frac{\\hat{\\gamma}}{\\|\\|w\\|\\|}\n\\\\]\n即，几何间隔等于函数间隔与\\\\(w\\\\)二阶范数的商。\n\n---------\n\n回到原始的问题中，我们需要最大间隔(几何间隔)分离超平面，即：\n\\\\[\n\\mathop{\\max\\_{w,b}} \\quad \\gamma\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(\\frac{w}{\\|\\|w\\|\\|}x\\_i+\\frac{b}{\\|\\|w\\|\\|}) \\geq \\gamma, \\quad i=1,2,...,n\n\\\\]\n\n考虑几何间隔和函数间隔的关系，可以将上面的问题改写成：\n\\\\[\n\\mathop{\\max\\_{w,b}} \\quad \\frac{\\hat{\\gamma}}{\\|\\|w\\|\\|}\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i+ b) \\geq \\hat{\\gamma}, \\quad i=1,2,...,n\n\\\\]\n\n由于函数间隔\\\\( \\hat{\\gamma} \\\\)的取值并不影响最优化问题的解**[可以认为函数间隔通过\\\\(\\lambda\\\\)放缩后变为1，这样最优化问题中含有一个常数因子]**，\n这样令函数间隔\\\\(\\hat{\\gamma}=1\\\\)代入上面的最优化问题，同时，\\\\( \\max \\frac{\\hat{\\lambda}}{\\|\\|w\\|\\|} \\\\)、\\\\( \\max \\frac{1}{\\|\\| \\lambda w \\|\\|} \\\\)与\\\\( \\min \\frac{1}{2} \\|\\|w\\|\\|^2 \\\\)是等价的。\n所以上述最优化问题最终可以归结为以下最优化问题：\n\\\\[\n\\mathop{\\min\\_{w,b}} \\frac{1}{2}\\|\\|w\\|\\|^2\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i + b) - 1 \\geq 0, \\quad i=1,2,...,n\n\\\\]\n\n现在目标函数和约束条件都是连续可微的凸函数，所以可以通过[拉格朗日对偶性](/2018/03/lagrange/)，通过求解与对偶问题等价的对偶问题得到原始问题的最优解，\n这样的优点：一、对偶问题往往更容易求解；二、自然的引入核函数，推广到非线性分类问题。\n\n### 2. 线性可分支持向量机学习算法 - 最大间隔法\n\n输入：线性可分训练数据集\\\\(T=\\lbrace(x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n)\\rbrace \\\\)，其中\\\\( x\\_i \\in \\chi=R^n, y\\_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace, i=1,2,...,n\\\\)\n\n输出：最大间隔分离超平面和分类决策函数\n\n（1）构造并求解约束最优化问题\n\\\\[\n\\mathop{\\min\\_{w,b}} \\frac{1}{2} \\|\\|w\\|\\|^2\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i +b) -1 \\geq 0, \\quad i=1,2,..,n\n\\\\]\n求的最优解\\\\( w^{\\ast},b^{\\ast} \\\\)。\n\n（2）由此得到分离超平面：\n\\\\[\nw^{\\ast} \\cdot x + b^{\\ast} = 0\n\\\\]\n分类决策函数\n\\\\[\nf(x) =  sign(w^{\\ast} \\cdot x + b^{\\ast})\n\\\\]\n\n\n--------------\n\n### 3. 线性可分支持向量机的对偶算法\n\n首先由最优化问题\\\\(式(9) \\sim 式(10)\\\\)，构建拉格朗日函数，引入拉格朗日乘子\\\\( \\alpha\\_i \\geq 0, i=1,2,...,n \\\\)\n\\\\[\nL(w,b,\\alpha) = \\frac{1}{2} \\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_i y\\_i (w \\cdot x\\_i + b) + \\sum\\_{i=i}^n \\alpha\\_i\n\\\\]\n其中\\\\( \\alpha = (\\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n)^T \\\\)。\n\n根据拉格朗日对偶性，原始问题：\n\\\\[\n\\mathop{\\min\\_{w,b} \\max\\_\\alpha} L(w,b,\\alpha)\n\\\\]\n**[这个地方有点绕，注意内部的极大后等价与原约束问题，理解拉格朗日对偶性]**\n\n对偶问题：\n\\\\[\n\\mathop{\\max\\_\\alpha \\min\\_{w,b}} L(w,b,\\alpha)\n\\\\]\n\n#### 求解对偶问题\n\n（1）求 \\\\( \\min\\_{w,b} L(w,b,\\alpha)\\\\)\n\n插播二范数的求导公式如下：\n\\\\[\n\\frac{\\partial(\\|\\| w \\|\\|^2)}{\\partial x} = \\frac{\\partial(w^T w)}{\\partial x} = 2w \\\\\\\n\\frac{\\partial(y^Tx)}{\\partial x} = y \\\\\\\n\\frac{\\partial(x^TAx)}{\\partial x} = (A+A^T)x\n\\\\]\n\n将拉格朗日函数\\\\(L(w,b,\\alpha)\\\\)分别对\\\\(w,b\\\\)求偏导数并令其等于0.\n\\\\[\n\\nabla\\_w L(w,b,\\alpha) = w - \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i = 0\n\\\\]\n\\\\[\n\\nabla\\_b L(w,b,\\alpha) = - \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n得\n\\\\[\nw = \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i\n\\\\]\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n将式(21)代入拉格朗日函数(15)，并利用式(22)，即得\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\alpha) \n& = \\frac{1}{2} \\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_iy\\_i(w x\\_i+b) + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^Tw - \\sum\\_{i=1}^n \\alpha\\_iy\\_iwx\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i b + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^T \\sum\\_{i=1}^n  \\alpha\\_i y\\_ix\\_i - \\sum\\_{i=1}^n \\alpha\\_iy\\_iwx\\_i - \\sum\\_{i=1}^n \\alpha\\_iy\\_ib + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_ix\\_i - w^T \\sum\\_{i=1}^n \\alpha\\_iy\\_ix\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_ib + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i b + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} (\\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i)^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\alpha\\_i y\\_i (x\\_i)^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i y\\_i (x\\_i)^T \\alpha\\_j y\\_j x\\_j - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\end{aligned}\n\\end{equation}\n\\\\]\n倒数第5步推导倒数第4步利用了线性代数的转置运算，由于\\\\(\\alpha\\_i\\\\)和\\\\(y\\_i\\\\)都是实数，因此转置后与自身一样。\n倒数第4步到倒数第3步使用了\\\\( (a+b+c+...)(a+b+c+...)=aa+ab+ac+ba+bb+bc+... \\\\)的乘法运算规则。\n\n即：\n\\\\[\n\\mathop{\\min\\_{w,b}}L(w,b,\\alpha) = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\n（2）求\\\\( \\min\\_{w,b} l(w,b,\\alpha) \\\\)对\\\\(\\alpha\\\\)的极大，即是对偶问题：\n\\\\[\n\\mathop{\\max\\_\\alpha} - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,..,n\n\\\\]\n将式(25)的目标函数由求极大转化为求极小，就得到下面与之等价的对偶最优化问题：\n\\\\[\n\\mathop{\\min\\_\\alpha} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,..,n\n\\\\]\n\\\\(式(28) \\sim 式(30)\\\\)要解决的是在参数\\\\(( \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n )^T\\\\)上求极值的问题，我们需要利用序列最小最优化(SMO)算法解决。\n限于篇幅及知识递进程度，具体SMO算法在[Support Vector Machines - Part 3](/2018/03/svm-3/)详细描述。\n\n（3）求参数\\\\(w^{\\ast}, b^{\\ast}\\\\)\n\n这样假设已经求出了\\\\( \\alpha^{\\ast}=(\\alpha\\_1^{\\ast}, \\alpha\\_2^{\\ast}, ..., \\alpha\\_n^{\\ast})^T \\\\)后，从而根据\\\\(KKT\\\\)条件得：\n\\\\[\nw^{\\ast} = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i\n\\\\]\n在\\\\(y=-1, y=1\\\\)的类别中，支持向量处于边界上，根据\\\\(KKT\\\\)条件\\\\( \\alpha\\_i^{\\ast}(y\\_i(w^{\\ast}x\\_i+b)-1)=0 \\\\)，\n至少有一个\\\\(\\alpha\\_j^{\\ast}>0\\\\)[反证法，假设\\\\(\\alpha^{\\ast}=0\\\\)，则\\\\(w^{\\ast}=0\\\\)，而\\\\(w^{\\ast}=0\\\\)不是原始最优化问题的解，产生矛盾！]，\n所以，对此\\\\(j\\\\)有\n\\\\[\n\\begin{cases}\ny\\_j(w^{\\ast}x\\_j+b^{\\ast})-1=0 \\\\\\\ny\\_j^2 = 1 \n\\end{cases}\n\\\\\\ \\Downarrow\n\\\\]\n\\\\[\nb^{\\ast} = y\\_j - \\sum \\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i (x\\_i^T \\cdot x\\_j)\n\\\\]\n\n即可求出\\\\(w^{\\ast},b^{\\ast}\\\\)，最终得出分离超平面\n\\\\[\nw^{\\ast} x + b^{\\ast} = 0\n\\\\]\n分类决策函数\n\\\\[\nf(x) = sign(w^{\\ast}\\cdot x + b^{\\ast})\n\\\\]\n\n\n--------------\n\n### 4. 特点\n\nSVM 函数间隔(functional margin)为\\\\( \\hat{\\gamma}=y(wx+b)=yf(x) \\\\)，其中的\\\\(y\\\\)是只取1和-1吗？\\\\(y\\\\)的唯一作用就是确保函数间隔(functional margin)的非负性？\n\n（1）二分类问题中，\\\\(y\\\\)只取两个值，而且这两个值是可以任意取的；\n\n（2）求解的超平面分开的两侧的函数值的符号是刚好相反的；\n\n（3）为了问题简单化，取了\\\\(y\\\\)的值为\\\\(1\\\\)和\\\\(-1\\\\)。\n\n--------------\n\n在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)，支持向量是使约束条件\\\\( y\\_i(w\\cdot x\\_i +b) -1=0 \\\\)成立的点，如下图\\\\(H\\_1和H\\_2\\\\)上的点。\n\n![supportvector](/posts_res/2018-03-28-svm/1-supportvector.png)\n\n在决定分离超平面时，只有支持向量起作用，其他样本点并不起作用。如果移动支持向量，将会改变分离超平面，移动其他样本点则无影响。支持向量一般个数较少。\n\n-----------------\n\n为什么令函数间隔为1？\n\n函数间隔可以表征样本被分到某一类的置信度，比如说\\\\(y\\_i=+1\\\\)时，如果\\\\(f(x\\_i) = w \\cdot x\\_i +b >0\\\\)且很大，说明\\\\((x\\_i,y\\_i)\\\\)离分类边界很远，我们有理由相信\\\\(x\\_i\\\\)是正类。\n\n> 另外我们知道成比例改变\\\\(w,b\\\\)，分离超平面不变，几何间隔也不改变\n\n因此可以做变量替换，将最优化问题改变为函数间隔为1。这样，在不影响最优化问题的前提下，改变了最优化函数并简化了计算。\n\n\n-------------\n\n### 参考\n\n> July - [支持向量机通俗导论](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n> 李航 - 《统计学习方法》\n\n> [SVM推导过程中的三个疑问](http://whatbeg.com/2017/04/13/svmlearning.html#)\n","source":"_posts/2018-03-28-svm-1.md","raw":"---\nlayout: post\ntitle: Support Vector Machines - Part 1\ndate: 2018-03-28 22:11 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>支持向量机 - SVM（Support Vector Machines）Part 1</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 基本概念\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n* 特点\n\n\n--------------\n\n### 1. 基本概念\n\n定义函数间隔(用\\\\(\\hat{\\gamma}\\\\)表示)为：\n\\\\[\n\\hat{\\gamma} = y(w x + b) = yf(x)\n\\\\]\n超平面\\\\((w,b)\\\\)关于训练数据集\\\\(T\\\\)中所有样本点\\\\( (x\\_i, y\\_i) \\\\)的函数间隔最小值，即为超平面\\\\( (w,b) \\\\)关于训练数据集\\\\(T\\\\)的函数间隔：\n\\\\[\n\\hat{\\gamma} = min \\hat{\\gamma\\_i}, \\quad i=1,2,...,n\n\\\\]\n其中\\\\(n\\\\)表示样本点数量。**[问题：将\\\\(w\\\\)和\\\\(b\\\\)成比例改变为\\\\(\\lambda w\\\\)和\\\\(\\lambda b\\\\)，函数间隔的值也会变化为原来的\\\\(\\lambda\\\\)倍]**，因此自然地引出几何间隔的概念。\n\n-----------\n\n定义几何间隔(用\\\\(\\gamma\\\\)表示)为：\n\\\\[\n\\gamma = \\frac{y(wx+b)}{\\|\\|w\\|\\|} = \\frac{yf(x)}{\\|\\|w\\|\\|} \\\\\\\n\\\\]\n其中\\\\(\\|\\|w\\|\\|\\\\)为\\\\(w\\\\)的\\\\(L\\_2\\\\)范数。 \n\n由上可知，\n\\\\[\n\\gamma = \\frac{\\hat{\\gamma}}{\\|\\|w\\|\\|}\n\\\\]\n即，几何间隔等于函数间隔与\\\\(w\\\\)二阶范数的商。\n\n---------\n\n回到原始的问题中，我们需要最大间隔(几何间隔)分离超平面，即：\n\\\\[\n\\mathop{\\max\\_{w,b}} \\quad \\gamma\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(\\frac{w}{\\|\\|w\\|\\|}x\\_i+\\frac{b}{\\|\\|w\\|\\|}) \\geq \\gamma, \\quad i=1,2,...,n\n\\\\]\n\n考虑几何间隔和函数间隔的关系，可以将上面的问题改写成：\n\\\\[\n\\mathop{\\max\\_{w,b}} \\quad \\frac{\\hat{\\gamma}}{\\|\\|w\\|\\|}\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i+ b) \\geq \\hat{\\gamma}, \\quad i=1,2,...,n\n\\\\]\n\n由于函数间隔\\\\( \\hat{\\gamma} \\\\)的取值并不影响最优化问题的解**[可以认为函数间隔通过\\\\(\\lambda\\\\)放缩后变为1，这样最优化问题中含有一个常数因子]**，\n这样令函数间隔\\\\(\\hat{\\gamma}=1\\\\)代入上面的最优化问题，同时，\\\\( \\max \\frac{\\hat{\\lambda}}{\\|\\|w\\|\\|} \\\\)、\\\\( \\max \\frac{1}{\\|\\| \\lambda w \\|\\|} \\\\)与\\\\( \\min \\frac{1}{2} \\|\\|w\\|\\|^2 \\\\)是等价的。\n所以上述最优化问题最终可以归结为以下最优化问题：\n\\\\[\n\\mathop{\\min\\_{w,b}} \\frac{1}{2}\\|\\|w\\|\\|^2\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i + b) - 1 \\geq 0, \\quad i=1,2,...,n\n\\\\]\n\n现在目标函数和约束条件都是连续可微的凸函数，所以可以通过[拉格朗日对偶性](/2018/03/lagrange/)，通过求解与对偶问题等价的对偶问题得到原始问题的最优解，\n这样的优点：一、对偶问题往往更容易求解；二、自然的引入核函数，推广到非线性分类问题。\n\n### 2. 线性可分支持向量机学习算法 - 最大间隔法\n\n输入：线性可分训练数据集\\\\(T=\\lbrace(x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n)\\rbrace \\\\)，其中\\\\( x\\_i \\in \\chi=R^n, y\\_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace, i=1,2,...,n\\\\)\n\n输出：最大间隔分离超平面和分类决策函数\n\n（1）构造并求解约束最优化问题\n\\\\[\n\\mathop{\\min\\_{w,b}} \\frac{1}{2} \\|\\|w\\|\\|^2\n\\\\]\n\\\\[\ns.t. \\quad y\\_i(w \\cdot x\\_i +b) -1 \\geq 0, \\quad i=1,2,..,n\n\\\\]\n求的最优解\\\\( w^{\\ast},b^{\\ast} \\\\)。\n\n（2）由此得到分离超平面：\n\\\\[\nw^{\\ast} \\cdot x + b^{\\ast} = 0\n\\\\]\n分类决策函数\n\\\\[\nf(x) =  sign(w^{\\ast} \\cdot x + b^{\\ast})\n\\\\]\n\n\n--------------\n\n### 3. 线性可分支持向量机的对偶算法\n\n首先由最优化问题\\\\(式(9) \\sim 式(10)\\\\)，构建拉格朗日函数，引入拉格朗日乘子\\\\( \\alpha\\_i \\geq 0, i=1,2,...,n \\\\)\n\\\\[\nL(w,b,\\alpha) = \\frac{1}{2} \\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_i y\\_i (w \\cdot x\\_i + b) + \\sum\\_{i=i}^n \\alpha\\_i\n\\\\]\n其中\\\\( \\alpha = (\\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n)^T \\\\)。\n\n根据拉格朗日对偶性，原始问题：\n\\\\[\n\\mathop{\\min\\_{w,b} \\max\\_\\alpha} L(w,b,\\alpha)\n\\\\]\n**[这个地方有点绕，注意内部的极大后等价与原约束问题，理解拉格朗日对偶性]**\n\n对偶问题：\n\\\\[\n\\mathop{\\max\\_\\alpha \\min\\_{w,b}} L(w,b,\\alpha)\n\\\\]\n\n#### 求解对偶问题\n\n（1）求 \\\\( \\min\\_{w,b} L(w,b,\\alpha)\\\\)\n\n插播二范数的求导公式如下：\n\\\\[\n\\frac{\\partial(\\|\\| w \\|\\|^2)}{\\partial x} = \\frac{\\partial(w^T w)}{\\partial x} = 2w \\\\\\\n\\frac{\\partial(y^Tx)}{\\partial x} = y \\\\\\\n\\frac{\\partial(x^TAx)}{\\partial x} = (A+A^T)x\n\\\\]\n\n将拉格朗日函数\\\\(L(w,b,\\alpha)\\\\)分别对\\\\(w,b\\\\)求偏导数并令其等于0.\n\\\\[\n\\nabla\\_w L(w,b,\\alpha) = w - \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i = 0\n\\\\]\n\\\\[\n\\nabla\\_b L(w,b,\\alpha) = - \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n得\n\\\\[\nw = \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i\n\\\\]\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n将式(21)代入拉格朗日函数(15)，并利用式(22)，即得\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\alpha) \n& = \\frac{1}{2} \\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_iy\\_i(w x\\_i+b) + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^Tw - \\sum\\_{i=1}^n \\alpha\\_iy\\_iwx\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i b + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^T \\sum\\_{i=1}^n  \\alpha\\_i y\\_ix\\_i - \\sum\\_{i=1}^n \\alpha\\_iy\\_iwx\\_i - \\sum\\_{i=1}^n \\alpha\\_iy\\_ib + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = \\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_ix\\_i - w^T \\sum\\_{i=1}^n \\alpha\\_iy\\_ix\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_ib + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i b + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} w^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} (\\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i)^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\alpha\\_i y\\_i (x\\_i)^T \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i y\\_i (x\\_i)^T \\alpha\\_j y\\_j x\\_j - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = -\\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j - b \\sum\\_{i=1}^n \\alpha\\_i y\\_i + \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\n& = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\end{aligned}\n\\end{equation}\n\\\\]\n倒数第5步推导倒数第4步利用了线性代数的转置运算，由于\\\\(\\alpha\\_i\\\\)和\\\\(y\\_i\\\\)都是实数，因此转置后与自身一样。\n倒数第4步到倒数第3步使用了\\\\( (a+b+c+...)(a+b+c+...)=aa+ab+ac+ba+bb+bc+... \\\\)的乘法运算规则。\n\n即：\n\\\\[\n\\mathop{\\min\\_{w,b}}L(w,b,\\alpha) = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\n（2）求\\\\( \\min\\_{w,b} l(w,b,\\alpha) \\\\)对\\\\(\\alpha\\\\)的极大，即是对偶问题：\n\\\\[\n\\mathop{\\max\\_\\alpha} - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,..,n\n\\\\]\n将式(25)的目标函数由求极大转化为求极小，就得到下面与之等价的对偶最优化问题：\n\\\\[\n\\mathop{\\min\\_\\alpha} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad \\alpha\\_i \\geq 0, \\quad i=1,2,..,n\n\\\\]\n\\\\(式(28) \\sim 式(30)\\\\)要解决的是在参数\\\\(( \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n )^T\\\\)上求极值的问题，我们需要利用序列最小最优化(SMO)算法解决。\n限于篇幅及知识递进程度，具体SMO算法在[Support Vector Machines - Part 3](/2018/03/svm-3/)详细描述。\n\n（3）求参数\\\\(w^{\\ast}, b^{\\ast}\\\\)\n\n这样假设已经求出了\\\\( \\alpha^{\\ast}=(\\alpha\\_1^{\\ast}, \\alpha\\_2^{\\ast}, ..., \\alpha\\_n^{\\ast})^T \\\\)后，从而根据\\\\(KKT\\\\)条件得：\n\\\\[\nw^{\\ast} = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i\n\\\\]\n在\\\\(y=-1, y=1\\\\)的类别中，支持向量处于边界上，根据\\\\(KKT\\\\)条件\\\\( \\alpha\\_i^{\\ast}(y\\_i(w^{\\ast}x\\_i+b)-1)=0 \\\\)，\n至少有一个\\\\(\\alpha\\_j^{\\ast}>0\\\\)[反证法，假设\\\\(\\alpha^{\\ast}=0\\\\)，则\\\\(w^{\\ast}=0\\\\)，而\\\\(w^{\\ast}=0\\\\)不是原始最优化问题的解，产生矛盾！]，\n所以，对此\\\\(j\\\\)有\n\\\\[\n\\begin{cases}\ny\\_j(w^{\\ast}x\\_j+b^{\\ast})-1=0 \\\\\\\ny\\_j^2 = 1 \n\\end{cases}\n\\\\\\ \\Downarrow\n\\\\]\n\\\\[\nb^{\\ast} = y\\_j - \\sum \\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i (x\\_i^T \\cdot x\\_j)\n\\\\]\n\n即可求出\\\\(w^{\\ast},b^{\\ast}\\\\)，最终得出分离超平面\n\\\\[\nw^{\\ast} x + b^{\\ast} = 0\n\\\\]\n分类决策函数\n\\\\[\nf(x) = sign(w^{\\ast}\\cdot x + b^{\\ast})\n\\\\]\n\n\n--------------\n\n### 4. 特点\n\nSVM 函数间隔(functional margin)为\\\\( \\hat{\\gamma}=y(wx+b)=yf(x) \\\\)，其中的\\\\(y\\\\)是只取1和-1吗？\\\\(y\\\\)的唯一作用就是确保函数间隔(functional margin)的非负性？\n\n（1）二分类问题中，\\\\(y\\\\)只取两个值，而且这两个值是可以任意取的；\n\n（2）求解的超平面分开的两侧的函数值的符号是刚好相反的；\n\n（3）为了问题简单化，取了\\\\(y\\\\)的值为\\\\(1\\\\)和\\\\(-1\\\\)。\n\n--------------\n\n在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)，支持向量是使约束条件\\\\( y\\_i(w\\cdot x\\_i +b) -1=0 \\\\)成立的点，如下图\\\\(H\\_1和H\\_2\\\\)上的点。\n\n![supportvector](/posts_res/2018-03-28-svm/1-supportvector.png)\n\n在决定分离超平面时，只有支持向量起作用，其他样本点并不起作用。如果移动支持向量，将会改变分离超平面，移动其他样本点则无影响。支持向量一般个数较少。\n\n-----------------\n\n为什么令函数间隔为1？\n\n函数间隔可以表征样本被分到某一类的置信度，比如说\\\\(y\\_i=+1\\\\)时，如果\\\\(f(x\\_i) = w \\cdot x\\_i +b >0\\\\)且很大，说明\\\\((x\\_i,y\\_i)\\\\)离分类边界很远，我们有理由相信\\\\(x\\_i\\\\)是正类。\n\n> 另外我们知道成比例改变\\\\(w,b\\\\)，分离超平面不变，几何间隔也不改变\n\n因此可以做变量替换，将最优化问题改变为函数间隔为1。这样，在不影响最优化问题的前提下，改变了最优化函数并简化了计算。\n\n\n-------------\n\n### 参考\n\n> July - [支持向量机通俗导论](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n> 李航 - 《统计学习方法》\n\n> [SVM推导过程中的三个疑问](http://whatbeg.com/2017/04/13/svmlearning.html#)\n","slug":"svm-1","published":1,"updated":"2019-08-17T09:33:30.856Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnva00122qwp5ydio618","content":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-1\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-1\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 1\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 1</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul><a id=\"more\"></a>\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>基本概念</li>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n<li>特点</li>\n</ul>\n<hr>\n<h3 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1. 基本概念\"></a>1. 基本概念</h3><p>定义函数间隔(用\\(\\hat{\\gamma}\\)表示)为：\n\\[\n\\hat{\\gamma} = y(w x + b) = yf(x)\n\\]\n超平面\\((w,b)\\)关于训练数据集\\(T\\)中所有样本点\\( (x_i, y_i) \\)的函数间隔最小值，即为超平面\\( (w,b) \\)关于训练数据集\\(T\\)的函数间隔：\n\\[\n\\hat{\\gamma} = min \\hat{\\gamma_i}, \\quad i=1,2,…,n\n\\]\n其中\\(n\\)表示样本点数量。<strong>[问题：将\\(w\\)和\\(b\\)成比例改变为\\(\\lambda w\\)和\\(\\lambda b\\)，函数间隔的值也会变化为原来的\\(\\lambda\\)倍]</strong>，因此自然地引出几何间隔的概念。</p>\n<hr>\n<p>定义几何间隔(用\\(\\gamma\\)表示)为：\n\\[\n\\gamma = \\frac{y(wx+b)}{||w||} = \\frac{yf(x)}{||w||} \\\\\n\\]\n其中\\(||w||\\)为\\(w\\)的\\(L_2\\)范数。 </p>\n<p>由上可知，\n\\[\n\\gamma = \\frac{\\hat{\\gamma}}{||w||}\n\\]\n即，几何间隔等于函数间隔与\\(w\\)二阶范数的商。</p>\n<hr>\n<p>回到原始的问题中，我们需要最大间隔(几何间隔)分离超平面，即：\n\\[\n\\mathop{\\max_{w,b}} \\quad \\gamma\n\\]\n\\[\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\geq \\gamma, \\quad i=1,2,…,n\n\\]</p>\n<p>考虑几何间隔和函数间隔的关系，可以将上面的问题改写成：\n\\[\n\\mathop{\\max_{w,b}} \\quad \\frac{\\hat{\\gamma}}{||w||}\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i+ b) \\geq \\hat{\\gamma}, \\quad i=1,2,…,n\n\\]</p>\n<p>由于函数间隔\\( \\hat{\\gamma} \\)的取值并不影响最优化问题的解<strong>[可以认为函数间隔通过\\(\\lambda\\)放缩后变为1，这样最优化问题中含有一个常数因子]</strong>，\n这样令函数间隔\\(\\hat{\\gamma}=1\\)代入上面的最优化问题，同时，\\( \\max \\frac{\\hat{\\lambda}}{||w||} \\)、\\( \\max \\frac{1}{|| \\lambda w ||} \\)与\\( \\min \\frac{1}{2} ||w||^2 \\)是等价的。\n所以上述最优化问题最终可以归结为以下最优化问题：\n\\[\n\\mathop{\\min_{w,b}} \\frac{1}{2}||w||^2\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i + b) - 1 \\geq 0, \\quad i=1,2,…,n\n\\]</p>\n<p>现在目标函数和约束条件都是连续可微的凸函数，所以可以通过<a href=\"/2018/03/lagrange/\">拉格朗日对偶性</a>，通过求解与对偶问题等价的对偶问题得到原始问题的最优解，\n这样的优点：一、对偶问题往往更容易求解；二、自然的引入核函数，推广到非线性分类问题。</p>\n<h3 id=\"2-线性可分支持向量机学习算法-最大间隔法\"><a href=\"#2-线性可分支持向量机学习算法-最大间隔法\" class=\"headerlink\" title=\"2. 线性可分支持向量机学习算法 - 最大间隔法\"></a>2. 线性可分支持向量机学习算法 - 最大间隔法</h3><p>输入：线性可分训练数据集\\(T=\\lbrace(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\\rbrace \\)，其中\\( x_i \\in \\chi=R^n, y_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace, i=1,2,…,n\\)</p>\n<p>输出：最大间隔分离超平面和分类决策函数</p>\n<p>（1）构造并求解约束最优化问题\n\\[\n\\mathop{\\min_{w,b}} \\frac{1}{2} ||w||^2\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i +b) -1 \\geq 0, \\quad i=1,2,..,n\n\\]\n求的最优解\\( w^{\\ast},b^{\\ast} \\)。</p>\n<p>（2）由此得到分离超平面：\n\\[\nw^{\\ast} \\cdot x + b^{\\ast} = 0\n\\]\n分类决策函数\n\\[\nf(x) =  sign(w^{\\ast} \\cdot x + b^{\\ast})\n\\]</p>\n<hr>\n<h3 id=\"3-线性可分支持向量机的对偶算法\"><a href=\"#3-线性可分支持向量机的对偶算法\" class=\"headerlink\" title=\"3. 线性可分支持向量机的对偶算法\"></a>3. 线性可分支持向量机的对偶算法</h3><p>首先由最优化问题\\(式(9) \\sim 式(10)\\)，构建拉格朗日函数，引入拉格朗日乘子\\( \\alpha_i \\geq 0, i=1,2,…,n \\)\n\\[\nL(w,b,\\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^n \\alpha_i y_i (w \\cdot x_i + b) + \\sum_{i=i}^n \\alpha_i\n\\]\n其中\\( \\alpha = (\\alpha_1, \\alpha_2, …, \\alpha_n)^T \\)。</p>\n<p>根据拉格朗日对偶性，原始问题：\n\\[\n\\mathop{\\min_{w,b} \\max_\\alpha} L(w,b,\\alpha)\n\\]\n<strong>[这个地方有点绕，注意内部的极大后等价与原约束问题，理解拉格朗日对偶性]</strong></p>\n<p>对偶问题：\n\\[\n\\mathop{\\max_\\alpha \\min_{w,b}} L(w,b,\\alpha)\n\\]</p>\n<h4 id=\"求解对偶问题\"><a href=\"#求解对偶问题\" class=\"headerlink\" title=\"求解对偶问题\"></a>求解对偶问题</h4><p>（1）求 \\( \\min_{w,b} L(w,b,\\alpha)\\)</p>\n<p>插播二范数的求导公式如下：\n\\[\n\\frac{\\partial(|| w ||^2)}{\\partial x} = \\frac{\\partial(w^T w)}{\\partial x} = 2w \\\\\n\\frac{\\partial(y^Tx)}{\\partial x} = y \\\\\n\\frac{\\partial(x^TAx)}{\\partial x} = (A+A^T)x\n\\]</p>\n<p>将拉格朗日函数\\(L(w,b,\\alpha)\\)分别对\\(w,b\\)求偏导数并令其等于0.\n\\[\n\\nabla_w L(w,b,\\alpha) = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0\n\\]\n\\[\n\\nabla_b L(w,b,\\alpha) = - \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n得\n\\[\nw = \\sum_{i=1}^n \\alpha_i y_i x_i\n\\]\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n将式(21)代入拉格朗日函数(15)，并利用式(22)，即得\n\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\alpha) \n&amp; = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^n \\alpha_iy_i(w x_i+b) + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^Tw - \\sum_{i=1}^n \\alpha_iy_iwx_i - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^T \\sum_{i=1}^n  \\alpha_i y_ix_i - \\sum_{i=1}^n \\alpha_iy_iwx_i - \\sum_{i=1}^n \\alpha_iy_ib + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_ix_i - w^T \\sum_{i=1}^n \\alpha_iy_ix_i - \\sum_{i=1}^n \\alpha_i y_ib + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_i x_i - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} (\\sum_{i=1}^n \\alpha_i y_i x_i)^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\alpha_i y_i (x_i)^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i y_i (x_i)^T \\alpha_j y_j x_j - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\end{aligned}\n\\end{equation}\n\\]\n倒数第5步推导倒数第4步利用了线性代数的转置运算，由于\\(\\alpha_i\\)和\\(y_i\\)都是实数，因此转置后与自身一样。\n倒数第4步到倒数第3步使用了\\( (a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+… \\)的乘法运算规则。</p>\n<p>即：\n\\[\n\\mathop{\\min_{w,b}}L(w,b,\\alpha) = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]</p>\n<p>（2）求\\( \\min_{w,b} l(w,b,\\alpha) \\)对\\(\\alpha\\)的极大，即是对偶问题：\n\\[\n\\mathop{\\max_\\alpha} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad \\alpha_i \\geq 0, \\quad i=1,2,..,n\n\\]\n将式(25)的目标函数由求极大转化为求极小，就得到下面与之等价的对偶最优化问题：\n\\[\n\\mathop{\\min_\\alpha} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad \\alpha_i \\geq 0, \\quad i=1,2,..,n\n\\]\n\\(式(28) \\sim 式(30)\\)要解决的是在参数\\(( \\alpha_1, \\alpha_2, …, \\alpha_n )^T\\)上求极值的问题，我们需要利用序列最小最优化(SMO)算法解决。\n限于篇幅及知识递进程度，具体SMO算法在<a href=\"/2018/03/svm-3/\">Support Vector Machines - Part 3</a>详细描述。</p>\n<p>（3）求参数\\(w^{\\ast}, b^{\\ast}\\)</p>\n<p>这样假设已经求出了\\( \\alpha^{\\ast}=(\\alpha_1^{\\ast}, \\alpha_2^{\\ast}, …, \\alpha_n^{\\ast})^T \\)后，从而根据\\(KKT\\)条件得：\n\\[\nw^{\\ast} = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i\n\\]\n在\\(y=-1, y=1\\)的类别中，支持向量处于边界上，根据\\(KKT\\)条件\\( \\alpha_i^{\\ast}(y_i(w^{\\ast}x_i+b)-1)=0 \\)，\n至少有一个\\(\\alpha_j^{\\ast}&gt;0\\)[反证法，假设\\(\\alpha^{\\ast}=0\\)，则\\(w^{\\ast}=0\\)，而\\(w^{\\ast}=0\\)不是原始最优化问题的解，产生矛盾！]，\n所以，对此\\(j\\)有\n\\[\n\\begin{cases}\ny_j(w^{\\ast}x_j+b^{\\ast})-1=0 \\\\\ny_j^2 = 1 \n\\end{cases}\n\\\\ \\Downarrow\n\\]\n\\[\nb^{\\ast} = y_j - \\sum _{i=1}^n \\alpha_i^{\\ast} y_i (x_i^T \\cdot x_j)\n\\]</p>\n<p>即可求出\\(w^{\\ast},b^{\\ast}\\)，最终得出分离超平面\n\\[\nw^{\\ast} x + b^{\\ast} = 0\n\\]\n分类决策函数\n\\[\nf(x) = sign(w^{\\ast}\\cdot x + b^{\\ast})\n\\]</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><p>SVM 函数间隔(functional margin)为\\( \\hat{\\gamma}=y(wx+b)=yf(x) \\)，其中的\\(y\\)是只取1和-1吗？\\(y\\)的唯一作用就是确保函数间隔(functional margin)的非负性？</p>\n<p>（1）二分类问题中，\\(y\\)只取两个值，而且这两个值是可以任意取的；</p>\n<p>（2）求解的超平面分开的两侧的函数值的符号是刚好相反的；</p>\n<p>（3）为了问题简单化，取了\\(y\\)的值为\\(1\\)和\\(-1\\)。</p>\n<hr>\n<p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)，支持向量是使约束条件\\( y_i(w\\cdot x_i +b) -1=0 \\)成立的点，如下图\\(H_1和H_2\\)上的点。</p>\n<p><img src=\"/posts_res/2018-03-28-svm/1-supportvector.png\" alt=\"supportvector\"></p>\n<p>在决定分离超平面时，只有支持向量起作用，其他样本点并不起作用。如果移动支持向量，将会改变分离超平面，移动其他样本点则无影响。支持向量一般个数较少。</p>\n<hr>\n<p>为什么令函数间隔为1？</p>\n<p>函数间隔可以表征样本被分到某一类的置信度，比如说\\(y_i=+1\\)时，如果\\(f(x_i) = w \\cdot x_i +b &gt;0\\)且很大，说明\\((x_i,y_i)\\)离分类边界很远，我们有理由相信\\(x_i\\)是正类。</p>\n<blockquote>\n<p>另外我们知道成比例改变\\(w,b\\)，分离超平面不变，几何间隔也不改变</p>\n</blockquote>\n<p>因此可以做变量替换，将最优化问题改变为函数间隔为1。这样，在不影响最优化问题的前提下，改变了最优化函数并简化了计算。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>July - <a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">支持向量机通俗导论</a></p>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"http://whatbeg.com/2017/04/13/svmlearning.html#\" target=\"_blank\" rel=\"noopener\">SVM推导过程中的三个疑问</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-1\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-1\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 1\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 1</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul>","more":"\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>基本概念</li>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n<li>特点</li>\n</ul>\n<hr>\n<h3 id=\"1-基本概念\"><a href=\"#1-基本概念\" class=\"headerlink\" title=\"1. 基本概念\"></a>1. 基本概念</h3><p>定义函数间隔(用\\(\\hat{\\gamma}\\)表示)为：\n\\[\n\\hat{\\gamma} = y(w x + b) = yf(x)\n\\]\n超平面\\((w,b)\\)关于训练数据集\\(T\\)中所有样本点\\( (x_i, y_i) \\)的函数间隔最小值，即为超平面\\( (w,b) \\)关于训练数据集\\(T\\)的函数间隔：\n\\[\n\\hat{\\gamma} = min \\hat{\\gamma_i}, \\quad i=1,2,…,n\n\\]\n其中\\(n\\)表示样本点数量。<strong>[问题：将\\(w\\)和\\(b\\)成比例改变为\\(\\lambda w\\)和\\(\\lambda b\\)，函数间隔的值也会变化为原来的\\(\\lambda\\)倍]</strong>，因此自然地引出几何间隔的概念。</p>\n<hr>\n<p>定义几何间隔(用\\(\\gamma\\)表示)为：\n\\[\n\\gamma = \\frac{y(wx+b)}{||w||} = \\frac{yf(x)}{||w||} \\\\\n\\]\n其中\\(||w||\\)为\\(w\\)的\\(L_2\\)范数。 </p>\n<p>由上可知，\n\\[\n\\gamma = \\frac{\\hat{\\gamma}}{||w||}\n\\]\n即，几何间隔等于函数间隔与\\(w\\)二阶范数的商。</p>\n<hr>\n<p>回到原始的问题中，我们需要最大间隔(几何间隔)分离超平面，即：\n\\[\n\\mathop{\\max_{w,b}} \\quad \\gamma\n\\]\n\\[\ns.t. \\quad y_i(\\frac{w}{||w||}x_i+\\frac{b}{||w||}) \\geq \\gamma, \\quad i=1,2,…,n\n\\]</p>\n<p>考虑几何间隔和函数间隔的关系，可以将上面的问题改写成：\n\\[\n\\mathop{\\max_{w,b}} \\quad \\frac{\\hat{\\gamma}}{||w||}\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i+ b) \\geq \\hat{\\gamma}, \\quad i=1,2,…,n\n\\]</p>\n<p>由于函数间隔\\( \\hat{\\gamma} \\)的取值并不影响最优化问题的解<strong>[可以认为函数间隔通过\\(\\lambda\\)放缩后变为1，这样最优化问题中含有一个常数因子]</strong>，\n这样令函数间隔\\(\\hat{\\gamma}=1\\)代入上面的最优化问题，同时，\\( \\max \\frac{\\hat{\\lambda}}{||w||} \\)、\\( \\max \\frac{1}{|| \\lambda w ||} \\)与\\( \\min \\frac{1}{2} ||w||^2 \\)是等价的。\n所以上述最优化问题最终可以归结为以下最优化问题：\n\\[\n\\mathop{\\min_{w,b}} \\frac{1}{2}||w||^2\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i + b) - 1 \\geq 0, \\quad i=1,2,…,n\n\\]</p>\n<p>现在目标函数和约束条件都是连续可微的凸函数，所以可以通过<a href=\"/2018/03/lagrange/\">拉格朗日对偶性</a>，通过求解与对偶问题等价的对偶问题得到原始问题的最优解，\n这样的优点：一、对偶问题往往更容易求解；二、自然的引入核函数，推广到非线性分类问题。</p>\n<h3 id=\"2-线性可分支持向量机学习算法-最大间隔法\"><a href=\"#2-线性可分支持向量机学习算法-最大间隔法\" class=\"headerlink\" title=\"2. 线性可分支持向量机学习算法 - 最大间隔法\"></a>2. 线性可分支持向量机学习算法 - 最大间隔法</h3><p>输入：线性可分训练数据集\\(T=\\lbrace(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\\rbrace \\)，其中\\( x_i \\in \\chi=R^n, y_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace, i=1,2,…,n\\)</p>\n<p>输出：最大间隔分离超平面和分类决策函数</p>\n<p>（1）构造并求解约束最优化问题\n\\[\n\\mathop{\\min_{w,b}} \\frac{1}{2} ||w||^2\n\\]\n\\[\ns.t. \\quad y_i(w \\cdot x_i +b) -1 \\geq 0, \\quad i=1,2,..,n\n\\]\n求的最优解\\( w^{\\ast},b^{\\ast} \\)。</p>\n<p>（2）由此得到分离超平面：\n\\[\nw^{\\ast} \\cdot x + b^{\\ast} = 0\n\\]\n分类决策函数\n\\[\nf(x) =  sign(w^{\\ast} \\cdot x + b^{\\ast})\n\\]</p>\n<hr>\n<h3 id=\"3-线性可分支持向量机的对偶算法\"><a href=\"#3-线性可分支持向量机的对偶算法\" class=\"headerlink\" title=\"3. 线性可分支持向量机的对偶算法\"></a>3. 线性可分支持向量机的对偶算法</h3><p>首先由最优化问题\\(式(9) \\sim 式(10)\\)，构建拉格朗日函数，引入拉格朗日乘子\\( \\alpha_i \\geq 0, i=1,2,…,n \\)\n\\[\nL(w,b,\\alpha) = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^n \\alpha_i y_i (w \\cdot x_i + b) + \\sum_{i=i}^n \\alpha_i\n\\]\n其中\\( \\alpha = (\\alpha_1, \\alpha_2, …, \\alpha_n)^T \\)。</p>\n<p>根据拉格朗日对偶性，原始问题：\n\\[\n\\mathop{\\min_{w,b} \\max_\\alpha} L(w,b,\\alpha)\n\\]\n<strong>[这个地方有点绕，注意内部的极大后等价与原约束问题，理解拉格朗日对偶性]</strong></p>\n<p>对偶问题：\n\\[\n\\mathop{\\max_\\alpha \\min_{w,b}} L(w,b,\\alpha)\n\\]</p>\n<h4 id=\"求解对偶问题\"><a href=\"#求解对偶问题\" class=\"headerlink\" title=\"求解对偶问题\"></a>求解对偶问题</h4><p>（1）求 \\( \\min_{w,b} L(w,b,\\alpha)\\)</p>\n<p>插播二范数的求导公式如下：\n\\[\n\\frac{\\partial(|| w ||^2)}{\\partial x} = \\frac{\\partial(w^T w)}{\\partial x} = 2w \\\\\n\\frac{\\partial(y^Tx)}{\\partial x} = y \\\\\n\\frac{\\partial(x^TAx)}{\\partial x} = (A+A^T)x\n\\]</p>\n<p>将拉格朗日函数\\(L(w,b,\\alpha)\\)分别对\\(w,b\\)求偏导数并令其等于0.\n\\[\n\\nabla_w L(w,b,\\alpha) = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0\n\\]\n\\[\n\\nabla_b L(w,b,\\alpha) = - \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n得\n\\[\nw = \\sum_{i=1}^n \\alpha_i y_i x_i\n\\]\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n将式(21)代入拉格朗日函数(15)，并利用式(22)，即得\n\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\alpha) \n&amp; = \\frac{1}{2} ||w||^2 - \\sum_{i=1}^n \\alpha_iy_i(w x_i+b) + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^Tw - \\sum_{i=1}^n \\alpha_iy_iwx_i - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^T \\sum_{i=1}^n  \\alpha_i y_ix_i - \\sum_{i=1}^n \\alpha_iy_iwx_i - \\sum_{i=1}^n \\alpha_iy_ib + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = \\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_ix_i - w^T \\sum_{i=1}^n \\alpha_iy_ix_i - \\sum_{i=1}^n \\alpha_i y_ib + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_i x_i - \\sum_{i=1}^n \\alpha_i y_i b + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} w^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} (\\sum_{i=1}^n \\alpha_i y_i x_i)^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\alpha_i y_i (x_i)^T \\sum_{i=1}^n \\alpha_i y_i x_i - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i y_i (x_i)^T \\alpha_j y_j x_j - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = -\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j - b \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n&amp; = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\end{aligned}\n\\end{equation}\n\\]\n倒数第5步推导倒数第4步利用了线性代数的转置运算，由于\\(\\alpha_i\\)和\\(y_i\\)都是实数，因此转置后与自身一样。\n倒数第4步到倒数第3步使用了\\( (a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+… \\)的乘法运算规则。</p>\n<p>即：\n\\[\n\\mathop{\\min_{w,b}}L(w,b,\\alpha) = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]</p>\n<p>（2）求\\( \\min_{w,b} l(w,b,\\alpha) \\)对\\(\\alpha\\)的极大，即是对偶问题：\n\\[\n\\mathop{\\max_\\alpha} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad \\alpha_i \\geq 0, \\quad i=1,2,..,n\n\\]\n将式(25)的目标函数由求极大转化为求极小，就得到下面与之等价的对偶最优化问题：\n\\[\n\\mathop{\\min_\\alpha} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad \\alpha_i \\geq 0, \\quad i=1,2,..,n\n\\]\n\\(式(28) \\sim 式(30)\\)要解决的是在参数\\(( \\alpha_1, \\alpha_2, …, \\alpha_n )^T\\)上求极值的问题，我们需要利用序列最小最优化(SMO)算法解决。\n限于篇幅及知识递进程度，具体SMO算法在<a href=\"/2018/03/svm-3/\">Support Vector Machines - Part 3</a>详细描述。</p>\n<p>（3）求参数\\(w^{\\ast}, b^{\\ast}\\)</p>\n<p>这样假设已经求出了\\( \\alpha^{\\ast}=(\\alpha_1^{\\ast}, \\alpha_2^{\\ast}, …, \\alpha_n^{\\ast})^T \\)后，从而根据\\(KKT\\)条件得：\n\\[\nw^{\\ast} = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i\n\\]\n在\\(y=-1, y=1\\)的类别中，支持向量处于边界上，根据\\(KKT\\)条件\\( \\alpha_i^{\\ast}(y_i(w^{\\ast}x_i+b)-1)=0 \\)，\n至少有一个\\(\\alpha_j^{\\ast}&gt;0\\)[反证法，假设\\(\\alpha^{\\ast}=0\\)，则\\(w^{\\ast}=0\\)，而\\(w^{\\ast}=0\\)不是原始最优化问题的解，产生矛盾！]，\n所以，对此\\(j\\)有\n\\[\n\\begin{cases}\ny_j(w^{\\ast}x_j+b^{\\ast})-1=0 \\\\\ny_j^2 = 1 \n\\end{cases}\n\\\\ \\Downarrow\n\\]\n\\[\nb^{\\ast} = y_j - \\sum _{i=1}^n \\alpha_i^{\\ast} y_i (x_i^T \\cdot x_j)\n\\]</p>\n<p>即可求出\\(w^{\\ast},b^{\\ast}\\)，最终得出分离超平面\n\\[\nw^{\\ast} x + b^{\\ast} = 0\n\\]\n分类决策函数\n\\[\nf(x) = sign(w^{\\ast}\\cdot x + b^{\\ast})\n\\]</p>\n<hr>\n<h3 id=\"4-特点\"><a href=\"#4-特点\" class=\"headerlink\" title=\"4. 特点\"></a>4. 特点</h3><p>SVM 函数间隔(functional margin)为\\( \\hat{\\gamma}=y(wx+b)=yf(x) \\)，其中的\\(y\\)是只取1和-1吗？\\(y\\)的唯一作用就是确保函数间隔(functional margin)的非负性？</p>\n<p>（1）二分类问题中，\\(y\\)只取两个值，而且这两个值是可以任意取的；</p>\n<p>（2）求解的超平面分开的两侧的函数值的符号是刚好相反的；</p>\n<p>（3）为了问题简单化，取了\\(y\\)的值为\\(1\\)和\\(-1\\)。</p>\n<hr>\n<p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(support vector)，支持向量是使约束条件\\( y_i(w\\cdot x_i +b) -1=0 \\)成立的点，如下图\\(H_1和H_2\\)上的点。</p>\n<p><img src=\"/posts_res/2018-03-28-svm/1-supportvector.png\" alt=\"supportvector\"></p>\n<p>在决定分离超平面时，只有支持向量起作用，其他样本点并不起作用。如果移动支持向量，将会改变分离超平面，移动其他样本点则无影响。支持向量一般个数较少。</p>\n<hr>\n<p>为什么令函数间隔为1？</p>\n<p>函数间隔可以表征样本被分到某一类的置信度，比如说\\(y_i=+1\\)时，如果\\(f(x_i) = w \\cdot x_i +b &gt;0\\)且很大，说明\\((x_i,y_i)\\)离分类边界很远，我们有理由相信\\(x_i\\)是正类。</p>\n<blockquote>\n<p>另外我们知道成比例改变\\(w,b\\)，分离超平面不变，几何间隔也不改变</p>\n</blockquote>\n<p>因此可以做变量替换，将最优化问题改变为函数间隔为1。这样，在不影响最优化问题的前提下，改变了最优化函数并简化了计算。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>July - <a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">支持向量机通俗导论</a></p>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"http://whatbeg.com/2017/04/13/svmlearning.html#\" target=\"_blank\" rel=\"noopener\">SVM推导过程中的三个疑问</a></p>\n</blockquote>\n"},{"layout":"post","title":"Support Vector Machines - Part 2","date":"2018-03-28T14:12:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>支持向量机 - SVM（Support Vector Machines）Part 2</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n\n--------------\n\n### 1. 线性支持向量机\n\n线性可分支持向量机的学习方法对于线性不可分训练数据是不适用的，因为问题中的不等式约束并不能都成立，所以我们要修改函数间隔。在[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)中的线性可分支持向量机的约束条件为\\\\(y\\_i(w\\cdot x\\_i +b) \\geq 1\\\\)，称为硬间隔最大化；现在我们对每个样本\\\\((x\\_i, y\\_i)\\\\)引入一个松弛变量\\\\(\\xi\\_i \\geq 0\\\\),使得函数间隔加上松弛变量大于等于1，此时的约束条件就变为\n\\\\[\ny\\_i(w\\cdot x\\_i +b) \\geq 1 - \\xi\\_i\n\\\\]\n同时，对每个松弛变量也要支付一个代价\\\\(\\xi\\_i\\\\)，目标函数也由原来的\\\\(\\frac{1}{2} \\|\\| w \\|\\|^2\\\\)变为\n\\\\[\n\\frac{1}{2}\\|\\| w \\|\\|^2 + C\\sum\\_{i=1}^n \\xi\\_i\n\\\\]\n相对于硬间隔最大化，此时称为软间隔最大化。\n\n线性不可分的线性支持向量机的学习问题变为凸二次规划问题(**原始问题**)：\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi}} \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i\n\\\\]\n\\\\[\ns.j. \\quad y\\_i(w \\cdot x\\_i + b) \\geq 1 - \\xi\\_i, \\quad i=1,2,...,n\n\\\\]\n\\\\[\n\\xi\\_i \\geq 0, \\quad, i=1,2,...,n\n\\\\]\n\n**下面求原始问题的对偶问题：**\n\n原始问题的拉格朗日函数是\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\xi,\\alpha,\\mu)\n& = \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i - \\sum\\_{i=1}^n \\alpha\\_i(y\\_i(w \\cdot x\\_i + b)-1+\\xi\\_i) - \\sum\\_{i=1}^n \\mu\\_i \\xi\\_i \\\\\\\n& = \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i (w \\cdot x +b) + \\sum\\_{i=1}^n \\alpha\\_i - \\sum\\_{i=1}^n \\alpha\\_i \\xi\\_i - \\sum\\_{i=1}^n \\mu\\_i \\xi\\_i \\\\\\\ns.t. \\quad \\alpha\\_i \\geq 0, \\quad \\mu\\_i \\geq 0\n\\end{aligned}\n\\end{equation}\n\\\\]\n原始问题为\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi} \\max\\_{\\alpha, \\mu}} L(w,b,\\xi, \\alpha, \\mu)\n\\\\]\n对偶问题为\n\\\\[\n\\mathop{\\max\\_{\\alpha, \\mu} \\min\\_{w,b,\\xi}} L(w,b,\\xi, \\alpha, \\mu)\n\\\\]\n\n因此，先求\\\\(L(w,b,\\xi,\\alpha,\\mu)\\\\)对\\\\(w,b,\\xi\\\\)的极小\n\\\\[\n\\nabla \\_w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i = 0 \\\\\\\n\\nabla \\_b L(w,b,\\xi,\\alpha,\\mu) = - \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n\\nabla\\_{\\xi\\_i} L(w,b,\\xi,\\alpha,\\mu) = C - \\alpha\\_i - \\mu\\_i = 0\n\\\\]\n得\n\\\\[\nw = \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i\n\\\\]\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\nC - \\alpha\\_i - \\mu\\_i = 0 \n\\\\]\n将\\\\( 式(10) \\sim 式(12) \\\\)代入\\\\(式(6)\\\\)，得\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu) = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n再对\\\\(\\mathop{\\min\\_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu)\\\\)求\\\\(\\alpha\\\\)的极大，即得对偶问题\n\\\\[\n\\mathop{\\max\\_{\\alpha}} - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j (x\\_i^T \\cdot x\\_j) + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\nC - \\alpha\\_i - \\mu\\_i = 0 \\\\\\\n\\alpha\\_i \\geq 0 \\\\\\\n\\mu\\_i \\geq 0, \\quad i=1,2,...,n\n\\\\]\n调整化简约束条件，得\n\\\\[\n\\begin{cases}\nC - \\alpha\\_i - \\mu\\_i = 0 \\\\\\\n\\alpha\\_i \\geq 0 \\\\\\\n\\mu\\_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\mu\\_i = C - \\alpha\\_i \\geq 0 \\\\\\\n\\alpha\\_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n0 \\leq \\alpha\\_i \\leq C\n\\\\]\n因此，原始问题的对偶问题为\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j (x\\_i^T \\cdot x\\_j) - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n\n\\\\]\n至此，我们得到一个和线性可分支持向量机对偶问题类似的问题，所以同样需要使用序列最小最优化(SMO)算法解决，详见[Support Vector Machines - Part 3](/2018/03/svm-3/)。\n\n<br />\n\n假设已经求出了\\\\( \\alpha^{\\ast}=(\\alpha\\_1 ^{\\ast}, \\alpha\\_2 ^{\\ast}, ..., \\alpha\\_n ^{\\ast})^T \\\\)是对偶问题的解，\n若存在\\\\(0 \\leq \\alpha\\_i \\leq C\\\\), 则利用\\\\(\\alpha^{\\ast}\\\\)求\\\\(w^{\\ast}\\\\)。\n\n原始问题是凸二次规划问题，解满足\\\\(KKT\\\\)条件，即得(下列公式也见《统计学习方法》page.111)\n\\\\[\n\\nabla \\_w L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = w^{\\ast} - \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i = 0 \\\\\\\n\\nabla \\_b L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = - \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i = 0 \\\\\\\n\\nabla\\_{\\xi\\_i} L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = C - \\alpha\\_i^{\\ast} - \\mu\\_i^{\\ast} = 0\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast} (y\\_i (w^{\\ast} \\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast}) = 0\n\\\\]\n\\\\[\n\\mu\\_i^{\\ast} \\xi\\_i^{\\ast} = (C - \\alpha\\_i^{\\ast}) \\xi\\_i = 0 \\\\\\\ny\\_i (w^{\\ast}\\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast} \\geq 0 \\\\\\\n\\xi\\_i^{\\ast} \\geq 0 \\\\\\\n\\alpha\\_i^{\\ast} \\geq 0 \\\\\\\n\\mu\\_i^{\\ast} \\geq 0, \\quad i=1,2,...,n\n\\\\]\n设我们找到一个支持向量\\\\(\\alpha\\_i^{\\ast} > 0\\\\)，则由\\\\( 式(21)\\\\)可以得到\n\\\\[ \ny\\_i (w^{\\ast}\\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast} = 0 \\Longrightarrow b^{\\ast} = y\\_i - \\xi\\_i^{\\ast} y\\_i - w^{\\ast} \\cdot x\\_i \n\\\\]\n想要求\\\\(b^{\\ast}\\\\)就必须先求\\\\(\\xi\\_i^{\\ast}\\\\)，没办法得到\\\\(\\xi\\_i^{\\ast}\\\\)，但是如果\\\\(\\xi\\_i^{\\ast}=0\\\\)的话，那么\n\\\\[\nb^{\\ast} = y\\_i - w^{\\ast} \\cdot x\\_i\n\\\\]\n所以为了让\\\\(\\xi\\_i^{\\ast}=0\\\\)，那么由\\\\( 式(22) \\\\)可知，\\\\(C - \\alpha\\_i^{\\ast} \\not= 0\\\\)，即\\\\( \\alpha\\_i^{\\ast} \\not= C \\Rightarrow \\alpha\\_i^{\\ast} <C \\\\)。\n\n所以，要求解\\\\( b^{\\ast} \\\\)，则需要找到一个\\\\( 0 < \\alpha\\_i^{\\ast} < C \\\\)，那么相应的\\\\(b^{\\ast} \\\\)就可以用\\\\(式(24)\\\\)进行计算。\n\n<br />\n\n综合上面的内容，可以看到线性支持向量机(软间隔)和线性可分支持向量机(硬间隔)几乎是一样的。\n相对于硬间隔来说，软间隔更灵活，可以通过调节\\\\(C\\\\)的值来控制，关心分隔超平面的间隔更大一些还是分类错误少一些，并且也不要求我们的数据是线性可分的，所以软间隔比硬间隔更加具有实际的应用价值。\n\n对于\\\\(\\alpha\\_i > 0\\\\)的样本点\\\\((x\\_i,y\\_i)\\\\)称为支持向量(软间隔的支持向量)，如下图所示，分离超平面由实线表示，间隔平面用虚线表示。\n* \\\\(0 < \\alpha\\_i < C, 则\\xi\\_i=0, \\quad \\\\)分类正确，支持向量落在间隔边界上[类似线性可分支持向量机]\n* \\\\(\\alpha\\_i = C, 0 < \\xi\\_i < 1, \\quad \\\\)分类正确，支持向量落在间隔边界和分离超平面之间\n* \\\\(\\alpha\\_i = C, \\xi\\_i=1, \\quad \\\\)，支持向量落在分离超平面上\n* \\\\(\\alpha\\_i = C, \\xi\\_i>1, \\quad \\\\)，分类错误，支持向量落在超平面另外一侧。\n\n<center><img src=\"/posts_res/2018-03-28-svm/2-supportvector.png\"></center>\n\n\n------------------\n\n### 2. 核函数及非线性支持向量机\n\n目前为止，SVM还只能处理线性或者近似线性的情况，下面引入核函数，进而将SVM推广到非线性问题上。\n\n对于一个数据点\\\\(x\\\\)分类，实际上就是通过把\\\\(x\\\\)带入到\\\\(f(x)=wx+b\\\\)中算出结果，然后根据结果的正负进行类别的划分，在之前的推导中，我们得到\n\\\\[\nw^{\\ast} = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i\n\\\\]\n因此分类函数为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = (\\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i)^T x + b \\\\\\\n& = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i \\langle x\\_i,x \\rangle + b\n\\end{aligned}\n\\end{equation}\n\\\\]\n其中\\\\( \\langle.,. \\rangle \\\\)表示内积。这里比较有趣的地方是，对于新点\\\\(x\\\\)的预测，只需要计算它与训练数据点的内积即可，**这是使用核函数\\\\(kernel\\\\)进行非线性推广的基本前提**，\n事实上，所有非支持向量的系数\\\\(\\alpha^{\\ast}\\\\)都是等于零的，因此对于新点的内积运算，不需要针对所有的训练数据，实际上只要针对少量的支持向量就可以了。\n\n<br />\n\n为什么非支持向量的系数为零?\n\n直观解释：间隔边界后面的点对分离超平面的确定没有影响，所以这些无关的点不会参与到分类问题的计算中。\n\n公式解释：在线性可分支持向量机(硬间隔)中，我们得到了拉格朗日函数\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\theta(w) \n& = \\mathop{\\max\\_{\\alpha\\_i \\geq 0}} L(w,b,\\alpha) \\\\\\\n& = \\mathop{\\max\\_{\\alpha\\_i \\geq 0}} \\frac{1}{2}\\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_i (y\\_i(wx\\_i +b)-1) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\alpha\\_i \\geq 0, \\quad i=1,2,...,n\n\\\\]\n**Notice:**\n\n如果\\\\( x\\_i \\\\)是支持向量的话，那么上式右面\\\\( y\\_i(wx\\_i +b)-1\\\\)部分是等于零的，因为支持向量的函数间隔等于1；\n而对于非支持向量，函数间隔大于零，所以\\\\( y\\_i(wx\\_i +b)-1\\\\)部分是大于零的，所以为了满足拉格朗日最大化，\\\\(\\alpha\\_i\\\\)必须等于零。\n\n<br />\n\n下面开始正式介绍核函数\\\\(Kernel\\\\)\n\n设\\\\(\\chi\\\\)是输入空间(欧式空间\\\\(R^n\\\\)的子集或离散集合)，又设\\\\(\\mathcal{H}\\\\)为特征空间(希尔伯特空间[完备的内积空间])，如果存在一个从\\\\(\\chi\\\\)到\\\\(\\mathcal{H}\\\\)的映射\n\\\\[\n\\varphi(x): \\chi \\to \\mathcal{H}\n\\\\]\n使得对所有\\\\(x,z \\in \\chi \\\\)，函数\\\\(K(x,z)\\\\)满足条件\n\\\\[\nK(x,z) = \\varphi(x) \\cdot \\varphi(z)\n\\\\]\n则称\\\\(K(x,z)\\\\)为核函数，\\\\(\\varphi(x)\\\\)为映射函数，式中\\\\(\\varphi(x) \\cdot \\varphi(z)\\\\)为\\\\(\\varphi(x)和\\varphi(z)\\\\)的内积。\n\n核函数的想法是，在学习和预测中只定义核函数\\\\(K(x,z)\\\\)，而不显式地定义映射函数\\\\(\\varphi(x)\\\\)。通常直接计算\\\\(K(x,z)\\\\)比较容易，而通过\\\\(\\varphi(x)和\\varphi(z)\\\\)计算\\\\(K(x,z)\\\\)并不容易。\n\n这样，处理非线性数据的情况时，线性支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中学习线性分离超平面，从而把原始空间中不好分的非线性数据分开。\n这样分类决策函数变为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = (\\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i)^T x + b \\\\\\\n& = \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**[其余核函数的举例不再赘述]**\n\n维度爆炸的问题！！\n\n按照核函数的定义[先找映射函数，之后再在高维空间中计算内积]，这样是很容易出现维度爆炸问题的。具体例子如下：\n\n设对向量\\\\( x=(u\\_1, u\\_2)^T \\\\)，映射的结果为\n\\\\[\n\\varphi(x)=u\\_1+u\\_1^2+u\\_2+u\\_2^2+u\\_1u\\_2\n\\\\]\n又由于\\\\( x\\_1=(\\eta\\_1, \\eta\\_2)^T, x\\_2=(\\xi\\_1, \\xi\\_2)^T \\\\)，则\\\\(x\\_1, x\\_2\\\\)的映射如下\n\\\\[\n\\varphi(x\\_1) = (\\eta\\_1, \\eta\\_1^2, \\eta\\_2, \\eta\\_2^2, \\eta\\_1\\eta\\_2) \\\\\\\n\\varphi(x\\_2) = (\\xi\\_1, \\xi\\_2^2, \\xi\\_2, \\xi\\_2^2, \\xi\\_1\\xi\\_2)\n\\\\]\n上面两式做内积运算即可得到\n\\\\[\n\\langle \\varphi(x\\_1), \\varphi(x\\_2) \\rangle = \\eta\\_1 \\xi\\_1 + \\eta\\_1^2 \\xi\\_2^2 + \\eta\\_2 \\xi\\_2 + \\eta\\_2^2 \\xi\\_2^2 + \\eta\\_1\\eta\\_2 \\xi\\_1\\xi\\_2\n\\\\]\n另外，如果我们计算下式\n\\\\[\n(\\langle x\\_1, x\\_2 \\rangle + 1)^2 = 2 \\eta\\_1 \\xi\\_1 + \\eta\\_1^2 \\xi\\_2^2 + \\eta\\_2 \\xi\\_2 + \\eta\\_2^2 \\xi\\_2^2 + 2 \\eta\\_1\\eta\\_2 \\xi\\_1\\xi\\_2 + 1\n\\\\]\n对比\\\\(式(33)和式(34)\\\\)，两者有很多相似的地方。实际上，我们只要把映射的某几个维度线性缩放一下，然后再加上一个常数维度，计算出来的结果和经过映射的两向量内积的结果是相等的。\n\n具体来说，如果对于向量\\\\(x=(a,b)^T\\\\)，设置新映射如下\n\\\\[\n\\phi = (\\sqrt{2}a, a^2, \\sqrt{2}b, b^2, \\sqrt{2}ab, 1)^T\n\\\\]\n则\\\\(式(34)\\\\)即\\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\)的结果和内积\\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)的结果是相等的。现在我们来研究一下二者的区别。\n\n1. \\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\): 直接在原来的低维空间中进行计算，不需要显示的写出映射后的结果\n2. \\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)：先映射到高维空间中，然后再根据内积进行计算\n\n回到刚才提到的维度爆炸，在\\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)的方法不能计算的情况下，另一种方法\\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\)却能从容处理，甚至无穷维度的情况也可以计算。\n这也就是核函数不显式的求映射关系，直接在低维计算，高维学习分离超平面的原因之一。\n\n这样，我们之前学习到的线性支持向量机，可以通过核函数学习非线性数据的情况：\n\\\\[\nf(x) = \\sum\\_{i=1}^n \\alpha\\_i y\\_i \\langle x\\_i, x \\rangle + b \\Longrightarrow f(x)= \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\\\]\n\n\n----------------\n\n### 3. 常用的核函数及其特点：\n\n* 多项式核函数(polynomial kernel function)\n\\\\[\nK(x\\_1, x\\_2) = (\\langle x\\_1, x\\_2 \\rangle + R)^d\n\\\\]\n该空间的维度是\\\\(C\\_{m+d}^d\\\\),其中\\\\(m\\\\)是原始空间的维度。\n\n* 高斯核函数(Gaussian kernel function) / 高斯径向基函数(Radial basis function)\n\\\\[\nK(x\\_1, x\\_2) = exp(- \\frac{\\|\\| x\\_1 - x\\_2 \\|\\|^2}{2\\sigma^2})\n\\\\]\n这个核会将原始空间映射为无穷维空间。不过，如果\\\\(\\sigma\\\\)选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；\n反过来，如果\\\\(\\sigma\\\\)选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。\n不过，总的来说，通过调控参数\\\\(\\sigma\\\\)，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间。\n\n![Gaussian](/posts_res/2018-03-28-svm/2-gaussian.png)\n\n* 线性核函数\n\\\\[\nK(x\\_1, x\\_2) = \\langle x\\_1, x\\_2 \\rangle\n\\\\]\n这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了 (意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板\n或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)\n\n[更多核函数进阶请看这里](http://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html)\n\n**last but not least.**\n\n#### 核函数的本质\n\n1. 实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去；\n2. 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(即维度爆炸)；\n3. 核函数登场，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。\n\n#### 核函数选择问题\n\n需要注意的是需要对数据归一化处理\n\n1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\n2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel\n3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n> [July - 支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n> [学习July博文总结——支持向量机(SVM)的深入理解](https://blog.csdn.net/ajianyingxiaoqinghan/article/details/72897399)\n\n> 周志华 - 《机器学习》\n\n> [\\_席达\\_ - SVM学习笔记](https://blog.csdn.net/robin_xu_shuai/article/details/77051258)\n","source":"_posts/2018-03-28-svm-2.md","raw":"---\nlayout: post\ntitle: Support Vector Machines - Part 2\ndate: 2018-03-28 22:12 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>支持向量机 - SVM（Support Vector Machines）Part 2</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n\n--------------\n\n### 1. 线性支持向量机\n\n线性可分支持向量机的学习方法对于线性不可分训练数据是不适用的，因为问题中的不等式约束并不能都成立，所以我们要修改函数间隔。在[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)中的线性可分支持向量机的约束条件为\\\\(y\\_i(w\\cdot x\\_i +b) \\geq 1\\\\)，称为硬间隔最大化；现在我们对每个样本\\\\((x\\_i, y\\_i)\\\\)引入一个松弛变量\\\\(\\xi\\_i \\geq 0\\\\),使得函数间隔加上松弛变量大于等于1，此时的约束条件就变为\n\\\\[\ny\\_i(w\\cdot x\\_i +b) \\geq 1 - \\xi\\_i\n\\\\]\n同时，对每个松弛变量也要支付一个代价\\\\(\\xi\\_i\\\\)，目标函数也由原来的\\\\(\\frac{1}{2} \\|\\| w \\|\\|^2\\\\)变为\n\\\\[\n\\frac{1}{2}\\|\\| w \\|\\|^2 + C\\sum\\_{i=1}^n \\xi\\_i\n\\\\]\n相对于硬间隔最大化，此时称为软间隔最大化。\n\n线性不可分的线性支持向量机的学习问题变为凸二次规划问题(**原始问题**)：\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi}} \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i\n\\\\]\n\\\\[\ns.j. \\quad y\\_i(w \\cdot x\\_i + b) \\geq 1 - \\xi\\_i, \\quad i=1,2,...,n\n\\\\]\n\\\\[\n\\xi\\_i \\geq 0, \\quad, i=1,2,...,n\n\\\\]\n\n**下面求原始问题的对偶问题：**\n\n原始问题的拉格朗日函数是\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\xi,\\alpha,\\mu)\n& = \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i - \\sum\\_{i=1}^n \\alpha\\_i(y\\_i(w \\cdot x\\_i + b)-1+\\xi\\_i) - \\sum\\_{i=1}^n \\mu\\_i \\xi\\_i \\\\\\\n& = \\frac{1}{2}\\|\\|w\\|\\|^2 + C \\sum\\_{i=1}^n \\xi\\_i - \\sum\\_{i=1}^n \\alpha\\_i y\\_i (w \\cdot x +b) + \\sum\\_{i=1}^n \\alpha\\_i - \\sum\\_{i=1}^n \\alpha\\_i \\xi\\_i - \\sum\\_{i=1}^n \\mu\\_i \\xi\\_i \\\\\\\ns.t. \\quad \\alpha\\_i \\geq 0, \\quad \\mu\\_i \\geq 0\n\\end{aligned}\n\\end{equation}\n\\\\]\n原始问题为\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi} \\max\\_{\\alpha, \\mu}} L(w,b,\\xi, \\alpha, \\mu)\n\\\\]\n对偶问题为\n\\\\[\n\\mathop{\\max\\_{\\alpha, \\mu} \\min\\_{w,b,\\xi}} L(w,b,\\xi, \\alpha, \\mu)\n\\\\]\n\n因此，先求\\\\(L(w,b,\\xi,\\alpha,\\mu)\\\\)对\\\\(w,b,\\xi\\\\)的极小\n\\\\[\n\\nabla \\_w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i = 0 \\\\\\\n\\nabla \\_b L(w,b,\\xi,\\alpha,\\mu) = - \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n\\nabla\\_{\\xi\\_i} L(w,b,\\xi,\\alpha,\\mu) = C - \\alpha\\_i - \\mu\\_i = 0\n\\\\]\n得\n\\\\[\nw = \\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i\n\\\\]\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\nC - \\alpha\\_i - \\mu\\_i = 0 \n\\\\]\n将\\\\( 式(10) \\sim 式(12) \\\\)代入\\\\(式(6)\\\\)，得\n\\\\[\n\\mathop{\\min\\_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu) = - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j  x\\_i^T x\\_j + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n再对\\\\(\\mathop{\\min\\_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu)\\\\)求\\\\(\\alpha\\\\)的极大，即得对偶问题\n\\\\[\n\\mathop{\\max\\_{\\alpha}} - \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j (x\\_i^T \\cdot x\\_j) + \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\nC - \\alpha\\_i - \\mu\\_i = 0 \\\\\\\n\\alpha\\_i \\geq 0 \\\\\\\n\\mu\\_i \\geq 0, \\quad i=1,2,...,n\n\\\\]\n调整化简约束条件，得\n\\\\[\n\\begin{cases}\nC - \\alpha\\_i - \\mu\\_i = 0 \\\\\\\n\\alpha\\_i \\geq 0 \\\\\\\n\\mu\\_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\mu\\_i = C - \\alpha\\_i \\geq 0 \\\\\\\n\\alpha\\_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n0 \\leq \\alpha\\_i \\leq C\n\\\\]\n因此，原始问题的对偶问题为\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j (x\\_i^T \\cdot x\\_j) - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n\n\\\\]\n至此，我们得到一个和线性可分支持向量机对偶问题类似的问题，所以同样需要使用序列最小最优化(SMO)算法解决，详见[Support Vector Machines - Part 3](/2018/03/svm-3/)。\n\n<br />\n\n假设已经求出了\\\\( \\alpha^{\\ast}=(\\alpha\\_1 ^{\\ast}, \\alpha\\_2 ^{\\ast}, ..., \\alpha\\_n ^{\\ast})^T \\\\)是对偶问题的解，\n若存在\\\\(0 \\leq \\alpha\\_i \\leq C\\\\), 则利用\\\\(\\alpha^{\\ast}\\\\)求\\\\(w^{\\ast}\\\\)。\n\n原始问题是凸二次规划问题，解满足\\\\(KKT\\\\)条件，即得(下列公式也见《统计学习方法》page.111)\n\\\\[\n\\nabla \\_w L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = w^{\\ast} - \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i = 0 \\\\\\\n\\nabla \\_b L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = - \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i = 0 \\\\\\\n\\nabla\\_{\\xi\\_i} L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = C - \\alpha\\_i^{\\ast} - \\mu\\_i^{\\ast} = 0\n\\\\]\n\\\\[\n\\alpha\\_i^{\\ast} (y\\_i (w^{\\ast} \\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast}) = 0\n\\\\]\n\\\\[\n\\mu\\_i^{\\ast} \\xi\\_i^{\\ast} = (C - \\alpha\\_i^{\\ast}) \\xi\\_i = 0 \\\\\\\ny\\_i (w^{\\ast}\\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast} \\geq 0 \\\\\\\n\\xi\\_i^{\\ast} \\geq 0 \\\\\\\n\\alpha\\_i^{\\ast} \\geq 0 \\\\\\\n\\mu\\_i^{\\ast} \\geq 0, \\quad i=1,2,...,n\n\\\\]\n设我们找到一个支持向量\\\\(\\alpha\\_i^{\\ast} > 0\\\\)，则由\\\\( 式(21)\\\\)可以得到\n\\\\[ \ny\\_i (w^{\\ast}\\cdot x\\_i + b^{\\ast}) -1 +  \\xi\\_i^{\\ast} = 0 \\Longrightarrow b^{\\ast} = y\\_i - \\xi\\_i^{\\ast} y\\_i - w^{\\ast} \\cdot x\\_i \n\\\\]\n想要求\\\\(b^{\\ast}\\\\)就必须先求\\\\(\\xi\\_i^{\\ast}\\\\)，没办法得到\\\\(\\xi\\_i^{\\ast}\\\\)，但是如果\\\\(\\xi\\_i^{\\ast}=0\\\\)的话，那么\n\\\\[\nb^{\\ast} = y\\_i - w^{\\ast} \\cdot x\\_i\n\\\\]\n所以为了让\\\\(\\xi\\_i^{\\ast}=0\\\\)，那么由\\\\( 式(22) \\\\)可知，\\\\(C - \\alpha\\_i^{\\ast} \\not= 0\\\\)，即\\\\( \\alpha\\_i^{\\ast} \\not= C \\Rightarrow \\alpha\\_i^{\\ast} <C \\\\)。\n\n所以，要求解\\\\( b^{\\ast} \\\\)，则需要找到一个\\\\( 0 < \\alpha\\_i^{\\ast} < C \\\\)，那么相应的\\\\(b^{\\ast} \\\\)就可以用\\\\(式(24)\\\\)进行计算。\n\n<br />\n\n综合上面的内容，可以看到线性支持向量机(软间隔)和线性可分支持向量机(硬间隔)几乎是一样的。\n相对于硬间隔来说，软间隔更灵活，可以通过调节\\\\(C\\\\)的值来控制，关心分隔超平面的间隔更大一些还是分类错误少一些，并且也不要求我们的数据是线性可分的，所以软间隔比硬间隔更加具有实际的应用价值。\n\n对于\\\\(\\alpha\\_i > 0\\\\)的样本点\\\\((x\\_i,y\\_i)\\\\)称为支持向量(软间隔的支持向量)，如下图所示，分离超平面由实线表示，间隔平面用虚线表示。\n* \\\\(0 < \\alpha\\_i < C, 则\\xi\\_i=0, \\quad \\\\)分类正确，支持向量落在间隔边界上[类似线性可分支持向量机]\n* \\\\(\\alpha\\_i = C, 0 < \\xi\\_i < 1, \\quad \\\\)分类正确，支持向量落在间隔边界和分离超平面之间\n* \\\\(\\alpha\\_i = C, \\xi\\_i=1, \\quad \\\\)，支持向量落在分离超平面上\n* \\\\(\\alpha\\_i = C, \\xi\\_i>1, \\quad \\\\)，分类错误，支持向量落在超平面另外一侧。\n\n<center><img src=\"/posts_res/2018-03-28-svm/2-supportvector.png\"></center>\n\n\n------------------\n\n### 2. 核函数及非线性支持向量机\n\n目前为止，SVM还只能处理线性或者近似线性的情况，下面引入核函数，进而将SVM推广到非线性问题上。\n\n对于一个数据点\\\\(x\\\\)分类，实际上就是通过把\\\\(x\\\\)带入到\\\\(f(x)=wx+b\\\\)中算出结果，然后根据结果的正负进行类别的划分，在之前的推导中，我们得到\n\\\\[\nw^{\\ast} = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i\n\\\\]\n因此分类函数为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = (\\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i x\\_i)^T x + b \\\\\\\n& = \\sum\\_{i=1}^n \\alpha\\_i^{\\ast} y\\_i \\langle x\\_i,x \\rangle + b\n\\end{aligned}\n\\end{equation}\n\\\\]\n其中\\\\( \\langle.,. \\rangle \\\\)表示内积。这里比较有趣的地方是，对于新点\\\\(x\\\\)的预测，只需要计算它与训练数据点的内积即可，**这是使用核函数\\\\(kernel\\\\)进行非线性推广的基本前提**，\n事实上，所有非支持向量的系数\\\\(\\alpha^{\\ast}\\\\)都是等于零的，因此对于新点的内积运算，不需要针对所有的训练数据，实际上只要针对少量的支持向量就可以了。\n\n<br />\n\n为什么非支持向量的系数为零?\n\n直观解释：间隔边界后面的点对分离超平面的确定没有影响，所以这些无关的点不会参与到分类问题的计算中。\n\n公式解释：在线性可分支持向量机(硬间隔)中，我们得到了拉格朗日函数\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\theta(w) \n& = \\mathop{\\max\\_{\\alpha\\_i \\geq 0}} L(w,b,\\alpha) \\\\\\\n& = \\mathop{\\max\\_{\\alpha\\_i \\geq 0}} \\frac{1}{2}\\|\\|w\\|\\|^2 - \\sum\\_{i=1}^n \\alpha\\_i (y\\_i(wx\\_i +b)-1) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\alpha\\_i \\geq 0, \\quad i=1,2,...,n\n\\\\]\n**Notice:**\n\n如果\\\\( x\\_i \\\\)是支持向量的话，那么上式右面\\\\( y\\_i(wx\\_i +b)-1\\\\)部分是等于零的，因为支持向量的函数间隔等于1；\n而对于非支持向量，函数间隔大于零，所以\\\\( y\\_i(wx\\_i +b)-1\\\\)部分是大于零的，所以为了满足拉格朗日最大化，\\\\(\\alpha\\_i\\\\)必须等于零。\n\n<br />\n\n下面开始正式介绍核函数\\\\(Kernel\\\\)\n\n设\\\\(\\chi\\\\)是输入空间(欧式空间\\\\(R^n\\\\)的子集或离散集合)，又设\\\\(\\mathcal{H}\\\\)为特征空间(希尔伯特空间[完备的内积空间])，如果存在一个从\\\\(\\chi\\\\)到\\\\(\\mathcal{H}\\\\)的映射\n\\\\[\n\\varphi(x): \\chi \\to \\mathcal{H}\n\\\\]\n使得对所有\\\\(x,z \\in \\chi \\\\)，函数\\\\(K(x,z)\\\\)满足条件\n\\\\[\nK(x,z) = \\varphi(x) \\cdot \\varphi(z)\n\\\\]\n则称\\\\(K(x,z)\\\\)为核函数，\\\\(\\varphi(x)\\\\)为映射函数，式中\\\\(\\varphi(x) \\cdot \\varphi(z)\\\\)为\\\\(\\varphi(x)和\\varphi(z)\\\\)的内积。\n\n核函数的想法是，在学习和预测中只定义核函数\\\\(K(x,z)\\\\)，而不显式地定义映射函数\\\\(\\varphi(x)\\\\)。通常直接计算\\\\(K(x,z)\\\\)比较容易，而通过\\\\(\\varphi(x)和\\varphi(z)\\\\)计算\\\\(K(x,z)\\\\)并不容易。\n\n这样，处理非线性数据的情况时，线性支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中学习线性分离超平面，从而把原始空间中不好分的非线性数据分开。\n这样分类决策函数变为\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n& = (\\sum\\_{i=1}^n \\alpha\\_i y\\_i x\\_i)^T x + b \\\\\\\n& = \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**[其余核函数的举例不再赘述]**\n\n维度爆炸的问题！！\n\n按照核函数的定义[先找映射函数，之后再在高维空间中计算内积]，这样是很容易出现维度爆炸问题的。具体例子如下：\n\n设对向量\\\\( x=(u\\_1, u\\_2)^T \\\\)，映射的结果为\n\\\\[\n\\varphi(x)=u\\_1+u\\_1^2+u\\_2+u\\_2^2+u\\_1u\\_2\n\\\\]\n又由于\\\\( x\\_1=(\\eta\\_1, \\eta\\_2)^T, x\\_2=(\\xi\\_1, \\xi\\_2)^T \\\\)，则\\\\(x\\_1, x\\_2\\\\)的映射如下\n\\\\[\n\\varphi(x\\_1) = (\\eta\\_1, \\eta\\_1^2, \\eta\\_2, \\eta\\_2^2, \\eta\\_1\\eta\\_2) \\\\\\\n\\varphi(x\\_2) = (\\xi\\_1, \\xi\\_2^2, \\xi\\_2, \\xi\\_2^2, \\xi\\_1\\xi\\_2)\n\\\\]\n上面两式做内积运算即可得到\n\\\\[\n\\langle \\varphi(x\\_1), \\varphi(x\\_2) \\rangle = \\eta\\_1 \\xi\\_1 + \\eta\\_1^2 \\xi\\_2^2 + \\eta\\_2 \\xi\\_2 + \\eta\\_2^2 \\xi\\_2^2 + \\eta\\_1\\eta\\_2 \\xi\\_1\\xi\\_2\n\\\\]\n另外，如果我们计算下式\n\\\\[\n(\\langle x\\_1, x\\_2 \\rangle + 1)^2 = 2 \\eta\\_1 \\xi\\_1 + \\eta\\_1^2 \\xi\\_2^2 + \\eta\\_2 \\xi\\_2 + \\eta\\_2^2 \\xi\\_2^2 + 2 \\eta\\_1\\eta\\_2 \\xi\\_1\\xi\\_2 + 1\n\\\\]\n对比\\\\(式(33)和式(34)\\\\)，两者有很多相似的地方。实际上，我们只要把映射的某几个维度线性缩放一下，然后再加上一个常数维度，计算出来的结果和经过映射的两向量内积的结果是相等的。\n\n具体来说，如果对于向量\\\\(x=(a,b)^T\\\\)，设置新映射如下\n\\\\[\n\\phi = (\\sqrt{2}a, a^2, \\sqrt{2}b, b^2, \\sqrt{2}ab, 1)^T\n\\\\]\n则\\\\(式(34)\\\\)即\\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\)的结果和内积\\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)的结果是相等的。现在我们来研究一下二者的区别。\n\n1. \\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\): 直接在原来的低维空间中进行计算，不需要显示的写出映射后的结果\n2. \\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)：先映射到高维空间中，然后再根据内积进行计算\n\n回到刚才提到的维度爆炸，在\\\\( \\langle \\phi(x\\_1), \\phi(x\\_2) \\rangle \\\\)的方法不能计算的情况下，另一种方法\\\\((\\langle x\\_1, x\\_2 \\rangle + 1)^2 \\\\)却能从容处理，甚至无穷维度的情况也可以计算。\n这也就是核函数不显式的求映射关系，直接在低维计算，高维学习分离超平面的原因之一。\n\n这样，我们之前学习到的线性支持向量机，可以通过核函数学习非线性数据的情况：\n\\\\[\nf(x) = \\sum\\_{i=1}^n \\alpha\\_i y\\_i \\langle x\\_i, x \\rangle + b \\Longrightarrow f(x)= \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\\\]\n\n\n----------------\n\n### 3. 常用的核函数及其特点：\n\n* 多项式核函数(polynomial kernel function)\n\\\\[\nK(x\\_1, x\\_2) = (\\langle x\\_1, x\\_2 \\rangle + R)^d\n\\\\]\n该空间的维度是\\\\(C\\_{m+d}^d\\\\),其中\\\\(m\\\\)是原始空间的维度。\n\n* 高斯核函数(Gaussian kernel function) / 高斯径向基函数(Radial basis function)\n\\\\[\nK(x\\_1, x\\_2) = exp(- \\frac{\\|\\| x\\_1 - x\\_2 \\|\\|^2}{2\\sigma^2})\n\\\\]\n这个核会将原始空间映射为无穷维空间。不过，如果\\\\(\\sigma\\\\)选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；\n反过来，如果\\\\(\\sigma\\\\)选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。\n不过，总的来说，通过调控参数\\\\(\\sigma\\\\)，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间。\n\n![Gaussian](/posts_res/2018-03-28-svm/2-gaussian.png)\n\n* 线性核函数\n\\\\[\nK(x\\_1, x\\_2) = \\langle x\\_1, x\\_2 \\rangle\n\\\\]\n这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了 (意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板\n或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)\n\n[更多核函数进阶请看这里](http://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html)\n\n**last but not least.**\n\n#### 核函数的本质\n\n1. 实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去；\n2. 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(即维度爆炸)；\n3. 核函数登场，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。\n\n#### 核函数选择问题\n\n需要注意的是需要对数据归一化处理\n\n1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\n2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel\n3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n> [July - 支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n> [学习July博文总结——支持向量机(SVM)的深入理解](https://blog.csdn.net/ajianyingxiaoqinghan/article/details/72897399)\n\n> 周志华 - 《机器学习》\n\n> [\\_席达\\_ - SVM学习笔记](https://blog.csdn.net/robin_xu_shuai/article/details/77051258)\n","slug":"svm-2","published":1,"updated":"2019-08-17T09:33:40.643Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvd00152qwpfu1vyrrl","content":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-2\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-2\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 2\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 2</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul><a id=\"more\"></a>\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul>\n<hr>\n<h3 id=\"1-线性支持向量机\"><a href=\"#1-线性支持向量机\" class=\"headerlink\" title=\"1. 线性支持向量机\"></a>1. 线性支持向量机</h3><p>线性可分支持向量机的学习方法对于线性不可分训练数据是不适用的，因为问题中的不等式约束并不能都成立，所以我们要修改函数间隔。在<a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a>中的线性可分支持向量机的约束条件为\\(y_i(w\\cdot x_i +b) \\geq 1\\)，称为硬间隔最大化；现在我们对每个样本\\((x_i, y_i)\\)引入一个松弛变量\\(\\xi_i \\geq 0\\),使得函数间隔加上松弛变量大于等于1，此时的约束条件就变为\n\\[\ny_i(w\\cdot x_i +b) \\geq 1 - \\xi_i\n\\]\n同时，对每个松弛变量也要支付一个代价\\(\\xi_i\\)，目标函数也由原来的\\(\\frac{1}{2} || w ||^2\\)变为\n\\[\n\\frac{1}{2}|| w ||^2 + C\\sum_{i=1}^n \\xi_i\n\\]\n相对于硬间隔最大化，此时称为软间隔最大化。</p>\n<p>线性不可分的线性支持向量机的学习问题变为凸二次规划问题(<strong>原始问题</strong>)：\n\\[\n\\mathop{\\min_{w,b,\\xi}} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i\n\\]\n\\[\ns.j. \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad i=1,2,…,n\n\\]\n\\[\n\\xi_i \\geq 0, \\quad, i=1,2,…,n\n\\]</p>\n<p><strong>下面求原始问题的对偶问题：</strong></p>\n<p>原始问题的拉格朗日函数是\n\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\xi,\\alpha,\\mu)\n&amp; = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b)-1+\\xi_i) - \\sum_{i=1}^n \\mu_i \\xi_i \\\\\n&amp; = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i y_i (w \\cdot x +b) + \\sum_{i=1}^n \\alpha_i - \\sum_{i=1}^n \\alpha_i \\xi_i - \\sum_{i=1}^n \\mu_i \\xi_i \\\\\ns.t. \\quad \\alpha_i \\geq 0, \\quad \\mu_i \\geq 0\n\\end{aligned}\n\\end{equation}\n\\]\n原始问题为\n\\[\n\\mathop{\\min_{w,b,\\xi} \\max_{\\alpha, \\mu}} L(w,b,\\xi, \\alpha, \\mu)\n\\]\n对偶问题为\n\\[\n\\mathop{\\max_{\\alpha, \\mu} \\min_{w,b,\\xi}} L(w,b,\\xi, \\alpha, \\mu)\n\\]</p>\n<p>因此，先求\\(L(w,b,\\xi,\\alpha,\\mu)\\)对\\(w,b,\\xi\\)的极小\n\\[\n\\nabla _w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0 \\\\\n\\nabla _b L(w,b,\\xi,\\alpha,\\mu) = - \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n\\nabla_{\\xi_i} L(w,b,\\xi,\\alpha,\\mu) = C - \\alpha_i - \\mu_i = 0\n\\]\n得\n\\[\nw = \\sum_{i=1}^n \\alpha_i y_i x_i\n\\]\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\nC - \\alpha_i - \\mu_i = 0 \n\\]\n将\\( 式(10) \\sim 式(12) \\)代入\\(式(6)\\)，得\n\\[\n\\mathop{\\min_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu) = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]\n再对\\(\\mathop{\\min_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu)\\)求\\(\\alpha\\)的极大，即得对偶问题\n\\[\n\\mathop{\\max_{\\alpha}} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) + \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\nC - \\alpha_i - \\mu_i = 0 \\\\\n\\alpha_i \\geq 0 \\\\\n\\mu_i \\geq 0, \\quad i=1,2,…,n\n\\]\n调整化简约束条件，得\n\\[\n\\begin{cases}\nC - \\alpha_i - \\mu_i = 0 \\\\\n\\alpha_i \\geq 0 \\\\\n\\mu_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\mu_i = C - \\alpha_i \\geq 0 \\\\\n\\alpha_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n0 \\leq \\alpha_i \\leq C\n\\]\n因此，原始问题的对偶问题为\n\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n\n\\]\n至此，我们得到一个和线性可分支持向量机对偶问题类似的问题，所以同样需要使用序列最小最优化(SMO)算法解决，详见<a href=\"/2018/03/svm-3/\">Support Vector Machines - Part 3</a>。</p>\n<p><br></p>\n<p>假设已经求出了\\( \\alpha^{\\ast}=(\\alpha_1 ^{\\ast}, \\alpha_2 ^{\\ast}, …, \\alpha_n ^{\\ast})^T \\)是对偶问题的解，\n若存在\\(0 \\leq \\alpha_i \\leq C\\), 则利用\\(\\alpha^{\\ast}\\)求\\(w^{\\ast}\\)。</p>\n<p>原始问题是凸二次规划问题，解满足\\(KKT\\)条件，即得(下列公式也见《统计学习方法》page.111)\n\\[\n\\nabla _w L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = w^{\\ast} - \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i = 0 \\\\\n\\nabla _b L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = - \\sum_{i=1}^n \\alpha_i^{\\ast} y_i = 0 \\\\\n\\nabla_{\\xi_i} L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = C - \\alpha_i^{\\ast} - \\mu_i^{\\ast} = 0\n\\]\n\\[\n\\alpha_i^{\\ast} (y_i (w^{\\ast} \\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast}) = 0\n\\]\n\\[\n\\mu_i^{\\ast} \\xi_i^{\\ast} = (C - \\alpha_i^{\\ast}) \\xi_i = 0 \\\\\ny_i (w^{\\ast}\\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast} \\geq 0 \\\\\n\\xi_i^{\\ast} \\geq 0 \\\\\n\\alpha_i^{\\ast} \\geq 0 \\\\\n\\mu_i^{\\ast} \\geq 0, \\quad i=1,2,…,n\n\\]\n设我们找到一个支持向量\\(\\alpha_i^{\\ast} &gt; 0\\)，则由\\( 式(21)\\)可以得到\n\\[ \ny_i (w^{\\ast}\\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast} = 0 \\Longrightarrow b^{\\ast} = y_i - \\xi_i^{\\ast} y_i - w^{\\ast} \\cdot x_i \n\\]\n想要求\\(b^{\\ast}\\)就必须先求\\(\\xi_i^{\\ast}\\)，没办法得到\\(\\xi_i^{\\ast}\\)，但是如果\\(\\xi_i^{\\ast}=0\\)的话，那么\n\\[\nb^{\\ast} = y_i - w^{\\ast} \\cdot x_i\n\\]\n所以为了让\\(\\xi_i^{\\ast}=0\\)，那么由\\( 式(22) \\)可知，\\(C - \\alpha_i^{\\ast} \\not= 0\\)，即\\( \\alpha_i^{\\ast} \\not= C \\Rightarrow \\alpha_i^{\\ast} &lt;C \\)。</p>\n<p>所以，要求解\\( b^{\\ast} \\)，则需要找到一个\\( 0 &lt; \\alpha_i^{\\ast} &lt; C \\)，那么相应的\\(b^{\\ast} \\)就可以用\\(式(24)\\)进行计算。</p>\n<p><br></p>\n<p>综合上面的内容，可以看到线性支持向量机(软间隔)和线性可分支持向量机(硬间隔)几乎是一样的。\n相对于硬间隔来说，软间隔更灵活，可以通过调节\\(C\\)的值来控制，关心分隔超平面的间隔更大一些还是分类错误少一些，并且也不要求我们的数据是线性可分的，所以软间隔比硬间隔更加具有实际的应用价值。</p>\n<p>对于\\(\\alpha_i &gt; 0\\)的样本点\\((x_i,y_i)\\)称为支持向量(软间隔的支持向量)，如下图所示，分离超平面由实线表示，间隔平面用虚线表示。</p>\n<ul>\n<li>\\(0 &lt; \\alpha_i &lt; C, 则\\xi_i=0, \\quad \\)分类正确，支持向量落在间隔边界上[类似线性可分支持向量机]</li>\n<li>\\(\\alpha_i = C, 0 &lt; \\xi_i &lt; 1, \\quad \\)分类正确，支持向量落在间隔边界和分离超平面之间</li>\n<li>\\(\\alpha_i = C, \\xi_i=1, \\quad \\)，支持向量落在分离超平面上</li>\n<li>\\(\\alpha_i = C, \\xi_i&gt;1, \\quad \\)，分类错误，支持向量落在超平面另外一侧。</li>\n</ul>\n<center><img src=\"/posts_res/2018-03-28-svm/2-supportvector.png\"></center>\n\n\n<hr>\n<h3 id=\"2-核函数及非线性支持向量机\"><a href=\"#2-核函数及非线性支持向量机\" class=\"headerlink\" title=\"2. 核函数及非线性支持向量机\"></a>2. 核函数及非线性支持向量机</h3><p>目前为止，SVM还只能处理线性或者近似线性的情况，下面引入核函数，进而将SVM推广到非线性问题上。</p>\n<p>对于一个数据点\\(x\\)分类，实际上就是通过把\\(x\\)带入到\\(f(x)=wx+b\\)中算出结果，然后根据结果的正负进行类别的划分，在之前的推导中，我们得到\n\\[\nw^{\\ast} = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i\n\\]\n因此分类函数为\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = (\\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i)^T x + b \\\\\n&amp; = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i \\langle x_i,x \\rangle + b\n\\end{aligned}\n\\end{equation}\n\\]\n其中\\( \\langle.,. \\rangle \\)表示内积。这里比较有趣的地方是，对于新点\\(x\\)的预测，只需要计算它与训练数据点的内积即可，<strong>这是使用核函数\\(kernel\\)进行非线性推广的基本前提</strong>，\n事实上，所有非支持向量的系数\\(\\alpha^{\\ast}\\)都是等于零的，因此对于新点的内积运算，不需要针对所有的训练数据，实际上只要针对少量的支持向量就可以了。</p>\n<p><br></p>\n<p>为什么非支持向量的系数为零?</p>\n<p>直观解释：间隔边界后面的点对分离超平面的确定没有影响，所以这些无关的点不会参与到分类问题的计算中。</p>\n<p>公式解释：在线性可分支持向量机(硬间隔)中，我们得到了拉格朗日函数\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\theta(w) \n&amp; = \\mathop{\\max_{\\alpha_i \\geq 0}} L(w,b,\\alpha) \\\\\n&amp; = \\mathop{\\max_{\\alpha_i \\geq 0}} \\frac{1}{2}||w||^2 - \\sum_{i=1}^n \\alpha_i (y_i(wx_i +b)-1) \\\\\n\\end{aligned}\n\\end{equation}\n\\alpha_i \\geq 0, \\quad i=1,2,…,n\n\\]\n<strong>Notice:</strong></p>\n<p>如果\\( x_i \\)是支持向量的话，那么上式右面\\( y_i(wx_i +b)-1\\)部分是等于零的，因为支持向量的函数间隔等于1；\n而对于非支持向量，函数间隔大于零，所以\\( y_i(wx_i +b)-1\\)部分是大于零的，所以为了满足拉格朗日最大化，\\(\\alpha_i\\)必须等于零。</p>\n<p><br></p>\n<p>下面开始正式介绍核函数\\(Kernel\\)</p>\n<p>设\\(\\chi\\)是输入空间(欧式空间\\(R^n\\)的子集或离散集合)，又设\\(\\mathcal{H}\\)为特征空间(希尔伯特空间[完备的内积空间])，如果存在一个从\\(\\chi\\)到\\(\\mathcal{H}\\)的映射\n\\[\n\\varphi(x): \\chi \\to \\mathcal{H}\n\\]\n使得对所有\\(x,z \\in \\chi \\)，函数\\(K(x,z)\\)满足条件\n\\[\nK(x,z) = \\varphi(x) \\cdot \\varphi(z)\n\\]\n则称\\(K(x,z)\\)为核函数，\\(\\varphi(x)\\)为映射函数，式中\\(\\varphi(x) \\cdot \\varphi(z)\\)为\\(\\varphi(x)和\\varphi(z)\\)的内积。</p>\n<p>核函数的想法是，在学习和预测中只定义核函数\\(K(x,z)\\)，而不显式地定义映射函数\\(\\varphi(x)\\)。通常直接计算\\(K(x,z)\\)比较容易，而通过\\(\\varphi(x)和\\varphi(z)\\)计算\\(K(x,z)\\)并不容易。</p>\n<p>这样，处理非线性数据的情况时，线性支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中学习线性分离超平面，从而把原始空间中不好分的非线性数据分开。\n这样分类决策函数变为\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = (\\sum_{i=1}^n \\alpha_i y_i x_i)^T x + b \\\\\n&amp; = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>[其余核函数的举例不再赘述]</strong></p>\n<p>维度爆炸的问题！！</p>\n<p>按照核函数的定义[先找映射函数，之后再在高维空间中计算内积]，这样是很容易出现维度爆炸问题的。具体例子如下：</p>\n<p>设对向量\\( x=(u_1, u_2)^T \\)，映射的结果为\n\\[\n\\varphi(x)=u_1+u_1^2+u_2+u_2^2+u_1u_2\n\\]\n又由于\\( x_1=(\\eta_1, \\eta_2)^T, x_2=(\\xi_1, \\xi_2)^T \\)，则\\(x_1, x_2\\)的映射如下\n\\[\n\\varphi(x_1) = (\\eta_1, \\eta_1^2, \\eta_2, \\eta_2^2, \\eta_1\\eta_2) \\\\\n\\varphi(x_2) = (\\xi_1, \\xi_2^2, \\xi_2, \\xi_2^2, \\xi_1\\xi_2)\n\\]\n上面两式做内积运算即可得到\n\\[\n\\langle \\varphi(x_1), \\varphi(x_2) \\rangle = \\eta_1 \\xi_1 + \\eta_1^2 \\xi_2^2 + \\eta_2 \\xi_2 + \\eta_2^2 \\xi_2^2 + \\eta_1\\eta_2 \\xi_1\\xi_2\n\\]\n另外，如果我们计算下式\n\\[\n(\\langle x_1, x_2 \\rangle + 1)^2 = 2 \\eta_1 \\xi_1 + \\eta_1^2 \\xi_2^2 + \\eta_2 \\xi_2 + \\eta_2^2 \\xi_2^2 + 2 \\eta_1\\eta_2 \\xi_1\\xi_2 + 1\n\\]\n对比\\(式(33)和式(34)\\)，两者有很多相似的地方。实际上，我们只要把映射的某几个维度线性缩放一下，然后再加上一个常数维度，计算出来的结果和经过映射的两向量内积的结果是相等的。</p>\n<p>具体来说，如果对于向量\\(x=(a,b)^T\\)，设置新映射如下\n\\[\n\\phi = (\\sqrt{2}a, a^2, \\sqrt{2}b, b^2, \\sqrt{2}ab, 1)^T\n\\]\n则\\(式(34)\\)即\\((\\langle x_1, x_2 \\rangle + 1)^2 \\)的结果和内积\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)的结果是相等的。现在我们来研究一下二者的区别。</p>\n<ol>\n<li>\\((\\langle x_1, x_2 \\rangle + 1)^2 \\): 直接在原来的低维空间中进行计算，不需要显示的写出映射后的结果</li>\n<li>\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)：先映射到高维空间中，然后再根据内积进行计算</li>\n</ol>\n<p>回到刚才提到的维度爆炸，在\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)的方法不能计算的情况下，另一种方法\\((\\langle x_1, x_2 \\rangle + 1)^2 \\)却能从容处理，甚至无穷维度的情况也可以计算。\n这也就是核函数不显式的求映射关系，直接在低维计算，高维学习分离超平面的原因之一。</p>\n<p>这样，我们之前学习到的线性支持向量机，可以通过核函数学习非线性数据的情况：\n\\[\nf(x) = \\sum_{i=1}^n \\alpha_i y_i \\langle x_i, x \\rangle + b \\Longrightarrow f(x)= \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\]</p>\n<hr>\n<h3 id=\"3-常用的核函数及其特点：\"><a href=\"#3-常用的核函数及其特点：\" class=\"headerlink\" title=\"3. 常用的核函数及其特点：\"></a>3. 常用的核函数及其特点：</h3><ul>\n<li><p>多项式核函数(polynomial kernel function)\n\\[\nK(x_1, x_2) = (\\langle x_1, x_2 \\rangle + R)^d\n\\]\n该空间的维度是\\(C_{m+d}^d\\),其中\\(m\\)是原始空间的维度。</p>\n</li>\n<li><p>高斯核函数(Gaussian kernel function) / 高斯径向基函数(Radial basis function)\n\\[\nK(x_1, x_2) = exp(- \\frac{|| x_1 - x_2 ||^2}{2\\sigma^2})\n\\]\n这个核会将原始空间映射为无穷维空间。不过，如果\\(\\sigma\\)选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；\n反过来，如果\\(\\sigma\\)选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。\n不过，总的来说，通过调控参数\\(\\sigma\\)，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间。</p>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-03-28-svm/2-gaussian.png\" alt=\"Gaussian\"></p>\n<ul>\n<li>线性核函数\n\\[\nK(x_1, x_2) = \\langle x_1, x_2 \\rangle\n\\]\n这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了 (意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板\n或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)</li>\n</ul>\n<p><a href=\"http://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html\" target=\"_blank\" rel=\"noopener\">更多核函数进阶请看这里</a></p>\n<p><strong>last but not least.</strong></p>\n<h4 id=\"核函数的本质\"><a href=\"#核函数的本质\" class=\"headerlink\" title=\"核函数的本质\"></a>核函数的本质</h4><ol>\n<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去；</li>\n<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(即维度爆炸)；</li>\n<li>核函数登场，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。</li>\n</ol>\n<h4 id=\"核函数选择问题\"><a href=\"#核函数选择问题\" class=\"headerlink\" title=\"核函数选择问题\"></a>核函数选择问题</h4><p>需要注意的是需要对数据归一化处理</p>\n<ol>\n<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>\n<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>\n<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">July - 支持向量机通俗导论（理解SVM的三层境界）</a></p>\n<p><a href=\"https://blog.csdn.net/ajianyingxiaoqinghan/article/details/72897399\" target=\"_blank\" rel=\"noopener\">学习July博文总结——支持向量机(SVM)的深入理解</a></p>\n<p>周志华 - 《机器学习》</p>\n<p><a href=\"https://blog.csdn.net/robin_xu_shuai/article/details/77051258\" target=\"_blank\" rel=\"noopener\">_席达_ - SVM学习笔记</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-2\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-2\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 2\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 2</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul>","more":"\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul>\n<hr>\n<h3 id=\"1-线性支持向量机\"><a href=\"#1-线性支持向量机\" class=\"headerlink\" title=\"1. 线性支持向量机\"></a>1. 线性支持向量机</h3><p>线性可分支持向量机的学习方法对于线性不可分训练数据是不适用的，因为问题中的不等式约束并不能都成立，所以我们要修改函数间隔。在<a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a>中的线性可分支持向量机的约束条件为\\(y_i(w\\cdot x_i +b) \\geq 1\\)，称为硬间隔最大化；现在我们对每个样本\\((x_i, y_i)\\)引入一个松弛变量\\(\\xi_i \\geq 0\\),使得函数间隔加上松弛变量大于等于1，此时的约束条件就变为\n\\[\ny_i(w\\cdot x_i +b) \\geq 1 - \\xi_i\n\\]\n同时，对每个松弛变量也要支付一个代价\\(\\xi_i\\)，目标函数也由原来的\\(\\frac{1}{2} || w ||^2\\)变为\n\\[\n\\frac{1}{2}|| w ||^2 + C\\sum_{i=1}^n \\xi_i\n\\]\n相对于硬间隔最大化，此时称为软间隔最大化。</p>\n<p>线性不可分的线性支持向量机的学习问题变为凸二次规划问题(<strong>原始问题</strong>)：\n\\[\n\\mathop{\\min_{w,b,\\xi}} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i\n\\]\n\\[\ns.j. \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad i=1,2,…,n\n\\]\n\\[\n\\xi_i \\geq 0, \\quad, i=1,2,…,n\n\\]</p>\n<p><strong>下面求原始问题的对偶问题：</strong></p>\n<p>原始问题的拉格朗日函数是\n\\[\n\\begin{equation}\n\\begin{aligned}\nL(w,b,\\xi,\\alpha,\\mu)\n&amp; = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i(y_i(w \\cdot x_i + b)-1+\\xi_i) - \\sum_{i=1}^n \\mu_i \\xi_i \\\\\n&amp; = \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i y_i (w \\cdot x +b) + \\sum_{i=1}^n \\alpha_i - \\sum_{i=1}^n \\alpha_i \\xi_i - \\sum_{i=1}^n \\mu_i \\xi_i \\\\\ns.t. \\quad \\alpha_i \\geq 0, \\quad \\mu_i \\geq 0\n\\end{aligned}\n\\end{equation}\n\\]\n原始问题为\n\\[\n\\mathop{\\min_{w,b,\\xi} \\max_{\\alpha, \\mu}} L(w,b,\\xi, \\alpha, \\mu)\n\\]\n对偶问题为\n\\[\n\\mathop{\\max_{\\alpha, \\mu} \\min_{w,b,\\xi}} L(w,b,\\xi, \\alpha, \\mu)\n\\]</p>\n<p>因此，先求\\(L(w,b,\\xi,\\alpha,\\mu)\\)对\\(w,b,\\xi\\)的极小\n\\[\n\\nabla _w L(w,b,\\xi,\\alpha,\\mu) = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0 \\\\\n\\nabla _b L(w,b,\\xi,\\alpha,\\mu) = - \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n\\nabla_{\\xi_i} L(w,b,\\xi,\\alpha,\\mu) = C - \\alpha_i - \\mu_i = 0\n\\]\n得\n\\[\nw = \\sum_{i=1}^n \\alpha_i y_i x_i\n\\]\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\nC - \\alpha_i - \\mu_i = 0 \n\\]\n将\\( 式(10) \\sim 式(12) \\)代入\\(式(6)\\)，得\n\\[\n\\mathop{\\min_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu) = - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j  x_i^T x_j + \\sum_{i=1}^n \\alpha_i\n\\]\n再对\\(\\mathop{\\min_{w,b,\\xi}} L(w,b,\\xi,\\alpha,\\mu)\\)求\\(\\alpha\\)的极大，即得对偶问题\n\\[\n\\mathop{\\max_{\\alpha}} - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) + \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\nC - \\alpha_i - \\mu_i = 0 \\\\\n\\alpha_i \\geq 0 \\\\\n\\mu_i \\geq 0, \\quad i=1,2,…,n\n\\]\n调整化简约束条件，得\n\\[\n\\begin{cases}\nC - \\alpha_i - \\mu_i = 0 \\\\\n\\alpha_i \\geq 0 \\\\\n\\mu_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\mu_i = C - \\alpha_i \\geq 0 \\\\\n\\alpha_i \\geq 0\n\\end{cases}\n\\Longrightarrow\n0 \\leq \\alpha_i \\leq C\n\\]\n因此，原始问题的对偶问题为\n\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (x_i^T \\cdot x_j) - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n\n\\]\n至此，我们得到一个和线性可分支持向量机对偶问题类似的问题，所以同样需要使用序列最小最优化(SMO)算法解决，详见<a href=\"/2018/03/svm-3/\">Support Vector Machines - Part 3</a>。</p>\n<p><br></p>\n<p>假设已经求出了\\( \\alpha^{\\ast}=(\\alpha_1 ^{\\ast}, \\alpha_2 ^{\\ast}, …, \\alpha_n ^{\\ast})^T \\)是对偶问题的解，\n若存在\\(0 \\leq \\alpha_i \\leq C\\), 则利用\\(\\alpha^{\\ast}\\)求\\(w^{\\ast}\\)。</p>\n<p>原始问题是凸二次规划问题，解满足\\(KKT\\)条件，即得(下列公式也见《统计学习方法》page.111)\n\\[\n\\nabla _w L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = w^{\\ast} - \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i = 0 \\\\\n\\nabla _b L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = - \\sum_{i=1}^n \\alpha_i^{\\ast} y_i = 0 \\\\\n\\nabla_{\\xi_i} L(w^{\\ast},b^{\\ast},\\xi,\\alpha,\\mu) = C - \\alpha_i^{\\ast} - \\mu_i^{\\ast} = 0\n\\]\n\\[\n\\alpha_i^{\\ast} (y_i (w^{\\ast} \\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast}) = 0\n\\]\n\\[\n\\mu_i^{\\ast} \\xi_i^{\\ast} = (C - \\alpha_i^{\\ast}) \\xi_i = 0 \\\\\ny_i (w^{\\ast}\\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast} \\geq 0 \\\\\n\\xi_i^{\\ast} \\geq 0 \\\\\n\\alpha_i^{\\ast} \\geq 0 \\\\\n\\mu_i^{\\ast} \\geq 0, \\quad i=1,2,…,n\n\\]\n设我们找到一个支持向量\\(\\alpha_i^{\\ast} &gt; 0\\)，则由\\( 式(21)\\)可以得到\n\\[ \ny_i (w^{\\ast}\\cdot x_i + b^{\\ast}) -1 +  \\xi_i^{\\ast} = 0 \\Longrightarrow b^{\\ast} = y_i - \\xi_i^{\\ast} y_i - w^{\\ast} \\cdot x_i \n\\]\n想要求\\(b^{\\ast}\\)就必须先求\\(\\xi_i^{\\ast}\\)，没办法得到\\(\\xi_i^{\\ast}\\)，但是如果\\(\\xi_i^{\\ast}=0\\)的话，那么\n\\[\nb^{\\ast} = y_i - w^{\\ast} \\cdot x_i\n\\]\n所以为了让\\(\\xi_i^{\\ast}=0\\)，那么由\\( 式(22) \\)可知，\\(C - \\alpha_i^{\\ast} \\not= 0\\)，即\\( \\alpha_i^{\\ast} \\not= C \\Rightarrow \\alpha_i^{\\ast} &lt;C \\)。</p>\n<p>所以，要求解\\( b^{\\ast} \\)，则需要找到一个\\( 0 &lt; \\alpha_i^{\\ast} &lt; C \\)，那么相应的\\(b^{\\ast} \\)就可以用\\(式(24)\\)进行计算。</p>\n<p><br></p>\n<p>综合上面的内容，可以看到线性支持向量机(软间隔)和线性可分支持向量机(硬间隔)几乎是一样的。\n相对于硬间隔来说，软间隔更灵活，可以通过调节\\(C\\)的值来控制，关心分隔超平面的间隔更大一些还是分类错误少一些，并且也不要求我们的数据是线性可分的，所以软间隔比硬间隔更加具有实际的应用价值。</p>\n<p>对于\\(\\alpha_i &gt; 0\\)的样本点\\((x_i,y_i)\\)称为支持向量(软间隔的支持向量)，如下图所示，分离超平面由实线表示，间隔平面用虚线表示。</p>\n<ul>\n<li>\\(0 &lt; \\alpha_i &lt; C, 则\\xi_i=0, \\quad \\)分类正确，支持向量落在间隔边界上[类似线性可分支持向量机]</li>\n<li>\\(\\alpha_i = C, 0 &lt; \\xi_i &lt; 1, \\quad \\)分类正确，支持向量落在间隔边界和分离超平面之间</li>\n<li>\\(\\alpha_i = C, \\xi_i=1, \\quad \\)，支持向量落在分离超平面上</li>\n<li>\\(\\alpha_i = C, \\xi_i&gt;1, \\quad \\)，分类错误，支持向量落在超平面另外一侧。</li>\n</ul>\n<center><img src=\"/posts_res/2018-03-28-svm/2-supportvector.png\"></center>\n\n\n<hr>\n<h3 id=\"2-核函数及非线性支持向量机\"><a href=\"#2-核函数及非线性支持向量机\" class=\"headerlink\" title=\"2. 核函数及非线性支持向量机\"></a>2. 核函数及非线性支持向量机</h3><p>目前为止，SVM还只能处理线性或者近似线性的情况，下面引入核函数，进而将SVM推广到非线性问题上。</p>\n<p>对于一个数据点\\(x\\)分类，实际上就是通过把\\(x\\)带入到\\(f(x)=wx+b\\)中算出结果，然后根据结果的正负进行类别的划分，在之前的推导中，我们得到\n\\[\nw^{\\ast} = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i\n\\]\n因此分类函数为\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = (\\sum_{i=1}^n \\alpha_i^{\\ast} y_i x_i)^T x + b \\\\\n&amp; = \\sum_{i=1}^n \\alpha_i^{\\ast} y_i \\langle x_i,x \\rangle + b\n\\end{aligned}\n\\end{equation}\n\\]\n其中\\( \\langle.,. \\rangle \\)表示内积。这里比较有趣的地方是，对于新点\\(x\\)的预测，只需要计算它与训练数据点的内积即可，<strong>这是使用核函数\\(kernel\\)进行非线性推广的基本前提</strong>，\n事实上，所有非支持向量的系数\\(\\alpha^{\\ast}\\)都是等于零的，因此对于新点的内积运算，不需要针对所有的训练数据，实际上只要针对少量的支持向量就可以了。</p>\n<p><br></p>\n<p>为什么非支持向量的系数为零?</p>\n<p>直观解释：间隔边界后面的点对分离超平面的确定没有影响，所以这些无关的点不会参与到分类问题的计算中。</p>\n<p>公式解释：在线性可分支持向量机(硬间隔)中，我们得到了拉格朗日函数\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\theta(w) \n&amp; = \\mathop{\\max_{\\alpha_i \\geq 0}} L(w,b,\\alpha) \\\\\n&amp; = \\mathop{\\max_{\\alpha_i \\geq 0}} \\frac{1}{2}||w||^2 - \\sum_{i=1}^n \\alpha_i (y_i(wx_i +b)-1) \\\\\n\\end{aligned}\n\\end{equation}\n\\alpha_i \\geq 0, \\quad i=1,2,…,n\n\\]\n<strong>Notice:</strong></p>\n<p>如果\\( x_i \\)是支持向量的话，那么上式右面\\( y_i(wx_i +b)-1\\)部分是等于零的，因为支持向量的函数间隔等于1；\n而对于非支持向量，函数间隔大于零，所以\\( y_i(wx_i +b)-1\\)部分是大于零的，所以为了满足拉格朗日最大化，\\(\\alpha_i\\)必须等于零。</p>\n<p><br></p>\n<p>下面开始正式介绍核函数\\(Kernel\\)</p>\n<p>设\\(\\chi\\)是输入空间(欧式空间\\(R^n\\)的子集或离散集合)，又设\\(\\mathcal{H}\\)为特征空间(希尔伯特空间[完备的内积空间])，如果存在一个从\\(\\chi\\)到\\(\\mathcal{H}\\)的映射\n\\[\n\\varphi(x): \\chi \\to \\mathcal{H}\n\\]\n使得对所有\\(x,z \\in \\chi \\)，函数\\(K(x,z)\\)满足条件\n\\[\nK(x,z) = \\varphi(x) \\cdot \\varphi(z)\n\\]\n则称\\(K(x,z)\\)为核函数，\\(\\varphi(x)\\)为映射函数，式中\\(\\varphi(x) \\cdot \\varphi(z)\\)为\\(\\varphi(x)和\\varphi(z)\\)的内积。</p>\n<p>核函数的想法是，在学习和预测中只定义核函数\\(K(x,z)\\)，而不显式地定义映射函数\\(\\varphi(x)\\)。通常直接计算\\(K(x,z)\\)比较容易，而通过\\(\\varphi(x)和\\varphi(z)\\)计算\\(K(x,z)\\)并不容易。</p>\n<p>这样，处理非线性数据的情况时，线性支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中学习线性分离超平面，从而把原始空间中不好分的非线性数据分开。\n这样分类决策函数变为\n\\[\n\\begin{equation}\n\\begin{aligned}\nf(x) \n&amp; = (\\sum_{i=1}^n \\alpha_i y_i x_i)^T x + b \\\\\n&amp; = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>[其余核函数的举例不再赘述]</strong></p>\n<p>维度爆炸的问题！！</p>\n<p>按照核函数的定义[先找映射函数，之后再在高维空间中计算内积]，这样是很容易出现维度爆炸问题的。具体例子如下：</p>\n<p>设对向量\\( x=(u_1, u_2)^T \\)，映射的结果为\n\\[\n\\varphi(x)=u_1+u_1^2+u_2+u_2^2+u_1u_2\n\\]\n又由于\\( x_1=(\\eta_1, \\eta_2)^T, x_2=(\\xi_1, \\xi_2)^T \\)，则\\(x_1, x_2\\)的映射如下\n\\[\n\\varphi(x_1) = (\\eta_1, \\eta_1^2, \\eta_2, \\eta_2^2, \\eta_1\\eta_2) \\\\\n\\varphi(x_2) = (\\xi_1, \\xi_2^2, \\xi_2, \\xi_2^2, \\xi_1\\xi_2)\n\\]\n上面两式做内积运算即可得到\n\\[\n\\langle \\varphi(x_1), \\varphi(x_2) \\rangle = \\eta_1 \\xi_1 + \\eta_1^2 \\xi_2^2 + \\eta_2 \\xi_2 + \\eta_2^2 \\xi_2^2 + \\eta_1\\eta_2 \\xi_1\\xi_2\n\\]\n另外，如果我们计算下式\n\\[\n(\\langle x_1, x_2 \\rangle + 1)^2 = 2 \\eta_1 \\xi_1 + \\eta_1^2 \\xi_2^2 + \\eta_2 \\xi_2 + \\eta_2^2 \\xi_2^2 + 2 \\eta_1\\eta_2 \\xi_1\\xi_2 + 1\n\\]\n对比\\(式(33)和式(34)\\)，两者有很多相似的地方。实际上，我们只要把映射的某几个维度线性缩放一下，然后再加上一个常数维度，计算出来的结果和经过映射的两向量内积的结果是相等的。</p>\n<p>具体来说，如果对于向量\\(x=(a,b)^T\\)，设置新映射如下\n\\[\n\\phi = (\\sqrt{2}a, a^2, \\sqrt{2}b, b^2, \\sqrt{2}ab, 1)^T\n\\]\n则\\(式(34)\\)即\\((\\langle x_1, x_2 \\rangle + 1)^2 \\)的结果和内积\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)的结果是相等的。现在我们来研究一下二者的区别。</p>\n<ol>\n<li>\\((\\langle x_1, x_2 \\rangle + 1)^2 \\): 直接在原来的低维空间中进行计算，不需要显示的写出映射后的结果</li>\n<li>\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)：先映射到高维空间中，然后再根据内积进行计算</li>\n</ol>\n<p>回到刚才提到的维度爆炸，在\\( \\langle \\phi(x_1), \\phi(x_2) \\rangle \\)的方法不能计算的情况下，另一种方法\\((\\langle x_1, x_2 \\rangle + 1)^2 \\)却能从容处理，甚至无穷维度的情况也可以计算。\n这也就是核函数不显式的求映射关系，直接在低维计算，高维学习分离超平面的原因之一。</p>\n<p>这样，我们之前学习到的线性支持向量机，可以通过核函数学习非线性数据的情况：\n\\[\nf(x) = \\sum_{i=1}^n \\alpha_i y_i \\langle x_i, x \\rangle + b \\Longrightarrow f(x)= \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\]</p>\n<hr>\n<h3 id=\"3-常用的核函数及其特点：\"><a href=\"#3-常用的核函数及其特点：\" class=\"headerlink\" title=\"3. 常用的核函数及其特点：\"></a>3. 常用的核函数及其特点：</h3><ul>\n<li><p>多项式核函数(polynomial kernel function)\n\\[\nK(x_1, x_2) = (\\langle x_1, x_2 \\rangle + R)^d\n\\]\n该空间的维度是\\(C_{m+d}^d\\),其中\\(m\\)是原始空间的维度。</p>\n</li>\n<li><p>高斯核函数(Gaussian kernel function) / 高斯径向基函数(Radial basis function)\n\\[\nK(x_1, x_2) = exp(- \\frac{|| x_1 - x_2 ||^2}{2\\sigma^2})\n\\]\n这个核会将原始空间映射为无穷维空间。不过，如果\\(\\sigma\\)选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；\n反过来，如果\\(\\sigma\\)选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。\n不过，总的来说，通过调控参数\\(\\sigma\\)，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间。</p>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-03-28-svm/2-gaussian.png\" alt=\"Gaussian\"></p>\n<ul>\n<li>线性核函数\n\\[\nK(x_1, x_2) = \\langle x_1, x_2 \\rangle\n\\]\n这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了 (意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板\n或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)</li>\n</ul>\n<p><a href=\"http://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html\" target=\"_blank\" rel=\"noopener\">更多核函数进阶请看这里</a></p>\n<p><strong>last but not least.</strong></p>\n<h4 id=\"核函数的本质\"><a href=\"#核函数的本质\" class=\"headerlink\" title=\"核函数的本质\"></a>核函数的本质</h4><ol>\n<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去；</li>\n<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(即维度爆炸)；</li>\n<li>核函数登场，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。</li>\n</ol>\n<h4 id=\"核函数选择问题\"><a href=\"#核函数选择问题\" class=\"headerlink\" title=\"核函数选择问题\"></a>核函数选择问题</h4><p>需要注意的是需要对数据归一化处理</p>\n<ol>\n<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>\n<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>\n<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">July - 支持向量机通俗导论（理解SVM的三层境界）</a></p>\n<p><a href=\"https://blog.csdn.net/ajianyingxiaoqinghan/article/details/72897399\" target=\"_blank\" rel=\"noopener\">学习July博文总结——支持向量机(SVM)的深入理解</a></p>\n<p>周志华 - 《机器学习》</p>\n<p><a href=\"https://blog.csdn.net/robin_xu_shuai/article/details/77051258\" target=\"_blank\" rel=\"noopener\">_席达_ - SVM学习笔记</a></p>\n</blockquote>\n"},{"layout":"post","title":"Support Vector Machines - Part 3","date":"2018-03-28T12:13:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>支持向量机 - SVM（Support Vector Machines）Part 3</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 序列最小最优化(SMO)算法\n* SMO算法步骤\n* 代码资料\n\n\n--------------\n\n### 1. 序列最小最优化(SMO)算法\n\n先看一下SMO要解决的问题:\n\n#### 线性可分支持向量机(硬间隔)\n\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n\\quad \\quad \\alpha\\_i \\geq 0, i=1,2,...,n\n\\\\]\n\n#### 线性支持向量机(软间隔)\n\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i \n\\Longleftrightarrow \\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j K\\_{ij} - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad 0 \\leq \\alpha\\_i \\leq C, i=1,2,...,n\n\\\\]\n其中\\\\(K\\_{ij}=K(x\\_i, x\\_j), i,j=1,2,...,n\\\\)\n\n下面要解决的问题是: \\\\( \\alpha = \\lbrace \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n \\rbrace \\\\)上求上述目标函数的最小化。SMO的基本思路是：\n如果所有的变量的解都满足此最优化问题的\\\\(KKT\\\\)条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，\n这个二次规划问题关于这两个变量的解更接近于原始二次规划问题的解。整个SMO算法包括两部分：\n* 求解这两个变量二次规划的解析方法\n* 选择变量的启发式方法\n\n----------------\n\n#### 1.1 两个变量二次规划的求解方法\n\n随机选择两个变量\\\\(\\alpha\\_1, \\alpha\\_2\\\\)，其他变量固定。于是SMO的最优化问题\\\\(式(2)\\\\)可以写成：\n\\\\[\n\\mathop{\\min\\_{\\alpha\\_1, \\alpha\\_2}} W(\\alpha\\_1, \\alpha\\_2) = \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1\\alpha\\_1\\sum\\_{i=3}^n y\\_i \\alpha\\_iK\\_{i1} + y\\_2\\alpha\\_2\\sum\\_{i=3}^ny\\_i\\alpha\\_iK\\_{i2}\n\\\\]\n\\\\[\ns.t. \\quad \\alpha\\_1y\\_1 + \\alpha\\_2y\\_2 = - \\sum\\_{i=3}^n y\\_i \\alpha\\_i = \\varsigma\n\\\\]\n\\\\[\n\\quad \\quad 0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n\n\\\\]\n其中\\\\(K\\_{ij}=K(x\\_i, x\\_j), i,j=1,2,...,n, \\quad \\varsigma\\\\)是常数，另外式\\\\((5)\\\\)中省略了不含\\\\(\\alpha\\_1, \\alpha\\_2\\\\)的常数项。\n\n这里，我们引入新的变量\n\\\\[\nv\\_i = \\sum\\_{j=3}^n \\alpha\\_jy\\_jK\\_{ij} = f(x\\_i) - b - \\alpha\\_1 y\\_1 K\\_{i1} - \\alpha\\_2 y\\_2 K\\_{i2}\n\\\\]\n其中\\\\( f(x) = \\sum\\_{i=1}^n \\alpha_iy\\_i K(x\\_i, x) + b\\\\)，则式\\\\((5)\\\\)中的目标函数可以重新写为\n\\\\[\nW(\\alpha\\_1, \\alpha\\_2) =  \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1v\\_1\\alpha\\_1 + y\\_2v\\_2\\alpha\\_2 \n\\\\]\n\n下面我们研究一下约束条件\\\\(式(6) \\sim 式(7)\\\\)，由式\\\\((7)\\\\)可知，两个变量均要落在[0,C]x[0,C]的一个矩阵中，\n考虑\\\\(\\alpha\\_2\\\\)单变量的最优化问题，设问题\\\\(式(5) \\sim 式(7)\\\\)的初始可行解为\\\\( \\alpha\\_1^{old}, \\alpha\\_2^{old}\\\\)，最优解为\\\\( \\alpha\\_1^{new}, \\alpha\\_2^{new} \\\\)，\n并且假设在沿着约束方向上未经剪辑时\\\\(\\alpha\\_2\\\\)的最优解为\\\\(\\alpha\\_2^{new,unc}\\\\)。假设\\\\(\\alpha\\_2^{new}\\\\)的上下边界分别为\\\\(H和L\\\\),那么有：\n\\\\[\nL \\leq \\alpha\\_2^{new} \\leq H\n\\\\]\n接下来综合约束条件\\\\(0 \\leq \\alpha\\_i \\leq C,i=1,2,...,n和\\alpha\\_1^{new}y\\_i+\\alpha\\_2^{new}y\\_2=\\alpha\\_1^{old}y\\_1+\\alpha\\_2^{old}y\\_2=\\varsigma\\\\)，求取上下边界的值。\n\n以\\\\(y\\_1 \\not= y\\_2\\\\)为例，由\\\\(\\alpha\\_1^{new}y\\_i+\\alpha\\_2^{new}y\\_2=\\alpha\\_1^{old}y\\_1+\\alpha\\_2^{old}y\\_2=\\varsigma\\\\)可得：\n\\\\[\n\\alpha\\_2 = \\alpha\\_1 - \\varsigma \\quad [下方线段] \\quad 或 \\quad \\quad \\alpha\\_2 = - \\alpha\\_1 + \\varsigma \\quad [上方线段]\n\\\\]\n所以，当\\\\(\\alpha\\_1=0\\\\)时，\\\\( L = max(0, -\\varsigma) = max(0, \\alpha\\_2^{old} - \\alpha\\_1^{old}) [上方线段] \\\\)；\n\n所以，当\\\\(\\alpha\\_1=C\\\\)时，\\\\( H = min(C, C-\\varsigma) = min(C, C - (\\alpha\\_1^{old} - \\alpha\\_2^{old})) [下方线段] \\\\)；\n\n二维图像如下图左图所示，右图为\\\\(y\\_1=y\\_2\\\\)时的图像。\n\n![img](/posts_res/2018-03-28-svm/3-variables.png)\n\n如此，根据\\\\(y\\_1和y\\_2\\\\)异号或同号，可以得到如下\\\\(\\alpha\\_2^{new}\\\\)的上下界分别为：\n\\\\[\n\\begin{cases}\nL = max(0, \\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad if \\quad y\\_1 \\not= y\\_2 \\\\\\\nL = max(0, \\alpha\\_2^{old}+\\alpha\\_1^{old}-C), \\quad H = min(C, \\alpha\\_2^{old}+\\alpha\\_1^{old}), \\quad \\quad \\quad if \\quad y\\_1 = y\\_2\n\\end{cases}\n\\\\]\n\n下面开始求沿着约束方向未经剪辑[即未考虑不等式约束\\\\((7)\\\\)]时\\\\(\\alpha\\_2\\\\)的最优解\\\\(\\alpha\\_2^{new,unc}\\\\)；之后再求剪辑后\\\\(\\alpha\\_2\\\\)的解\\\\(\\alpha\\_2^{new}\\\\)。\n\\\\[\nf(x) = \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\\\]\n\\\\[\nE\\_i = f(x\\_i) - y\\_i = (\\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b) - y\\_i\n\\\\]\n\n由\\\\( \\alpha\\_1y\\_1 + \\alpha\\_2y\\_2 = \\varsigma及y\\_i^2 = 1 \\\\)可得，\n\\\\[\n\\alpha_1 = (\\varsigma - \\alpha\\_2y\\_2)y\\_1\n\\\\]\n将\\\\(式(15)\\\\)代入\\\\(式(9)\\\\)中，得：\n\\\\[\nW(\\alpha\\_1, \\alpha\\_2) = \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1v\\_1\\alpha\\_1 + y\\_2v\\_2\\alpha\\_2\n\\\\]\n\\\\[\\Downarrow\\\\]\n\\\\[\nW(\\alpha\\_2) = \\frac{1}{2}K\\_{11}(\\varsigma-\\alpha\\_2y\\_2)^2+\\frac{1}{2}K\\_{22}\\alpha\\_2^2+y\\_2K\\_{12}(\\varsigma-\\alpha\\_2y\\_2)\\alpha\\_2 - (\\varsigma-\\alpha\\_2y\\_2)y\\_1 - \\alpha\\_2 + v\\_1(\\varsigma-\\alpha\\_2y\\_2)+y\\_2v\\_2\\alpha\\_2\n\\\\]\n之后对\\\\(\\alpha\\_2\\\\)求导数[Notice: \\\\( y\\_i^2 = 1 \\\\)]并令其等于零：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial W}{\\partial \\alpha\\_2}\n& = K\\_{11}(\\varsigma - \\alpha\\_2y\\_2)(-y\\_2) + K\\_{22}\\alpha\\_2 + y\\_2K\\_{12}\\varsigma - 2y\\_2K\\_{12}y\\_2\\alpha\\_2+y\\_1y\\_2 -1 - v\\_1y\\_2 + v\\_2y\\_2 \\\\\\\n& = K\\_{11}\\alpha\\_2 + K\\_{22}\\alpha\\_2 - 2K\\_{12}\\alpha\\_2 - K\\_{11}\\varsigma y\\_2 + K\\_{12}\\varsigma y\\_2 -1 -v\\_1y\\_2 + v\\_2y\\_2 \\\\\\\n& = 0\n\\end{aligned}\n\\end{equation}\n\\\\]\n得：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n(K\\_{11}+K\\_{22}-2K\\_{12})\\alpha\\_2 \n& = y\\_2(y\\_2 - y\\_1 + \\varsigma K\\_{11} - \\varsigma K\\_{12}+v\\_1-v\\_2) \\\\\\\n& = y\\_2 \\lbrace y\\_2 - y\\_1 + \\varsigma K\\_{11} - \\varsigma K\\_{12}+ [f(x\\_1) - \\sum\\_{j=1}^2 \\alpha\\_jy\\_jK\\_{1j} - b] - [f(x\\_2) - \\sum\\_{j=1}^2 \\alpha\\_jy\\_jK\\_{2j} - b] \\rbrace \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n将\\\\( \\varsigma = \\alpha\\_1^{old}y\\_1 + \\alpha\\_2^{old}y\\_2 \\\\)代入式\\\\((20)\\\\)，得：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n(K\\_{11}+K\\_{22}-2K\\_{12})\\alpha\\_2^{new,unc} \n& = y\\_2 \\lbrace (K\\_{11}+K\\_{12} - 2K\\_{12})\\alpha\\_2^{old}y\\_2 + y\\_2 - y\\_1 + f(x\\_1) - f(x\\_2) \\rbrace \\\\\\\n& = (K\\_{11}+K\\_{12} - 2K\\_{12})\\alpha\\_2^{old} + y\\_2(E\\_1 - E\\_2) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n令\\\\(\\eta = K\\_{11}+K\\_{22}-2K\\_{12}\\\\)，即得：\n\\\\[\n\\alpha\\_2^{new,unc} = \\alpha\\_2^{old} + \\frac{y\\_2(E\\_1-E\\_2)}{\\eta}\n\\\\]\n之后剪辑\\\\(\\alpha\\_2^{new,unc}\\\\)得：\n\\\\[\n\\alpha\\_2^{new} = \n\\begin{cases}\nH, \\quad \\quad \\quad \\quad \\alpha\\_2^{new,unc} > H \\\\\\\n\\alpha\\_2^{new,unc}, \\quad \\quad L \\leq \\alpha\\_2^{new,unc} \\leq H \\\\\\\nL, \\quad \\quad \\quad \\quad \\alpha\\_2^{new,unc} < L\n\\end{cases}\n\\\\\\ where, \\quad \\quad \n\\begin{cases}\nL = max(0, \\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad if \\quad y\\_1 \\not= y\\_2 \\\\\\\nL = max(0, \\alpha\\_2^{old}+\\alpha\\_1^{old}-C), \\quad H = min(C, \\alpha\\_2^{old}+\\alpha\\_1^{old}), \\quad \\quad \\quad if \\quad y\\_1 = y\\_2\n\\end{cases}\n\\\\]\n然后由\\\\( \\alpha\\_2^{new}求得 \\alpha\\_1^{new} \\\\)为：\n\\\\[\n\\alpha\\_1^{new} = \\alpha\\_1^{old} + y\\_1y\\_2(\\alpha\\_2^{old} - \\alpha\\_2^{new})\n\\\\]\n至此，我们选择的两个变量，固定其他变量的最优化过程结束。\n\n\n#### 1.2 选择变量的启发式方法\n\n##### 1.2.1 第1个变量的选择\n\nSMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选择违反\\\\(KKT\\\\)条件最严重的样本点，并将其对应的变量作为第一个变量。\n具体地，检验样本点\\\\( (x\\_i, y\\_i) \\\\)是否满足\\\\(KKT\\\\)条件，即：\n\\\\[\n\\alpha\\_i = 0 \\Leftrightarrow y\\_if(x\\_i) \\geq 1 \\\\\\\n\\\\]\n\\\\[\n0 < \\alpha\\_i < C \\Leftrightarrow y\\_if(x\\_i)=1 \\\\\\\n\\\\]\n\\\\[\n\\alpha\\_i = C \\Leftrightarrow y\\_if(x\\_i) \\leq 1\n\\\\]\n其中\\\\( f(x\\_i) = \\sum\\_{j=1}^n \\alpha\\_j y\\_j K(x\\_i, x\\_j) +b \\\\)。\n\n以下几种情况出现将会导致\\\\(KKT\\\\)条件不满足：\n* \\\\(y\\_if(x\\_i) \\leq 1\\\\)但是\\\\(\\alpha\\_i <C\\\\)不满足，原本\\\\( \\alpha\\_i=C\\\\)；\n* \\\\(y\\_if(x\\_i) \\geq 1\\\\)但是\\\\(\\alpha\\_i > 0\\\\)不满足，原本\\\\( \\alpha\\_i=0\\\\)；\n* \\\\(y\\_if(x\\_i) = 1\\\\)但是\\\\(\\alpha\\_i=0或\\alpha\\_i=C\\\\)不满足，原本\\\\(0 < \\alpha\\_i < C\\\\)；\n\n##### 1.2.2 第2个变量的选择\n\nSMO称选择第二个变量的过程为内层循环，第二个变量选择的标准是希望能使\\\\(\\alpha\\_2\\\\)有足够大的变化。\n所以，对于第二个变变量，通常选择满足下式的样本点对应的变量：\n\\\\[\nmax\\| E\\_1 - E\\_2 \\|\n\\\\]\n\n特殊情况下，如果内存循环通过以上方法选择的\\\\( \\alpha\\_2 \\\\)不能使目标函数有足够的下降，那么采用以下启发式规则继续选择\\\\( \\alpha\\_2 \\\\)。\n* 遍历在间隔边界上的支持向量点，一次将其对应的变量作为\\\\( \\alpha\\_2 \\\\)试用，知道目标函数有足够的下降；\n* 若找不到合适的\\\\( \\alpha\\_2 \\\\)，那么遍历训练数据集；\n* 若仍找不到合适的\\\\( \\alpha\\_2 \\\\)，则放弃第一个\\\\( \\alpha\\_1 \\\\)，再通过外层循环寻求另外的\\\\( \\alpha\\_2 \\\\)。\n\n##### 1.2.3 计算阈值\\\\(b和差值E\\_i\\\\)\n\n每次完成两个变量的优化后，都要重新计算阈值\\\\(b\\\\)。当\\\\(0<\\alpha\\_i^{new}<C\\\\)时，由\\\\(KKT\\\\)条件可知：\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i K\\_{i1} +b = y\\_1\n\\\\]\n于是，\n\\\\[\n\\begin{cases}\nb\\_1^{new} = y\\_1 - \\sum\\_{i=3}^n \\alpha\\_i y\\_i K\\_{i1} - \\alpha\\_1^{new}y\\_1K\\_{11} - \\alpha\\_2^{new} y\\_2K\\_{21} \\\\\\\nE\\_1 = \\sum\\_{i=3}^n\\alpha\\_iy\\_iK\\_{i1} + \\alpha\\_1^{old}y\\_1K\\_{11} + \\alpha\\_2^{old}y\\_2K\\_{21} + b^{old} -y\\_1\n\\end{cases}\n\\\\\\ \\Downarrow\n\\\\]\n\\\\[\nb\\_1^{new} = -E\\_1 - y\\_1K\\_{11}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{21}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}\n\\\\]\n同样，如果\\\\( 0<\\alpha\\_2^{new} < C \\\\)，那么\n\\\\[\nb\\_2^{new} = -E\\_2 - y\\_1K\\_{12}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{22}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}\n\\\\]\n如果\\\\(\\alpha\\_1^{new}, \\alpha\\_2^{new}是0或C\\\\)，那么\\\\(b\\_1^{new}和b\\_2^{new}\\\\)以及它们之间的数都是符合\\\\(KKT\\\\)条件的阈值，此时选择它们的中点作为\\\\(b^{new}\\\\)。同时，每次完成两个变量的优化之后，还必须更新对应的\\\\(E\\_i\\\\)值。\n\\\\[\nE\\_i^{new} = \\sum\\_S y\\_j \\alpha\\_j K(x\\_i, x\\_j) + b^{new} - y\\_i\n\\\\]\n其中，\\\\(S\\\\)是所有支持向量\\\\(x\\_j\\\\)的集合。\n\n以上：\n\\\\[\nb = \n\\begin{cases}\nb\\_1^{new} = -E\\_1 - y\\_1K\\_{11}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{21}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}, \\quad 0 < \\alpha\\_1^{new} <C \\\\\\\nb\\_2^{new} = -E\\_2 - y\\_1K\\_{12}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{22}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}, \\quad 0 < \\alpha\\_2^{new} <C \\\\\\\n(b\\_1^{new} + b\\_2^{new}) / 2, \\quad otherwise\n\\end{cases}\n\\\\]\n\n\n--------------\n\n### 2. SMO算法步骤\n\n输入：训练数据集\\\\(T=\\lbrace (x\\_1,y\\_1), (x\\_2,y\\_2), ..., (x\\_n,y\\_n) \\rbrace\\\\)，其中，\n\\\\( x\\_i \\in \\chi = R^n, \\quad y\\_i \\in \\mathcal{y}=\\lbrace -1, +1 \\rbrace, \\quad i=1,2,...,n \\\\)，精度\\\\(\\epsilon\\\\)。\n\n输出：近似解\\\\( \\hat{\\alpha}\\\\)\n\n* (1)取初值\\\\( \\alpha^{(0)}=0, \\quad 令k=0 \\\\)；\n* (2)选取最优变量\\\\( \\alpha\\_1^{(k)}, \\alpha\\_2^{(k)} \\\\)，解析求解两个变量的最优化问题\\\\(式() \\sim 式() \\\\)，\n求的最优解\\\\( \\alpha\\_1^{(k+1)}, \\alpha\\_2^{(k+1)} \\\\)，更新\\\\( \\alpha^{k}为 \\alpha^{k+1} \\\\)；\n* (3)若在精度\\\\(\\epsilon\\\\)范围内满足停机条件\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n \\\\\\\ny\\_i \\cdot f(x\\_i) = \n\\begin{cases}\n\\geq 1, \\quad \\lbrace x\\_i \\| \\alpha\\_i = 0 \\rbrace \\\\\\\n= 1, \\quad \\lbrace x\\_i \\| 0 < \\alpha\\_i < C \\rbrace \\\\\\\n\\leq 1, \\quad \\lbrace x\\_i \\| \\alpha\\_i = C \\rbrace\n\\end{cases}\n\\\\]\n其中，\\\\[ f(x\\_i) = \\sum\\_{j=1}^n \\alpha\\_j y\\_j K(x\\_j, x\\_i) + b \\\\]\n满足则转(4)；否则令\\\\(k=k+1\\\\)，转(2)。\n* (4)取\\\\(\\hat{\\alpha} = \\alpha^{k+1} \\\\)。\n\n\n--------------\n\n### 3. 代码资料\n\n台湾的林智仁教授写了一个封装SVM算法的[libsvm库](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)，\n此外上海交通大学模式分析与机器智能实验室有一个[libsvm的注释文档](http://www.pami.sjtu.edu.cn/people/gpliu/document/libsvm_src.pdf)\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n> [July - 支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n","source":"_posts/2018-03-28-svm-3.md","raw":"---\nlayout: post\ntitle: Support Vector Machines - Part 3\ndate: 2018-03-28 20:13 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>支持向量机 - SVM（Support Vector Machines）Part 3</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n#### 目录\n* 序列最小最优化(SMO)算法\n* SMO算法步骤\n* 代码资料\n\n\n--------------\n\n### 1. 序列最小最优化(SMO)算法\n\n先看一下SMO要解决的问题:\n\n#### 线性可分支持向量机(硬间隔)\n\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i \\\\\\\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n\\quad \\quad \\alpha\\_i \\geq 0, i=1,2,...,n\n\\\\]\n\n#### 线性支持向量机(软间隔)\n\n\\\\[\n\\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j x\\_i^T x\\_j - \\sum\\_{i=1}^n \\alpha\\_i \n\\Longleftrightarrow \\mathop{\\min\\_{\\alpha}} \\frac{1}{2} \\sum\\_{i=1}^n \\sum\\_{j=1}^n \\alpha\\_i \\alpha\\_j y\\_i y\\_j K\\_{ij} - \\sum\\_{i=1}^n \\alpha\\_i\n\\\\]\n\\\\[\ns.t. \\quad \\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0\n\\\\]\n\\\\[\n\\quad \\quad 0 \\leq \\alpha\\_i \\leq C, i=1,2,...,n\n\\\\]\n其中\\\\(K\\_{ij}=K(x\\_i, x\\_j), i,j=1,2,...,n\\\\)\n\n下面要解决的问题是: \\\\( \\alpha = \\lbrace \\alpha\\_1, \\alpha\\_2, ..., \\alpha\\_n \\rbrace \\\\)上求上述目标函数的最小化。SMO的基本思路是：\n如果所有的变量的解都满足此最优化问题的\\\\(KKT\\\\)条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，\n这个二次规划问题关于这两个变量的解更接近于原始二次规划问题的解。整个SMO算法包括两部分：\n* 求解这两个变量二次规划的解析方法\n* 选择变量的启发式方法\n\n----------------\n\n#### 1.1 两个变量二次规划的求解方法\n\n随机选择两个变量\\\\(\\alpha\\_1, \\alpha\\_2\\\\)，其他变量固定。于是SMO的最优化问题\\\\(式(2)\\\\)可以写成：\n\\\\[\n\\mathop{\\min\\_{\\alpha\\_1, \\alpha\\_2}} W(\\alpha\\_1, \\alpha\\_2) = \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1\\alpha\\_1\\sum\\_{i=3}^n y\\_i \\alpha\\_iK\\_{i1} + y\\_2\\alpha\\_2\\sum\\_{i=3}^ny\\_i\\alpha\\_iK\\_{i2}\n\\\\]\n\\\\[\ns.t. \\quad \\alpha\\_1y\\_1 + \\alpha\\_2y\\_2 = - \\sum\\_{i=3}^n y\\_i \\alpha\\_i = \\varsigma\n\\\\]\n\\\\[\n\\quad \\quad 0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n\n\\\\]\n其中\\\\(K\\_{ij}=K(x\\_i, x\\_j), i,j=1,2,...,n, \\quad \\varsigma\\\\)是常数，另外式\\\\((5)\\\\)中省略了不含\\\\(\\alpha\\_1, \\alpha\\_2\\\\)的常数项。\n\n这里，我们引入新的变量\n\\\\[\nv\\_i = \\sum\\_{j=3}^n \\alpha\\_jy\\_jK\\_{ij} = f(x\\_i) - b - \\alpha\\_1 y\\_1 K\\_{i1} - \\alpha\\_2 y\\_2 K\\_{i2}\n\\\\]\n其中\\\\( f(x) = \\sum\\_{i=1}^n \\alpha_iy\\_i K(x\\_i, x) + b\\\\)，则式\\\\((5)\\\\)中的目标函数可以重新写为\n\\\\[\nW(\\alpha\\_1, \\alpha\\_2) =  \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1v\\_1\\alpha\\_1 + y\\_2v\\_2\\alpha\\_2 \n\\\\]\n\n下面我们研究一下约束条件\\\\(式(6) \\sim 式(7)\\\\)，由式\\\\((7)\\\\)可知，两个变量均要落在[0,C]x[0,C]的一个矩阵中，\n考虑\\\\(\\alpha\\_2\\\\)单变量的最优化问题，设问题\\\\(式(5) \\sim 式(7)\\\\)的初始可行解为\\\\( \\alpha\\_1^{old}, \\alpha\\_2^{old}\\\\)，最优解为\\\\( \\alpha\\_1^{new}, \\alpha\\_2^{new} \\\\)，\n并且假设在沿着约束方向上未经剪辑时\\\\(\\alpha\\_2\\\\)的最优解为\\\\(\\alpha\\_2^{new,unc}\\\\)。假设\\\\(\\alpha\\_2^{new}\\\\)的上下边界分别为\\\\(H和L\\\\),那么有：\n\\\\[\nL \\leq \\alpha\\_2^{new} \\leq H\n\\\\]\n接下来综合约束条件\\\\(0 \\leq \\alpha\\_i \\leq C,i=1,2,...,n和\\alpha\\_1^{new}y\\_i+\\alpha\\_2^{new}y\\_2=\\alpha\\_1^{old}y\\_1+\\alpha\\_2^{old}y\\_2=\\varsigma\\\\)，求取上下边界的值。\n\n以\\\\(y\\_1 \\not= y\\_2\\\\)为例，由\\\\(\\alpha\\_1^{new}y\\_i+\\alpha\\_2^{new}y\\_2=\\alpha\\_1^{old}y\\_1+\\alpha\\_2^{old}y\\_2=\\varsigma\\\\)可得：\n\\\\[\n\\alpha\\_2 = \\alpha\\_1 - \\varsigma \\quad [下方线段] \\quad 或 \\quad \\quad \\alpha\\_2 = - \\alpha\\_1 + \\varsigma \\quad [上方线段]\n\\\\]\n所以，当\\\\(\\alpha\\_1=0\\\\)时，\\\\( L = max(0, -\\varsigma) = max(0, \\alpha\\_2^{old} - \\alpha\\_1^{old}) [上方线段] \\\\)；\n\n所以，当\\\\(\\alpha\\_1=C\\\\)时，\\\\( H = min(C, C-\\varsigma) = min(C, C - (\\alpha\\_1^{old} - \\alpha\\_2^{old})) [下方线段] \\\\)；\n\n二维图像如下图左图所示，右图为\\\\(y\\_1=y\\_2\\\\)时的图像。\n\n![img](/posts_res/2018-03-28-svm/3-variables.png)\n\n如此，根据\\\\(y\\_1和y\\_2\\\\)异号或同号，可以得到如下\\\\(\\alpha\\_2^{new}\\\\)的上下界分别为：\n\\\\[\n\\begin{cases}\nL = max(0, \\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad if \\quad y\\_1 \\not= y\\_2 \\\\\\\nL = max(0, \\alpha\\_2^{old}+\\alpha\\_1^{old}-C), \\quad H = min(C, \\alpha\\_2^{old}+\\alpha\\_1^{old}), \\quad \\quad \\quad if \\quad y\\_1 = y\\_2\n\\end{cases}\n\\\\]\n\n下面开始求沿着约束方向未经剪辑[即未考虑不等式约束\\\\((7)\\\\)]时\\\\(\\alpha\\_2\\\\)的最优解\\\\(\\alpha\\_2^{new,unc}\\\\)；之后再求剪辑后\\\\(\\alpha\\_2\\\\)的解\\\\(\\alpha\\_2^{new}\\\\)。\n\\\\[\nf(x) = \\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b\n\\\\]\n\\\\[\nE\\_i = f(x\\_i) - y\\_i = (\\sum\\_{i=1}^n \\alpha\\_i y\\_i K(x\\_i, x) + b) - y\\_i\n\\\\]\n\n由\\\\( \\alpha\\_1y\\_1 + \\alpha\\_2y\\_2 = \\varsigma及y\\_i^2 = 1 \\\\)可得，\n\\\\[\n\\alpha_1 = (\\varsigma - \\alpha\\_2y\\_2)y\\_1\n\\\\]\n将\\\\(式(15)\\\\)代入\\\\(式(9)\\\\)中，得：\n\\\\[\nW(\\alpha\\_1, \\alpha\\_2) = \\frac{1}{2}K\\_{11}\\alpha\\_1^2 + \\frac{1}{2}K\\_{22}\\alpha\\_2^2 + y\\_1y\\_2K\\_{12}\\alpha\\_1 \\alpha\\_2 - (\\alpha\\_1+\\alpha\\_2) + y\\_1v\\_1\\alpha\\_1 + y\\_2v\\_2\\alpha\\_2\n\\\\]\n\\\\[\\Downarrow\\\\]\n\\\\[\nW(\\alpha\\_2) = \\frac{1}{2}K\\_{11}(\\varsigma-\\alpha\\_2y\\_2)^2+\\frac{1}{2}K\\_{22}\\alpha\\_2^2+y\\_2K\\_{12}(\\varsigma-\\alpha\\_2y\\_2)\\alpha\\_2 - (\\varsigma-\\alpha\\_2y\\_2)y\\_1 - \\alpha\\_2 + v\\_1(\\varsigma-\\alpha\\_2y\\_2)+y\\_2v\\_2\\alpha\\_2\n\\\\]\n之后对\\\\(\\alpha\\_2\\\\)求导数[Notice: \\\\( y\\_i^2 = 1 \\\\)]并令其等于零：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial W}{\\partial \\alpha\\_2}\n& = K\\_{11}(\\varsigma - \\alpha\\_2y\\_2)(-y\\_2) + K\\_{22}\\alpha\\_2 + y\\_2K\\_{12}\\varsigma - 2y\\_2K\\_{12}y\\_2\\alpha\\_2+y\\_1y\\_2 -1 - v\\_1y\\_2 + v\\_2y\\_2 \\\\\\\n& = K\\_{11}\\alpha\\_2 + K\\_{22}\\alpha\\_2 - 2K\\_{12}\\alpha\\_2 - K\\_{11}\\varsigma y\\_2 + K\\_{12}\\varsigma y\\_2 -1 -v\\_1y\\_2 + v\\_2y\\_2 \\\\\\\n& = 0\n\\end{aligned}\n\\end{equation}\n\\\\]\n得：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n(K\\_{11}+K\\_{22}-2K\\_{12})\\alpha\\_2 \n& = y\\_2(y\\_2 - y\\_1 + \\varsigma K\\_{11} - \\varsigma K\\_{12}+v\\_1-v\\_2) \\\\\\\n& = y\\_2 \\lbrace y\\_2 - y\\_1 + \\varsigma K\\_{11} - \\varsigma K\\_{12}+ [f(x\\_1) - \\sum\\_{j=1}^2 \\alpha\\_jy\\_jK\\_{1j} - b] - [f(x\\_2) - \\sum\\_{j=1}^2 \\alpha\\_jy\\_jK\\_{2j} - b] \\rbrace \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n将\\\\( \\varsigma = \\alpha\\_1^{old}y\\_1 + \\alpha\\_2^{old}y\\_2 \\\\)代入式\\\\((20)\\\\)，得：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n(K\\_{11}+K\\_{22}-2K\\_{12})\\alpha\\_2^{new,unc} \n& = y\\_2 \\lbrace (K\\_{11}+K\\_{12} - 2K\\_{12})\\alpha\\_2^{old}y\\_2 + y\\_2 - y\\_1 + f(x\\_1) - f(x\\_2) \\rbrace \\\\\\\n& = (K\\_{11}+K\\_{12} - 2K\\_{12})\\alpha\\_2^{old} + y\\_2(E\\_1 - E\\_2) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n令\\\\(\\eta = K\\_{11}+K\\_{22}-2K\\_{12}\\\\)，即得：\n\\\\[\n\\alpha\\_2^{new,unc} = \\alpha\\_2^{old} + \\frac{y\\_2(E\\_1-E\\_2)}{\\eta}\n\\\\]\n之后剪辑\\\\(\\alpha\\_2^{new,unc}\\\\)得：\n\\\\[\n\\alpha\\_2^{new} = \n\\begin{cases}\nH, \\quad \\quad \\quad \\quad \\alpha\\_2^{new,unc} > H \\\\\\\n\\alpha\\_2^{new,unc}, \\quad \\quad L \\leq \\alpha\\_2^{new,unc} \\leq H \\\\\\\nL, \\quad \\quad \\quad \\quad \\alpha\\_2^{new,unc} < L\n\\end{cases}\n\\\\\\ where, \\quad \\quad \n\\begin{cases}\nL = max(0, \\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha\\_2^{old}-\\alpha\\_1^{old}), \\quad if \\quad y\\_1 \\not= y\\_2 \\\\\\\nL = max(0, \\alpha\\_2^{old}+\\alpha\\_1^{old}-C), \\quad H = min(C, \\alpha\\_2^{old}+\\alpha\\_1^{old}), \\quad \\quad \\quad if \\quad y\\_1 = y\\_2\n\\end{cases}\n\\\\]\n然后由\\\\( \\alpha\\_2^{new}求得 \\alpha\\_1^{new} \\\\)为：\n\\\\[\n\\alpha\\_1^{new} = \\alpha\\_1^{old} + y\\_1y\\_2(\\alpha\\_2^{old} - \\alpha\\_2^{new})\n\\\\]\n至此，我们选择的两个变量，固定其他变量的最优化过程结束。\n\n\n#### 1.2 选择变量的启发式方法\n\n##### 1.2.1 第1个变量的选择\n\nSMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选择违反\\\\(KKT\\\\)条件最严重的样本点，并将其对应的变量作为第一个变量。\n具体地，检验样本点\\\\( (x\\_i, y\\_i) \\\\)是否满足\\\\(KKT\\\\)条件，即：\n\\\\[\n\\alpha\\_i = 0 \\Leftrightarrow y\\_if(x\\_i) \\geq 1 \\\\\\\n\\\\]\n\\\\[\n0 < \\alpha\\_i < C \\Leftrightarrow y\\_if(x\\_i)=1 \\\\\\\n\\\\]\n\\\\[\n\\alpha\\_i = C \\Leftrightarrow y\\_if(x\\_i) \\leq 1\n\\\\]\n其中\\\\( f(x\\_i) = \\sum\\_{j=1}^n \\alpha\\_j y\\_j K(x\\_i, x\\_j) +b \\\\)。\n\n以下几种情况出现将会导致\\\\(KKT\\\\)条件不满足：\n* \\\\(y\\_if(x\\_i) \\leq 1\\\\)但是\\\\(\\alpha\\_i <C\\\\)不满足，原本\\\\( \\alpha\\_i=C\\\\)；\n* \\\\(y\\_if(x\\_i) \\geq 1\\\\)但是\\\\(\\alpha\\_i > 0\\\\)不满足，原本\\\\( \\alpha\\_i=0\\\\)；\n* \\\\(y\\_if(x\\_i) = 1\\\\)但是\\\\(\\alpha\\_i=0或\\alpha\\_i=C\\\\)不满足，原本\\\\(0 < \\alpha\\_i < C\\\\)；\n\n##### 1.2.2 第2个变量的选择\n\nSMO称选择第二个变量的过程为内层循环，第二个变量选择的标准是希望能使\\\\(\\alpha\\_2\\\\)有足够大的变化。\n所以，对于第二个变变量，通常选择满足下式的样本点对应的变量：\n\\\\[\nmax\\| E\\_1 - E\\_2 \\|\n\\\\]\n\n特殊情况下，如果内存循环通过以上方法选择的\\\\( \\alpha\\_2 \\\\)不能使目标函数有足够的下降，那么采用以下启发式规则继续选择\\\\( \\alpha\\_2 \\\\)。\n* 遍历在间隔边界上的支持向量点，一次将其对应的变量作为\\\\( \\alpha\\_2 \\\\)试用，知道目标函数有足够的下降；\n* 若找不到合适的\\\\( \\alpha\\_2 \\\\)，那么遍历训练数据集；\n* 若仍找不到合适的\\\\( \\alpha\\_2 \\\\)，则放弃第一个\\\\( \\alpha\\_1 \\\\)，再通过外层循环寻求另外的\\\\( \\alpha\\_2 \\\\)。\n\n##### 1.2.3 计算阈值\\\\(b和差值E\\_i\\\\)\n\n每次完成两个变量的优化后，都要重新计算阈值\\\\(b\\\\)。当\\\\(0<\\alpha\\_i^{new}<C\\\\)时，由\\\\(KKT\\\\)条件可知：\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i K\\_{i1} +b = y\\_1\n\\\\]\n于是，\n\\\\[\n\\begin{cases}\nb\\_1^{new} = y\\_1 - \\sum\\_{i=3}^n \\alpha\\_i y\\_i K\\_{i1} - \\alpha\\_1^{new}y\\_1K\\_{11} - \\alpha\\_2^{new} y\\_2K\\_{21} \\\\\\\nE\\_1 = \\sum\\_{i=3}^n\\alpha\\_iy\\_iK\\_{i1} + \\alpha\\_1^{old}y\\_1K\\_{11} + \\alpha\\_2^{old}y\\_2K\\_{21} + b^{old} -y\\_1\n\\end{cases}\n\\\\\\ \\Downarrow\n\\\\]\n\\\\[\nb\\_1^{new} = -E\\_1 - y\\_1K\\_{11}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{21}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}\n\\\\]\n同样，如果\\\\( 0<\\alpha\\_2^{new} < C \\\\)，那么\n\\\\[\nb\\_2^{new} = -E\\_2 - y\\_1K\\_{12}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{22}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}\n\\\\]\n如果\\\\(\\alpha\\_1^{new}, \\alpha\\_2^{new}是0或C\\\\)，那么\\\\(b\\_1^{new}和b\\_2^{new}\\\\)以及它们之间的数都是符合\\\\(KKT\\\\)条件的阈值，此时选择它们的中点作为\\\\(b^{new}\\\\)。同时，每次完成两个变量的优化之后，还必须更新对应的\\\\(E\\_i\\\\)值。\n\\\\[\nE\\_i^{new} = \\sum\\_S y\\_j \\alpha\\_j K(x\\_i, x\\_j) + b^{new} - y\\_i\n\\\\]\n其中，\\\\(S\\\\)是所有支持向量\\\\(x\\_j\\\\)的集合。\n\n以上：\n\\\\[\nb = \n\\begin{cases}\nb\\_1^{new} = -E\\_1 - y\\_1K\\_{11}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{21}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}, \\quad 0 < \\alpha\\_1^{new} <C \\\\\\\nb\\_2^{new} = -E\\_2 - y\\_1K\\_{12}(\\alpha\\_1^{new} - \\alpha\\_1^{old}) - y\\_2K\\_{22}(\\alpha\\_2^{new} - \\alpha\\_2^{old}) + b^{old}, \\quad 0 < \\alpha\\_2^{new} <C \\\\\\\n(b\\_1^{new} + b\\_2^{new}) / 2, \\quad otherwise\n\\end{cases}\n\\\\]\n\n\n--------------\n\n### 2. SMO算法步骤\n\n输入：训练数据集\\\\(T=\\lbrace (x\\_1,y\\_1), (x\\_2,y\\_2), ..., (x\\_n,y\\_n) \\rbrace\\\\)，其中，\n\\\\( x\\_i \\in \\chi = R^n, \\quad y\\_i \\in \\mathcal{y}=\\lbrace -1, +1 \\rbrace, \\quad i=1,2,...,n \\\\)，精度\\\\(\\epsilon\\\\)。\n\n输出：近似解\\\\( \\hat{\\alpha}\\\\)\n\n* (1)取初值\\\\( \\alpha^{(0)}=0, \\quad 令k=0 \\\\)；\n* (2)选取最优变量\\\\( \\alpha\\_1^{(k)}, \\alpha\\_2^{(k)} \\\\)，解析求解两个变量的最优化问题\\\\(式() \\sim 式() \\\\)，\n求的最优解\\\\( \\alpha\\_1^{(k+1)}, \\alpha\\_2^{(k+1)} \\\\)，更新\\\\( \\alpha^{k}为 \\alpha^{k+1} \\\\)；\n* (3)若在精度\\\\(\\epsilon\\\\)范围内满足停机条件\n\\\\[\n\\sum\\_{i=1}^n \\alpha\\_i y\\_i = 0 \\\\\\\n0 \\leq \\alpha\\_i \\leq C, \\quad i=1,2,...,n \\\\\\\ny\\_i \\cdot f(x\\_i) = \n\\begin{cases}\n\\geq 1, \\quad \\lbrace x\\_i \\| \\alpha\\_i = 0 \\rbrace \\\\\\\n= 1, \\quad \\lbrace x\\_i \\| 0 < \\alpha\\_i < C \\rbrace \\\\\\\n\\leq 1, \\quad \\lbrace x\\_i \\| \\alpha\\_i = C \\rbrace\n\\end{cases}\n\\\\]\n其中，\\\\[ f(x\\_i) = \\sum\\_{j=1}^n \\alpha\\_j y\\_j K(x\\_j, x\\_i) + b \\\\]\n满足则转(4)；否则令\\\\(k=k+1\\\\)，转(2)。\n* (4)取\\\\(\\hat{\\alpha} = \\alpha^{k+1} \\\\)。\n\n\n--------------\n\n### 3. 代码资料\n\n台湾的林智仁教授写了一个封装SVM算法的[libsvm库](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)，\n此外上海交通大学模式分析与机器智能实验室有一个[libsvm的注释文档](http://www.pami.sjtu.edu.cn/people/gpliu/document/libsvm_src.pdf)\n\n\n-------------\n\n### 参考\n\n> 李航 - 《统计学习方法》\n\n> [July - 支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)\n\n","slug":"svm-3","published":1,"updated":"2019-08-17T09:33:46.881Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvg00182qwpyc9uiih4","content":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-3\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-3\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 3\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 3</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul><a id=\"more\"></a>\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>序列最小最优化(SMO)算法</li>\n<li>SMO算法步骤</li>\n<li>代码资料</li>\n</ul>\n<hr>\n<h3 id=\"1-序列最小最优化-SMO-算法\"><a href=\"#1-序列最小最优化-SMO-算法\" class=\"headerlink\" title=\"1. 序列最小最优化(SMO)算法\"></a>1. 序列最小最优化(SMO)算法</h3><p>先看一下SMO要解决的问题:</p>\n<h4 id=\"线性可分支持向量机-硬间隔\"><a href=\"#线性可分支持向量机-硬间隔\" class=\"headerlink\" title=\"线性可分支持向量机(硬间隔)\"></a>线性可分支持向量机(硬间隔)</h4><p>\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^n \\alpha_i \\\\\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n\\quad \\quad \\alpha_i \\geq 0, i=1,2,…,n\n\\]</p>\n<h4 id=\"线性支持向量机-软间隔\"><a href=\"#线性支持向量机-软间隔\" class=\"headerlink\" title=\"线性支持向量机(软间隔)\"></a>线性支持向量机(软间隔)</h4><p>\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^n \\alpha_i \n\\Longleftrightarrow \\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K_{ij} - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad 0 \\leq \\alpha_i \\leq C, i=1,2,…,n\n\\]\n其中\\(K_{ij}=K(x_i, x_j), i,j=1,2,…,n\\)</p>\n<p>下面要解决的问题是: \\( \\alpha = \\lbrace \\alpha_1, \\alpha_2, …, \\alpha_n \\rbrace \\)上求上述目标函数的最小化。SMO的基本思路是：\n如果所有的变量的解都满足此最优化问题的\\(KKT\\)条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，\n这个二次规划问题关于这两个变量的解更接近于原始二次规划问题的解。整个SMO算法包括两部分：</p>\n<ul>\n<li>求解这两个变量二次规划的解析方法</li>\n<li>选择变量的启发式方法</li>\n</ul>\n<hr>\n<h4 id=\"1-1-两个变量二次规划的求解方法\"><a href=\"#1-1-两个变量二次规划的求解方法\" class=\"headerlink\" title=\"1.1 两个变量二次规划的求解方法\"></a>1.1 两个变量二次规划的求解方法</h4><p>随机选择两个变量\\(\\alpha_1, \\alpha_2\\)，其他变量固定。于是SMO的最优化问题\\(式(2)\\)可以写成：\n\\[\n\\mathop{\\min_{\\alpha_1, \\alpha_2}} W(\\alpha_1, \\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1\\alpha_1\\sum_{i=3}^n y_i \\alpha_iK_{i1} + y_2\\alpha_2\\sum_{i=3}^ny_i\\alpha_iK_{i2}\n\\]\n\\[\ns.t. \\quad \\alpha_1y_1 + \\alpha_2y_2 = - \\sum_{i=3}^n y_i \\alpha_i = \\varsigma\n\\]\n\\[\n\\quad \\quad 0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n\n\\]\n其中\\(K_{ij}=K(x_i, x_j), i,j=1,2,…,n, \\quad \\varsigma\\)是常数，另外式\\((5)\\)中省略了不含\\(\\alpha_1, \\alpha_2\\)的常数项。</p>\n<p>这里，我们引入新的变量\n\\[\nv_i = \\sum_{j=3}^n \\alpha_jy_jK_{ij} = f(x_i) - b - \\alpha_1 y_1 K_{i1} - \\alpha_2 y_2 K_{i2}\n\\]\n其中\\( f(x) = \\sum_{i=1}^n \\alpha<em>iy_i K(x_i, x) + b\\)，则式\\((5)\\)中的目标函数可以重新写为\n\\[\nW(\\alpha_1, \\alpha_2) =  \\frac{1}{2}K\\</em>{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1v_1\\alpha_1 + y_2v_2\\alpha_2 \n\\]</p>\n<p>下面我们研究一下约束条件\\(式(6) \\sim 式(7)\\)，由式\\((7)\\)可知，两个变量均要落在[0,C]x[0,C]的一个矩阵中，\n考虑\\(\\alpha_2\\)单变量的最优化问题，设问题\\(式(5) \\sim 式(7)\\)的初始可行解为\\( \\alpha_1^{old}, \\alpha_2^{old}\\)，最优解为\\( \\alpha_1^{new}, \\alpha_2^{new} \\)，\n并且假设在沿着约束方向上未经剪辑时\\(\\alpha_2\\)的最优解为\\(\\alpha_2^{new,unc}\\)。假设\\(\\alpha_2^{new}\\)的上下边界分别为\\(H和L\\),那么有：\n\\[\nL \\leq \\alpha_2^{new} \\leq H\n\\]\n接下来综合约束条件\\(0 \\leq \\alpha_i \\leq C,i=1,2,…,n和\\alpha_1^{new}y_i+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\varsigma\\)，求取上下边界的值。</p>\n<p>以\\(y_1 \\not= y_2\\)为例，由\\(\\alpha_1^{new}y_i+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\varsigma\\)可得：\n\\[\n\\alpha_2 = \\alpha_1 - \\varsigma \\quad [下方线段] \\quad 或 \\quad \\quad \\alpha_2 = - \\alpha_1 + \\varsigma \\quad [上方线段]\n\\]\n所以，当\\(\\alpha_1=0\\)时，\\( L = max(0, -\\varsigma) = max(0, \\alpha_2^{old} - \\alpha_1^{old}) [上方线段] \\)；</p>\n<p>所以，当\\(\\alpha_1=C\\)时，\\( H = min(C, C-\\varsigma) = min(C, C - (\\alpha_1^{old} - \\alpha_2^{old})) [下方线段] \\)；</p>\n<p>二维图像如下图左图所示，右图为\\(y_1=y_2\\)时的图像。</p>\n<p><img src=\"/posts_res/2018-03-28-svm/3-variables.png\" alt=\"img\"></p>\n<p>如此，根据\\(y_1和y_2\\)异号或同号，可以得到如下\\(\\alpha_2^{new}\\)的上下界分别为：\n\\[\n\\begin{cases}\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha_2^{old}-\\alpha_1^{old}), \\quad if \\quad y_1 \\not= y_2 \\\\\nL = max(0, \\alpha_2^{old}+\\alpha_1^{old}-C), \\quad H = min(C, \\alpha_2^{old}+\\alpha_1^{old}), \\quad \\quad \\quad if \\quad y_1 = y_2\n\\end{cases}\n\\]</p>\n<p>下面开始求沿着约束方向未经剪辑[即未考虑不等式约束\\((7)\\)]时\\(\\alpha_2\\)的最优解\\(\\alpha_2^{new,unc}\\)；之后再求剪辑后\\(\\alpha_2\\)的解\\(\\alpha_2^{new}\\)。\n\\[\nf(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\]\n\\[\nE_i = f(x_i) - y_i = (\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b) - y_i\n\\]</p>\n<p>由\\( \\alpha_1y_1 + \\alpha_2y_2 = \\varsigma及y_i^2 = 1 \\)可得，\n\\[\n\\alpha<em>1 = (\\varsigma - \\alpha_2y_2)y_1\n\\]\n将\\(式(15)\\)代入\\(式(9)\\)中，得：\n\\[\nW(\\alpha_1, \\alpha_2) = \\frac{1}{2}K\\</em>{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1v_1\\alpha_1 + y_2v_2\\alpha_2\n\\]\n\\[\\Downarrow\\]\n\\[\nW(\\alpha_2) = \\frac{1}{2}K_{11}(\\varsigma-\\alpha_2y_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_2K_{12}(\\varsigma-\\alpha_2y_2)\\alpha_2 - (\\varsigma-\\alpha_2y_2)y_1 - \\alpha_2 + v_1(\\varsigma-\\alpha_2y_2)+y_2v_2\\alpha_2\n\\]\n之后对\\(\\alpha_2\\)求导数[Notice: \\( y_i^2 = 1 \\)]并令其等于零：\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial W}{\\partial \\alpha_2}\n&amp; = K_{11}(\\varsigma - \\alpha_2y_2)(-y_2) + K_{22}\\alpha_2 + y_2K_{12}\\varsigma - 2y_2K_{12}y_2\\alpha_2+y_1y_2 -1 - v_1y_2 + v_2y_2 \\\\\n&amp; = K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2 - K_{11}\\varsigma y_2 + K_{12}\\varsigma y_2 -1 -v_1y_2 + v_2y_2 \\\\\n&amp; = 0\n\\end{aligned}\n\\end{equation}\n\\]\n得：\n\\[\n\\begin{equation}\n\\begin{aligned}\n(K_{11}+K_{22}-2K_{12})\\alpha_2 \n&amp; = y_2(y_2 - y_1 + \\varsigma K_{11} - \\varsigma K_{12}+v_1-v_2) \\\\\n&amp; = y_2 \\lbrace y_2 - y_1 + \\varsigma K_{11} - \\varsigma K_{12}+ [f(x_1) - \\sum_{j=1}^2 \\alpha_jy_jK_{1j} - b] - [f(x_2) - \\sum_{j=1}^2 \\alpha_jy_jK_{2j} - b] \\rbrace \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n将\\( \\varsigma = \\alpha_1^{old}y_1 + \\alpha_2^{old}y_2 \\)代入式\\((20)\\)，得：\n\\[\n\\begin{equation}\n\\begin{aligned}\n(K_{11}+K_{22}-2K_{12})\\alpha_2^{new,unc} \n&amp; = y_2 \\lbrace (K_{11}+K_{12} - 2K_{12})\\alpha_2^{old}y_2 + y_2 - y_1 + f(x_1) - f(x_2) \\rbrace \\\\\n&amp; = (K_{11}+K_{12} - 2K_{12})\\alpha_2^{old} + y_2(E_1 - E_2) \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n令\\(\\eta = K_{11}+K_{22}-2K_{12}\\)，即得：\n\\[\n\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1-E_2)}{\\eta}\n\\]\n之后剪辑\\(\\alpha_2^{new,unc}\\)得：\n\\[\n\\alpha_2^{new} = \n\\begin{cases}\nH, \\quad \\quad \\quad \\quad \\alpha_2^{new,unc} &gt; H \\\\\n\\alpha_2^{new,unc}, \\quad \\quad L \\leq \\alpha_2^{new,unc} \\leq H \\\\\nL, \\quad \\quad \\quad \\quad \\alpha_2^{new,unc} &lt; L\n\\end{cases}\n\\\\ where, \\quad \\quad \n\\begin{cases}\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha_2^{old}-\\alpha_1^{old}), \\quad if \\quad y_1 \\not= y_2 \\\\\nL = max(0, \\alpha_2^{old}+\\alpha_1^{old}-C), \\quad H = min(C, \\alpha_2^{old}+\\alpha_1^{old}), \\quad \\quad \\quad if \\quad y_1 = y_2\n\\end{cases}\n\\]\n然后由\\( \\alpha_2^{new}求得 \\alpha_1^{new} \\)为：\n\\[\n\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new})\n\\]\n至此，我们选择的两个变量，固定其他变量的最优化过程结束。</p>\n<h4 id=\"1-2-选择变量的启发式方法\"><a href=\"#1-2-选择变量的启发式方法\" class=\"headerlink\" title=\"1.2 选择变量的启发式方法\"></a>1.2 选择变量的启发式方法</h4><h5 id=\"1-2-1-第1个变量的选择\"><a href=\"#1-2-1-第1个变量的选择\" class=\"headerlink\" title=\"1.2.1 第1个变量的选择\"></a>1.2.1 第1个变量的选择</h5><p>SMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选择违反\\(KKT\\)条件最严重的样本点，并将其对应的变量作为第一个变量。\n具体地，检验样本点\\( (x_i, y_i) \\)是否满足\\(KKT\\)条件，即：\n\\[\n\\alpha_i = 0 \\Leftrightarrow y_if(x_i) \\geq 1 \\\\\n\\]\n\\[\n0 &lt; \\alpha_i &lt; C \\Leftrightarrow y_if(x_i)=1 \\\\\n\\]\n\\[\n\\alpha_i = C \\Leftrightarrow y_if(x_i) \\leq 1\n\\]\n其中\\( f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_i, x_j) +b \\)。</p>\n<p>以下几种情况出现将会导致\\(KKT\\)条件不满足：</p>\n<ul>\n<li>\\(y_if(x_i) \\leq 1\\)但是\\(\\alpha_i &lt;C\\)不满足，原本\\( \\alpha_i=C\\)；</li>\n<li>\\(y_if(x_i) \\geq 1\\)但是\\(\\alpha_i &gt; 0\\)不满足，原本\\( \\alpha_i=0\\)；</li>\n<li>\\(y_if(x_i) = 1\\)但是\\(\\alpha_i=0或\\alpha_i=C\\)不满足，原本\\(0 &lt; \\alpha_i &lt; C\\)；</li>\n</ul>\n<h5 id=\"1-2-2-第2个变量的选择\"><a href=\"#1-2-2-第2个变量的选择\" class=\"headerlink\" title=\"1.2.2 第2个变量的选择\"></a>1.2.2 第2个变量的选择</h5><p>SMO称选择第二个变量的过程为内层循环，第二个变量选择的标准是希望能使\\(\\alpha_2\\)有足够大的变化。\n所以，对于第二个变变量，通常选择满足下式的样本点对应的变量：\n\\[\nmax| E_1 - E_2 |\n\\]</p>\n<p>特殊情况下，如果内存循环通过以上方法选择的\\( \\alpha_2 \\)不能使目标函数有足够的下降，那么采用以下启发式规则继续选择\\( \\alpha_2 \\)。</p>\n<ul>\n<li>遍历在间隔边界上的支持向量点，一次将其对应的变量作为\\( \\alpha_2 \\)试用，知道目标函数有足够的下降；</li>\n<li>若找不到合适的\\( \\alpha_2 \\)，那么遍历训练数据集；</li>\n<li>若仍找不到合适的\\( \\alpha_2 \\)，则放弃第一个\\( \\alpha_1 \\)，再通过外层循环寻求另外的\\( \\alpha_2 \\)。</li>\n</ul>\n<h5 id=\"1-2-3-计算阈值-b和差值E-i\"><a href=\"#1-2-3-计算阈值-b和差值E-i\" class=\"headerlink\" title=\"1.2.3 计算阈值\\(b和差值E_i\\)\"></a>1.2.3 计算阈值\\(b和差值E_i\\)</h5><p>每次完成两个变量的优化后，都要重新计算阈值\\(b\\)。当\\(0&lt;\\alpha_i^{new}&lt;C\\)时，由\\(KKT\\)条件可知：\n\\[\n\\sum_{i=1}^n \\alpha_i y_i K_{i1} +b = y_1\n\\]\n于是，\n\\[\n\\begin{cases}\nb_1^{new} = y_1 - \\sum_{i=3}^n \\alpha_i y_i K_{i1} - \\alpha_1^{new}y_1K_{11} - \\alpha_2^{new} y_2K_{21} \\\\\nE_1 = \\sum_{i=3}^n\\alpha_iy_iK_{i1} + \\alpha_1^{old}y_1K_{11} + \\alpha_2^{old}y_2K_{21} + b^{old} -y_1\n\\end{cases}\n\\\\ \\Downarrow\n\\]\n\\[\nb_1^{new} = -E_1 - y_1K_{11}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{21}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}\n\\]\n同样，如果\\( 0&lt;\\alpha_2^{new} &lt; C \\)，那么\n\\[\nb_2^{new} = -E_2 - y_1K_{12}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{22}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}\n\\]\n如果\\(\\alpha_1^{new}, \\alpha_2^{new}是0或C\\)，那么\\(b_1^{new}和b_2^{new}\\)以及它们之间的数都是符合\\(KKT\\)条件的阈值，此时选择它们的中点作为\\(b^{new}\\)。同时，每次完成两个变量的优化之后，还必须更新对应的\\(E_i\\)值。\n\\[\nE_i^{new} = \\sum_S y_j \\alpha_j K(x_i, x_j) + b^{new} - y_i\n\\]\n其中，\\(S\\)是所有支持向量\\(x_j\\)的集合。</p>\n<p>以上：\n\\[\nb = \n\\begin{cases}\nb_1^{new} = -E_1 - y_1K_{11}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{21}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}, \\quad 0 &lt; \\alpha_1^{new} &lt;C \\\\\nb_2^{new} = -E_2 - y_1K_{12}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{22}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}, \\quad 0 &lt; \\alpha_2^{new} &lt;C \\\\\n(b_1^{new} + b_2^{new}) / 2, \\quad otherwise\n\\end{cases}\n\\]</p>\n<hr>\n<h3 id=\"2-SMO算法步骤\"><a href=\"#2-SMO算法步骤\" class=\"headerlink\" title=\"2. SMO算法步骤\"></a>2. SMO算法步骤</h3><p>输入：训练数据集\\(T=\\lbrace (x_1,y_1), (x_2,y_2), …, (x_n,y_n) \\rbrace\\)，其中，\n\\( x_i \\in \\chi = R^n, \\quad y_i \\in \\mathcal{y}=\\lbrace -1, +1 \\rbrace, \\quad i=1,2,…,n \\)，精度\\(\\epsilon\\)。</p>\n<p>输出：近似解\\( \\hat{\\alpha}\\)</p>\n<ul>\n<li>(1)取初值\\( \\alpha^{(0)}=0, \\quad 令k=0 \\)；</li>\n<li>(2)选取最优变量\\( \\alpha_1^{(k)}, \\alpha_2^{(k)} \\)，解析求解两个变量的最优化问题\\(式() \\sim 式() \\)，\n求的最优解\\( \\alpha_1^{(k+1)}, \\alpha_2^{(k+1)} \\)，更新\\( \\alpha^{k}为 \\alpha^{k+1} \\)；</li>\n<li>(3)若在精度\\(\\epsilon\\)范围内满足停机条件\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n \\\\\ny_i \\cdot f(x_i) = \n\\begin{cases}\n\\geq 1, \\quad \\lbrace x_i | \\alpha_i = 0 \\rbrace \\\\\n= 1, \\quad \\lbrace x_i | 0 &lt; \\alpha_i &lt; C \\rbrace \\\\\n\\leq 1, \\quad \\lbrace x_i | \\alpha_i = C \\rbrace\n\\end{cases}\n\\]\n其中，\\[ f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b \\]\n满足则转(4)；否则令\\(k=k+1\\)，转(2)。</li>\n<li>(4)取\\(\\hat{\\alpha} = \\alpha^{k+1} \\)。</li>\n</ul>\n<hr>\n<h3 id=\"3-代码资料\"><a href=\"#3-代码资料\" class=\"headerlink\" title=\"3. 代码资料\"></a>3. 代码资料</h3><p>台湾的林智仁教授写了一个封装SVM算法的<a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" target=\"_blank\" rel=\"noopener\">libsvm库</a>，\n此外上海交通大学模式分析与机器智能实验室有一个<a href=\"http://www.pami.sjtu.edu.cn/people/gpliu/document/libsvm_src.pdf\" target=\"_blank\" rel=\"noopener\">libsvm的注释文档</a></p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">July - 支持向量机通俗导论（理解SVM的三层境界）</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-3\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-3\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 3\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 3</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul>","more":"\n\n\n\n\n\n\n\n<h4 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h4><ul>\n<li>序列最小最优化(SMO)算法</li>\n<li>SMO算法步骤</li>\n<li>代码资料</li>\n</ul>\n<hr>\n<h3 id=\"1-序列最小最优化-SMO-算法\"><a href=\"#1-序列最小最优化-SMO-算法\" class=\"headerlink\" title=\"1. 序列最小最优化(SMO)算法\"></a>1. 序列最小最优化(SMO)算法</h3><p>先看一下SMO要解决的问题:</p>\n<h4 id=\"线性可分支持向量机-硬间隔\"><a href=\"#线性可分支持向量机-硬间隔\" class=\"headerlink\" title=\"线性可分支持向量机(硬间隔)\"></a>线性可分支持向量机(硬间隔)</h4><p>\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^n \\alpha_i \\\\\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n\\quad \\quad \\alpha_i \\geq 0, i=1,2,…,n\n\\]</p>\n<h4 id=\"线性支持向量机-软间隔\"><a href=\"#线性支持向量机-软间隔\" class=\"headerlink\" title=\"线性支持向量机(软间隔)\"></a>线性支持向量机(软间隔)</h4><p>\\[\n\\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^n \\alpha_i \n\\Longleftrightarrow \\mathop{\\min_{\\alpha}} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K_{ij} - \\sum_{i=1}^n \\alpha_i\n\\]\n\\[\ns.t. \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\n\\[\n\\quad \\quad 0 \\leq \\alpha_i \\leq C, i=1,2,…,n\n\\]\n其中\\(K_{ij}=K(x_i, x_j), i,j=1,2,…,n\\)</p>\n<p>下面要解决的问题是: \\( \\alpha = \\lbrace \\alpha_1, \\alpha_2, …, \\alpha_n \\rbrace \\)上求上述目标函数的最小化。SMO的基本思路是：\n如果所有的变量的解都满足此最优化问题的\\(KKT\\)条件，那么这个最优化问题的解就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，\n这个二次规划问题关于这两个变量的解更接近于原始二次规划问题的解。整个SMO算法包括两部分：</p>\n<ul>\n<li>求解这两个变量二次规划的解析方法</li>\n<li>选择变量的启发式方法</li>\n</ul>\n<hr>\n<h4 id=\"1-1-两个变量二次规划的求解方法\"><a href=\"#1-1-两个变量二次规划的求解方法\" class=\"headerlink\" title=\"1.1 两个变量二次规划的求解方法\"></a>1.1 两个变量二次规划的求解方法</h4><p>随机选择两个变量\\(\\alpha_1, \\alpha_2\\)，其他变量固定。于是SMO的最优化问题\\(式(2)\\)可以写成：\n\\[\n\\mathop{\\min_{\\alpha_1, \\alpha_2}} W(\\alpha_1, \\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1\\alpha_1\\sum_{i=3}^n y_i \\alpha_iK_{i1} + y_2\\alpha_2\\sum_{i=3}^ny_i\\alpha_iK_{i2}\n\\]\n\\[\ns.t. \\quad \\alpha_1y_1 + \\alpha_2y_2 = - \\sum_{i=3}^n y_i \\alpha_i = \\varsigma\n\\]\n\\[\n\\quad \\quad 0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n\n\\]\n其中\\(K_{ij}=K(x_i, x_j), i,j=1,2,…,n, \\quad \\varsigma\\)是常数，另外式\\((5)\\)中省略了不含\\(\\alpha_1, \\alpha_2\\)的常数项。</p>\n<p>这里，我们引入新的变量\n\\[\nv_i = \\sum_{j=3}^n \\alpha_jy_jK_{ij} = f(x_i) - b - \\alpha_1 y_1 K_{i1} - \\alpha_2 y_2 K_{i2}\n\\]\n其中\\( f(x) = \\sum_{i=1}^n \\alpha<em>iy_i K(x_i, x) + b\\)，则式\\((5)\\)中的目标函数可以重新写为\n\\[\nW(\\alpha_1, \\alpha_2) =  \\frac{1}{2}K\\</em>{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1v_1\\alpha_1 + y_2v_2\\alpha_2 \n\\]</p>\n<p>下面我们研究一下约束条件\\(式(6) \\sim 式(7)\\)，由式\\((7)\\)可知，两个变量均要落在[0,C]x[0,C]的一个矩阵中，\n考虑\\(\\alpha_2\\)单变量的最优化问题，设问题\\(式(5) \\sim 式(7)\\)的初始可行解为\\( \\alpha_1^{old}, \\alpha_2^{old}\\)，最优解为\\( \\alpha_1^{new}, \\alpha_2^{new} \\)，\n并且假设在沿着约束方向上未经剪辑时\\(\\alpha_2\\)的最优解为\\(\\alpha_2^{new,unc}\\)。假设\\(\\alpha_2^{new}\\)的上下边界分别为\\(H和L\\),那么有：\n\\[\nL \\leq \\alpha_2^{new} \\leq H\n\\]\n接下来综合约束条件\\(0 \\leq \\alpha_i \\leq C,i=1,2,…,n和\\alpha_1^{new}y_i+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\varsigma\\)，求取上下边界的值。</p>\n<p>以\\(y_1 \\not= y_2\\)为例，由\\(\\alpha_1^{new}y_i+\\alpha_2^{new}y_2=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2=\\varsigma\\)可得：\n\\[\n\\alpha_2 = \\alpha_1 - \\varsigma \\quad [下方线段] \\quad 或 \\quad \\quad \\alpha_2 = - \\alpha_1 + \\varsigma \\quad [上方线段]\n\\]\n所以，当\\(\\alpha_1=0\\)时，\\( L = max(0, -\\varsigma) = max(0, \\alpha_2^{old} - \\alpha_1^{old}) [上方线段] \\)；</p>\n<p>所以，当\\(\\alpha_1=C\\)时，\\( H = min(C, C-\\varsigma) = min(C, C - (\\alpha_1^{old} - \\alpha_2^{old})) [下方线段] \\)；</p>\n<p>二维图像如下图左图所示，右图为\\(y_1=y_2\\)时的图像。</p>\n<p><img src=\"/posts_res/2018-03-28-svm/3-variables.png\" alt=\"img\"></p>\n<p>如此，根据\\(y_1和y_2\\)异号或同号，可以得到如下\\(\\alpha_2^{new}\\)的上下界分别为：\n\\[\n\\begin{cases}\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha_2^{old}-\\alpha_1^{old}), \\quad if \\quad y_1 \\not= y_2 \\\\\nL = max(0, \\alpha_2^{old}+\\alpha_1^{old}-C), \\quad H = min(C, \\alpha_2^{old}+\\alpha_1^{old}), \\quad \\quad \\quad if \\quad y_1 = y_2\n\\end{cases}\n\\]</p>\n<p>下面开始求沿着约束方向未经剪辑[即未考虑不等式约束\\((7)\\)]时\\(\\alpha_2\\)的最优解\\(\\alpha_2^{new,unc}\\)；之后再求剪辑后\\(\\alpha_2\\)的解\\(\\alpha_2^{new}\\)。\n\\[\nf(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\]\n\\[\nE_i = f(x_i) - y_i = (\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b) - y_i\n\\]</p>\n<p>由\\( \\alpha_1y_1 + \\alpha_2y_2 = \\varsigma及y_i^2 = 1 \\)可得，\n\\[\n\\alpha<em>1 = (\\varsigma - \\alpha_2y_2)y_1\n\\]\n将\\(式(15)\\)代入\\(式(9)\\)中，得：\n\\[\nW(\\alpha_1, \\alpha_2) = \\frac{1}{2}K\\</em>{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + y_1y_2K_{12}\\alpha_1 \\alpha_2 - (\\alpha_1+\\alpha_2) + y_1v_1\\alpha_1 + y_2v_2\\alpha_2\n\\]\n\\[\\Downarrow\\]\n\\[\nW(\\alpha_2) = \\frac{1}{2}K_{11}(\\varsigma-\\alpha_2y_2)^2+\\frac{1}{2}K_{22}\\alpha_2^2+y_2K_{12}(\\varsigma-\\alpha_2y_2)\\alpha_2 - (\\varsigma-\\alpha_2y_2)y_1 - \\alpha_2 + v_1(\\varsigma-\\alpha_2y_2)+y_2v_2\\alpha_2\n\\]\n之后对\\(\\alpha_2\\)求导数[Notice: \\( y_i^2 = 1 \\)]并令其等于零：\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial W}{\\partial \\alpha_2}\n&amp; = K_{11}(\\varsigma - \\alpha_2y_2)(-y_2) + K_{22}\\alpha_2 + y_2K_{12}\\varsigma - 2y_2K_{12}y_2\\alpha_2+y_1y_2 -1 - v_1y_2 + v_2y_2 \\\\\n&amp; = K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2 - K_{11}\\varsigma y_2 + K_{12}\\varsigma y_2 -1 -v_1y_2 + v_2y_2 \\\\\n&amp; = 0\n\\end{aligned}\n\\end{equation}\n\\]\n得：\n\\[\n\\begin{equation}\n\\begin{aligned}\n(K_{11}+K_{22}-2K_{12})\\alpha_2 \n&amp; = y_2(y_2 - y_1 + \\varsigma K_{11} - \\varsigma K_{12}+v_1-v_2) \\\\\n&amp; = y_2 \\lbrace y_2 - y_1 + \\varsigma K_{11} - \\varsigma K_{12}+ [f(x_1) - \\sum_{j=1}^2 \\alpha_jy_jK_{1j} - b] - [f(x_2) - \\sum_{j=1}^2 \\alpha_jy_jK_{2j} - b] \\rbrace \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n将\\( \\varsigma = \\alpha_1^{old}y_1 + \\alpha_2^{old}y_2 \\)代入式\\((20)\\)，得：\n\\[\n\\begin{equation}\n\\begin{aligned}\n(K_{11}+K_{22}-2K_{12})\\alpha_2^{new,unc} \n&amp; = y_2 \\lbrace (K_{11}+K_{12} - 2K_{12})\\alpha_2^{old}y_2 + y_2 - y_1 + f(x_1) - f(x_2) \\rbrace \\\\\n&amp; = (K_{11}+K_{12} - 2K_{12})\\alpha_2^{old} + y_2(E_1 - E_2) \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n令\\(\\eta = K_{11}+K_{22}-2K_{12}\\)，即得：\n\\[\n\\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1-E_2)}{\\eta}\n\\]\n之后剪辑\\(\\alpha_2^{new,unc}\\)得：\n\\[\n\\alpha_2^{new} = \n\\begin{cases}\nH, \\quad \\quad \\quad \\quad \\alpha_2^{new,unc} &gt; H \\\\\n\\alpha_2^{new,unc}, \\quad \\quad L \\leq \\alpha_2^{new,unc} \\leq H \\\\\nL, \\quad \\quad \\quad \\quad \\alpha_2^{new,unc} &lt; L\n\\end{cases}\n\\\\ where, \\quad \\quad \n\\begin{cases}\nL = max(0, \\alpha_2^{old}-\\alpha_1^{old}), \\quad \\quad \\quad H = min(C, C+\\alpha_2^{old}-\\alpha_1^{old}), \\quad if \\quad y_1 \\not= y_2 \\\\\nL = max(0, \\alpha_2^{old}+\\alpha_1^{old}-C), \\quad H = min(C, \\alpha_2^{old}+\\alpha_1^{old}), \\quad \\quad \\quad if \\quad y_1 = y_2\n\\end{cases}\n\\]\n然后由\\( \\alpha_2^{new}求得 \\alpha_1^{new} \\)为：\n\\[\n\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new})\n\\]\n至此，我们选择的两个变量，固定其他变量的最优化过程结束。</p>\n<h4 id=\"1-2-选择变量的启发式方法\"><a href=\"#1-2-选择变量的启发式方法\" class=\"headerlink\" title=\"1.2 选择变量的启发式方法\"></a>1.2 选择变量的启发式方法</h4><h5 id=\"1-2-1-第1个变量的选择\"><a href=\"#1-2-1-第1个变量的选择\" class=\"headerlink\" title=\"1.2.1 第1个变量的选择\"></a>1.2.1 第1个变量的选择</h5><p>SMO称选择第一个变量的过程为外层循环，外层循环在训练样本中选择违反\\(KKT\\)条件最严重的样本点，并将其对应的变量作为第一个变量。\n具体地，检验样本点\\( (x_i, y_i) \\)是否满足\\(KKT\\)条件，即：\n\\[\n\\alpha_i = 0 \\Leftrightarrow y_if(x_i) \\geq 1 \\\\\n\\]\n\\[\n0 &lt; \\alpha_i &lt; C \\Leftrightarrow y_if(x_i)=1 \\\\\n\\]\n\\[\n\\alpha_i = C \\Leftrightarrow y_if(x_i) \\leq 1\n\\]\n其中\\( f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_i, x_j) +b \\)。</p>\n<p>以下几种情况出现将会导致\\(KKT\\)条件不满足：</p>\n<ul>\n<li>\\(y_if(x_i) \\leq 1\\)但是\\(\\alpha_i &lt;C\\)不满足，原本\\( \\alpha_i=C\\)；</li>\n<li>\\(y_if(x_i) \\geq 1\\)但是\\(\\alpha_i &gt; 0\\)不满足，原本\\( \\alpha_i=0\\)；</li>\n<li>\\(y_if(x_i) = 1\\)但是\\(\\alpha_i=0或\\alpha_i=C\\)不满足，原本\\(0 &lt; \\alpha_i &lt; C\\)；</li>\n</ul>\n<h5 id=\"1-2-2-第2个变量的选择\"><a href=\"#1-2-2-第2个变量的选择\" class=\"headerlink\" title=\"1.2.2 第2个变量的选择\"></a>1.2.2 第2个变量的选择</h5><p>SMO称选择第二个变量的过程为内层循环，第二个变量选择的标准是希望能使\\(\\alpha_2\\)有足够大的变化。\n所以，对于第二个变变量，通常选择满足下式的样本点对应的变量：\n\\[\nmax| E_1 - E_2 |\n\\]</p>\n<p>特殊情况下，如果内存循环通过以上方法选择的\\( \\alpha_2 \\)不能使目标函数有足够的下降，那么采用以下启发式规则继续选择\\( \\alpha_2 \\)。</p>\n<ul>\n<li>遍历在间隔边界上的支持向量点，一次将其对应的变量作为\\( \\alpha_2 \\)试用，知道目标函数有足够的下降；</li>\n<li>若找不到合适的\\( \\alpha_2 \\)，那么遍历训练数据集；</li>\n<li>若仍找不到合适的\\( \\alpha_2 \\)，则放弃第一个\\( \\alpha_1 \\)，再通过外层循环寻求另外的\\( \\alpha_2 \\)。</li>\n</ul>\n<h5 id=\"1-2-3-计算阈值-b和差值E-i\"><a href=\"#1-2-3-计算阈值-b和差值E-i\" class=\"headerlink\" title=\"1.2.3 计算阈值\\(b和差值E_i\\)\"></a>1.2.3 计算阈值\\(b和差值E_i\\)</h5><p>每次完成两个变量的优化后，都要重新计算阈值\\(b\\)。当\\(0&lt;\\alpha_i^{new}&lt;C\\)时，由\\(KKT\\)条件可知：\n\\[\n\\sum_{i=1}^n \\alpha_i y_i K_{i1} +b = y_1\n\\]\n于是，\n\\[\n\\begin{cases}\nb_1^{new} = y_1 - \\sum_{i=3}^n \\alpha_i y_i K_{i1} - \\alpha_1^{new}y_1K_{11} - \\alpha_2^{new} y_2K_{21} \\\\\nE_1 = \\sum_{i=3}^n\\alpha_iy_iK_{i1} + \\alpha_1^{old}y_1K_{11} + \\alpha_2^{old}y_2K_{21} + b^{old} -y_1\n\\end{cases}\n\\\\ \\Downarrow\n\\]\n\\[\nb_1^{new} = -E_1 - y_1K_{11}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{21}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}\n\\]\n同样，如果\\( 0&lt;\\alpha_2^{new} &lt; C \\)，那么\n\\[\nb_2^{new} = -E_2 - y_1K_{12}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{22}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}\n\\]\n如果\\(\\alpha_1^{new}, \\alpha_2^{new}是0或C\\)，那么\\(b_1^{new}和b_2^{new}\\)以及它们之间的数都是符合\\(KKT\\)条件的阈值，此时选择它们的中点作为\\(b^{new}\\)。同时，每次完成两个变量的优化之后，还必须更新对应的\\(E_i\\)值。\n\\[\nE_i^{new} = \\sum_S y_j \\alpha_j K(x_i, x_j) + b^{new} - y_i\n\\]\n其中，\\(S\\)是所有支持向量\\(x_j\\)的集合。</p>\n<p>以上：\n\\[\nb = \n\\begin{cases}\nb_1^{new} = -E_1 - y_1K_{11}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{21}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}, \\quad 0 &lt; \\alpha_1^{new} &lt;C \\\\\nb_2^{new} = -E_2 - y_1K_{12}(\\alpha_1^{new} - \\alpha_1^{old}) - y_2K_{22}(\\alpha_2^{new} - \\alpha_2^{old}) + b^{old}, \\quad 0 &lt; \\alpha_2^{new} &lt;C \\\\\n(b_1^{new} + b_2^{new}) / 2, \\quad otherwise\n\\end{cases}\n\\]</p>\n<hr>\n<h3 id=\"2-SMO算法步骤\"><a href=\"#2-SMO算法步骤\" class=\"headerlink\" title=\"2. SMO算法步骤\"></a>2. SMO算法步骤</h3><p>输入：训练数据集\\(T=\\lbrace (x_1,y_1), (x_2,y_2), …, (x_n,y_n) \\rbrace\\)，其中，\n\\( x_i \\in \\chi = R^n, \\quad y_i \\in \\mathcal{y}=\\lbrace -1, +1 \\rbrace, \\quad i=1,2,…,n \\)，精度\\(\\epsilon\\)。</p>\n<p>输出：近似解\\( \\hat{\\alpha}\\)</p>\n<ul>\n<li>(1)取初值\\( \\alpha^{(0)}=0, \\quad 令k=0 \\)；</li>\n<li>(2)选取最优变量\\( \\alpha_1^{(k)}, \\alpha_2^{(k)} \\)，解析求解两个变量的最优化问题\\(式() \\sim 式() \\)，\n求的最优解\\( \\alpha_1^{(k+1)}, \\alpha_2^{(k+1)} \\)，更新\\( \\alpha^{k}为 \\alpha^{k+1} \\)；</li>\n<li>(3)若在精度\\(\\epsilon\\)范围内满足停机条件\n\\[\n\\sum_{i=1}^n \\alpha_i y_i = 0 \\\\\n0 \\leq \\alpha_i \\leq C, \\quad i=1,2,…,n \\\\\ny_i \\cdot f(x_i) = \n\\begin{cases}\n\\geq 1, \\quad \\lbrace x_i | \\alpha_i = 0 \\rbrace \\\\\n= 1, \\quad \\lbrace x_i | 0 &lt; \\alpha_i &lt; C \\rbrace \\\\\n\\leq 1, \\quad \\lbrace x_i | \\alpha_i = C \\rbrace\n\\end{cases}\n\\]\n其中，\\[ f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b \\]\n满足则转(4)；否则令\\(k=k+1\\)，转(2)。</li>\n<li>(4)取\\(\\hat{\\alpha} = \\alpha^{k+1} \\)。</li>\n</ul>\n<hr>\n<h3 id=\"3-代码资料\"><a href=\"#3-代码资料\" class=\"headerlink\" title=\"3. 代码资料\"></a>3. 代码资料</h3><p>台湾的林智仁教授写了一个封装SVM算法的<a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\" target=\"_blank\" rel=\"noopener\">libsvm库</a>，\n此外上海交通大学模式分析与机器智能实验室有一个<a href=\"http://www.pami.sjtu.edu.cn/people/gpliu/document/libsvm_src.pdf\" target=\"_blank\" rel=\"noopener\">libsvm的注释文档</a></p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n<p><a href=\"https://blog.csdn.net/v_july_v/article/details/7624837\" target=\"_blank\" rel=\"noopener\">July - 支持向量机通俗导论（理解SVM的三层境界）</a></p>\n</blockquote>\n"},{"layout":"post","title":"Support Vector Machines - Part 4","date":"2018-03-28T14:14:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>支持向量机 - SVM（Support Vector Machines）Part 4</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n目录\n\n* 支持向量回归 - SVR\n\n\n----------\n\n### 支持向量回归 - SVR\n\n回归和分类从某种意义上讲，本质上是一回事。\n* SVM分类，就是找到一个平面，让两个分类集合的支持向量或者所有的数据离分类平面最远；\n* SVR回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。\n\n假设某个元素到回归平面的距离为$ r = d(x) − g(x) $ 。另外，由于数据不可能都在回归平面上，距离之和还是挺大，因此所有数据到回归平面的距离可以给定一个容忍值$ \\epsilon$ 防止过拟合。该参数是经验参数，需要人工给定。如果数据元素到回归平面的距离小于$ \\epsilon $，则代价为0。SVR的代价函数可以表示为：\n\n$$ cost(x) = \\max ( 0, \\quad \\| d(x)-g(x) \\| - \\epsilon ) $$\n\n其中$d(x)$为样本的标签。考虑松弛变量$ \\zeta_i, \\zeta_i^{\\ast} $，分别表示上下边界的松弛因子，约束条件即为：\n\n$$\n\\begin{cases}\nd(x_i) - g(x_i) < \\epsilon + \\zeta_i, \\qquad \\zeta_i \\geq 0 \\\\\ng(x_i) - d(x_i) < \\epsilon + \\zeta_i^{\\ast}, \\quad \\zeta_i^{\\ast} \\geq 0\n\\end{cases}\n$$\n\n实际上要最小化$ \\zeta_i, \\zeta_i^{\\ast} $，为了获得$w$的稀疏解，且假设$w$的计算结果满足正态分布，根据贝叶斯线性回归模型，对$w$有$L_2$范数约束。\nSVR可以转化为最优化问题：\n\n$$\n\\psi (x) = \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} C' w^T w \\\\\n\\Downarrow \\\\\n\\psi (x) = C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} w^T w \n$$\n\n其中$C$是惩罚因子，是人为给定的经验参数。考虑约束条件，引入拉格朗日算子$\\alpha, \\alpha^{\\ast}, \\beta, \\beta^{\\ast}$，\n将最优化问题转化为对偶问题：\n\n$$ \nJ=\\frac{1}{2} w^T w + C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) \n+\\sum_i \\alpha_i \\left[ d(x_i) − g(x_i) − \\epsilon − \\zeta_i \\right] + \\sum_i \\alpha_i^{\\ast} \\left[ g(x_i) − d(x_i) − \\epsilon - \\zeta_i^{\\ast} \\right]\n−\\sum_i \\beta_i \\zeta_i − \\sum_i \\beta_i^{\\ast} \\zeta_i^{\\ast}\n$$\n\n然后分别求导得到：\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &= w − ( \\sum_i \\alpha_i x_i − \\sum_i \\alpha_i^{\\ast} x_i) = 0 \\\\\n\\frac{\\partial J}{\\partial b} &= \\sum_i ( \\alpha_i − \\alpha_i^{\\ast} ) = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i} &= C − \\alpha_i − \\beta_i = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i^{\\ast}} &= C − \\alpha_i^{\\ast} − \\beta_i^{\\ast} = 0 \\\\\nC &= \\alpha_i + \\beta_i = \\alpha_i^{\\ast} + \\beta_i^{\\ast}\n\\end{aligned}\n\\end{equation}\n$$　　\n\n将上述式子代入$J$函数有： \n\n$$\n\\begin{equation}\n\\begin{aligned}\nJ\n&= \\frac{1}{2}w^Tw − \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) w x_i − b \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\\\ \n&+ \\underbrace{C \\sum_i (\\zeta_i + \\zeta_i^{\\ast})− \\sum_i \\alpha_i \\zeta_i − \\sum_i \\alpha_i^{\\ast} \\zeta_i^{\\ast} − \\sum_i (C−\\alpha_i)\\zeta_i − \\sum_i(C−\\alpha_i^{\\ast}) \\zeta_i^{\\ast}}_{这部分可以相互抵消，变为0} \\\\\n&= \\frac{1}{2} (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) d(x_i) \\\\\n&= − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) ( \\alpha_j − \\alpha_j^{\\ast} ) x_i x_j − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\\\ \ns.t. \\qquad 0 \\leq \\quad \\alpha_i, \\alpha_i^{\\ast} \\quad \\leq C\n\\end{aligned}\n\\end{equation}\n$$\n\n其中$\\zeta, \\zeta^{\\ast}, \\beta, \\beta^{\\ast}$都在计算过程中抵消了。$\\epsilon, C$则是人为给定的参数，是常量。如果要使用核函数，可以将上式写成：\n\n$$\nJ = − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) (\\alpha_j − \\alpha_j^{\\ast}) K(x_i, x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast} ) \\epsilon + \\sum_i ( \\alpha_i − \\alpha_i^{\\ast})d(x_i)\n$$\n\nSVR的代价函数和SVM的很相似，但是最优化的对象却不同，对偶式有很大不同，解法同样都是基于拉格朗日的最优化问题解法。\n求解这类问题的早期解法非常复杂，后来出来很多新的较为简单的解法，对数学和编程水平要求高，对大部分工程学人士来说还是颇为复杂和难以实现，\n因此大牛们推出了一些SVM库。比较出名的有libSVM，该库同时实现了SVM和SVR。 \n\n\n---------------\n\n>\n[SVR支持向量机回归](https://blog.csdn.net/lpsl1882/article/details/52411987)\n>","source":"_posts/2018-03-28-svm-4.md","raw":"---\nlayout: post\ntitle: Support Vector Machines - Part 4\ndate: 2018-03-28 22:14 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>支持向量机 - SVM（Support Vector Machines）Part 4</center>\n\n[支持向量机 - SVM（Support Vector Machines）Part 1](/2018/03/svm-1/)\n* 线性可分支持向量机学习算法 - 最大间隔法\n* 线性可分支持向量机的对偶算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 2](/2018/03/svm-2/)\n* 线性支持向量机\n* 核函数及非线性支持向量机\n* 常用的核函数及其特点\n\n[支持向量机 - SVM（Support Vector Machines）Part 3](/2018/03/svm-3/)\n* 序列最小最优化(SMO)算法\n\n[支持向量机 - SVM（Support Vector Machines）Part 4](/2018/03/svm-4/)\n* 支持向量回归 - SVR\n\n\n目录\n\n* 支持向量回归 - SVR\n\n\n----------\n\n### 支持向量回归 - SVR\n\n回归和分类从某种意义上讲，本质上是一回事。\n* SVM分类，就是找到一个平面，让两个分类集合的支持向量或者所有的数据离分类平面最远；\n* SVR回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。\n\n假设某个元素到回归平面的距离为$ r = d(x) − g(x) $ 。另外，由于数据不可能都在回归平面上，距离之和还是挺大，因此所有数据到回归平面的距离可以给定一个容忍值$ \\epsilon$ 防止过拟合。该参数是经验参数，需要人工给定。如果数据元素到回归平面的距离小于$ \\epsilon $，则代价为0。SVR的代价函数可以表示为：\n\n$$ cost(x) = \\max ( 0, \\quad \\| d(x)-g(x) \\| - \\epsilon ) $$\n\n其中$d(x)$为样本的标签。考虑松弛变量$ \\zeta_i, \\zeta_i^{\\ast} $，分别表示上下边界的松弛因子，约束条件即为：\n\n$$\n\\begin{cases}\nd(x_i) - g(x_i) < \\epsilon + \\zeta_i, \\qquad \\zeta_i \\geq 0 \\\\\ng(x_i) - d(x_i) < \\epsilon + \\zeta_i^{\\ast}, \\quad \\zeta_i^{\\ast} \\geq 0\n\\end{cases}\n$$\n\n实际上要最小化$ \\zeta_i, \\zeta_i^{\\ast} $，为了获得$w$的稀疏解，且假设$w$的计算结果满足正态分布，根据贝叶斯线性回归模型，对$w$有$L_2$范数约束。\nSVR可以转化为最优化问题：\n\n$$\n\\psi (x) = \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} C' w^T w \\\\\n\\Downarrow \\\\\n\\psi (x) = C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} w^T w \n$$\n\n其中$C$是惩罚因子，是人为给定的经验参数。考虑约束条件，引入拉格朗日算子$\\alpha, \\alpha^{\\ast}, \\beta, \\beta^{\\ast}$，\n将最优化问题转化为对偶问题：\n\n$$ \nJ=\\frac{1}{2} w^T w + C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) \n+\\sum_i \\alpha_i \\left[ d(x_i) − g(x_i) − \\epsilon − \\zeta_i \\right] + \\sum_i \\alpha_i^{\\ast} \\left[ g(x_i) − d(x_i) − \\epsilon - \\zeta_i^{\\ast} \\right]\n−\\sum_i \\beta_i \\zeta_i − \\sum_i \\beta_i^{\\ast} \\zeta_i^{\\ast}\n$$\n\n然后分别求导得到：\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &= w − ( \\sum_i \\alpha_i x_i − \\sum_i \\alpha_i^{\\ast} x_i) = 0 \\\\\n\\frac{\\partial J}{\\partial b} &= \\sum_i ( \\alpha_i − \\alpha_i^{\\ast} ) = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i} &= C − \\alpha_i − \\beta_i = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i^{\\ast}} &= C − \\alpha_i^{\\ast} − \\beta_i^{\\ast} = 0 \\\\\nC &= \\alpha_i + \\beta_i = \\alpha_i^{\\ast} + \\beta_i^{\\ast}\n\\end{aligned}\n\\end{equation}\n$$　　\n\n将上述式子代入$J$函数有： \n\n$$\n\\begin{equation}\n\\begin{aligned}\nJ\n&= \\frac{1}{2}w^Tw − \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) w x_i − b \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\\\ \n&+ \\underbrace{C \\sum_i (\\zeta_i + \\zeta_i^{\\ast})− \\sum_i \\alpha_i \\zeta_i − \\sum_i \\alpha_i^{\\ast} \\zeta_i^{\\ast} − \\sum_i (C−\\alpha_i)\\zeta_i − \\sum_i(C−\\alpha_i^{\\ast}) \\zeta_i^{\\ast}}_{这部分可以相互抵消，变为0} \\\\\n&= \\frac{1}{2} (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) d(x_i) \\\\\n&= − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) ( \\alpha_j − \\alpha_j^{\\ast} ) x_i x_j − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\\\ \ns.t. \\qquad 0 \\leq \\quad \\alpha_i, \\alpha_i^{\\ast} \\quad \\leq C\n\\end{aligned}\n\\end{equation}\n$$\n\n其中$\\zeta, \\zeta^{\\ast}, \\beta, \\beta^{\\ast}$都在计算过程中抵消了。$\\epsilon, C$则是人为给定的参数，是常量。如果要使用核函数，可以将上式写成：\n\n$$\nJ = − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) (\\alpha_j − \\alpha_j^{\\ast}) K(x_i, x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast} ) \\epsilon + \\sum_i ( \\alpha_i − \\alpha_i^{\\ast})d(x_i)\n$$\n\nSVR的代价函数和SVM的很相似，但是最优化的对象却不同，对偶式有很大不同，解法同样都是基于拉格朗日的最优化问题解法。\n求解这类问题的早期解法非常复杂，后来出来很多新的较为简单的解法，对数学和编程水平要求高，对大部分工程学人士来说还是颇为复杂和难以实现，\n因此大牛们推出了一些SVM库。比较出名的有libSVM，该库同时实现了SVM和SVR。 \n\n\n---------------\n\n>\n[SVR支持向量机回归](https://blog.csdn.net/lpsl1882/article/details/52411987)\n>","slug":"svm-4","published":1,"updated":"2019-08-17T09:33:51.041Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvi001c2qwpvrd5c93y","content":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-4\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-4\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 4\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 4</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul><a id=\"more\"></a>\n\n\n\n\n\n\n\n<p>目录</p>\n<ul>\n<li>支持向量回归 - SVR</li>\n</ul>\n<hr>\n<h3 id=\"支持向量回归-SVR\"><a href=\"#支持向量回归-SVR\" class=\"headerlink\" title=\"支持向量回归 - SVR\"></a>支持向量回归 - SVR</h3><p>回归和分类从某种意义上讲，本质上是一回事。</p>\n<ul>\n<li>SVM分类，就是找到一个平面，让两个分类集合的支持向量或者所有的数据离分类平面最远；</li>\n<li>SVR回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。</li>\n</ul>\n<p>假设某个元素到回归平面的距离为$ r = d(x) − g(x) $ 。另外，由于数据不可能都在回归平面上，距离之和还是挺大，因此所有数据到回归平面的距离可以给定一个容忍值$ \\epsilon$ 防止过拟合。该参数是经验参数，需要人工给定。如果数据元素到回归平面的距离小于$ \\epsilon $，则代价为0。SVR的代价函数可以表示为：</p>\n<script type=\"math/tex; mode=display\">cost(x) = \\max ( 0, \\quad \\| d(x)-g(x) \\| - \\epsilon )</script><p>其中$d(x)$为样本的标签。考虑松弛变量$ \\zeta_i, \\zeta_i^{\\ast} $，分别表示上下边界的松弛因子，约束条件即为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nd(x_i) - g(x_i) < \\epsilon + \\zeta_i, \\qquad \\zeta_i \\geq 0 \\\\\ng(x_i) - d(x_i) < \\epsilon + \\zeta_i^{\\ast}, \\quad \\zeta_i^{\\ast} \\geq 0\n\\end{cases}</script><p>实际上要最小化$ \\zeta_i, \\zeta_i^{\\ast} $，为了获得$w$的稀疏解，且假设$w$的计算结果满足正态分布，根据贝叶斯线性回归模型，对$w$有$L_2$范数约束。\nSVR可以转化为最优化问题：</p>\n<script type=\"math/tex; mode=display\">\n\\psi (x) = \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} C' w^T w \\\\\n\\Downarrow \\\\\n\\psi (x) = C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} w^T w</script><p>其中$C$是惩罚因子，是人为给定的经验参数。考虑约束条件，引入拉格朗日算子$\\alpha, \\alpha^{\\ast}, \\beta, \\beta^{\\ast}$，\n将最优化问题转化为对偶问题：</p>\n<script type=\"math/tex; mode=display\">\nJ=\\frac{1}{2} w^T w + C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) \n+\\sum_i \\alpha_i \\left[ d(x_i) − g(x_i) − \\epsilon − \\zeta_i \\right] + \\sum_i \\alpha_i^{\\ast} \\left[ g(x_i) − d(x_i) − \\epsilon - \\zeta_i^{\\ast} \\right]\n−\\sum_i \\beta_i \\zeta_i − \\sum_i \\beta_i^{\\ast} \\zeta_i^{\\ast}</script><p>然后分别求导得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &= w − ( \\sum_i \\alpha_i x_i − \\sum_i \\alpha_i^{\\ast} x_i) = 0 \\\\\n\\frac{\\partial J}{\\partial b} &= \\sum_i ( \\alpha_i − \\alpha_i^{\\ast} ) = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i} &= C − \\alpha_i − \\beta_i = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i^{\\ast}} &= C − \\alpha_i^{\\ast} − \\beta_i^{\\ast} = 0 \\\\\nC &= \\alpha_i + \\beta_i = \\alpha_i^{\\ast} + \\beta_i^{\\ast}\n\\end{aligned}\n\\end{equation}\n$$　　\n\n将上述式子代入$J$函数有：</script><p>\\begin{equation}\n\\begin{aligned}\nJ\n&amp;= \\frac{1}{2}w^Tw − \\sum<em>i (\\alpha_i − \\alpha_i^{\\ast}) w x_i − b \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\ \n&amp;+ \\underbrace{C \\sum_i (\\zeta_i + \\zeta_i^{\\ast})− \\sum_i \\alpha_i \\zeta_i − \\sum_i \\alpha_i^{\\ast} \\zeta_i^{\\ast} − \\sum_i (C−\\alpha_i)\\zeta_i − \\sum_i(C−\\alpha_i^{\\ast}) \\zeta_i^{\\ast}}</em>{这部分可以相互抵消，变为0} \\\n&amp;= \\frac{1}{2} (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) d(x_i) \\\n&amp;= − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) ( \\alpha_j − \\alpha_j^{\\ast} ) x_i x_j − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\ \ns.t. \\qquad 0 \\leq \\quad \\alpha_i, \\alpha_i^{\\ast} \\quad \\leq C\n\\end{aligned}\n\\end{equation}</p>\n<script type=\"math/tex; mode=display\">\n\n其中$\\zeta, \\zeta^{\\ast}, \\beta, \\beta^{\\ast}$都在计算过程中抵消了。$\\epsilon, C$则是人为给定的参数，是常量。如果要使用核函数，可以将上式写成：</script><p>J = − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) (\\alpha_j − \\alpha_j^{\\ast}) K(x_i, x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast} ) \\epsilon + \\sum_i ( \\alpha_i − \\alpha_i^{\\ast})d(x_i)</p>\n<p>$$</p>\n<p>SVR的代价函数和SVM的很相似，但是最优化的对象却不同，对偶式有很大不同，解法同样都是基于拉格朗日的最优化问题解法。\n求解这类问题的早期解法非常复杂，后来出来很多新的较为简单的解法，对数学和编程水平要求高，对大部分工程学人士来说还是颇为复杂和难以实现，\n因此大牛们推出了一些SVM库。比较出名的有libSVM，该库同时实现了SVM和SVR。 </p>\n<hr>\n<p>&gt;\n<a href=\"https://blog.csdn.net/lpsl1882/article/details/52411987\" target=\"_blank\" rel=\"noopener\">SVR支持向量机回归</a>\n&gt;</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"支持向量机-SVM（Support-Vector-Machines）Part-4\"><a href=\"#支持向量机-SVM（Support-Vector-Machines）Part-4\" class=\"headerlink\" title=\"支持向量机 - SVM（Support Vector Machines）Part 4\"></a><center>支持向量机 - SVM（Support Vector Machines）Part 4</center></h2><p><a href=\"/2018/03/svm-1/\">支持向量机 - SVM（Support Vector Machines）Part 1</a></p><ul>\n<li>线性可分支持向量机学习算法 - 最大间隔法</li>\n<li>线性可分支持向量机的对偶算法</li>\n</ul><p><a href=\"/2018/03/svm-2/\">支持向量机 - SVM（Support Vector Machines）Part 2</a></p><ul>\n<li>线性支持向量机</li>\n<li>核函数及非线性支持向量机</li>\n<li>常用的核函数及其特点</li>\n</ul><p><a href=\"/2018/03/svm-3/\">支持向量机 - SVM（Support Vector Machines）Part 3</a></p><ul>\n<li>序列最小最优化(SMO)算法</li>\n</ul><p><a href=\"/2018/03/svm-4/\">支持向量机 - SVM（Support Vector Machines）Part 4</a></p><ul>\n<li>支持向量回归 - SVR</li>\n</ul>","more":"\n\n\n\n\n\n\n\n<p>目录</p>\n<ul>\n<li>支持向量回归 - SVR</li>\n</ul>\n<hr>\n<h3 id=\"支持向量回归-SVR\"><a href=\"#支持向量回归-SVR\" class=\"headerlink\" title=\"支持向量回归 - SVR\"></a>支持向量回归 - SVR</h3><p>回归和分类从某种意义上讲，本质上是一回事。</p>\n<ul>\n<li>SVM分类，就是找到一个平面，让两个分类集合的支持向量或者所有的数据离分类平面最远；</li>\n<li>SVR回归，就是找到一个回归平面，让一个集合的所有数据到该平面的距离最近。</li>\n</ul>\n<p>假设某个元素到回归平面的距离为$ r = d(x) − g(x) $ 。另外，由于数据不可能都在回归平面上，距离之和还是挺大，因此所有数据到回归平面的距离可以给定一个容忍值$ \\epsilon$ 防止过拟合。该参数是经验参数，需要人工给定。如果数据元素到回归平面的距离小于$ \\epsilon $，则代价为0。SVR的代价函数可以表示为：</p>\n<script type=\"math/tex; mode=display\">cost(x) = \\max ( 0, \\quad \\| d(x)-g(x) \\| - \\epsilon )</script><p>其中$d(x)$为样本的标签。考虑松弛变量$ \\zeta_i, \\zeta_i^{\\ast} $，分别表示上下边界的松弛因子，约束条件即为：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nd(x_i) - g(x_i) < \\epsilon + \\zeta_i, \\qquad \\zeta_i \\geq 0 \\\\\ng(x_i) - d(x_i) < \\epsilon + \\zeta_i^{\\ast}, \\quad \\zeta_i^{\\ast} \\geq 0\n\\end{cases}</script><p>实际上要最小化$ \\zeta_i, \\zeta_i^{\\ast} $，为了获得$w$的稀疏解，且假设$w$的计算结果满足正态分布，根据贝叶斯线性回归模型，对$w$有$L_2$范数约束。\nSVR可以转化为最优化问题：</p>\n<script type=\"math/tex; mode=display\">\n\\psi (x) = \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} C' w^T w \\\\\n\\Downarrow \\\\\n\\psi (x) = C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) + \\frac{1}{2} w^T w</script><p>其中$C$是惩罚因子，是人为给定的经验参数。考虑约束条件，引入拉格朗日算子$\\alpha, \\alpha^{\\ast}, \\beta, \\beta^{\\ast}$，\n将最优化问题转化为对偶问题：</p>\n<script type=\"math/tex; mode=display\">\nJ=\\frac{1}{2} w^T w + C \\sum_i (\\zeta_i + \\zeta_i^{\\ast}) \n+\\sum_i \\alpha_i \\left[ d(x_i) − g(x_i) − \\epsilon − \\zeta_i \\right] + \\sum_i \\alpha_i^{\\ast} \\left[ g(x_i) − d(x_i) − \\epsilon - \\zeta_i^{\\ast} \\right]\n−\\sum_i \\beta_i \\zeta_i − \\sum_i \\beta_i^{\\ast} \\zeta_i^{\\ast}</script><p>然后分别求导得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial J}{\\partial w} &= w − ( \\sum_i \\alpha_i x_i − \\sum_i \\alpha_i^{\\ast} x_i) = 0 \\\\\n\\frac{\\partial J}{\\partial b} &= \\sum_i ( \\alpha_i − \\alpha_i^{\\ast} ) = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i} &= C − \\alpha_i − \\beta_i = 0 \\\\\n\\frac{\\partial J}{\\partial \\zeta_i^{\\ast}} &= C − \\alpha_i^{\\ast} − \\beta_i^{\\ast} = 0 \\\\\nC &= \\alpha_i + \\beta_i = \\alpha_i^{\\ast} + \\beta_i^{\\ast}\n\\end{aligned}\n\\end{equation}\n$$　　\n\n将上述式子代入$J$函数有：</script><p>\\begin{equation}\n\\begin{aligned}\nJ\n&amp;= \\frac{1}{2}w^Tw − \\sum<em>i (\\alpha_i − \\alpha_i^{\\ast}) w x_i − b \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\ \n&amp;+ \\underbrace{C \\sum_i (\\zeta_i + \\zeta_i^{\\ast})− \\sum_i \\alpha_i \\zeta_i − \\sum_i \\alpha_i^{\\ast} \\zeta_i^{\\ast} − \\sum_i (C−\\alpha_i)\\zeta_i − \\sum_i(C−\\alpha_i^{\\ast}) \\zeta_i^{\\ast}}</em>{这部分可以相互抵消，变为0} \\\n&amp;= \\frac{1}{2} (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − (\\sum_i \\alpha_i x_i - \\sum_i \\alpha_i^{\\ast} x_i)(\\sum_j \\alpha_j x_j - \\sum_j \\alpha_j^{\\ast} x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast}) d(x_i) \\\n&amp;= − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) ( \\alpha_j − \\alpha_j^{\\ast} ) x_i x_j − \\sum_i (\\alpha_i + \\alpha_i^{\\ast}) \\epsilon + \\sum_i (\\alpha_i − \\alpha_i^{\\ast})d(x_i) \\ \ns.t. \\qquad 0 \\leq \\quad \\alpha_i, \\alpha_i^{\\ast} \\quad \\leq C\n\\end{aligned}\n\\end{equation}</p>\n<script type=\"math/tex; mode=display\">\n\n其中$\\zeta, \\zeta^{\\ast}, \\beta, \\beta^{\\ast}$都在计算过程中抵消了。$\\epsilon, C$则是人为给定的参数，是常量。如果要使用核函数，可以将上式写成：</script><p>J = − \\frac{1}{2} \\sum_i \\sum_j (\\alpha_i − \\alpha_i^{\\ast} ) (\\alpha_j − \\alpha_j^{\\ast}) K(x_i, x_j) − \\sum_i ( \\alpha_i + \\alpha_i^{\\ast} ) \\epsilon + \\sum_i ( \\alpha_i − \\alpha_i^{\\ast})d(x_i)</p>\n<p>$$</p>\n<p>SVR的代价函数和SVM的很相似，但是最优化的对象却不同，对偶式有很大不同，解法同样都是基于拉格朗日的最优化问题解法。\n求解这类问题的早期解法非常复杂，后来出来很多新的较为简单的解法，对数学和编程水平要求高，对大部分工程学人士来说还是颇为复杂和难以实现，\n因此大牛们推出了一些SVM库。比较出名的有libSVM，该库同时实现了SVM和SVR。 </p>\n<hr>\n<p>&gt;\n<a href=\"https://blog.csdn.net/lpsl1882/article/details/52411987\" target=\"_blank\" rel=\"noopener\">SVR支持向量机回归</a>\n&gt;</p>\n"},{"layout":"post","title":"Adaboost & 前向分布算法","date":"2018-04-10T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center>Adaboost & 前向分布算法</center>\n\n目录\n* Adaboost\n* 前向分布算法\n* 前向分布算法推导Adaboost\n* Adaboost特点\n\n----------\n\n### 1. Adaboost\n\nAdaboost提高那些被前一轮弱学习器错误分类样本的权值，而降低那些被正确分类样本的权值。这样没有正确分类的样本在下一轮学习中将获得更大的关注；之后Adaboost采用加权多数表决的方法，加大错误率低的弱学习器的权值，减小错误率高的弱学习器的权值。\n\n#### 1.1 Adaboost算法\n\n输入：训练数据集\\\\(T =\\lbrace (x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n) \\rbrace \\\\)，其中\\\\( x\\_i \\in \\chi \\subseteq R^n, y\\_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace \\\\)； 弱学习器学习算法；\n\n输出：最终的强学习器\\\\(G(x)\\\\)\n\n（1）初始化训练数据的权值分布\n\\\\[\nD\\_1 = (w\\_{11}, w\\_{12}, ..., w\\_{1n}), \\quad w\\_{1i}=\\frac{1}{n}, \\quad i=1,2,...,n\n\\\\]\n假设训练数据集具有均匀分布的权值分布，保证第1步能在原始数据集上学习弱学习器\\\\(G\\_1(x)\\\\)。\n\n（2）对\\\\(m=1,2,...,M\\\\)\n\n（2.1）使用具有权值分布\\\\(D\\_m\\\\)的训练数据集学习，得到基本分类器\n\\\\[\nG\\_m(x):\\chi \\longrightarrow \\lbrace -1,+1 \\rbrace\n\\\\]\n\n（2.2）计算\\\\(G\\_m(x)\\\\)在训练数据集上的误差率\n\\\\[\ne\\_m = \\sum\\_{i=1}^n P(G\\_m(x\\_i) \\not= y\\_i) = \\sum\\_{i=1}^n w\\_{mi}I(G\\_m(x\\_i) \\not= y\\_i)\n\\\\]\n\n（2.3）计算\\\\(G\\_m(x\\_i)\\\\)的系数\n\\\\[\n\\alpha\\_m = \\frac{1}{2} ln \\frac{1-e\\_m}{e\\_m}\n\\\\]\n这里可知，当\\\\(e\\_m \\leq \\frac{1}{2}时，\\alpha\\_m \\geq 0\\\\)，并且\\\\(\\alpha\\_m\\\\)随着\\\\(e\\_m\\\\)的减小而增大，所以误差率小的弱学习器在最终的强学习器中具有更大的作用。当\\\\( e\\_m > \\frac{1}{2} \\\\)算法停止。\n\n（2.4）更新训练数据集的权值分布\n\\\\[\nD\\_{m+1} = (w\\_{m+1,1}, w\\_{m+1,2}, ..., w\\_{m+1,n}) \\\\\\\nw\\_{m+1,i} = \\frac{w\\_{mi}}{Z\\_m} exp(-\\alpha\\_m y\\_i G\\_m(x\\_i)), \\quad i=1,2,...,n\n\\\\]\n这里的\\\\(Z\\_m\\\\)是规范化因子，它使\\\\(D\\_{m+1}\\\\)成为一个概率分布。\n\\\\[\nZ\\_m = \\sum\\_{i=1}^n w\\_{mi} exp(-\\alpha\\_m y\\_i G\\_m(x\\_i))\n\\\\]\n\n（3）构建弱学习器的线性组合\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x)\n\\\\]\n得到最终强学习器\n\\\\[\nG(x) = sign(f(x)) = sign( \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x))\n\\\\]\n\n**Adaboost的例子见《统计学习方法》page,140.**\n\nAdaboost的另外一个解释：认为Adaboost算法是模型为*加法模型、损失函数为指数函数、学习算法为前向分步算法*的二类分类学习方法。\n\n\n-------------\n\n### 2. 前向分布算法\n\n输入：训练数据集\\\\(T=\\lbrace (x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n) \\rbrace\\\\)；损失函数\\\\(L(y,f(x))\\\\)；基函数集\\\\( \\lbrace b(x;\\gamma) \\rbrace \\\\)；\n\n输出：加法模型\\\\(f(x)\\\\)。\n\n（1）初始化\\\\(f\\_0(x)=0\\\\)\n\n（2）对\\\\(m=1,2,...,M\\\\)\n\n（2.1）极小化损失函数\n\\\\[\n(\\beta\\_m, \\gamma\\_m) = \\mathop{\\arg\\min\\_{\\beta,\\gamma}} \\sum\\_{i=1}^n L(y\\_i, f\\_{m-1}(x\\_i)+\\beta b(x\\_i;\\gamma))\n\\\\]\n得到参数\\\\(\\beta\\_m, \\gamma\\_m\\\\)。\n\n（2.2）更新\n\\\\[\nf\\_m(x) = f\\_{m-1}(x) + \\beta\\_m b(x;\\gamma\\_m)\n\\\\]\n\n（3）得到加法模型\n\\\\[\nf(x) = f\\_M(x) = \\sum\\_{i=1}^M \\beta\\_m b(x;\\gamma\\_m)\n\\\\]\n\n这样，前向分布算法将同时求解\\\\(m=1\\\\)到\\\\(M\\\\)所有参数\\\\(\\beta\\_m, \\gamma\\_m\\\\)的优化问题简化为逐次求解各个\\\\(\\beta\\_m, \\gamma\\_m\\\\)的优化问题。\n\n\n---------\n\n### 3. 前向分布算法推导Adaboost\n\n前向分布算法学习的是加法模型，所以该加法模型等价于Adaboost的最终的强学习器\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x)\n\\\\]\n前向分步算法逐一学习基函数与Adaboost逐一学习弱分类器的过程一致。\n\n下面证明前向分布算法的损失函数是指数损失函数时，其学习的具体操作等价于Adaboost算法学习的具体操作。\n\\\\[\nL(y, f(x)) = exp[-y f(x)]\n\\\\]\n假设经过\\\\(m-1\\\\)轮迭代，前向分布算法得到\\\\(f\\_{m-1}(x)\\\\)：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf\\_{m-1}(x) = \n& = f\\_{m-2}(x) + \\alpha\\_{m-1}G\\_{m-1}(x) \\\\\\\n& = \\alpha\\_1 G\\_1(x) + ... + \\alpha\\_{m-1} G\\_{m-1}(x)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n在第\\\\(m\\\\)次迭代得到\\\\(\\alpha\\_m, G\\_m(x)和f\\_m(x)\\\\)。\n\\\\[\nf\\_m(x) = f\\_{m-1}(x) + \\alpha\\_m G\\_m(x)\n\\\\]\n\n我们的目标是使用前向分步算法，得到\\\\(\\alpha\\_m, G\\_m(x)\\\\)使\\\\(f(x)\\\\)在训练数据集上的指数损失最小，即\n\\\\[\n(\\alpha\\_m, G\\_m(x)) = \\mathop{\\arg\\min\\_{\\alpha, G}} \\sum\\_{i=1}^n exp[-y\\_i (f\\_{m-1}(x\\_i) + \\alpha G(x\\_i))]\n\\\\]\n重新整理\\\\(式(16)\\\\)得：\n\\\\[\n(\\alpha\\_m, G\\_m(x)) = \\mathop{\\arg\\min\\_{\\alpha, G}} \\sum\\_{i=1}^n \\bar{w}\\_{mi} exp[-y\\_i \\alpha G(x\\_i)]\n\\\\]\n其中\\\\(\\bar{w}\\_{mi} = exp[-y\\_i f\\_{m-1}(x\\_i)]\\\\)，由于\\\\(\\bar{w}\\_{mi}\\\\)既不依赖\\\\(\\alpha\\\\)也不依赖于\\\\(G\\\\)，所以与最小化无关。\n\n首先求\\\\( G\\_m^{\\ast} \\\\)， 对任意\\\\(\\alpha > 0\\\\)，使\\\\( 式(8.21) \\\\)最小的\\\\(G(x)\\\\)由下式得到：\n\\\\[\nG\\_m^{\\ast} = \\mathop{\\arg\\min\\_G} \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G(x\\_i))\n\\\\]\n之后求\\\\(\\alpha\\_m^{\\ast}\\\\)，令：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nW\n& = \\sum\\_{i=1}^n \\bar{w}\\_{mi} exp[ - y\\_i \\alpha G(x\\_i)] \\\\\\\n& = \\sum\\_{y\\_i = G\\_m(x\\_i)} \\bar{w}\\_{mi} e^{-\\alpha} + \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} e^{\\alpha} \\\\\\\n& = e^{-\\alpha} \\sum\\_{y\\_i = G\\_m(x\\_i)} \\bar{w}\\_{mi} + e^{\\alpha} \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\\\\\\n& = e^{-\\alpha} \\lbrace \\sum\\_{i=1}^n \\bar{w}\\_{mi} - \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\rbrace + e^{\\alpha} \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\\\\\\n& = e^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} + (e^{\\alpha} - e^{-\\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m(x\\_i))\n\\end{aligned}\n\\end{equation}\n\\\\]\n将式\\\\((18)\\\\)求得的\\\\( G\\_m^{\\ast}(x) \\\\)代入上式，对\\\\(\\alpha\\\\)求导并令导数为零，即得：\n\\\\[\n\\frac{\\partial W}{\\partial \\alpha} = - e^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} + (e^{\\alpha} + e^{- \\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m^{\\ast}(x\\_i)) = 0\n\\\\\\ \\Downarrow \\\\\\\ne^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} = (e^{\\alpha} + e^{- \\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m^{\\ast}(x\\_i))\n\\\\\\ \\Downarrow \\\\\\\n\\alpha\\_m^{\\ast} = \\frac{1}{2} log \\frac{1-e\\_m}{e\\_m} \\\\\\ \nwhere, \\quad e\\_m = \\frac{\\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m(x\\_i))}{\\sum\\_{i=1}^n \\bar{w}\\_{mi}}\n\\\\]\n这里的\\\\(\\alpha\\_m^{\\ast}\\\\)与Adaboost算法第(2.3)步的\\\\(\\alpha\\_m\\\\)完全一致。\n\n接下来求权值更新公式：\n\\\\[\n\\begin{cases}\nf\\_m(x) = f\\_{m-1}(x) + \\alpha\\_m G\\_m(x) \\\\\\\n\\bar{w}\\_{mi} = exp[-y\\_i f\\_{m-1}(x\\_i)]\n\\end{cases}\n\\quad \\Longrightarrow \\quad \n\\bar{w}\\_{m+1,i} = \\bar{w}\\_{m,i}exp[-y\\_i \\alpha\\_m G\\_m(x)]\n\\\\]\n这与Adaboost算法的第(2.4)步的样本均值的更新，只相差规范化因子，因而等价。\n\n\n------------\n\n### 4. Adaboost特点\n\n* 优点：分类精度高；灵活的弱学习器；模型简单易解释；不易过拟合\n* 缺点：异常值敏感\n\n\n--------------\n\n### 参考\n>李航 - 《统计学习方法》\n\n\n\n","source":"_posts/2018-04-10-adaboost.md","raw":"---\nlayout: post\ntitle: Adaboost & 前向分布算法\ndate: 2018-04-10 12:10 +0800\ncategories: 机器学习\ntags:\n- 集成学习\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center>Adaboost & 前向分布算法</center>\n\n目录\n* Adaboost\n* 前向分布算法\n* 前向分布算法推导Adaboost\n* Adaboost特点\n\n----------\n\n### 1. Adaboost\n\nAdaboost提高那些被前一轮弱学习器错误分类样本的权值，而降低那些被正确分类样本的权值。这样没有正确分类的样本在下一轮学习中将获得更大的关注；之后Adaboost采用加权多数表决的方法，加大错误率低的弱学习器的权值，减小错误率高的弱学习器的权值。\n\n#### 1.1 Adaboost算法\n\n输入：训练数据集\\\\(T =\\lbrace (x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n) \\rbrace \\\\)，其中\\\\( x\\_i \\in \\chi \\subseteq R^n, y\\_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace \\\\)； 弱学习器学习算法；\n\n输出：最终的强学习器\\\\(G(x)\\\\)\n\n（1）初始化训练数据的权值分布\n\\\\[\nD\\_1 = (w\\_{11}, w\\_{12}, ..., w\\_{1n}), \\quad w\\_{1i}=\\frac{1}{n}, \\quad i=1,2,...,n\n\\\\]\n假设训练数据集具有均匀分布的权值分布，保证第1步能在原始数据集上学习弱学习器\\\\(G\\_1(x)\\\\)。\n\n（2）对\\\\(m=1,2,...,M\\\\)\n\n（2.1）使用具有权值分布\\\\(D\\_m\\\\)的训练数据集学习，得到基本分类器\n\\\\[\nG\\_m(x):\\chi \\longrightarrow \\lbrace -1,+1 \\rbrace\n\\\\]\n\n（2.2）计算\\\\(G\\_m(x)\\\\)在训练数据集上的误差率\n\\\\[\ne\\_m = \\sum\\_{i=1}^n P(G\\_m(x\\_i) \\not= y\\_i) = \\sum\\_{i=1}^n w\\_{mi}I(G\\_m(x\\_i) \\not= y\\_i)\n\\\\]\n\n（2.3）计算\\\\(G\\_m(x\\_i)\\\\)的系数\n\\\\[\n\\alpha\\_m = \\frac{1}{2} ln \\frac{1-e\\_m}{e\\_m}\n\\\\]\n这里可知，当\\\\(e\\_m \\leq \\frac{1}{2}时，\\alpha\\_m \\geq 0\\\\)，并且\\\\(\\alpha\\_m\\\\)随着\\\\(e\\_m\\\\)的减小而增大，所以误差率小的弱学习器在最终的强学习器中具有更大的作用。当\\\\( e\\_m > \\frac{1}{2} \\\\)算法停止。\n\n（2.4）更新训练数据集的权值分布\n\\\\[\nD\\_{m+1} = (w\\_{m+1,1}, w\\_{m+1,2}, ..., w\\_{m+1,n}) \\\\\\\nw\\_{m+1,i} = \\frac{w\\_{mi}}{Z\\_m} exp(-\\alpha\\_m y\\_i G\\_m(x\\_i)), \\quad i=1,2,...,n\n\\\\]\n这里的\\\\(Z\\_m\\\\)是规范化因子，它使\\\\(D\\_{m+1}\\\\)成为一个概率分布。\n\\\\[\nZ\\_m = \\sum\\_{i=1}^n w\\_{mi} exp(-\\alpha\\_m y\\_i G\\_m(x\\_i))\n\\\\]\n\n（3）构建弱学习器的线性组合\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x)\n\\\\]\n得到最终强学习器\n\\\\[\nG(x) = sign(f(x)) = sign( \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x))\n\\\\]\n\n**Adaboost的例子见《统计学习方法》page,140.**\n\nAdaboost的另外一个解释：认为Adaboost算法是模型为*加法模型、损失函数为指数函数、学习算法为前向分步算法*的二类分类学习方法。\n\n\n-------------\n\n### 2. 前向分布算法\n\n输入：训练数据集\\\\(T=\\lbrace (x\\_1,y\\_1),(x\\_2,y\\_2),...,(x\\_n,y\\_n) \\rbrace\\\\)；损失函数\\\\(L(y,f(x))\\\\)；基函数集\\\\( \\lbrace b(x;\\gamma) \\rbrace \\\\)；\n\n输出：加法模型\\\\(f(x)\\\\)。\n\n（1）初始化\\\\(f\\_0(x)=0\\\\)\n\n（2）对\\\\(m=1,2,...,M\\\\)\n\n（2.1）极小化损失函数\n\\\\[\n(\\beta\\_m, \\gamma\\_m) = \\mathop{\\arg\\min\\_{\\beta,\\gamma}} \\sum\\_{i=1}^n L(y\\_i, f\\_{m-1}(x\\_i)+\\beta b(x\\_i;\\gamma))\n\\\\]\n得到参数\\\\(\\beta\\_m, \\gamma\\_m\\\\)。\n\n（2.2）更新\n\\\\[\nf\\_m(x) = f\\_{m-1}(x) + \\beta\\_m b(x;\\gamma\\_m)\n\\\\]\n\n（3）得到加法模型\n\\\\[\nf(x) = f\\_M(x) = \\sum\\_{i=1}^M \\beta\\_m b(x;\\gamma\\_m)\n\\\\]\n\n这样，前向分布算法将同时求解\\\\(m=1\\\\)到\\\\(M\\\\)所有参数\\\\(\\beta\\_m, \\gamma\\_m\\\\)的优化问题简化为逐次求解各个\\\\(\\beta\\_m, \\gamma\\_m\\\\)的优化问题。\n\n\n---------\n\n### 3. 前向分布算法推导Adaboost\n\n前向分布算法学习的是加法模型，所以该加法模型等价于Adaboost的最终的强学习器\n\\\\[\nf(x) = \\sum\\_{m=1}^M \\alpha\\_m G\\_m(x)\n\\\\]\n前向分步算法逐一学习基函数与Adaboost逐一学习弱分类器的过程一致。\n\n下面证明前向分布算法的损失函数是指数损失函数时，其学习的具体操作等价于Adaboost算法学习的具体操作。\n\\\\[\nL(y, f(x)) = exp[-y f(x)]\n\\\\]\n假设经过\\\\(m-1\\\\)轮迭代，前向分布算法得到\\\\(f\\_{m-1}(x)\\\\)：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nf\\_{m-1}(x) = \n& = f\\_{m-2}(x) + \\alpha\\_{m-1}G\\_{m-1}(x) \\\\\\\n& = \\alpha\\_1 G\\_1(x) + ... + \\alpha\\_{m-1} G\\_{m-1}(x)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n在第\\\\(m\\\\)次迭代得到\\\\(\\alpha\\_m, G\\_m(x)和f\\_m(x)\\\\)。\n\\\\[\nf\\_m(x) = f\\_{m-1}(x) + \\alpha\\_m G\\_m(x)\n\\\\]\n\n我们的目标是使用前向分步算法，得到\\\\(\\alpha\\_m, G\\_m(x)\\\\)使\\\\(f(x)\\\\)在训练数据集上的指数损失最小，即\n\\\\[\n(\\alpha\\_m, G\\_m(x)) = \\mathop{\\arg\\min\\_{\\alpha, G}} \\sum\\_{i=1}^n exp[-y\\_i (f\\_{m-1}(x\\_i) + \\alpha G(x\\_i))]\n\\\\]\n重新整理\\\\(式(16)\\\\)得：\n\\\\[\n(\\alpha\\_m, G\\_m(x)) = \\mathop{\\arg\\min\\_{\\alpha, G}} \\sum\\_{i=1}^n \\bar{w}\\_{mi} exp[-y\\_i \\alpha G(x\\_i)]\n\\\\]\n其中\\\\(\\bar{w}\\_{mi} = exp[-y\\_i f\\_{m-1}(x\\_i)]\\\\)，由于\\\\(\\bar{w}\\_{mi}\\\\)既不依赖\\\\(\\alpha\\\\)也不依赖于\\\\(G\\\\)，所以与最小化无关。\n\n首先求\\\\( G\\_m^{\\ast} \\\\)， 对任意\\\\(\\alpha > 0\\\\)，使\\\\( 式(8.21) \\\\)最小的\\\\(G(x)\\\\)由下式得到：\n\\\\[\nG\\_m^{\\ast} = \\mathop{\\arg\\min\\_G} \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G(x\\_i))\n\\\\]\n之后求\\\\(\\alpha\\_m^{\\ast}\\\\)，令：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nW\n& = \\sum\\_{i=1}^n \\bar{w}\\_{mi} exp[ - y\\_i \\alpha G(x\\_i)] \\\\\\\n& = \\sum\\_{y\\_i = G\\_m(x\\_i)} \\bar{w}\\_{mi} e^{-\\alpha} + \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} e^{\\alpha} \\\\\\\n& = e^{-\\alpha} \\sum\\_{y\\_i = G\\_m(x\\_i)} \\bar{w}\\_{mi} + e^{\\alpha} \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\\\\\\n& = e^{-\\alpha} \\lbrace \\sum\\_{i=1}^n \\bar{w}\\_{mi} - \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\rbrace + e^{\\alpha} \\sum\\_{y\\_i \\not= G\\_m(x\\_i)} \\bar{w}\\_{mi} \\\\\\\n& = e^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} + (e^{\\alpha} - e^{-\\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m(x\\_i))\n\\end{aligned}\n\\end{equation}\n\\\\]\n将式\\\\((18)\\\\)求得的\\\\( G\\_m^{\\ast}(x) \\\\)代入上式，对\\\\(\\alpha\\\\)求导并令导数为零，即得：\n\\\\[\n\\frac{\\partial W}{\\partial \\alpha} = - e^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} + (e^{\\alpha} + e^{- \\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m^{\\ast}(x\\_i)) = 0\n\\\\\\ \\Downarrow \\\\\\\ne^{-\\alpha} \\sum\\_{i=1}^n \\bar{w}\\_{mi} = (e^{\\alpha} + e^{- \\alpha}) \\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m^{\\ast}(x\\_i))\n\\\\\\ \\Downarrow \\\\\\\n\\alpha\\_m^{\\ast} = \\frac{1}{2} log \\frac{1-e\\_m}{e\\_m} \\\\\\ \nwhere, \\quad e\\_m = \\frac{\\sum\\_{i=1}^n \\bar{w}\\_{mi} I(y\\_i \\not= G\\_m(x\\_i))}{\\sum\\_{i=1}^n \\bar{w}\\_{mi}}\n\\\\]\n这里的\\\\(\\alpha\\_m^{\\ast}\\\\)与Adaboost算法第(2.3)步的\\\\(\\alpha\\_m\\\\)完全一致。\n\n接下来求权值更新公式：\n\\\\[\n\\begin{cases}\nf\\_m(x) = f\\_{m-1}(x) + \\alpha\\_m G\\_m(x) \\\\\\\n\\bar{w}\\_{mi} = exp[-y\\_i f\\_{m-1}(x\\_i)]\n\\end{cases}\n\\quad \\Longrightarrow \\quad \n\\bar{w}\\_{m+1,i} = \\bar{w}\\_{m,i}exp[-y\\_i \\alpha\\_m G\\_m(x)]\n\\\\]\n这与Adaboost算法的第(2.4)步的样本均值的更新，只相差规范化因子，因而等价。\n\n\n------------\n\n### 4. Adaboost特点\n\n* 优点：分类精度高；灵活的弱学习器；模型简单易解释；不易过拟合\n* 缺点：异常值敏感\n\n\n--------------\n\n### 参考\n>李航 - 《统计学习方法》\n\n\n\n","slug":"adaboost","published":1,"updated":"2019-08-17T09:33:56.782Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvk001g2qwpfzf8533b","content":"<h2 id=\"Adaboost-前向分布算法\"><a href=\"#Adaboost-前向分布算法\" class=\"headerlink\" title=\"Adaboost & 前向分布算法\"></a><center>Adaboost & 前向分布算法</center></h2><p>目录</p><ul>\n<li>Adaboost</li>\n<li>前向分布算法</li>\n<li>前向分布算法推导Adaboost</li>\n<li>Adaboost特点</li>\n</ul><hr><h3 id=\"1-Adaboost\"><a href=\"#1-Adaboost\" class=\"headerlink\" title=\"1. Adaboost\"></a>1. Adaboost</h3><p>Adaboost提高那些被前一轮弱学习器错误分类样本的权值，而降低那些被正确分类样本的权值。这样没有正确分类的样本在下一轮学习中将获得更大的关注；之后Adaboost采用加权多数表决的方法，加大错误率低的弱学习器的权值，减小错误率高的弱学习器的权值。</p><h4 id=\"1-1-Adaboost算法\"><a href=\"#1-1-Adaboost算法\" class=\"headerlink\" title=\"1.1 Adaboost算法\"></a>1.1 Adaboost算法</h4><p>输入：训练数据集\\(T =\\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \\rbrace \\)，其中\\( x_i \\in \\chi \\subseteq R^n, y_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace \\)； 弱学习器学习算法；</p><a id=\"more\"></a>\n\n\n\n\n<p>输出：最终的强学习器\\(G(x)\\)</p>\n<p>（1）初始化训练数据的权值分布\n\\[\nD_1 = (w_{11}, w_{12}, …, w_{1n}), \\quad w_{1i}=\\frac{1}{n}, \\quad i=1,2,…,n\n\\]\n假设训练数据集具有均匀分布的权值分布，保证第1步能在原始数据集上学习弱学习器\\(G_1(x)\\)。</p>\n<p>（2）对\\(m=1,2,…,M\\)</p>\n<p>（2.1）使用具有权值分布\\(D_m\\)的训练数据集学习，得到基本分类器\n\\[\nG_m(x):\\chi \\longrightarrow \\lbrace -1,+1 \\rbrace\n\\]</p>\n<p>（2.2）计算\\(G_m(x)\\)在训练数据集上的误差率\n\\[\ne_m = \\sum_{i=1}^n P(G_m(x_i) \\not= y_i) = \\sum_{i=1}^n w_{mi}I(G_m(x_i) \\not= y_i)\n\\]</p>\n<p>（2.3）计算\\(G_m(x_i)\\)的系数\n\\[\n\\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m}\n\\]\n这里可知，当\\(e_m \\leq \\frac{1}{2}时，\\alpha_m \\geq 0\\)，并且\\(\\alpha_m\\)随着\\(e_m\\)的减小而增大，所以误差率小的弱学习器在最终的强学习器中具有更大的作用。当\\( e_m &gt; \\frac{1}{2} \\)算法停止。</p>\n<p>（2.4）更新训练数据集的权值分布\n\\[\nD_{m+1} = (w_{m+1,1}, w_{m+1,2}, …, w_{m+1,n}) \\\\\nw_{m+1,i} = \\frac{w_{mi}}{Z_m} exp(-\\alpha_m y_i G_m(x_i)), \\quad i=1,2,…,n\n\\]\n这里的\\(Z_m\\)是规范化因子，它使\\(D_{m+1}\\)成为一个概率分布。\n\\[\nZ_m = \\sum_{i=1}^n w_{mi} exp(-\\alpha_m y_i G_m(x_i))\n\\]</p>\n<p>（3）构建弱学习器的线性组合\n\\[\nf(x) = \\sum_{m=1}^M \\alpha_m G_m(x)\n\\]\n得到最终强学习器\n\\[\nG(x) = sign(f(x)) = sign( \\sum_{m=1}^M \\alpha_m G_m(x))\n\\]</p>\n<p><strong>Adaboost的例子见《统计学习方法》page,140.</strong></p>\n<p>Adaboost的另外一个解释：认为Adaboost算法是模型为<em>加法模型、损失函数为指数函数、学习算法为前向分步算法</em>的二类分类学习方法。</p>\n<hr>\n<h3 id=\"2-前向分布算法\"><a href=\"#2-前向分布算法\" class=\"headerlink\" title=\"2. 前向分布算法\"></a>2. 前向分布算法</h3><p>输入：训练数据集\\(T=\\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \\rbrace\\)；损失函数\\(L(y,f(x))\\)；基函数集\\( \\lbrace b(x;\\gamma) \\rbrace \\)；</p>\n<p>输出：加法模型\\(f(x)\\)。</p>\n<p>（1）初始化\\(f_0(x)=0\\)</p>\n<p>（2）对\\(m=1,2,…,M\\)</p>\n<p>（2.1）极小化损失函数\n\\[\n(\\beta_m, \\gamma_m) = \\mathop{\\arg\\min_{\\beta,\\gamma}} \\sum_{i=1}^n L(y_i, f_{m-1}(x_i)+\\beta b(x_i;\\gamma))\n\\]\n得到参数\\(\\beta_m, \\gamma_m\\)。</p>\n<p>（2.2）更新\n\\[\nf_m(x) = f_{m-1}(x) + \\beta_m b(x;\\gamma_m)\n\\]</p>\n<p>（3）得到加法模型\n\\[\nf(x) = f_M(x) = \\sum_{i=1}^M \\beta_m b(x;\\gamma_m)\n\\]</p>\n<p>这样，前向分布算法将同时求解\\(m=1\\)到\\(M\\)所有参数\\(\\beta_m, \\gamma_m\\)的优化问题简化为逐次求解各个\\(\\beta_m, \\gamma_m\\)的优化问题。</p>\n<hr>\n<h3 id=\"3-前向分布算法推导Adaboost\"><a href=\"#3-前向分布算法推导Adaboost\" class=\"headerlink\" title=\"3. 前向分布算法推导Adaboost\"></a>3. 前向分布算法推导Adaboost</h3><p>前向分布算法学习的是加法模型，所以该加法模型等价于Adaboost的最终的强学习器\n\\[\nf(x) = \\sum_{m=1}^M \\alpha_m G_m(x)\n\\]\n前向分步算法逐一学习基函数与Adaboost逐一学习弱分类器的过程一致。</p>\n<p>下面证明前向分布算法的损失函数是指数损失函数时，其学习的具体操作等价于Adaboost算法学习的具体操作。\n\\[\nL(y, f(x)) = exp[-y f(x)]\n\\]\n假设经过\\(m-1\\)轮迭代，前向分布算法得到\\(f_{m-1}(x)\\)：\n\\[\n\\begin{equation}\n\\begin{aligned}\nf_{m-1}(x) = \n&amp; = f_{m-2}(x) + \\alpha_{m-1}G_{m-1}(x) \\\\\n&amp; = \\alpha_1 G_1(x) + … + \\alpha_{m-1} G_{m-1}(x)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>在第\\(m\\)次迭代得到\\(\\alpha_m, G_m(x)和f_m(x)\\)。\n\\[\nf_m(x) = f_{m-1}(x) + \\alpha_m G_m(x)\n\\]</p>\n<p>我们的目标是使用前向分步算法，得到\\(\\alpha_m, G_m(x)\\)使\\(f(x)\\)在训练数据集上的指数损失最小，即\n\\[\n(\\alpha_m, G_m(x)) = \\mathop{\\arg\\min_{\\alpha, G}} \\sum_{i=1}^n exp[-y_i (f_{m-1}(x_i) + \\alpha G(x_i))]\n\\]\n重新整理\\(式(16)\\)得：\n\\[\n(\\alpha_m, G_m(x)) = \\mathop{\\arg\\min_{\\alpha, G}} \\sum_{i=1}^n \\bar{w}_{mi} exp[-y_i \\alpha G(x_i)]\n\\]\n其中\\(\\bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)]\\)，由于\\(\\bar{w}_{mi}\\)既不依赖\\(\\alpha\\)也不依赖于\\(G\\)，所以与最小化无关。</p>\n<p>首先求\\( G_m^{\\ast} \\)， 对任意\\(\\alpha &gt; 0\\)，使\\( 式(8.21) \\)最小的\\(G(x)\\)由下式得到：\n\\[\nG_m^{\\ast} = \\mathop{\\arg\\min_G} \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G(x_i))\n\\]\n之后求\\(\\alpha_m^{\\ast}\\)，令：\n\\[\n\\begin{equation}\n\\begin{aligned}\nW\n&amp; = \\sum_{i=1}^n \\bar{w}_{mi} exp[ - y_i \\alpha G(x_i)] \\\\\n&amp; = \\sum_{y_i = G_m(x_i)} \\bar{w}_{mi} e^{-\\alpha} + \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} e^{\\alpha} \\\\\n&amp; = e^{-\\alpha} \\sum_{y_i = G_m(x_i)} \\bar{w}_{mi} + e^{\\alpha} \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\\\\n&amp; = e^{-\\alpha} \\lbrace \\sum_{i=1}^n \\bar{w}_{mi} - \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\rbrace + e^{\\alpha} \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\\\\n&amp; = e^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} + (e^{\\alpha} - e^{-\\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m(x_i))\n\\end{aligned}\n\\end{equation}\n\\]\n将式\\((18)\\)求得的\\( G_m^{\\ast}(x) \\)代入上式，对\\(\\alpha\\)求导并令导数为零，即得：\n\\[\n\\frac{\\partial W}{\\partial \\alpha} = - e^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} + (e^{\\alpha} + e^{- \\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m^{\\ast}(x_i)) = 0\n\\\\ \\Downarrow \\\\\ne^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} = (e^{\\alpha} + e^{- \\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m^{\\ast}(x_i))\n\\\\ \\Downarrow \\\\\n\\alpha_m^{\\ast} = \\frac{1}{2} log \\frac{1-e_m}{e_m} \\\\ \nwhere, \\quad e_m = \\frac{\\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m(x_i))}{\\sum_{i=1}^n \\bar{w}_{mi}}\n\\]\n这里的\\(\\alpha_m^{\\ast}\\)与Adaboost算法第(2.3)步的\\(\\alpha_m\\)完全一致。</p>\n<p>接下来求权值更新公式：\n\\[\n\\begin{cases}\nf_m(x) = f_{m-1}(x) + \\alpha_m G_m(x) \\\\\n\\bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)]\n\\end{cases}\n\\quad \\Longrightarrow \\quad \n\\bar{w}_{m+1,i} = \\bar{w}_{m,i}exp[-y_i \\alpha_m G_m(x)]\n\\]\n这与Adaboost算法的第(2.4)步的样本均值的更新，只相差规范化因子，因而等价。</p>\n<hr>\n<h3 id=\"4-Adaboost特点\"><a href=\"#4-Adaboost特点\" class=\"headerlink\" title=\"4. Adaboost特点\"></a>4. Adaboost特点</h3><ul>\n<li>优点：分类精度高；灵活的弱学习器；模型简单易解释；不易过拟合</li>\n<li>缺点：异常值敏感</li>\n</ul>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"Adaboost-前向分布算法\"><a href=\"#Adaboost-前向分布算法\" class=\"headerlink\" title=\"Adaboost & 前向分布算法\"></a><center>Adaboost & 前向分布算法</center></h2><p>目录</p><ul>\n<li>Adaboost</li>\n<li>前向分布算法</li>\n<li>前向分布算法推导Adaboost</li>\n<li>Adaboost特点</li>\n</ul><hr><h3 id=\"1-Adaboost\"><a href=\"#1-Adaboost\" class=\"headerlink\" title=\"1. Adaboost\"></a>1. Adaboost</h3><p>Adaboost提高那些被前一轮弱学习器错误分类样本的权值，而降低那些被正确分类样本的权值。这样没有正确分类的样本在下一轮学习中将获得更大的关注；之后Adaboost采用加权多数表决的方法，加大错误率低的弱学习器的权值，减小错误率高的弱学习器的权值。</p><h4 id=\"1-1-Adaboost算法\"><a href=\"#1-1-Adaboost算法\" class=\"headerlink\" title=\"1.1 Adaboost算法\"></a>1.1 Adaboost算法</h4><p>输入：训练数据集\\(T =\\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \\rbrace \\)，其中\\( x_i \\in \\chi \\subseteq R^n, y_i \\in \\mathcal{y} = \\lbrace -1, +1 \\rbrace \\)； 弱学习器学习算法；</p>","more":"\n\n\n\n\n<p>输出：最终的强学习器\\(G(x)\\)</p>\n<p>（1）初始化训练数据的权值分布\n\\[\nD_1 = (w_{11}, w_{12}, …, w_{1n}), \\quad w_{1i}=\\frac{1}{n}, \\quad i=1,2,…,n\n\\]\n假设训练数据集具有均匀分布的权值分布，保证第1步能在原始数据集上学习弱学习器\\(G_1(x)\\)。</p>\n<p>（2）对\\(m=1,2,…,M\\)</p>\n<p>（2.1）使用具有权值分布\\(D_m\\)的训练数据集学习，得到基本分类器\n\\[\nG_m(x):\\chi \\longrightarrow \\lbrace -1,+1 \\rbrace\n\\]</p>\n<p>（2.2）计算\\(G_m(x)\\)在训练数据集上的误差率\n\\[\ne_m = \\sum_{i=1}^n P(G_m(x_i) \\not= y_i) = \\sum_{i=1}^n w_{mi}I(G_m(x_i) \\not= y_i)\n\\]</p>\n<p>（2.3）计算\\(G_m(x_i)\\)的系数\n\\[\n\\alpha_m = \\frac{1}{2} ln \\frac{1-e_m}{e_m}\n\\]\n这里可知，当\\(e_m \\leq \\frac{1}{2}时，\\alpha_m \\geq 0\\)，并且\\(\\alpha_m\\)随着\\(e_m\\)的减小而增大，所以误差率小的弱学习器在最终的强学习器中具有更大的作用。当\\( e_m &gt; \\frac{1}{2} \\)算法停止。</p>\n<p>（2.4）更新训练数据集的权值分布\n\\[\nD_{m+1} = (w_{m+1,1}, w_{m+1,2}, …, w_{m+1,n}) \\\\\nw_{m+1,i} = \\frac{w_{mi}}{Z_m} exp(-\\alpha_m y_i G_m(x_i)), \\quad i=1,2,…,n\n\\]\n这里的\\(Z_m\\)是规范化因子，它使\\(D_{m+1}\\)成为一个概率分布。\n\\[\nZ_m = \\sum_{i=1}^n w_{mi} exp(-\\alpha_m y_i G_m(x_i))\n\\]</p>\n<p>（3）构建弱学习器的线性组合\n\\[\nf(x) = \\sum_{m=1}^M \\alpha_m G_m(x)\n\\]\n得到最终强学习器\n\\[\nG(x) = sign(f(x)) = sign( \\sum_{m=1}^M \\alpha_m G_m(x))\n\\]</p>\n<p><strong>Adaboost的例子见《统计学习方法》page,140.</strong></p>\n<p>Adaboost的另外一个解释：认为Adaboost算法是模型为<em>加法模型、损失函数为指数函数、学习算法为前向分步算法</em>的二类分类学习方法。</p>\n<hr>\n<h3 id=\"2-前向分布算法\"><a href=\"#2-前向分布算法\" class=\"headerlink\" title=\"2. 前向分布算法\"></a>2. 前向分布算法</h3><p>输入：训练数据集\\(T=\\lbrace (x_1,y_1),(x_2,y_2),…,(x_n,y_n) \\rbrace\\)；损失函数\\(L(y,f(x))\\)；基函数集\\( \\lbrace b(x;\\gamma) \\rbrace \\)；</p>\n<p>输出：加法模型\\(f(x)\\)。</p>\n<p>（1）初始化\\(f_0(x)=0\\)</p>\n<p>（2）对\\(m=1,2,…,M\\)</p>\n<p>（2.1）极小化损失函数\n\\[\n(\\beta_m, \\gamma_m) = \\mathop{\\arg\\min_{\\beta,\\gamma}} \\sum_{i=1}^n L(y_i, f_{m-1}(x_i)+\\beta b(x_i;\\gamma))\n\\]\n得到参数\\(\\beta_m, \\gamma_m\\)。</p>\n<p>（2.2）更新\n\\[\nf_m(x) = f_{m-1}(x) + \\beta_m b(x;\\gamma_m)\n\\]</p>\n<p>（3）得到加法模型\n\\[\nf(x) = f_M(x) = \\sum_{i=1}^M \\beta_m b(x;\\gamma_m)\n\\]</p>\n<p>这样，前向分布算法将同时求解\\(m=1\\)到\\(M\\)所有参数\\(\\beta_m, \\gamma_m\\)的优化问题简化为逐次求解各个\\(\\beta_m, \\gamma_m\\)的优化问题。</p>\n<hr>\n<h3 id=\"3-前向分布算法推导Adaboost\"><a href=\"#3-前向分布算法推导Adaboost\" class=\"headerlink\" title=\"3. 前向分布算法推导Adaboost\"></a>3. 前向分布算法推导Adaboost</h3><p>前向分布算法学习的是加法模型，所以该加法模型等价于Adaboost的最终的强学习器\n\\[\nf(x) = \\sum_{m=1}^M \\alpha_m G_m(x)\n\\]\n前向分步算法逐一学习基函数与Adaboost逐一学习弱分类器的过程一致。</p>\n<p>下面证明前向分布算法的损失函数是指数损失函数时，其学习的具体操作等价于Adaboost算法学习的具体操作。\n\\[\nL(y, f(x)) = exp[-y f(x)]\n\\]\n假设经过\\(m-1\\)轮迭代，前向分布算法得到\\(f_{m-1}(x)\\)：\n\\[\n\\begin{equation}\n\\begin{aligned}\nf_{m-1}(x) = \n&amp; = f_{m-2}(x) + \\alpha_{m-1}G_{m-1}(x) \\\\\n&amp; = \\alpha_1 G_1(x) + … + \\alpha_{m-1} G_{m-1}(x)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>在第\\(m\\)次迭代得到\\(\\alpha_m, G_m(x)和f_m(x)\\)。\n\\[\nf_m(x) = f_{m-1}(x) + \\alpha_m G_m(x)\n\\]</p>\n<p>我们的目标是使用前向分步算法，得到\\(\\alpha_m, G_m(x)\\)使\\(f(x)\\)在训练数据集上的指数损失最小，即\n\\[\n(\\alpha_m, G_m(x)) = \\mathop{\\arg\\min_{\\alpha, G}} \\sum_{i=1}^n exp[-y_i (f_{m-1}(x_i) + \\alpha G(x_i))]\n\\]\n重新整理\\(式(16)\\)得：\n\\[\n(\\alpha_m, G_m(x)) = \\mathop{\\arg\\min_{\\alpha, G}} \\sum_{i=1}^n \\bar{w}_{mi} exp[-y_i \\alpha G(x_i)]\n\\]\n其中\\(\\bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)]\\)，由于\\(\\bar{w}_{mi}\\)既不依赖\\(\\alpha\\)也不依赖于\\(G\\)，所以与最小化无关。</p>\n<p>首先求\\( G_m^{\\ast} \\)， 对任意\\(\\alpha &gt; 0\\)，使\\( 式(8.21) \\)最小的\\(G(x)\\)由下式得到：\n\\[\nG_m^{\\ast} = \\mathop{\\arg\\min_G} \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G(x_i))\n\\]\n之后求\\(\\alpha_m^{\\ast}\\)，令：\n\\[\n\\begin{equation}\n\\begin{aligned}\nW\n&amp; = \\sum_{i=1}^n \\bar{w}_{mi} exp[ - y_i \\alpha G(x_i)] \\\\\n&amp; = \\sum_{y_i = G_m(x_i)} \\bar{w}_{mi} e^{-\\alpha} + \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} e^{\\alpha} \\\\\n&amp; = e^{-\\alpha} \\sum_{y_i = G_m(x_i)} \\bar{w}_{mi} + e^{\\alpha} \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\\\\n&amp; = e^{-\\alpha} \\lbrace \\sum_{i=1}^n \\bar{w}_{mi} - \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\rbrace + e^{\\alpha} \\sum_{y_i \\not= G_m(x_i)} \\bar{w}_{mi} \\\\\n&amp; = e^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} + (e^{\\alpha} - e^{-\\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m(x_i))\n\\end{aligned}\n\\end{equation}\n\\]\n将式\\((18)\\)求得的\\( G_m^{\\ast}(x) \\)代入上式，对\\(\\alpha\\)求导并令导数为零，即得：\n\\[\n\\frac{\\partial W}{\\partial \\alpha} = - e^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} + (e^{\\alpha} + e^{- \\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m^{\\ast}(x_i)) = 0\n\\\\ \\Downarrow \\\\\ne^{-\\alpha} \\sum_{i=1}^n \\bar{w}_{mi} = (e^{\\alpha} + e^{- \\alpha}) \\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m^{\\ast}(x_i))\n\\\\ \\Downarrow \\\\\n\\alpha_m^{\\ast} = \\frac{1}{2} log \\frac{1-e_m}{e_m} \\\\ \nwhere, \\quad e_m = \\frac{\\sum_{i=1}^n \\bar{w}_{mi} I(y_i \\not= G_m(x_i))}{\\sum_{i=1}^n \\bar{w}_{mi}}\n\\]\n这里的\\(\\alpha_m^{\\ast}\\)与Adaboost算法第(2.3)步的\\(\\alpha_m\\)完全一致。</p>\n<p>接下来求权值更新公式：\n\\[\n\\begin{cases}\nf_m(x) = f_{m-1}(x) + \\alpha_m G_m(x) \\\\\n\\bar{w}_{mi} = exp[-y_i f_{m-1}(x_i)]\n\\end{cases}\n\\quad \\Longrightarrow \\quad \n\\bar{w}_{m+1,i} = \\bar{w}_{m,i}exp[-y_i \\alpha_m G_m(x)]\n\\]\n这与Adaboost算法的第(2.4)步的样本均值的更新，只相差规范化因子，因而等价。</p>\n<hr>\n<h3 id=\"4-Adaboost特点\"><a href=\"#4-Adaboost特点\" class=\"headerlink\" title=\"4. Adaboost特点\"></a>4. Adaboost特点</h3><ul>\n<li>优点：分类精度高；灵活的弱学习器；模型简单易解释；不易过拟合</li>\n<li>缺点：异常值敏感</li>\n</ul>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p>李航 - 《统计学习方法》</p>\n</blockquote>\n"},{"layout":"post","title":"GBDT & XGBoost","date":"2018-04-12T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n## <center>GBDT & XGBoost</center>\n\n目录\n\n* 符号定义\n* GBDT = GBRT = MART\n* XGBoost\n* 正则化\n* GBDT与XGBoost的比较\n\n-------------\n\n### 1. 符号定义\n\n决策树\n\\\\[\nf(x; \\lbrace R\\_j, b\\_j\\rbrace ^{J}\\_1) = \\sum\\_{j=1}^J b\\_j I(x\\in R\\_j)\n\\\\]\n\\\\( \\lbrace R\\_j \\rbrace ^{J}\\_1 \\\\)和\\\\( \\lbrace b\\_j \\rbrace ^{J}\\_1 \\\\)表示决策树的参数，前者为分段空间(disjoint空间)，后者为这些空间上的输出值[其他地方称为打分值]，\\\\(J\\\\)是叶子结点的数量，下文中用\\\\(f(x)\\\\)省略表示\\\\( f(x; \\lbrace R\\_j, b\\_j\\rbrace ^{J}\\_1) \\\\)\n\n决策树的Ensemble\n\\\\[\nF = \\sum\\_{i=0}^K f\\_i\n\\\\]\n其中\\\\(f\\_0\\\\)是模型初始值，通常为统计样本计算出的常数[论文中为median]，同时定义\\\\( F\\_k = \\sum\\_{i=0}^k f\\_i \\\\)。\n\n\\\\(D = \\lbrace (x\\_i, y\\_i) \\rbrace ^N\\_1\\\\)，训练样本。\n\n\\\\[\n\\mathfrak{L} = \\mathfrak{L}( \\lbrace y\\_i, F(x\\_i) \\rbrace ^N\\_1 ) = \\underbrace{\\sum\\_{i=1}^N L(y\\_i, F(x\\_i))}\\_{\\text{训练误差项}} + \\underbrace{\\sum\\_{k=1}^K \\Omega(f\\_k)}\\_{\\text{正则化项}}\n\\\\]\n目标函数(损失函数)，第一项\\\\(L\\\\)是针对训练数据的\\\\(Loss\\\\)，可以选择：绝对值误差、平方误差、logistic loss等；第二项\\\\( \\Omega \\\\)是正则化函数，惩罚\\\\(f\\_k\\\\)的复杂度。\n\n\n----------------\n\n### 2. GBDT\n\n#### 2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]\n\n输入：\\\\( \\lbrace (x\\_i, y\\_i) \\rbrace^N\\_1 , K, L, ...\\\\)\n\n输出：\\\\(F\\_k\\\\)\n\n（1）初始化\\\\(f\\_0\\\\)\n\n（2）对k=1,2,...,K, 计算\n\n（2.1）\\\\( \\tilde{y}\\_i = - \\frac{\\partial L(y\\_i, F\\_{k-1}(x\\_i))}{\\partial F\\_{k-1}}, i=1,2,...,N \\\\)\n\n计算响应[response]\\\\(\\tilde{y}\\_i\\\\)，它是一个和残差[residual, \\\\(y\\_i-F\\_{k-1}(x\\_i)\\\\)]正相关的变量。\n\n（2.2）\\\\( \\lbrace R\\_j, b\\_j \\rbrace^{J^{\\ast}}\\_1 = \\mathop{\\arg\\min}\\_{\\lbrace R\\_j, b\\_j \\rbrace^J\\_1} \\sum\\_{i=1}^N [\\tilde{y}\\_i - f\\_k(x\\_i;\\lbrace R\\_j,b\\_j \\rbrace^J\\_1)]^2 \\\\)\n\n使用平方误差训练一颗决策树\\\\(f\\_k\\\\)，拟合数据\\\\( \\lbrace (x\\_i, \\tilde{y}\\_i) \\rbrace^N\\_1 \\\\)\n\n（2.3）\\\\( \\rho^{\\ast} = \\mathop{\\arg\\min}\\_{\\rho} \\mathfrak{L}(\\lbrace y\\_i, F\\_{k-1}(x\\_i)+\\rho f\\_k(x\\_i) \\rbrace ^N\\_1)= \\mathop{\\arg\\min}\\_{\\rho} \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}(x\\_i) + \\rho f \\_k(x\\_i)) + \\Omega(f\\_k) \\\\)\n\n求一个步长\\\\(\\rho^{\\ast}\\\\)，最小化损失。\n\n（2.4）令\\\\( f\\_k = \\rho^{\\ast}f\\_k, \\quad  F\\_k = F\\_{k-1}+f\\_k \\\\)\n\n将训练出来的\\\\(f\\_k\\\\)叠加到\\\\(F\\\\)。\n\n*总体来说，GBDT就是一个不断拟合响应(残差)并叠加到F上的过程，在这个过程中，响应不断变小，Loss不断接近最小值。*\n\n\n#### 2.2 GBDT例子\n\nGBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。\n\n假设我们现在有一个训练集，训练集只有4个人A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：\n\n![fulltree](/posts_res/2018-04-12-gbdtxgboost/1-1.png)\n\n现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：\n\n![gbdt](/posts_res/2018-04-12-gbdtxgboost/1-2.png)\n\n在第一棵树分枝和第一张图片中一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1,-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。\n\n最后GBDT的预测结果为：\n* A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14；\n* B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16；\n* C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24；\n* D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26。\n\n那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。\n\n注：两张图片中的最终效果相同，为何还需要GBDT呢？答案是过拟合。\n\n*其他例子请见李航博士《统计学习方法》page.149*\n\nGBRT 的优点:\n\n* 对混合型数据的自然处理（异构特征）\n* 强大的预测能力\n* 在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)\n\nGBDT 的缺点:\n\n* 可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBDT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.\n\n[附GBDT论文Friedman J. - Greedy Function Approximation_A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n\n[梯度提升树(GBDT)原理小结 - 刘建平Pinard](https://www.cnblogs.com/pinard/p/6140514.html)\n\n\n-------------\n\n### 3. XGBoost\n\nxgboost中使用的正则化函数为：\n\\\\[\n\\Omega(f\\_k) = \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b^2\\_j\n\\\\]\n我们的目标是求\\\\(f\\_k\\\\)，它最小化目标函数[式\\\\(3\\\\)]\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}\\_k \n& = \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}(x\\_i)+ \\rho f\\_k(x\\_i)) + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}+\\rho f\\_k) + \\Omega(f\\_k) \\quad \\quad \\quad [泰勒二阶展开可得下步] \\\\\\\n& \\approx \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + \\underbrace{\\frac{\\partial L(y\\_i, F\\_{k-1})}{\\partial F\\_{k-1}}}\\_{\\text{= \\\\(g\\_i\\\\)}} f\\_k + \\frac{1}{2}\\underbrace{\\frac{\\partial^2 L(y\\_i, F\\_{k-1})}{\\partial F^2\\_{k-1}}}\\_{\\text{=\\\\(h\\_i\\\\)}} f^2\\_k \\rgroup + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + g\\_i f\\_k + \\frac{1}{2}h\\_i f\\_k^2 \\rgroup + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2} h\\_i \\sum\\_{j=1}^J b\\_j^2  \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n整理出和\\\\(\\lbrace R\\_j \\rbrace^J\\_1, \\lbrace b\\_j \\rbrace^J\\_1\\\\)有关的项：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}(\\lbrace R\\_j \\rbrace^J\\_1, \\lbrace b\\_j \\rbrace^J\\_1) \n& = \\sum\\_{i=1}^N \\lgroup g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2}h\\_i \\sum\\_{j=1}^J b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{x\\_i \\in R\\_j} \\lgroup g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2}h\\_i \\sum\\_{j=1}^J b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup \\sum\\_{x\\_i \\in R\\_j} g\\_i b\\_j + \\sum\\_{x\\_i \\in R\\_j} \\frac{1}{2} h\\_i b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup \\underbrace{\\sum\\_{x\\_i \\in R\\_j}g\\_i}\\_{\\text{\\\\(=G\\_j\\\\)}}b\\_j + \\frac{1}{2} \\lgroup \\underbrace{\\sum\\_{x\\_i \\in R\\_j}h\\_i}\\_{\\text{\\\\(=H\\_j \\\\)}} + \\lambda \\rgroup b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup G\\_jb\\_j + \\frac{1}{2}(H\\_j + \\lambda)b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J\n\\end{aligned}\n\\end{equation}\n\\\\]\n\\\\(式(6)\\\\)对\\\\(b\\_j\\\\)求导，并令其等于零，得：\n\\\\[\nb\\_j^{\\ast} = - \\frac{G\\_j}{H\\_j + \\lambda}, \\quad j=1,2,...,J\n\\\\]\n\\\\(式(7)代入式(6)\\\\)中，化简得最小的\\\\(\\mathfrak{L}\\\\)：\n\\\\[\n\\mathfrak{L}^{\\ast}\\_k = - \\frac{1}{2} \\sum\\_{j=1}^J \\frac{G\\_j^2}{H\\_j+\\lambda} + \\frac{\\gamma}{2}J\n\\\\]\n\n求\\\\(\\lbrace R\\_j\\rbrace^J\\_1\\\\)与求\\\\( \\lbrace b\\_j \\rbrace^J\\_1 \\\\)的方法不同，前者它是对输入\\\\(x\\\\)所属空间的一种划分方法不连续，无法求导。\n精确得到划分\\\\(\\lbrace R\\_j \\rbrace^J\\_1\\\\)是一个NP难问 题，取而代之使用贪心法，即分裂某节点时，只考虑对当前节点分裂后哪个分裂方案能得到最小的\\\\(\\mathfrak{L}\\_k\\\\)。\n像传统决策树一样，CART中的办法也是遍历\\\\(x\\\\)的每个维度的每个分裂点，选择具有最小\\\\(\\\\mathfrak{L}\\_k\\)的维度和分裂点进行。\n那么定义：当前节点 \\\\( R\\_j \\\\) 分裂成\\\\( R\\_L \\\\)和\\\\( R\\_R \\\\)使得分裂后整棵树的\\\\( \\mathfrak{L}\\_k \\\\)最小。\n\n从\\\\(式(8)\\\\)可知，整棵树的最小\\\\(\\mathfrak{L}\\_k\\\\)等于每个叶子结点上(最小)Loss的和，由于整个分裂过程中只涉及到3个结点，其他任何结点的Loss在分裂过程中不变，这个问题等价于：\n\\\\[\n\\mathop{\\max}\\_{R\\_L, R\\_R} \\frac{G\\_L^2}{H\\_L+\\lambda} + \\frac{G\\_R^2}{H\\_R+\\lambda} - \\frac{(G\\_L+G\\_R)^2}{H\\_L+H\\_R+\\lambda} - \\frac{\\gamma}{2}\n\\\\]\n\\\\(式(9)\\\\)的含义是：前两项分别加上新生成的叶子结点的最小Loss，第三项是指减去被分裂的叶子结点的最小Loss，第四项是分裂后增加叶结点带来的模型复杂度。它是将结点\\\\(R\\_j\\\\)分裂成\\\\(R\\_L和R\\_R\\\\)后，**整棵树最小\\\\(\\mathfrak{L}\\_k\\\\)的降低量，这个量越大越好**。\n\n\n#### 3.1 xgboost树的分裂算法\n\n![treesplitalgorithm](/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png)\n\n#### 3.2 xgboost调参\n\n[1-Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n\n[2-Hyperopt](https://blog.csdn.net/a819825294/article/details/51775418)\n\n[3-GridSearchCV](https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn)\n\n\n-------------\n\n### 4. 正则化\n\nGBDT有非常快降低Loss的能力，这也会造成一个问题：Loss迅速下降，模型低bias，高variance，造成过拟合。\n\n下面一一介绍GBDT中抵抗过拟合的技巧：\n\n1. 限制树的复杂度。\\\\(\\Omega\\\\)函数对树的节点数，和节点上预测值\\\\( \\lbrace b\\_j \\rbrace^J\\_1 \\\\)的平方和均有惩罚，除此之外，我们通常在终止条件上还会增加一条为树的深度。\n2. 采样。即训练每个树的时候只使用一部分的样本。\n3. 列采样。即训练每个树的时候只使用一部分的特征，**这是Xgboost的创新，它将随机森林中的思想引入了GBDT**。\n4. Shrinkage。进一步惩罚\\\\(\\lbrace b\\_j \\rbrace^J\\_1\\\\)，给它们乘一个小于1的系数，也可以理解为设置了一个较低的学习率。\n5. Early stop。因为GBDT的可叠加性我们使用的模型不一定是最终的ensemble，而根据测试集的测试情况，选择使用前若干棵树。\n\n\n------------\n\n### 5. GBDT与XGBoost的比较\n\n以下来自知乎，作者：wepon \n\n1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。\n2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。\n3. xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。\n4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）\n5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。\n6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。\n7. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\n8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n9. 为了限制树的生长，加入阈值gamma，当增益大于阈值时才让节点分裂，它是正则项里叶子节点数J的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性\n\n\n-------------\n\n### 参考\n\n> [kimmyzhang - GBDT详解上 + 下 + 后补](http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/)\n\n> [Friedman - Greedy Function Approximation:A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n\n> [GBDT(MART)迭代决策树入门教程简介](https://blog.csdn.net/w28971023/article/details/8240756)\n\n> [雪伦 - xgboost原理](https://blog.csdn.net/a819825294/article/details/51206410)\n\n> [机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？](https://www.zhihu.com/question/41354392)\n\n> [Tianqi Chen - XGBoost_A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)\n","source":"_posts/2018-04-12-gbdtxgboost.md","raw":"---\nlayout: post\ntitle: GBDT & XGBoost\ndate: 2018-04-12 12:10 +0800\ncategories: 机器学习\ntags:\n- 集成学习\n- 模型算法\nmathjax: true\ncopyright: false\n---\n\n## <center>GBDT & XGBoost</center>\n\n目录\n\n* 符号定义\n* GBDT = GBRT = MART\n* XGBoost\n* 正则化\n* GBDT与XGBoost的比较\n\n-------------\n\n### 1. 符号定义\n\n决策树\n\\\\[\nf(x; \\lbrace R\\_j, b\\_j\\rbrace ^{J}\\_1) = \\sum\\_{j=1}^J b\\_j I(x\\in R\\_j)\n\\\\]\n\\\\( \\lbrace R\\_j \\rbrace ^{J}\\_1 \\\\)和\\\\( \\lbrace b\\_j \\rbrace ^{J}\\_1 \\\\)表示决策树的参数，前者为分段空间(disjoint空间)，后者为这些空间上的输出值[其他地方称为打分值]，\\\\(J\\\\)是叶子结点的数量，下文中用\\\\(f(x)\\\\)省略表示\\\\( f(x; \\lbrace R\\_j, b\\_j\\rbrace ^{J}\\_1) \\\\)\n\n决策树的Ensemble\n\\\\[\nF = \\sum\\_{i=0}^K f\\_i\n\\\\]\n其中\\\\(f\\_0\\\\)是模型初始值，通常为统计样本计算出的常数[论文中为median]，同时定义\\\\( F\\_k = \\sum\\_{i=0}^k f\\_i \\\\)。\n\n\\\\(D = \\lbrace (x\\_i, y\\_i) \\rbrace ^N\\_1\\\\)，训练样本。\n\n\\\\[\n\\mathfrak{L} = \\mathfrak{L}( \\lbrace y\\_i, F(x\\_i) \\rbrace ^N\\_1 ) = \\underbrace{\\sum\\_{i=1}^N L(y\\_i, F(x\\_i))}\\_{\\text{训练误差项}} + \\underbrace{\\sum\\_{k=1}^K \\Omega(f\\_k)}\\_{\\text{正则化项}}\n\\\\]\n目标函数(损失函数)，第一项\\\\(L\\\\)是针对训练数据的\\\\(Loss\\\\)，可以选择：绝对值误差、平方误差、logistic loss等；第二项\\\\( \\Omega \\\\)是正则化函数，惩罚\\\\(f\\_k\\\\)的复杂度。\n\n\n----------------\n\n### 2. GBDT\n\n#### 2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]\n\n输入：\\\\( \\lbrace (x\\_i, y\\_i) \\rbrace^N\\_1 , K, L, ...\\\\)\n\n输出：\\\\(F\\_k\\\\)\n\n（1）初始化\\\\(f\\_0\\\\)\n\n（2）对k=1,2,...,K, 计算\n\n（2.1）\\\\( \\tilde{y}\\_i = - \\frac{\\partial L(y\\_i, F\\_{k-1}(x\\_i))}{\\partial F\\_{k-1}}, i=1,2,...,N \\\\)\n\n计算响应[response]\\\\(\\tilde{y}\\_i\\\\)，它是一个和残差[residual, \\\\(y\\_i-F\\_{k-1}(x\\_i)\\\\)]正相关的变量。\n\n（2.2）\\\\( \\lbrace R\\_j, b\\_j \\rbrace^{J^{\\ast}}\\_1 = \\mathop{\\arg\\min}\\_{\\lbrace R\\_j, b\\_j \\rbrace^J\\_1} \\sum\\_{i=1}^N [\\tilde{y}\\_i - f\\_k(x\\_i;\\lbrace R\\_j,b\\_j \\rbrace^J\\_1)]^2 \\\\)\n\n使用平方误差训练一颗决策树\\\\(f\\_k\\\\)，拟合数据\\\\( \\lbrace (x\\_i, \\tilde{y}\\_i) \\rbrace^N\\_1 \\\\)\n\n（2.3）\\\\( \\rho^{\\ast} = \\mathop{\\arg\\min}\\_{\\rho} \\mathfrak{L}(\\lbrace y\\_i, F\\_{k-1}(x\\_i)+\\rho f\\_k(x\\_i) \\rbrace ^N\\_1)= \\mathop{\\arg\\min}\\_{\\rho} \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}(x\\_i) + \\rho f \\_k(x\\_i)) + \\Omega(f\\_k) \\\\)\n\n求一个步长\\\\(\\rho^{\\ast}\\\\)，最小化损失。\n\n（2.4）令\\\\( f\\_k = \\rho^{\\ast}f\\_k, \\quad  F\\_k = F\\_{k-1}+f\\_k \\\\)\n\n将训练出来的\\\\(f\\_k\\\\)叠加到\\\\(F\\\\)。\n\n*总体来说，GBDT就是一个不断拟合响应(残差)并叠加到F上的过程，在这个过程中，响应不断变小，Loss不断接近最小值。*\n\n\n#### 2.2 GBDT例子\n\nGBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。\n\n假设我们现在有一个训练集，训练集只有4个人A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：\n\n![fulltree](/posts_res/2018-04-12-gbdtxgboost/1-1.png)\n\n现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：\n\n![gbdt](/posts_res/2018-04-12-gbdtxgboost/1-2.png)\n\n在第一棵树分枝和第一张图片中一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1,-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。\n\n最后GBDT的预测结果为：\n* A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14；\n* B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16；\n* C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24；\n* D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26。\n\n那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。\n\n注：两张图片中的最终效果相同，为何还需要GBDT呢？答案是过拟合。\n\n*其他例子请见李航博士《统计学习方法》page.149*\n\nGBRT 的优点:\n\n* 对混合型数据的自然处理（异构特征）\n* 强大的预测能力\n* 在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)\n\nGBDT 的缺点:\n\n* 可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBDT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.\n\n[附GBDT论文Friedman J. - Greedy Function Approximation_A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n\n[梯度提升树(GBDT)原理小结 - 刘建平Pinard](https://www.cnblogs.com/pinard/p/6140514.html)\n\n\n-------------\n\n### 3. XGBoost\n\nxgboost中使用的正则化函数为：\n\\\\[\n\\Omega(f\\_k) = \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b^2\\_j\n\\\\]\n我们的目标是求\\\\(f\\_k\\\\)，它最小化目标函数[式\\\\(3\\\\)]\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}\\_k \n& = \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}(x\\_i)+ \\rho f\\_k(x\\_i)) + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N L(y\\_i, F\\_{k-1}+\\rho f\\_k) + \\Omega(f\\_k) \\quad \\quad \\quad [泰勒二阶展开可得下步] \\\\\\\n& \\approx \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + \\underbrace{\\frac{\\partial L(y\\_i, F\\_{k-1})}{\\partial F\\_{k-1}}}\\_{\\text{= \\\\(g\\_i\\\\)}} f\\_k + \\frac{1}{2}\\underbrace{\\frac{\\partial^2 L(y\\_i, F\\_{k-1})}{\\partial F^2\\_{k-1}}}\\_{\\text{=\\\\(h\\_i\\\\)}} f^2\\_k \\rgroup + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + g\\_i f\\_k + \\frac{1}{2}h\\_i f\\_k^2 \\rgroup + \\Omega(f\\_k) \\\\\\\n& = \\sum\\_{i=1}^N \\lgroup L(y\\_i, F\\_{k-1}) + g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2} h\\_i \\sum\\_{j=1}^J b\\_j^2  \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n整理出和\\\\(\\lbrace R\\_j \\rbrace^J\\_1, \\lbrace b\\_j \\rbrace^J\\_1\\\\)有关的项：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}(\\lbrace R\\_j \\rbrace^J\\_1, \\lbrace b\\_j \\rbrace^J\\_1) \n& = \\sum\\_{i=1}^N \\lgroup g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2}h\\_i \\sum\\_{j=1}^J b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{x\\_i \\in R\\_j} \\lgroup g\\_i \\sum\\_{j=1}^J b\\_j + \\frac{1}{2}h\\_i \\sum\\_{j=1}^J b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup \\sum\\_{x\\_i \\in R\\_j} g\\_i b\\_j + \\sum\\_{x\\_i \\in R\\_j} \\frac{1}{2} h\\_i b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum\\_{j=1}^J b\\_j^2 \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup \\underbrace{\\sum\\_{x\\_i \\in R\\_j}g\\_i}\\_{\\text{\\\\(=G\\_j\\\\)}}b\\_j + \\frac{1}{2} \\lgroup \\underbrace{\\sum\\_{x\\_i \\in R\\_j}h\\_i}\\_{\\text{\\\\(=H\\_j \\\\)}} + \\lambda \\rgroup b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J \\\\\\\n& = \\sum\\_{j=1}^J \\lgroup G\\_jb\\_j + \\frac{1}{2}(H\\_j + \\lambda)b\\_j^2 \\rgroup + \\frac{\\gamma}{2} J\n\\end{aligned}\n\\end{equation}\n\\\\]\n\\\\(式(6)\\\\)对\\\\(b\\_j\\\\)求导，并令其等于零，得：\n\\\\[\nb\\_j^{\\ast} = - \\frac{G\\_j}{H\\_j + \\lambda}, \\quad j=1,2,...,J\n\\\\]\n\\\\(式(7)代入式(6)\\\\)中，化简得最小的\\\\(\\mathfrak{L}\\\\)：\n\\\\[\n\\mathfrak{L}^{\\ast}\\_k = - \\frac{1}{2} \\sum\\_{j=1}^J \\frac{G\\_j^2}{H\\_j+\\lambda} + \\frac{\\gamma}{2}J\n\\\\]\n\n求\\\\(\\lbrace R\\_j\\rbrace^J\\_1\\\\)与求\\\\( \\lbrace b\\_j \\rbrace^J\\_1 \\\\)的方法不同，前者它是对输入\\\\(x\\\\)所属空间的一种划分方法不连续，无法求导。\n精确得到划分\\\\(\\lbrace R\\_j \\rbrace^J\\_1\\\\)是一个NP难问 题，取而代之使用贪心法，即分裂某节点时，只考虑对当前节点分裂后哪个分裂方案能得到最小的\\\\(\\mathfrak{L}\\_k\\\\)。\n像传统决策树一样，CART中的办法也是遍历\\\\(x\\\\)的每个维度的每个分裂点，选择具有最小\\\\(\\\\mathfrak{L}\\_k\\)的维度和分裂点进行。\n那么定义：当前节点 \\\\( R\\_j \\\\) 分裂成\\\\( R\\_L \\\\)和\\\\( R\\_R \\\\)使得分裂后整棵树的\\\\( \\mathfrak{L}\\_k \\\\)最小。\n\n从\\\\(式(8)\\\\)可知，整棵树的最小\\\\(\\mathfrak{L}\\_k\\\\)等于每个叶子结点上(最小)Loss的和，由于整个分裂过程中只涉及到3个结点，其他任何结点的Loss在分裂过程中不变，这个问题等价于：\n\\\\[\n\\mathop{\\max}\\_{R\\_L, R\\_R} \\frac{G\\_L^2}{H\\_L+\\lambda} + \\frac{G\\_R^2}{H\\_R+\\lambda} - \\frac{(G\\_L+G\\_R)^2}{H\\_L+H\\_R+\\lambda} - \\frac{\\gamma}{2}\n\\\\]\n\\\\(式(9)\\\\)的含义是：前两项分别加上新生成的叶子结点的最小Loss，第三项是指减去被分裂的叶子结点的最小Loss，第四项是分裂后增加叶结点带来的模型复杂度。它是将结点\\\\(R\\_j\\\\)分裂成\\\\(R\\_L和R\\_R\\\\)后，**整棵树最小\\\\(\\mathfrak{L}\\_k\\\\)的降低量，这个量越大越好**。\n\n\n#### 3.1 xgboost树的分裂算法\n\n![treesplitalgorithm](/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png)\n\n#### 3.2 xgboost调参\n\n[1-Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n\n[2-Hyperopt](https://blog.csdn.net/a819825294/article/details/51775418)\n\n[3-GridSearchCV](https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn)\n\n\n-------------\n\n### 4. 正则化\n\nGBDT有非常快降低Loss的能力，这也会造成一个问题：Loss迅速下降，模型低bias，高variance，造成过拟合。\n\n下面一一介绍GBDT中抵抗过拟合的技巧：\n\n1. 限制树的复杂度。\\\\(\\Omega\\\\)函数对树的节点数，和节点上预测值\\\\( \\lbrace b\\_j \\rbrace^J\\_1 \\\\)的平方和均有惩罚，除此之外，我们通常在终止条件上还会增加一条为树的深度。\n2. 采样。即训练每个树的时候只使用一部分的样本。\n3. 列采样。即训练每个树的时候只使用一部分的特征，**这是Xgboost的创新，它将随机森林中的思想引入了GBDT**。\n4. Shrinkage。进一步惩罚\\\\(\\lbrace b\\_j \\rbrace^J\\_1\\\\)，给它们乘一个小于1的系数，也可以理解为设置了一个较低的学习率。\n5. Early stop。因为GBDT的可叠加性我们使用的模型不一定是最终的ensemble，而根据测试集的测试情况，选择使用前若干棵树。\n\n\n------------\n\n### 5. GBDT与XGBoost的比较\n\n以下来自知乎，作者：wepon \n\n1. 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。\n2. 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。\n3. xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。\n4. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）\n5. 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。\n6. 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。\n7. xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\n8. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n9. 为了限制树的生长，加入阈值gamma，当增益大于阈值时才让节点分裂，它是正则项里叶子节点数J的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性\n\n\n-------------\n\n### 参考\n\n> [kimmyzhang - GBDT详解上 + 下 + 后补](http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/)\n\n> [Friedman - Greedy Function Approximation:A Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n\n> [GBDT(MART)迭代决策树入门教程简介](https://blog.csdn.net/w28971023/article/details/8240756)\n\n> [雪伦 - xgboost原理](https://blog.csdn.net/a819825294/article/details/51206410)\n\n> [机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？](https://www.zhihu.com/question/41354392)\n\n> [Tianqi Chen - XGBoost_A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754.pdf)\n","slug":"gbdtxgboost","published":1,"updated":"2019-08-17T09:34:13.035Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvl001k2qwp1d79evah","content":"<h2 id=\"GBDT-XGBoost\"><a href=\"#GBDT-XGBoost\" class=\"headerlink\" title=\"GBDT & XGBoost\"></a><center>GBDT & XGBoost</center></h2><p>目录</p><ul>\n<li>符号定义</li>\n<li>GBDT = GBRT = MART</li>\n<li>XGBoost</li>\n<li>正则化</li>\n<li>GBDT与XGBoost的比较</li>\n</ul><hr><h3 id=\"1-符号定义\"><a href=\"#1-符号定义\" class=\"headerlink\" title=\"1. 符号定义\"></a>1. 符号定义</h3><p>决策树\n\\[\nf(x; \\lbrace R_j, b_j\\rbrace ^{J}_1) = \\sum_{j=1}^J b_j I(x\\in R_j)\n\\]\n\\( \\lbrace R_j \\rbrace ^{J}_1 \\)和\\( \\lbrace b_j \\rbrace ^{J}_1 \\)表示决策树的参数，前者为分段空间(disjoint空间)，后者为这些空间上的输出值[其他地方称为打分值]，\\(J\\)是叶子结点的数量，下文中用\\(f(x)\\)省略表示\\( f(x; \\lbrace R_j, b_j\\rbrace ^{J}_1) \\)</p><a id=\"more\"></a>\n\n\n\n<p>决策树的Ensemble\n\\[\nF = \\sum_{i=0}^K f_i\n\\]\n其中\\(f_0\\)是模型初始值，通常为统计样本计算出的常数[论文中为median]，同时定义\\( F_k = \\sum_{i=0}^k f_i \\)。</p>\n<p>\\(D = \\lbrace (x_i, y_i) \\rbrace ^N_1\\)，训练样本。</p>\n<p>\\[\n\\mathfrak{L} = \\mathfrak{L}( \\lbrace y_i, F(x_i) \\rbrace ^N_1 ) = \\underbrace{\\sum_{i=1}^N L(y_i, F(x_i))}_{\\text{训练误差项}} + \\underbrace{\\sum_{k=1}^K \\Omega(f_k)}_{\\text{正则化项}}\n\\]\n目标函数(损失函数)，第一项\\(L\\)是针对训练数据的\\(Loss\\)，可以选择：绝对值误差、平方误差、logistic loss等；第二项\\( \\Omega \\)是正则化函数，惩罚\\(f_k\\)的复杂度。</p>\n<hr>\n<h3 id=\"2-GBDT\"><a href=\"#2-GBDT\" class=\"headerlink\" title=\"2. GBDT\"></a>2. GBDT</h3><h4 id=\"2-1-算法1-GBDT算法-论文中：Algorithm1-Gradient-Boost\"><a href=\"#2-1-算法1-GBDT算法-论文中：Algorithm1-Gradient-Boost\" class=\"headerlink\" title=\"2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]\"></a>2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]</h4><p>输入：\\( \\lbrace (x_i, y_i) \\rbrace^N_1 , K, L, …\\)</p>\n<p>输出：\\(F_k\\)</p>\n<p>（1）初始化\\(f_0\\)</p>\n<p>（2）对k=1,2,…,K, 计算</p>\n<p>（2.1）\\( \\tilde{y}_i = - \\frac{\\partial L(y_i, F_{k-1}(x_i))}{\\partial F_{k-1}}, i=1,2,…,N \\)</p>\n<p>计算响应[response]\\(\\tilde{y}_i\\)，它是一个和残差[residual, \\(y_i-F_{k-1}(x_i)\\)]正相关的变量。</p>\n<p>（2.2）\\( \\lbrace R_j, b_j \\rbrace^{J^{\\ast}}_1 = \\mathop{\\arg\\min}_{\\lbrace R_j, b_j \\rbrace^J_1} \\sum_{i=1}^N [\\tilde{y}_i - f_k(x_i;\\lbrace R_j,b_j \\rbrace^J_1)]^2 \\)</p>\n<p>使用平方误差训练一颗决策树\\(f_k\\)，拟合数据\\( \\lbrace (x_i, \\tilde{y}_i) \\rbrace^N_1 \\)</p>\n<p>（2.3）\\( \\rho^{\\ast} = \\mathop{\\arg\\min}_{\\rho} \\mathfrak{L}(\\lbrace y_i, F_{k-1}(x_i)+\\rho f_k(x_i) \\rbrace ^N_1)= \\mathop{\\arg\\min}_{\\rho} \\sum_{i=1}^N L(y_i, F_{k-1}(x_i) + \\rho f _k(x_i)) + \\Omega(f_k) \\)</p>\n<p>求一个步长\\(\\rho^{\\ast}\\)，最小化损失。</p>\n<p>（2.4）令\\( f_k = \\rho^{\\ast}f_k, \\quad  F_k = F_{k-1}+f_k \\)</p>\n<p>将训练出来的\\(f_k\\)叠加到\\(F\\)。</p>\n<p><em>总体来说，GBDT就是一个不断拟合响应(残差)并叠加到F上的过程，在这个过程中，响应不断变小，Loss不断接近最小值。</em></p>\n<h4 id=\"2-2-GBDT例子\"><a href=\"#2-2-GBDT例子\" class=\"headerlink\" title=\"2.2 GBDT例子\"></a>2.2 GBDT例子</h4><p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。</p>\n<p>假设我们现在有一个训练集，训练集只有4个人A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：</p>\n<p><img src=\"/posts_res/2018-04-12-gbdtxgboost/1-1.png\" alt=\"fulltree\"></p>\n<p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：</p>\n<p><img src=\"/posts_res/2018-04-12-gbdtxgboost/1-2.png\" alt=\"gbdt\"></p>\n<p>在第一棵树分枝和第一张图片中一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1,-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。</p>\n<p>最后GBDT的预测结果为：</p>\n<ul>\n<li>A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14；</li>\n<li>B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16；</li>\n<li>C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24；</li>\n<li>D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26。</li>\n</ul>\n<p>那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。</p>\n<p>注：两张图片中的最终效果相同，为何还需要GBDT呢？答案是过拟合。</p>\n<p><em>其他例子请见李航博士《统计学习方法》page.149</em></p>\n<p>GBRT 的优点:</p>\n<ul>\n<li>对混合型数据的自然处理（异构特征）</li>\n<li>强大的预测能力</li>\n<li>在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)</li>\n</ul>\n<p>GBDT 的缺点:</p>\n<ul>\n<li>可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBDT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.</li>\n</ul>\n<p><a href=\"https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\" target=\"_blank\" rel=\"noopener\">附GBDT论文Friedman J. - Greedy Function Approximation_A Gradient Boosting Machine</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6140514.html\" target=\"_blank\" rel=\"noopener\">梯度提升树(GBDT)原理小结 - 刘建平Pinard</a></p>\n<hr>\n<h3 id=\"3-XGBoost\"><a href=\"#3-XGBoost\" class=\"headerlink\" title=\"3. XGBoost\"></a>3. XGBoost</h3><p>xgboost中使用的正则化函数为：\n\\[\n\\Omega(f_k) = \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b^2_j\n\\]\n我们的目标是求\\(f_k\\)，它最小化目标函数[式\\(3\\)]\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}_k \n&amp; = \\sum_{i=1}^N L(y_i, F_{k-1}(x_i)+ \\rho f_k(x_i)) + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N L(y_i, F_{k-1}+\\rho f_k) + \\Omega(f_k) \\quad \\quad \\quad [泰勒二阶展开可得下步] \\\\\n&amp; \\approx \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + \\underbrace{\\frac{\\partial L(y_i, F_{k-1})}{\\partial F_{k-1}}}_{\\text{= \\(g_i\\)}} f_k + \\frac{1}{2}\\underbrace{\\frac{\\partial^2 L(y_i, F_{k-1})}{\\partial F^2_{k-1}}}_{\\text{=\\(h_i\\)}} f^2_k \\rgroup + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + g_i f_k + \\frac{1}{2}h_i f_k^2 \\rgroup + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + g_i \\sum_{j=1}^J b_j + \\frac{1}{2} h_i \\sum_{j=1}^J b_j^2  \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n整理出和\\(\\lbrace R_j \\rbrace^J_1, \\lbrace b_j \\rbrace^J_1\\)有关的项：\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}(\\lbrace R_j \\rbrace^J_1, \\lbrace b_j \\rbrace^J_1) \n&amp; = \\sum_{i=1}^N \\lgroup g_i \\sum_{j=1}^J b_j + \\frac{1}{2}h_i \\sum_{j=1}^J b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{x_i \\in R_j} \\lgroup g_i \\sum_{j=1}^J b_j + \\frac{1}{2}h_i \\sum_{j=1}^J b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{j=1}^J \\lgroup \\sum_{x_i \\in R_j} g_i b_j + \\sum_{x_i \\in R_j} \\frac{1}{2} h_i b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{j=1}^J \\lgroup \\underbrace{\\sum_{x_i \\in R_j}g_i}_{\\text{\\(=G_j\\)}}b_j + \\frac{1}{2} \\lgroup \\underbrace{\\sum_{x_i \\in R_j}h_i}_{\\text{\\(=H_j \\)}} + \\lambda \\rgroup b_j^2 \\rgroup + \\frac{\\gamma}{2} J \\\\\n&amp; = \\sum_{j=1}^J \\lgroup G_jb_j + \\frac{1}{2}(H_j + \\lambda)b_j^2 \\rgroup + \\frac{\\gamma}{2} J\n\\end{aligned}\n\\end{equation}\n\\]\n\\(式(6)\\)对\\(b_j\\)求导，并令其等于零，得：\n\\[\nb_j^{\\ast} = - \\frac{G_j}{H_j + \\lambda}, \\quad j=1,2,…,J\n\\]\n\\(式(7)代入式(6)\\)中，化简得最小的\\(\\mathfrak{L}\\)：\n\\[\n\\mathfrak{L}^{\\ast}_k = - \\frac{1}{2} \\sum_{j=1}^J \\frac{G_j^2}{H_j+\\lambda} + \\frac{\\gamma}{2}J\n\\]</p>\n<p>求\\(\\lbrace R_j\\rbrace^J_1\\)与求\\( \\lbrace b_j \\rbrace^J_1 \\)的方法不同，前者它是对输入\\(x\\)所属空间的一种划分方法不连续，无法求导。\n精确得到划分\\(\\lbrace R_j \\rbrace^J_1\\)是一个NP难问 题，取而代之使用贪心法，即分裂某节点时，只考虑对当前节点分裂后哪个分裂方案能得到最小的\\(\\mathfrak{L}_k\\)。\n像传统决策树一样，CART中的办法也是遍历\\(x\\)的每个维度的每个分裂点，选择具有最小\\(\\mathfrak{L}_k)的维度和分裂点进行。\n那么定义：当前节点 \\( R_j \\) 分裂成\\( R_L \\)和\\( R_R \\)使得分裂后整棵树的\\( \\mathfrak{L}_k \\)最小。</p>\n<p>从\\(式(8)\\)可知，整棵树的最小\\(\\mathfrak{L}_k\\)等于每个叶子结点上(最小)Loss的和，由于整个分裂过程中只涉及到3个结点，其他任何结点的Loss在分裂过程中不变，这个问题等价于：\n\\[\n\\mathop{\\max}_{R_L, R_R} \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda} - \\frac{\\gamma}{2}\n\\]\n\\(式(9)\\)的含义是：前两项分别加上新生成的叶子结点的最小Loss，第三项是指减去被分裂的叶子结点的最小Loss，第四项是分裂后增加叶结点带来的模型复杂度。它是将结点\\(R_j\\)分裂成\\(R_L和R_R\\)后，<strong>整棵树最小\\(\\mathfrak{L}_k\\)的降低量，这个量越大越好</strong>。</p>\n<h4 id=\"3-1-xgboost树的分裂算法\"><a href=\"#3-1-xgboost树的分裂算法\" class=\"headerlink\" title=\"3.1 xgboost树的分裂算法\"></a>3.1 xgboost树的分裂算法</h4><p><img src=\"/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png\" alt=\"treesplitalgorithm\"></p>\n<h4 id=\"3-2-xgboost调参\"><a href=\"#3-2-xgboost调参\" class=\"headerlink\" title=\"3.2 xgboost调参\"></a>3.2 xgboost调参</h4><p><a href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\" target=\"_blank\" rel=\"noopener\">1-Complete Guide to Parameter Tuning in XGBoost</a></p>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51775418\" target=\"_blank\" rel=\"noopener\">2-Hyperopt</a></p>\n<p><a href=\"https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn\" target=\"_blank\" rel=\"noopener\">3-GridSearchCV</a></p>\n<hr>\n<h3 id=\"4-正则化\"><a href=\"#4-正则化\" class=\"headerlink\" title=\"4. 正则化\"></a>4. 正则化</h3><p>GBDT有非常快降低Loss的能力，这也会造成一个问题：Loss迅速下降，模型低bias，高variance，造成过拟合。</p>\n<p>下面一一介绍GBDT中抵抗过拟合的技巧：</p>\n<ol>\n<li>限制树的复杂度。\\(\\Omega\\)函数对树的节点数，和节点上预测值\\( \\lbrace b_j \\rbrace^J_1 \\)的平方和均有惩罚，除此之外，我们通常在终止条件上还会增加一条为树的深度。</li>\n<li>采样。即训练每个树的时候只使用一部分的样本。</li>\n<li>列采样。即训练每个树的时候只使用一部分的特征，<strong>这是Xgboost的创新，它将随机森林中的思想引入了GBDT</strong>。</li>\n<li>Shrinkage。进一步惩罚\\(\\lbrace b_j \\rbrace^J_1\\)，给它们乘一个小于1的系数，也可以理解为设置了一个较低的学习率。</li>\n<li>Early stop。因为GBDT的可叠加性我们使用的模型不一定是最终的ensemble，而根据测试集的测试情况，选择使用前若干棵树。</li>\n</ol>\n<hr>\n<h3 id=\"5-GBDT与XGBoost的比较\"><a href=\"#5-GBDT与XGBoost的比较\" class=\"headerlink\" title=\"5. GBDT与XGBoost的比较\"></a>5. GBDT与XGBoost的比较</h3><p>以下来自知乎，作者：wepon </p>\n<ol>\n<li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>\n<li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</li>\n<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li>\n<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</li>\n<li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li>\n<li>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>\n<li>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>\n<li>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</li>\n<li>为了限制树的生长，加入阈值gamma，当增益大于阈值时才让节点分裂，它是正则项里叶子节点数J的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/\" target=\"_blank\" rel=\"noopener\">kimmyzhang - GBDT详解上 + 下 + 后补</a></p>\n<p><a href=\"https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\" target=\"_blank\" rel=\"noopener\">Friedman - Greedy Function Approximation:A Gradient Boosting Machine</a></p>\n<p><a href=\"https://blog.csdn.net/w28971023/article/details/8240756\" target=\"_blank\" rel=\"noopener\">GBDT(MART)迭代决策树入门教程简介</a></p>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51206410\" target=\"_blank\" rel=\"noopener\">雪伦 - xgboost原理</a></p>\n<p><a href=\"https://www.zhihu.com/question/41354392\" target=\"_blank\" rel=\"noopener\">机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？</a></p>\n<p><a href=\"https://arxiv.org/pdf/1603.02754.pdf\" target=\"_blank\" rel=\"noopener\">Tianqi Chen - XGBoost_A Scalable Tree Boosting System</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"GBDT-XGBoost\"><a href=\"#GBDT-XGBoost\" class=\"headerlink\" title=\"GBDT & XGBoost\"></a><center>GBDT & XGBoost</center></h2><p>目录</p><ul>\n<li>符号定义</li>\n<li>GBDT = GBRT = MART</li>\n<li>XGBoost</li>\n<li>正则化</li>\n<li>GBDT与XGBoost的比较</li>\n</ul><hr><h3 id=\"1-符号定义\"><a href=\"#1-符号定义\" class=\"headerlink\" title=\"1. 符号定义\"></a>1. 符号定义</h3><p>决策树\n\\[\nf(x; \\lbrace R_j, b_j\\rbrace ^{J}_1) = \\sum_{j=1}^J b_j I(x\\in R_j)\n\\]\n\\( \\lbrace R_j \\rbrace ^{J}_1 \\)和\\( \\lbrace b_j \\rbrace ^{J}_1 \\)表示决策树的参数，前者为分段空间(disjoint空间)，后者为这些空间上的输出值[其他地方称为打分值]，\\(J\\)是叶子结点的数量，下文中用\\(f(x)\\)省略表示\\( f(x; \\lbrace R_j, b_j\\rbrace ^{J}_1) \\)</p>","more":"\n\n\n\n<p>决策树的Ensemble\n\\[\nF = \\sum_{i=0}^K f_i\n\\]\n其中\\(f_0\\)是模型初始值，通常为统计样本计算出的常数[论文中为median]，同时定义\\( F_k = \\sum_{i=0}^k f_i \\)。</p>\n<p>\\(D = \\lbrace (x_i, y_i) \\rbrace ^N_1\\)，训练样本。</p>\n<p>\\[\n\\mathfrak{L} = \\mathfrak{L}( \\lbrace y_i, F(x_i) \\rbrace ^N_1 ) = \\underbrace{\\sum_{i=1}^N L(y_i, F(x_i))}_{\\text{训练误差项}} + \\underbrace{\\sum_{k=1}^K \\Omega(f_k)}_{\\text{正则化项}}\n\\]\n目标函数(损失函数)，第一项\\(L\\)是针对训练数据的\\(Loss\\)，可以选择：绝对值误差、平方误差、logistic loss等；第二项\\( \\Omega \\)是正则化函数，惩罚\\(f_k\\)的复杂度。</p>\n<hr>\n<h3 id=\"2-GBDT\"><a href=\"#2-GBDT\" class=\"headerlink\" title=\"2. GBDT\"></a>2. GBDT</h3><h4 id=\"2-1-算法1-GBDT算法-论文中：Algorithm1-Gradient-Boost\"><a href=\"#2-1-算法1-GBDT算法-论文中：Algorithm1-Gradient-Boost\" class=\"headerlink\" title=\"2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]\"></a>2.1 算法1 - GBDT算法[论文中：Algorithm1:Gradient Boost]</h4><p>输入：\\( \\lbrace (x_i, y_i) \\rbrace^N_1 , K, L, …\\)</p>\n<p>输出：\\(F_k\\)</p>\n<p>（1）初始化\\(f_0\\)</p>\n<p>（2）对k=1,2,…,K, 计算</p>\n<p>（2.1）\\( \\tilde{y}_i = - \\frac{\\partial L(y_i, F_{k-1}(x_i))}{\\partial F_{k-1}}, i=1,2,…,N \\)</p>\n<p>计算响应[response]\\(\\tilde{y}_i\\)，它是一个和残差[residual, \\(y_i-F_{k-1}(x_i)\\)]正相关的变量。</p>\n<p>（2.2）\\( \\lbrace R_j, b_j \\rbrace^{J^{\\ast}}_1 = \\mathop{\\arg\\min}_{\\lbrace R_j, b_j \\rbrace^J_1} \\sum_{i=1}^N [\\tilde{y}_i - f_k(x_i;\\lbrace R_j,b_j \\rbrace^J_1)]^2 \\)</p>\n<p>使用平方误差训练一颗决策树\\(f_k\\)，拟合数据\\( \\lbrace (x_i, \\tilde{y}_i) \\rbrace^N_1 \\)</p>\n<p>（2.3）\\( \\rho^{\\ast} = \\mathop{\\arg\\min}_{\\rho} \\mathfrak{L}(\\lbrace y_i, F_{k-1}(x_i)+\\rho f_k(x_i) \\rbrace ^N_1)= \\mathop{\\arg\\min}_{\\rho} \\sum_{i=1}^N L(y_i, F_{k-1}(x_i) + \\rho f _k(x_i)) + \\Omega(f_k) \\)</p>\n<p>求一个步长\\(\\rho^{\\ast}\\)，最小化损失。</p>\n<p>（2.4）令\\( f_k = \\rho^{\\ast}f_k, \\quad  F_k = F_{k-1}+f_k \\)</p>\n<p>将训练出来的\\(f_k\\)叠加到\\(F\\)。</p>\n<p><em>总体来说，GBDT就是一个不断拟合响应(残差)并叠加到F上的过程，在这个过程中，响应不断变小，Loss不断接近最小值。</em></p>\n<h4 id=\"2-2-GBDT例子\"><a href=\"#2-2-GBDT例子\" class=\"headerlink\" title=\"2.2 GBDT例子\"></a>2.2 GBDT例子</h4><p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。</p>\n<p>假设我们现在有一个训练集，训练集只有4个人A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：</p>\n<p><img src=\"/posts_res/2018-04-12-gbdtxgboost/1-1.png\" alt=\"fulltree\"></p>\n<p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：</p>\n<p><img src=\"/posts_res/2018-04-12-gbdtxgboost/1-2.png\" alt=\"gbdt\"></p>\n<p>在第一棵树分枝和第一张图片中一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1,-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。</p>\n<p>最后GBDT的预测结果为：</p>\n<ul>\n<li>A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14；</li>\n<li>B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16；</li>\n<li>C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24；</li>\n<li>D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26。</li>\n</ul>\n<p>那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。</p>\n<p>注：两张图片中的最终效果相同，为何还需要GBDT呢？答案是过拟合。</p>\n<p><em>其他例子请见李航博士《统计学习方法》page.149</em></p>\n<p>GBRT 的优点:</p>\n<ul>\n<li>对混合型数据的自然处理（异构特征）</li>\n<li>强大的预测能力</li>\n<li>在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)</li>\n</ul>\n<p>GBDT 的缺点:</p>\n<ul>\n<li>可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBDT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.</li>\n</ul>\n<p><a href=\"https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\" target=\"_blank\" rel=\"noopener\">附GBDT论文Friedman J. - Greedy Function Approximation_A Gradient Boosting Machine</a></p>\n<p><a href=\"https://www.cnblogs.com/pinard/p/6140514.html\" target=\"_blank\" rel=\"noopener\">梯度提升树(GBDT)原理小结 - 刘建平Pinard</a></p>\n<hr>\n<h3 id=\"3-XGBoost\"><a href=\"#3-XGBoost\" class=\"headerlink\" title=\"3. XGBoost\"></a>3. XGBoost</h3><p>xgboost中使用的正则化函数为：\n\\[\n\\Omega(f_k) = \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b^2_j\n\\]\n我们的目标是求\\(f_k\\)，它最小化目标函数[式\\(3\\)]\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}_k \n&amp; = \\sum_{i=1}^N L(y_i, F_{k-1}(x_i)+ \\rho f_k(x_i)) + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N L(y_i, F_{k-1}+\\rho f_k) + \\Omega(f_k) \\quad \\quad \\quad [泰勒二阶展开可得下步] \\\\\n&amp; \\approx \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + \\underbrace{\\frac{\\partial L(y_i, F_{k-1})}{\\partial F_{k-1}}}_{\\text{= \\(g_i\\)}} f_k + \\frac{1}{2}\\underbrace{\\frac{\\partial^2 L(y_i, F_{k-1})}{\\partial F^2_{k-1}}}_{\\text{=\\(h_i\\)}} f^2_k \\rgroup + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + g_i f_k + \\frac{1}{2}h_i f_k^2 \\rgroup + \\Omega(f_k) \\\\\n&amp; = \\sum_{i=1}^N \\lgroup L(y_i, F_{k-1}) + g_i \\sum_{j=1}^J b_j + \\frac{1}{2} h_i \\sum_{j=1}^J b_j^2  \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n\\end{aligned}\n\\end{equation}\n\\]\n整理出和\\(\\lbrace R_j \\rbrace^J_1, \\lbrace b_j \\rbrace^J_1\\)有关的项：\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathfrak{L}(\\lbrace R_j \\rbrace^J_1, \\lbrace b_j \\rbrace^J_1) \n&amp; = \\sum_{i=1}^N \\lgroup g_i \\sum_{j=1}^J b_j + \\frac{1}{2}h_i \\sum_{j=1}^J b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{x_i \\in R_j} \\lgroup g_i \\sum_{j=1}^J b_j + \\frac{1}{2}h_i \\sum_{j=1}^J b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{j=1}^J \\lgroup \\sum_{x_i \\in R_j} g_i b_j + \\sum_{x_i \\in R_j} \\frac{1}{2} h_i b_j^2 \\rgroup + \\frac{\\gamma}{2} J + \\frac{\\lambda}{2} \\sum_{j=1}^J b_j^2 \\\\\n&amp; = \\sum_{j=1}^J \\lgroup \\underbrace{\\sum_{x_i \\in R_j}g_i}_{\\text{\\(=G_j\\)}}b_j + \\frac{1}{2} \\lgroup \\underbrace{\\sum_{x_i \\in R_j}h_i}_{\\text{\\(=H_j \\)}} + \\lambda \\rgroup b_j^2 \\rgroup + \\frac{\\gamma}{2} J \\\\\n&amp; = \\sum_{j=1}^J \\lgroup G_jb_j + \\frac{1}{2}(H_j + \\lambda)b_j^2 \\rgroup + \\frac{\\gamma}{2} J\n\\end{aligned}\n\\end{equation}\n\\]\n\\(式(6)\\)对\\(b_j\\)求导，并令其等于零，得：\n\\[\nb_j^{\\ast} = - \\frac{G_j}{H_j + \\lambda}, \\quad j=1,2,…,J\n\\]\n\\(式(7)代入式(6)\\)中，化简得最小的\\(\\mathfrak{L}\\)：\n\\[\n\\mathfrak{L}^{\\ast}_k = - \\frac{1}{2} \\sum_{j=1}^J \\frac{G_j^2}{H_j+\\lambda} + \\frac{\\gamma}{2}J\n\\]</p>\n<p>求\\(\\lbrace R_j\\rbrace^J_1\\)与求\\( \\lbrace b_j \\rbrace^J_1 \\)的方法不同，前者它是对输入\\(x\\)所属空间的一种划分方法不连续，无法求导。\n精确得到划分\\(\\lbrace R_j \\rbrace^J_1\\)是一个NP难问 题，取而代之使用贪心法，即分裂某节点时，只考虑对当前节点分裂后哪个分裂方案能得到最小的\\(\\mathfrak{L}_k\\)。\n像传统决策树一样，CART中的办法也是遍历\\(x\\)的每个维度的每个分裂点，选择具有最小\\(\\mathfrak{L}_k)的维度和分裂点进行。\n那么定义：当前节点 \\( R_j \\) 分裂成\\( R_L \\)和\\( R_R \\)使得分裂后整棵树的\\( \\mathfrak{L}_k \\)最小。</p>\n<p>从\\(式(8)\\)可知，整棵树的最小\\(\\mathfrak{L}_k\\)等于每个叶子结点上(最小)Loss的和，由于整个分裂过程中只涉及到3个结点，其他任何结点的Loss在分裂过程中不变，这个问题等价于：\n\\[\n\\mathop{\\max}_{R_L, R_R} \\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda} - \\frac{\\gamma}{2}\n\\]\n\\(式(9)\\)的含义是：前两项分别加上新生成的叶子结点的最小Loss，第三项是指减去被分裂的叶子结点的最小Loss，第四项是分裂后增加叶结点带来的模型复杂度。它是将结点\\(R_j\\)分裂成\\(R_L和R_R\\)后，<strong>整棵树最小\\(\\mathfrak{L}_k\\)的降低量，这个量越大越好</strong>。</p>\n<h4 id=\"3-1-xgboost树的分裂算法\"><a href=\"#3-1-xgboost树的分裂算法\" class=\"headerlink\" title=\"3.1 xgboost树的分裂算法\"></a>3.1 xgboost树的分裂算法</h4><p><img src=\"/posts_res/2018-04-12-gbdtxgboost/3-treesplit.png\" alt=\"treesplitalgorithm\"></p>\n<h4 id=\"3-2-xgboost调参\"><a href=\"#3-2-xgboost调参\" class=\"headerlink\" title=\"3.2 xgboost调参\"></a>3.2 xgboost调参</h4><p><a href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\" target=\"_blank\" rel=\"noopener\">1-Complete Guide to Parameter Tuning in XGBoost</a></p>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51775418\" target=\"_blank\" rel=\"noopener\">2-Hyperopt</a></p>\n<p><a href=\"https://www.kaggle.com/tanitter/grid-search-xgboost-with-scikit-learn\" target=\"_blank\" rel=\"noopener\">3-GridSearchCV</a></p>\n<hr>\n<h3 id=\"4-正则化\"><a href=\"#4-正则化\" class=\"headerlink\" title=\"4. 正则化\"></a>4. 正则化</h3><p>GBDT有非常快降低Loss的能力，这也会造成一个问题：Loss迅速下降，模型低bias，高variance，造成过拟合。</p>\n<p>下面一一介绍GBDT中抵抗过拟合的技巧：</p>\n<ol>\n<li>限制树的复杂度。\\(\\Omega\\)函数对树的节点数，和节点上预测值\\( \\lbrace b_j \\rbrace^J_1 \\)的平方和均有惩罚，除此之外，我们通常在终止条件上还会增加一条为树的深度。</li>\n<li>采样。即训练每个树的时候只使用一部分的样本。</li>\n<li>列采样。即训练每个树的时候只使用一部分的特征，<strong>这是Xgboost的创新，它将随机森林中的思想引入了GBDT</strong>。</li>\n<li>Shrinkage。进一步惩罚\\(\\lbrace b_j \\rbrace^J_1\\)，给它们乘一个小于1的系数，也可以理解为设置了一个较低的学习率。</li>\n<li>Early stop。因为GBDT的可叠加性我们使用的模型不一定是最终的ensemble，而根据测试集的测试情况，选择使用前若干棵树。</li>\n</ol>\n<hr>\n<h3 id=\"5-GBDT与XGBoost的比较\"><a href=\"#5-GBDT与XGBoost的比较\" class=\"headerlink\" title=\"5. GBDT与XGBoost的比较\"></a>5. GBDT与XGBoost的比较</h3><p>以下来自知乎，作者：wepon </p>\n<ol>\n<li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li>\n<li>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</li>\n<li>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。</li>\n<li>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</li>\n<li>列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。</li>\n<li>对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。</li>\n<li>xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</li>\n<li>可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</li>\n<li>为了限制树的生长，加入阈值gamma，当增益大于阈值时才让节点分裂，它是正则项里叶子节点数J的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/\" target=\"_blank\" rel=\"noopener\">kimmyzhang - GBDT详解上 + 下 + 后补</a></p>\n<p><a href=\"https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\" target=\"_blank\" rel=\"noopener\">Friedman - Greedy Function Approximation:A Gradient Boosting Machine</a></p>\n<p><a href=\"https://blog.csdn.net/w28971023/article/details/8240756\" target=\"_blank\" rel=\"noopener\">GBDT(MART)迭代决策树入门教程简介</a></p>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51206410\" target=\"_blank\" rel=\"noopener\">雪伦 - xgboost原理</a></p>\n<p><a href=\"https://www.zhihu.com/question/41354392\" target=\"_blank\" rel=\"noopener\">机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？</a></p>\n<p><a href=\"https://arxiv.org/pdf/1603.02754.pdf\" target=\"_blank\" rel=\"noopener\">Tianqi Chen - XGBoost_A Scalable Tree Boosting System</a></p>\n</blockquote>\n"},{"layout":"post","title":"使用sklearn大规模机器学习","date":"2018-04-03T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n## <center> [转]使用sklearn大规模机器学习 </center>\n\n目录\n\n* 核外学习(out-of-core learning)\n* 磁盘上数据流式化\n* sklearn 中的 SGD\n* 流式数据中的特征工程\n* 总结\n\n转载自:[吴良超的学习笔记](http://wulc.me/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)\n\n------\n\n### 核外学习(out-of-core learning)\n\n核外学习指的是机器的内存无法容纳训练的数据集，但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。\n\n#### sampling（采样）\n\n采样可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。\n\n解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。\n\n\n#### mini-batch learning（小批量学习/增量学习）\n\n这种方法不同于 sampling，利用了全部的数据，只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。\n\n通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息（如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负）。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。\n\n\n----------\n\n### 磁盘上数据流式化\n\n#### 文件读取\n\n这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现\n\n```python3\nwith open(source_file, 'r') as fp:\n    line = fp.readline()\n    while line:\n        # data processing\n        # training\n        line = fp.readline()\n```\n\n而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取，\n下面以这个数据文件为例说明：\n[Bike-Sharing-Dataset.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip)\n[备用地址:Bike-Sharing-Dataset.zip](/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip), \n\n```python3\nimport csv\nSEP = \",\"\nwith open(source_file, 'r') as fp:\n    iterator = csv.reader(fp, delimiter=SEP)\n    for n, row in enumerate(iterator):\n        if n == 0:\n            header = row\n        else:\n            # data processing\n            # training\n            pass\n    print('Total rows: %i' % (n+1))\n    print('Header: %s' % ', '.join(header))\n    print('Sample values: %s' % ', '.join(row))\n```\n\n输出为\n\n```text\nTotal rows: 17380\nHeader: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt\nSample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49\n```\n\n在上面的例子中，每个样本是就是一个 row（list类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为\n\n```python3\nimport csv\nSEP = \",\"\nwith open(source_file, 'r') as R:\n    iterator = csv.DictReader(R, delimiter=SEP)\n    for n, row in enumerate(iterator):\n        # data processing\n        # training\n        pass\n    print ('Total rows: %i' % (n+1))\n    print ('Sample values: %s' % row)\n```\n\n输出为\n\n```text\nTotal rows: 17379\nSample values: {'temp': '0.26', 'cnt': '49', 'yr': '1', 'windspeed': '0.1343', 'casual': '12', 'registered': '37', 'season': '1', 'weathersit': '1', 'dteday': '2012-12-31', 'hr': '23', 'weekday': '1', 'atemp': '0.2727', 'hum': '0.65', 'holiday': '0', 'instant': '17379', 'mnth': '12', 'workingday': '1'}\n```\n\n除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示:\n\n```python3\nimport pandas as pd\nCHUNK_SIZE = 1000\nwith open(source_file, 'rb') as R:\n    iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n    for n, data_chunk in enumerate(iterator):\n        print ('Size of uploaded chunk: %i instances, %i features' % (data_chunk.shape))\n        # data processing\n        # training\n        pass\n    print ('Sample values: \\n%s' % str(data_chunk.iloc[0]))\n```\n\n对应的输出为\n\n```text\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 379 instances, 17 features\nSample values: \ninstant            17001\ndteday        2012-12-16\nseason                 4\nyr                     1\nmnth                  12\nhr                     3\nholiday                0\nweekday                0\nworkingday             0\nweathersit             2\ntemp                0.34\natemp             0.3333\nhum                 0.87\nwindspeed          0.194\ncasual                 1\nregistered            37\ncnt                   38\nName: 17000, dtype: object\n```\n\n#### 数据库读取\n\n上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不仅能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。\n\n假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下(**未验证**)\n\n```python3\nimport sqlite3\nimport pandas as pd\n\nDB_NAME = 'bikesharing.sqlite'\nCHUNK_SIZE = 2500\n\nconn = sqlite3.connect(DB_NAME)\nconn.text_factory = str  # allows utf-8 data to be stored     \nsql = \"SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)\"\nDB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)\nfor j,data_chunk in enumerate(DB_stream):\n    print ('Chunk %i -' % (j+1)),\n    print ('Size of uploaded chunk: %i istances, %i features' % (data_chunk.shape))\n    # data processing\n    # training\n    pass\n```\n\n输出为\n\n```text\nChunk 1 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 2 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 3 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 4 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 5 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 6 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 7 - Size of uploaded chunk: 2379 istances, 18 features\n```\n\n#### 样本的读取顺序\n\n上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。\n\n假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。\n\n这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。\n\n假如内存能够容纳这些数据，那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接，下面是这两种 shuffle 方法的实现代码。\n\n在**内存**中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下\n\n```python3\nimport zlib\nfrom random import shuffle\n\ndef ram_shuffle(filename_in, filename_out, header=True):\n    with open(filename_in, 'r') as f:\n        zlines = [zlib.compress(line, 9) for line in f]\n        if header:\n            first_row = zlines.pop(0)\n    shuffle(zlines)\n    with open(filename_out, 'w') as f:\n        if header:\n            f.write(zlib.decompress(first_row))\n        for zline in zlines:\n            f.write(zlib.decompress(zline))\n```\n\n基于**磁盘**的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下\n\n```python3\nfrom random import shuffle\nimport pandas as pd\nimport numpy as np\nimport os\n\ndef disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=','):\n    for i in range(iterations):\n        with open(filename_in, 'rb') as R:\n            iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n            for n, df in enumerate(iterator):\n                if n==0 and header:\n                    header_cols =SEP.join(df.columns)+'\\n'\n                df.iloc[np.random.permutation(len(df))].to_csv(str(n)+'_chunk.csv', index=False, header=False, sep=SEP)\n        ordering = list(range(0,n+1))\n        shuffle(ordering)\n        with open(filename_out, 'wb') as W:\n            if header:\n                W.write(header_cols)\n            for f in ordering:\n                with open(str(f)+'_chunk.csv', 'r') as R:\n                    for line in R:\n                        W.write(line)\n                os.remove(str(f)+'_chunk.csv')\n        filename_in = filename_out\n        CHUNK_SIZE = int(CHUNK_SIZE / 2)\n```\n\n\n-----------\n\n### sklearn 中的 SGD\n\n通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。\n\n需要注意的参数有：\n\n* loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等\n* penalty：正则项，用于防止过拟合(默认为 L2 正则项)\n* learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档\n* \n需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。\n\n需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。\n\n```python3\nfrom sklearn import linear_model\nimport pandas as pd\n\nCHUNK_SIZE = 1000\nn_iter = 10  # number of iteration over the whole dataset\nn_class = 7\n\nmodel = linear_model.SGDClassifier(loss = 'hinge', penalty ='l1',)\nfor _ in range(n_iter): \n    with open(source_file, 'rb') as R:\n        iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n        for n, data_chunk in enumerate(iterator):\n            model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class)))\n```\n\n\n---------\n\n### 流式数据中的特征工程\n\n#### feature scaling\n\n对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。\n\n由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为(\\\\(\\mu\\\\)为均值)：\n\\\\[\n\\sigma ^2= \\frac{1}{n} \\sum\\_x (x−μ)^2\n\\\\]\n\n只有知道均值才能求解，这意味着只有遍历一次求得\\\\(\\mu\\\\)后才能求\\\\(\\sigma^2\\\\)，这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得\\\\(\\mu和 \\sigma^2 \\\\)能够同时求出。假如当前有 n 个样本，当前的均值 \\\\( \\mu' \\\\) 可以简单求出，而当前的方差 \\\\( \\sigma'^2 \\\\) 可通以下公式求解\n\\\\[\n\\sigma′^2 = \\frac{1}{n} \\sum\\_x (x^2 − 2 x \\mu′+\\mu′^2 ) = \\frac{1}{n} \\sum\\_x (x^2) − \\frac{1}{n} (2n\\mu′^2 − n \\mu′^2)=\\frac{1}{n}\\sum\\_x (x^2) − \\mu′^2\n\\\\]\n\n通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。\n\n```python3\n# calculate the running mean,standard deviation, and range reporting the final result\nimport os, csv\nraw_source = 'bikesharing/hour.csv' # unshuffle\nshuffle_source = 'bikesharing/shuffled_hour.csv'\n\ndef running_statistic(source):\n    SEP=','\n    running_mean = list()\n    running_std = list()\n    with open(local_path+'/'+source, 'rb') as R:\n        iterator = csv.DictReader(R, delimiter=SEP)\n        x = 0.0\n        x_squared = 0.0\n        for n, row in enumerate(iterator):\n            temp = float(row['temp'])\n            if n == 0:\n                max_x, min_x = temp, temp\n            else:\n                max_x, min_x = max(temp, max_x),min(temp, min_x)\n            x += temp\n            x_squared += temp**2\n            running_mean.append(x / (n+1))\n            running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5)\n            # DATA PROCESSING placeholder\n            # MACHINE LEARNING placeholder\n            pass\n        print ('Total rows: %i' % (n+1))\n        print ('Feature \\'temp\\': mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f' \\\n               % (running_mean[-1], max_x, min_x, running_std[-1]))\n        return running_mean, running_std\nprint '===========raw data file==========='\nraw_running_mean, raw_running_std = running_statistic(raw_source)\nprint '===========shuffle data file==========='\nshuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source)\n```\n\n输出如下\n\n```text\n===========raw data file===========\nTotal rows: 17379\nFeature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193\n===========shuffle data file===========\nTotal rows: 17379\nFeature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193\n```\n\n两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化\n\n```python3\n# plot how such stats changed as data was streamed from disk\n# get an idea about how many instances are required before getting a stable mean and standard deviation estimate\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)):\n    plt.plot(mean,'r-', label='mean')\n    plt.plot(std,'b-', label='standard deviation')\n    plt.ylim(0.0,0.6)\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Value') \n    plt.legend(loc='lower right', numpoints= 1)\n    plt.show()\n# The difference in the two charts reminds us of the importance of randomizing the order of the observations.\n```\n\n得到的结果如下\n\n原始的文件\n\n![original](/posts_res/2018-04-19-bigscaledataml/1.png)\n\nshuffle 后的文件\n\n![after](/posts_res/2018-04-19-bigscaledataml/2.png)\n\n可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。\n\n\n#### hasing trick\n\n对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。\n\nhahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子\n\n```python3\nfrom sklearn.feature_extraction.text import HashingVectorizer\nh = HashingVectorizer(n_features=1000, binary=True, norm=None)\nsparse_vector = h.transform(['A simple toy example will make clear how it works.'])\nprint(sparse_vector)\n```\n\n输出如下\n\n```text\n(0, 61)\t1.0\n(0, 271)\t1.0\n(0, 287)\t1.0\n(0, 452)\t1.0\n(0, 462)\t1.0\n(0, 539)\t1.0\n(0, 605)\t1.0\n(0, 726)\t1.0\n(0, 918)\t1.0\n```\n\n这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。\n\n### 总结\n\n本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新，\n在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。\n\n","source":"_posts/2018-04-19-bigscaledataml.md","raw":"---\nlayout: post\ntitle: 使用sklearn大规模机器学习\ndate: 2018-04-03 12:10 +0800\ncategories: 机器学习\nmathjax: true\ncopyright: false\n---\n\n## <center> [转]使用sklearn大规模机器学习 </center>\n\n目录\n\n* 核外学习(out-of-core learning)\n* 磁盘上数据流式化\n* sklearn 中的 SGD\n* 流式数据中的特征工程\n* 总结\n\n转载自:[吴良超的学习笔记](http://wulc.me/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/)\n\n------\n\n### 核外学习(out-of-core learning)\n\n核外学习指的是机器的内存无法容纳训练的数据集，但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。\n\n#### sampling（采样）\n\n采样可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。\n\n解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。\n\n\n#### mini-batch learning（小批量学习/增量学习）\n\n这种方法不同于 sampling，利用了全部的数据，只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。\n\n通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息（如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负）。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。\n\n\n----------\n\n### 磁盘上数据流式化\n\n#### 文件读取\n\n这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现\n\n```python3\nwith open(source_file, 'r') as fp:\n    line = fp.readline()\n    while line:\n        # data processing\n        # training\n        line = fp.readline()\n```\n\n而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取，\n下面以这个数据文件为例说明：\n[Bike-Sharing-Dataset.zip](https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip)\n[备用地址:Bike-Sharing-Dataset.zip](/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip), \n\n```python3\nimport csv\nSEP = \",\"\nwith open(source_file, 'r') as fp:\n    iterator = csv.reader(fp, delimiter=SEP)\n    for n, row in enumerate(iterator):\n        if n == 0:\n            header = row\n        else:\n            # data processing\n            # training\n            pass\n    print('Total rows: %i' % (n+1))\n    print('Header: %s' % ', '.join(header))\n    print('Sample values: %s' % ', '.join(row))\n```\n\n输出为\n\n```text\nTotal rows: 17380\nHeader: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt\nSample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49\n```\n\n在上面的例子中，每个样本是就是一个 row（list类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为\n\n```python3\nimport csv\nSEP = \",\"\nwith open(source_file, 'r') as R:\n    iterator = csv.DictReader(R, delimiter=SEP)\n    for n, row in enumerate(iterator):\n        # data processing\n        # training\n        pass\n    print ('Total rows: %i' % (n+1))\n    print ('Sample values: %s' % row)\n```\n\n输出为\n\n```text\nTotal rows: 17379\nSample values: {'temp': '0.26', 'cnt': '49', 'yr': '1', 'windspeed': '0.1343', 'casual': '12', 'registered': '37', 'season': '1', 'weathersit': '1', 'dteday': '2012-12-31', 'hr': '23', 'weekday': '1', 'atemp': '0.2727', 'hum': '0.65', 'holiday': '0', 'instant': '17379', 'mnth': '12', 'workingday': '1'}\n```\n\n除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示:\n\n```python3\nimport pandas as pd\nCHUNK_SIZE = 1000\nwith open(source_file, 'rb') as R:\n    iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n    for n, data_chunk in enumerate(iterator):\n        print ('Size of uploaded chunk: %i instances, %i features' % (data_chunk.shape))\n        # data processing\n        # training\n        pass\n    print ('Sample values: \\n%s' % str(data_chunk.iloc[0]))\n```\n\n对应的输出为\n\n```text\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 1000 instances, 17 features\nSize of uploaded chunk: 379 instances, 17 features\nSample values: \ninstant            17001\ndteday        2012-12-16\nseason                 4\nyr                     1\nmnth                  12\nhr                     3\nholiday                0\nweekday                0\nworkingday             0\nweathersit             2\ntemp                0.34\natemp             0.3333\nhum                 0.87\nwindspeed          0.194\ncasual                 1\nregistered            37\ncnt                   38\nName: 17000, dtype: object\n```\n\n#### 数据库读取\n\n上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不仅能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。\n\n假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下(**未验证**)\n\n```python3\nimport sqlite3\nimport pandas as pd\n\nDB_NAME = 'bikesharing.sqlite'\nCHUNK_SIZE = 2500\n\nconn = sqlite3.connect(DB_NAME)\nconn.text_factory = str  # allows utf-8 data to be stored     \nsql = \"SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)\"\nDB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)\nfor j,data_chunk in enumerate(DB_stream):\n    print ('Chunk %i -' % (j+1)),\n    print ('Size of uploaded chunk: %i istances, %i features' % (data_chunk.shape))\n    # data processing\n    # training\n    pass\n```\n\n输出为\n\n```text\nChunk 1 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 2 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 3 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 4 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 5 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 6 - Size of uploaded chunk: 2500 istances, 18 features\nChunk 7 - Size of uploaded chunk: 2379 istances, 18 features\n```\n\n#### 样本的读取顺序\n\n上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。\n\n假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。\n\n这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。\n\n假如内存能够容纳这些数据，那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接，下面是这两种 shuffle 方法的实现代码。\n\n在**内存**中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下\n\n```python3\nimport zlib\nfrom random import shuffle\n\ndef ram_shuffle(filename_in, filename_out, header=True):\n    with open(filename_in, 'r') as f:\n        zlines = [zlib.compress(line, 9) for line in f]\n        if header:\n            first_row = zlines.pop(0)\n    shuffle(zlines)\n    with open(filename_out, 'w') as f:\n        if header:\n            f.write(zlib.decompress(first_row))\n        for zline in zlines:\n            f.write(zlib.decompress(zline))\n```\n\n基于**磁盘**的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下\n\n```python3\nfrom random import shuffle\nimport pandas as pd\nimport numpy as np\nimport os\n\ndef disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=','):\n    for i in range(iterations):\n        with open(filename_in, 'rb') as R:\n            iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n            for n, df in enumerate(iterator):\n                if n==0 and header:\n                    header_cols =SEP.join(df.columns)+'\\n'\n                df.iloc[np.random.permutation(len(df))].to_csv(str(n)+'_chunk.csv', index=False, header=False, sep=SEP)\n        ordering = list(range(0,n+1))\n        shuffle(ordering)\n        with open(filename_out, 'wb') as W:\n            if header:\n                W.write(header_cols)\n            for f in ordering:\n                with open(str(f)+'_chunk.csv', 'r') as R:\n                    for line in R:\n                        W.write(line)\n                os.remove(str(f)+'_chunk.csv')\n        filename_in = filename_out\n        CHUNK_SIZE = int(CHUNK_SIZE / 2)\n```\n\n\n-----------\n\n### sklearn 中的 SGD\n\n通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。\n\n需要注意的参数有：\n\n* loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等\n* penalty：正则项，用于防止过拟合(默认为 L2 正则项)\n* learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档\n* \n需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。\n\n需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。\n\n```python3\nfrom sklearn import linear_model\nimport pandas as pd\n\nCHUNK_SIZE = 1000\nn_iter = 10  # number of iteration over the whole dataset\nn_class = 7\n\nmodel = linear_model.SGDClassifier(loss = 'hinge', penalty ='l1',)\nfor _ in range(n_iter): \n    with open(source_file, 'rb') as R:\n        iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) \n        for n, data_chunk in enumerate(iterator):\n            model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class)))\n```\n\n\n---------\n\n### 流式数据中的特征工程\n\n#### feature scaling\n\n对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。\n\n由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为(\\\\(\\mu\\\\)为均值)：\n\\\\[\n\\sigma ^2= \\frac{1}{n} \\sum\\_x (x−μ)^2\n\\\\]\n\n只有知道均值才能求解，这意味着只有遍历一次求得\\\\(\\mu\\\\)后才能求\\\\(\\sigma^2\\\\)，这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得\\\\(\\mu和 \\sigma^2 \\\\)能够同时求出。假如当前有 n 个样本，当前的均值 \\\\( \\mu' \\\\) 可以简单求出，而当前的方差 \\\\( \\sigma'^2 \\\\) 可通以下公式求解\n\\\\[\n\\sigma′^2 = \\frac{1}{n} \\sum\\_x (x^2 − 2 x \\mu′+\\mu′^2 ) = \\frac{1}{n} \\sum\\_x (x^2) − \\frac{1}{n} (2n\\mu′^2 − n \\mu′^2)=\\frac{1}{n}\\sum\\_x (x^2) − \\mu′^2\n\\\\]\n\n通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。\n\n```python3\n# calculate the running mean,standard deviation, and range reporting the final result\nimport os, csv\nraw_source = 'bikesharing/hour.csv' # unshuffle\nshuffle_source = 'bikesharing/shuffled_hour.csv'\n\ndef running_statistic(source):\n    SEP=','\n    running_mean = list()\n    running_std = list()\n    with open(local_path+'/'+source, 'rb') as R:\n        iterator = csv.DictReader(R, delimiter=SEP)\n        x = 0.0\n        x_squared = 0.0\n        for n, row in enumerate(iterator):\n            temp = float(row['temp'])\n            if n == 0:\n                max_x, min_x = temp, temp\n            else:\n                max_x, min_x = max(temp, max_x),min(temp, min_x)\n            x += temp\n            x_squared += temp**2\n            running_mean.append(x / (n+1))\n            running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5)\n            # DATA PROCESSING placeholder\n            # MACHINE LEARNING placeholder\n            pass\n        print ('Total rows: %i' % (n+1))\n        print ('Feature \\'temp\\': mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f' \\\n               % (running_mean[-1], max_x, min_x, running_std[-1]))\n        return running_mean, running_std\nprint '===========raw data file==========='\nraw_running_mean, raw_running_std = running_statistic(raw_source)\nprint '===========shuffle data file==========='\nshuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source)\n```\n\n输出如下\n\n```text\n===========raw data file===========\nTotal rows: 17379\nFeature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193\n===========shuffle data file===========\nTotal rows: 17379\nFeature 'temp': mean=0.497, max=1.000, min=0.020,sd=0.193\n```\n\n两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化\n\n```python3\n# plot how such stats changed as data was streamed from disk\n# get an idea about how many instances are required before getting a stable mean and standard deviation estimate\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)):\n    plt.plot(mean,'r-', label='mean')\n    plt.plot(std,'b-', label='standard deviation')\n    plt.ylim(0.0,0.6)\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Value') \n    plt.legend(loc='lower right', numpoints= 1)\n    plt.show()\n# The difference in the two charts reminds us of the importance of randomizing the order of the observations.\n```\n\n得到的结果如下\n\n原始的文件\n\n![original](/posts_res/2018-04-19-bigscaledataml/1.png)\n\nshuffle 后的文件\n\n![after](/posts_res/2018-04-19-bigscaledataml/2.png)\n\n可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。\n\n\n#### hasing trick\n\n对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。\n\nhahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子\n\n```python3\nfrom sklearn.feature_extraction.text import HashingVectorizer\nh = HashingVectorizer(n_features=1000, binary=True, norm=None)\nsparse_vector = h.transform(['A simple toy example will make clear how it works.'])\nprint(sparse_vector)\n```\n\n输出如下\n\n```text\n(0, 61)\t1.0\n(0, 271)\t1.0\n(0, 287)\t1.0\n(0, 452)\t1.0\n(0, 462)\t1.0\n(0, 539)\t1.0\n(0, 605)\t1.0\n(0, 726)\t1.0\n(0, 918)\t1.0\n```\n\n这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。\n\n### 总结\n\n本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新，\n在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。\n\n","slug":"bigscaledataml","published":1,"updated":"2019-08-17T09:35:02.664Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvn001n2qwpkae2hhdb","content":"<h2 id=\"转-使用sklearn大规模机器学习\"><a href=\"#转-使用sklearn大规模机器学习\" class=\"headerlink\" title=\" [转]使用sklearn大规模机器学习 \"></a><center> [转]使用sklearn大规模机器学习 </center></h2><p>目录</p><ul>\n<li>核外学习(out-of-core learning)</li>\n<li>磁盘上数据流式化</li>\n<li>sklearn 中的 SGD</li>\n<li>流式数据中的特征工程</li>\n<li>总结</li>\n</ul><p>转载自:<a href=\"http://wulc.me/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/\" target=\"_blank\" rel=\"noopener\">吴良超的学习笔记</a></p><hr><h3 id=\"核外学习-out-of-core-learning\"><a href=\"#核外学习-out-of-core-learning\" class=\"headerlink\" title=\"核外学习(out-of-core learning)\"></a>核外学习(out-of-core learning)</h3><p>核外学习指的是机器的内存无法容纳训练的数据集，但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。</p><h4 id=\"sampling（采样）\"><a href=\"#sampling（采样）\" class=\"headerlink\" title=\"sampling（采样）\"></a>sampling（采样）</h4><p>采样可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。</p><a id=\"more\"></a>\n\n\n\n\n\n<p>解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。</p>\n<h4 id=\"mini-batch-learning（小批量学习-增量学习）\"><a href=\"#mini-batch-learning（小批量学习-增量学习）\" class=\"headerlink\" title=\"mini-batch learning（小批量学习/增量学习）\"></a>mini-batch learning（小批量学习/增量学习）</h4><p>这种方法不同于 sampling，利用了全部的数据，只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。</p>\n<p>通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息（如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负）。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。</p>\n<hr>\n<h3 id=\"磁盘上数据流式化\"><a href=\"#磁盘上数据流式化\" class=\"headerlink\" title=\"磁盘上数据流式化\"></a>磁盘上数据流式化</h3><h4 id=\"文件读取\"><a href=\"#文件读取\" class=\"headerlink\" title=\"文件读取\"></a>文件读取</h4><p>这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">with open(source_file, &apos;r&apos;) as fp:</span><br><span class=\"line\">    line = fp.readline()</span><br><span class=\"line\">    while line:</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        line = fp.readline()</span><br></pre></td></tr></table></figure>\n<p>而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取，\n下面以这个数据文件为例说明：\n<a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\" target=\"_blank\" rel=\"noopener\">Bike-Sharing-Dataset.zip</a>\n<a href=\"/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip\">备用地址:Bike-Sharing-Dataset.zip</a>, </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">SEP = &quot;,&quot;</span><br><span class=\"line\">with open(source_file, &apos;r&apos;) as fp:</span><br><span class=\"line\">    iterator = csv.reader(fp, delimiter=SEP)</span><br><span class=\"line\">    for n, row in enumerate(iterator):</span><br><span class=\"line\">        if n == 0:</span><br><span class=\"line\">            header = row</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            # data processing</span><br><span class=\"line\">            # training</span><br><span class=\"line\">            pass</span><br><span class=\"line\">    print(&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">    print(&apos;Header: %s&apos; % &apos;, &apos;.join(header))</span><br><span class=\"line\">    print(&apos;Sample values: %s&apos; % &apos;, &apos;.join(row))</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Total rows: 17380</span><br><span class=\"line\">Header: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt</span><br><span class=\"line\">Sample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49</span><br></pre></td></tr></table></figure>\n<p>在上面的例子中，每个样本是就是一个 row（list类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">SEP = &quot;,&quot;</span><br><span class=\"line\">with open(source_file, &apos;r&apos;) as R:</span><br><span class=\"line\">    iterator = csv.DictReader(R, delimiter=SEP)</span><br><span class=\"line\">    for n, row in enumerate(iterator):</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        pass</span><br><span class=\"line\">    print (&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">    print (&apos;Sample values: %s&apos; % row)</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Sample values: &#123;&apos;temp&apos;: &apos;0.26&apos;, &apos;cnt&apos;: &apos;49&apos;, &apos;yr&apos;: &apos;1&apos;, &apos;windspeed&apos;: &apos;0.1343&apos;, &apos;casual&apos;: &apos;12&apos;, &apos;registered&apos;: &apos;37&apos;, &apos;season&apos;: &apos;1&apos;, &apos;weathersit&apos;: &apos;1&apos;, &apos;dteday&apos;: &apos;2012-12-31&apos;, &apos;hr&apos;: &apos;23&apos;, &apos;weekday&apos;: &apos;1&apos;, &apos;atemp&apos;: &apos;0.2727&apos;, &apos;hum&apos;: &apos;0.65&apos;, &apos;holiday&apos;: &apos;0&apos;, &apos;instant&apos;: &apos;17379&apos;, &apos;mnth&apos;: &apos;12&apos;, &apos;workingday&apos;: &apos;1&apos;&#125;</span><br></pre></td></tr></table></figure>\n<p>除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\">CHUNK_SIZE = 1000</span><br><span class=\"line\">with open(source_file, &apos;rb&apos;) as R:</span><br><span class=\"line\">    iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">    for n, data_chunk in enumerate(iterator):</span><br><span class=\"line\">        print (&apos;Size of uploaded chunk: %i instances, %i features&apos; % (data_chunk.shape))</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        pass</span><br><span class=\"line\">    print (&apos;Sample values: \\n%s&apos; % str(data_chunk.iloc[0]))</span><br></pre></td></tr></table></figure>\n<p>对应的输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 379 instances, 17 features</span><br><span class=\"line\">Sample values: </span><br><span class=\"line\">instant            17001</span><br><span class=\"line\">dteday        2012-12-16</span><br><span class=\"line\">season                 4</span><br><span class=\"line\">yr                     1</span><br><span class=\"line\">mnth                  12</span><br><span class=\"line\">hr                     3</span><br><span class=\"line\">holiday                0</span><br><span class=\"line\">weekday                0</span><br><span class=\"line\">workingday             0</span><br><span class=\"line\">weathersit             2</span><br><span class=\"line\">temp                0.34</span><br><span class=\"line\">atemp             0.3333</span><br><span class=\"line\">hum                 0.87</span><br><span class=\"line\">windspeed          0.194</span><br><span class=\"line\">casual                 1</span><br><span class=\"line\">registered            37</span><br><span class=\"line\">cnt                   38</span><br><span class=\"line\">Name: 17000, dtype: object</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据库读取\"><a href=\"#数据库读取\" class=\"headerlink\" title=\"数据库读取\"></a>数据库读取</h4><p>上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不仅能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。</p>\n<p>假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下(<strong>未验证</strong>)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import sqlite3</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\"></span><br><span class=\"line\">DB_NAME = &apos;bikesharing.sqlite&apos;</span><br><span class=\"line\">CHUNK_SIZE = 2500</span><br><span class=\"line\"></span><br><span class=\"line\">conn = sqlite3.connect(DB_NAME)</span><br><span class=\"line\">conn.text_factory = str  # allows utf-8 data to be stored     </span><br><span class=\"line\">sql = &quot;SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)&quot;</span><br><span class=\"line\">DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)</span><br><span class=\"line\">for j,data_chunk in enumerate(DB_stream):</span><br><span class=\"line\">    print (&apos;Chunk %i -&apos; % (j+1)),</span><br><span class=\"line\">    print (&apos;Size of uploaded chunk: %i istances, %i features&apos; % (data_chunk.shape))</span><br><span class=\"line\">    # data processing</span><br><span class=\"line\">    # training</span><br><span class=\"line\">    pass</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Chunk 1 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 2 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 3 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 4 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 5 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 6 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 7 - Size of uploaded chunk: 2379 istances, 18 features</span><br></pre></td></tr></table></figure>\n<h4 id=\"样本的读取顺序\"><a href=\"#样本的读取顺序\" class=\"headerlink\" title=\"样本的读取顺序\"></a>样本的读取顺序</h4><p>上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。</p>\n<p>假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。</p>\n<p>这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。</p>\n<p>假如内存能够容纳这些数据，那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接，下面是这两种 shuffle 方法的实现代码。</p>\n<p>在<strong>内存</strong>中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import zlib</span><br><span class=\"line\">from random import shuffle</span><br><span class=\"line\"></span><br><span class=\"line\">def ram_shuffle(filename_in, filename_out, header=True):</span><br><span class=\"line\">    with open(filename_in, &apos;r&apos;) as f:</span><br><span class=\"line\">        zlines = [zlib.compress(line, 9) for line in f]</span><br><span class=\"line\">        if header:</span><br><span class=\"line\">            first_row = zlines.pop(0)</span><br><span class=\"line\">    shuffle(zlines)</span><br><span class=\"line\">    with open(filename_out, &apos;w&apos;) as f:</span><br><span class=\"line\">        if header:</span><br><span class=\"line\">            f.write(zlib.decompress(first_row))</span><br><span class=\"line\">        for zline in zlines:</span><br><span class=\"line\">            f.write(zlib.decompress(zline))</span><br></pre></td></tr></table></figure>\n<p>基于<strong>磁盘</strong>的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from random import shuffle</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">def disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=&apos;,&apos;):</span><br><span class=\"line\">    for i in range(iterations):</span><br><span class=\"line\">        with open(filename_in, &apos;rb&apos;) as R:</span><br><span class=\"line\">            iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">            for n, df in enumerate(iterator):</span><br><span class=\"line\">                if n==0 and header:</span><br><span class=\"line\">                    header_cols =SEP.join(df.columns)+&apos;\\n&apos;</span><br><span class=\"line\">                df.iloc[np.random.permutation(len(df))].to_csv(str(n)+&apos;_chunk.csv&apos;, index=False, header=False, sep=SEP)</span><br><span class=\"line\">        ordering = list(range(0,n+1))</span><br><span class=\"line\">        shuffle(ordering)</span><br><span class=\"line\">        with open(filename_out, &apos;wb&apos;) as W:</span><br><span class=\"line\">            if header:</span><br><span class=\"line\">                W.write(header_cols)</span><br><span class=\"line\">            for f in ordering:</span><br><span class=\"line\">                with open(str(f)+&apos;_chunk.csv&apos;, &apos;r&apos;) as R:</span><br><span class=\"line\">                    for line in R:</span><br><span class=\"line\">                        W.write(line)</span><br><span class=\"line\">                os.remove(str(f)+&apos;_chunk.csv&apos;)</span><br><span class=\"line\">        filename_in = filename_out</span><br><span class=\"line\">        CHUNK_SIZE = int(CHUNK_SIZE / 2)</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"sklearn-中的-SGD\"><a href=\"#sklearn-中的-SGD\" class=\"headerlink\" title=\"sklearn 中的 SGD\"></a>sklearn 中的 SGD</h3><p>通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。</p>\n<p>需要注意的参数有：</p>\n<ul>\n<li>loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等</li>\n<li>penalty：正则项，用于防止过拟合(默认为 L2 正则项)</li>\n<li>learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档</li>\n<li>需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。</li>\n</ul>\n<p>需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn import linear_model</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\"></span><br><span class=\"line\">CHUNK_SIZE = 1000</span><br><span class=\"line\">n_iter = 10  # number of iteration over the whole dataset</span><br><span class=\"line\">n_class = 7</span><br><span class=\"line\"></span><br><span class=\"line\">model = linear_model.SGDClassifier(loss = &apos;hinge&apos;, penalty =&apos;l1&apos;,)</span><br><span class=\"line\">for _ in range(n_iter): </span><br><span class=\"line\">    with open(source_file, &apos;rb&apos;) as R:</span><br><span class=\"line\">        iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">        for n, data_chunk in enumerate(iterator):</span><br><span class=\"line\">            model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class)))</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"流式数据中的特征工程\"><a href=\"#流式数据中的特征工程\" class=\"headerlink\" title=\"流式数据中的特征工程\"></a>流式数据中的特征工程</h3><h4 id=\"feature-scaling\"><a href=\"#feature-scaling\" class=\"headerlink\" title=\"feature scaling\"></a>feature scaling</h4><p>对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。</p>\n<p>由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为(\\(\\mu\\)为均值)：\n\\[\n\\sigma ^2= \\frac{1}{n} \\sum_x (x−μ)^2\n\\]</p>\n<p>只有知道均值才能求解，这意味着只有遍历一次求得\\(\\mu\\)后才能求\\(\\sigma^2\\)，这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得\\(\\mu和 \\sigma^2 \\)能够同时求出。假如当前有 n 个样本，当前的均值 \\( \\mu’ \\) 可以简单求出，而当前的方差 \\( \\sigma’^2 \\) 可通以下公式求解\n\\[\n\\sigma′^2 = \\frac{1}{n} \\sum_x (x^2 − 2 x \\mu′+\\mu′^2 ) = \\frac{1}{n} \\sum_x (x^2) − \\frac{1}{n} (2n\\mu′^2 − n \\mu′^2)=\\frac{1}{n}\\sum_x (x^2) − \\mu′^2\n\\]</p>\n<p>通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># calculate the running mean,standard deviation, and range reporting the final result</span><br><span class=\"line\">import os, csv</span><br><span class=\"line\">raw_source = &apos;bikesharing/hour.csv&apos; # unshuffle</span><br><span class=\"line\">shuffle_source = &apos;bikesharing/shuffled_hour.csv&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">def running_statistic(source):</span><br><span class=\"line\">    SEP=&apos;,&apos;</span><br><span class=\"line\">    running_mean = list()</span><br><span class=\"line\">    running_std = list()</span><br><span class=\"line\">    with open(local_path+&apos;/&apos;+source, &apos;rb&apos;) as R:</span><br><span class=\"line\">        iterator = csv.DictReader(R, delimiter=SEP)</span><br><span class=\"line\">        x = 0.0</span><br><span class=\"line\">        x_squared = 0.0</span><br><span class=\"line\">        for n, row in enumerate(iterator):</span><br><span class=\"line\">            temp = float(row[&apos;temp&apos;])</span><br><span class=\"line\">            if n == 0:</span><br><span class=\"line\">                max_x, min_x = temp, temp</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                max_x, min_x = max(temp, max_x),min(temp, min_x)</span><br><span class=\"line\">            x += temp</span><br><span class=\"line\">            x_squared += temp**2</span><br><span class=\"line\">            running_mean.append(x / (n+1))</span><br><span class=\"line\">            running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5)</span><br><span class=\"line\">            # DATA PROCESSING placeholder</span><br><span class=\"line\">            # MACHINE LEARNING placeholder</span><br><span class=\"line\">            pass</span><br><span class=\"line\">        print (&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">        print (&apos;Feature \\&apos;temp\\&apos;: mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f&apos; \\</span><br><span class=\"line\">               % (running_mean[-1], max_x, min_x, running_std[-1]))</span><br><span class=\"line\">        return running_mean, running_std</span><br><span class=\"line\">print &apos;===========raw data file===========&apos;</span><br><span class=\"line\">raw_running_mean, raw_running_std = running_statistic(raw_source)</span><br><span class=\"line\">print &apos;===========shuffle data file===========&apos;</span><br><span class=\"line\">shuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source)</span><br></pre></td></tr></table></figure>\n<p>输出如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===========raw data file===========</span><br><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193</span><br><span class=\"line\">===========shuffle data file===========</span><br><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193</span><br></pre></td></tr></table></figure>\n<p>两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># plot how such stats changed as data was streamed from disk</span><br><span class=\"line\"># get an idea about how many instances are required before getting a stable mean and standard deviation estimate</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">for mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)):</span><br><span class=\"line\">    plt.plot(mean,&apos;r-&apos;, label=&apos;mean&apos;)</span><br><span class=\"line\">    plt.plot(std,&apos;b-&apos;, label=&apos;standard deviation&apos;)</span><br><span class=\"line\">    plt.ylim(0.0,0.6)</span><br><span class=\"line\">    plt.xlabel(&apos;Number of training examples&apos;)</span><br><span class=\"line\">    plt.ylabel(&apos;Value&apos;) </span><br><span class=\"line\">    plt.legend(loc=&apos;lower right&apos;, numpoints= 1)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"># The difference in the two charts reminds us of the importance of randomizing the order of the observations.</span><br></pre></td></tr></table></figure>\n<p>得到的结果如下</p>\n<p>原始的文件</p>\n<p><img src=\"/posts_res/2018-04-19-bigscaledataml/1.png\" alt=\"original\"></p>\n<p>shuffle 后的文件</p>\n<p><img src=\"/posts_res/2018-04-19-bigscaledataml/2.png\" alt=\"after\"></p>\n<p>可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。</p>\n<h4 id=\"hasing-trick\"><a href=\"#hasing-trick\" class=\"headerlink\" title=\"hasing trick\"></a>hasing trick</h4><p>对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。</p>\n<p>hahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.feature_extraction.text import HashingVectorizer</span><br><span class=\"line\">h = HashingVectorizer(n_features=1000, binary=True, norm=None)</span><br><span class=\"line\">sparse_vector = h.transform([&apos;A simple toy example will make clear how it works.&apos;])</span><br><span class=\"line\">print(sparse_vector)</span><br></pre></td></tr></table></figure>\n<p>输出如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(0, 61)\t1.0</span><br><span class=\"line\">(0, 271)\t1.0</span><br><span class=\"line\">(0, 287)\t1.0</span><br><span class=\"line\">(0, 452)\t1.0</span><br><span class=\"line\">(0, 462)\t1.0</span><br><span class=\"line\">(0, 539)\t1.0</span><br><span class=\"line\">(0, 605)\t1.0</span><br><span class=\"line\">(0, 726)\t1.0</span><br><span class=\"line\">(0, 918)\t1.0</span><br></pre></td></tr></table></figure>\n<p>这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新，\n在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"转-使用sklearn大规模机器学习\"><a href=\"#转-使用sklearn大规模机器学习\" class=\"headerlink\" title=\" [转]使用sklearn大规模机器学习 \"></a><center> [转]使用sklearn大规模机器学习 </center></h2><p>目录</p><ul>\n<li>核外学习(out-of-core learning)</li>\n<li>磁盘上数据流式化</li>\n<li>sklearn 中的 SGD</li>\n<li>流式数据中的特征工程</li>\n<li>总结</li>\n</ul><p>转载自:<a href=\"http://wulc.me/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/\" target=\"_blank\" rel=\"noopener\">吴良超的学习笔记</a></p><hr><h3 id=\"核外学习-out-of-core-learning\"><a href=\"#核外学习-out-of-core-learning\" class=\"headerlink\" title=\"核外学习(out-of-core learning)\"></a>核外学习(out-of-core learning)</h3><p>核外学习指的是机器的内存无法容纳训练的数据集，但是硬盘可容纳这些数据，这种情况在数据集较大的时候比较常见，一般有两种解决方法：sampling 与 mini-batch learning。</p><h4 id=\"sampling（采样）\"><a href=\"#sampling（采样）\" class=\"headerlink\" title=\"sampling（采样）\"></a>sampling（采样）</h4><p>采样可以针对样本数目或样本特征，能够减少样本的数量或者feature的数目，两者均能减小整个数据集所占内存，但是采样无可避免地会丢失掉原来数据集中的一些信息（当数据没有冗余的时候），这会导致 variance inflation 问题，也就是进行若干次采样，每次训练得出的模型之间差异都比较大。用 bias-variance 来解释就是出现了high variance，原因是每次采样得到的数据中都有随机噪声，而模型拟合了这些没有规律的噪声，从而导致了每次得到的模型都不一样。</p>","more":"\n\n\n\n\n\n<p>解决采样带来的 high-variance 问题，可以通过训练多个采样模型，然后将其进行集成，采用这种思路典型的方法有 bagging。</p>\n<h4 id=\"mini-batch-learning（小批量学习-增量学习）\"><a href=\"#mini-batch-learning（小批量学习-增量学习）\" class=\"headerlink\" title=\"mini-batch learning（小批量学习/增量学习）\"></a>mini-batch learning（小批量学习/增量学习）</h4><p>这种方法不同于 sampling，利用了全部的数据，只是每次只用一部分样本（可以是一个样本，也可以是多个样本）来训练模型，通过增加迭代的次数可以近似用全部数据集训练的效果，这种方法需要训练的算法的支持，SGD 恰好就能够提供这种模式的训练，因此 SGD 是这种模式训练的核心。下面也主要针对这种方法进行讲述。</p>\n<p>通过 SGD 进行训练时，需要流式（streaming）读取训练样本，同时注意的是要将样本的顺序随机打乱，以消除样本顺序带来的信息（如先用正样本训练，再用负样本训练，模型会偏向于将样本预测为负）。下面主要讲述如何将磁盘上的数据流式化并送入到模型中进行训练。</p>\n<hr>\n<h3 id=\"磁盘上数据流式化\"><a href=\"#磁盘上数据流式化\" class=\"headerlink\" title=\"磁盘上数据流式化\"></a>磁盘上数据流式化</h3><h4 id=\"文件读取\"><a href=\"#文件读取\" class=\"headerlink\" title=\"文件读取\"></a>文件读取</h4><p>这里读取的数据的格式是每行存储一个样本。最简单的方法就是通过 python 读取文件的 readline 方法实现</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">with open(source_file, &apos;r&apos;) as fp:</span><br><span class=\"line\">    line = fp.readline()</span><br><span class=\"line\">    while line:</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        line = fp.readline()</span><br></pre></td></tr></table></figure>\n<p>而往往训练文件都是 csv 格式的，此时需要丢弃第一行，同时可通过 csv 模块进行读取，\n下面以这个数据文件为例说明：\n<a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\" target=\"_blank\" rel=\"noopener\">Bike-Sharing-Dataset.zip</a>\n<a href=\"/posts_res/2018-04-19-bigscaledataml/Bike-Sharing-Dataset.zip\">备用地址:Bike-Sharing-Dataset.zip</a>, </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">SEP = &quot;,&quot;</span><br><span class=\"line\">with open(source_file, &apos;r&apos;) as fp:</span><br><span class=\"line\">    iterator = csv.reader(fp, delimiter=SEP)</span><br><span class=\"line\">    for n, row in enumerate(iterator):</span><br><span class=\"line\">        if n == 0:</span><br><span class=\"line\">            header = row</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            # data processing</span><br><span class=\"line\">            # training</span><br><span class=\"line\">            pass</span><br><span class=\"line\">    print(&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">    print(&apos;Header: %s&apos; % &apos;, &apos;.join(header))</span><br><span class=\"line\">    print(&apos;Sample values: %s&apos; % &apos;, &apos;.join(row))</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Total rows: 17380</span><br><span class=\"line\">Header: instant, dteday, season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, casual, registered, cnt</span><br><span class=\"line\">Sample values: 17379, 2012-12-31, 1, 1, 12, 23, 0, 1, 1, 1, 0.26, 0.2727, 0.65, 0.1343, 12, 37, 49</span><br></pre></td></tr></table></figure>\n<p>在上面的例子中，每个样本是就是一个 row（list类型），样本的 feature 只能通过 row[index] 方式获取，假如要通过 header 中的名称获取， 可以修改上面获取 iterator 的代码，用 csv.DictReader(f, delimiter = SEP) 来获取 iterator，此时得到的 row 会是一个 dictionary，key 为 header 中的列名，value 为对应的值; 对应的代码为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import csv</span><br><span class=\"line\">SEP = &quot;,&quot;</span><br><span class=\"line\">with open(source_file, &apos;r&apos;) as R:</span><br><span class=\"line\">    iterator = csv.DictReader(R, delimiter=SEP)</span><br><span class=\"line\">    for n, row in enumerate(iterator):</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        pass</span><br><span class=\"line\">    print (&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">    print (&apos;Sample values: %s&apos; % row)</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Sample values: &#123;&apos;temp&apos;: &apos;0.26&apos;, &apos;cnt&apos;: &apos;49&apos;, &apos;yr&apos;: &apos;1&apos;, &apos;windspeed&apos;: &apos;0.1343&apos;, &apos;casual&apos;: &apos;12&apos;, &apos;registered&apos;: &apos;37&apos;, &apos;season&apos;: &apos;1&apos;, &apos;weathersit&apos;: &apos;1&apos;, &apos;dteday&apos;: &apos;2012-12-31&apos;, &apos;hr&apos;: &apos;23&apos;, &apos;weekday&apos;: &apos;1&apos;, &apos;atemp&apos;: &apos;0.2727&apos;, &apos;hum&apos;: &apos;0.65&apos;, &apos;holiday&apos;: &apos;0&apos;, &apos;instant&apos;: &apos;17379&apos;, &apos;mnth&apos;: &apos;12&apos;, &apos;workingday&apos;: &apos;1&apos;&#125;</span><br></pre></td></tr></table></figure>\n<p>除了通过 csv 模块进行读取，还可以通过 pandas 模块进行读取，pandas 模块可以说是处理 csv 文件的神器，csv 每次只能读取一条数据，而 pandas 可以指定每次读取的数据的数目，如下所示:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\">CHUNK_SIZE = 1000</span><br><span class=\"line\">with open(source_file, &apos;rb&apos;) as R:</span><br><span class=\"line\">    iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">    for n, data_chunk in enumerate(iterator):</span><br><span class=\"line\">        print (&apos;Size of uploaded chunk: %i instances, %i features&apos; % (data_chunk.shape))</span><br><span class=\"line\">        # data processing</span><br><span class=\"line\">        # training</span><br><span class=\"line\">        pass</span><br><span class=\"line\">    print (&apos;Sample values: \\n%s&apos; % str(data_chunk.iloc[0]))</span><br></pre></td></tr></table></figure>\n<p>对应的输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 1000 instances, 17 features</span><br><span class=\"line\">Size of uploaded chunk: 379 instances, 17 features</span><br><span class=\"line\">Sample values: </span><br><span class=\"line\">instant            17001</span><br><span class=\"line\">dteday        2012-12-16</span><br><span class=\"line\">season                 4</span><br><span class=\"line\">yr                     1</span><br><span class=\"line\">mnth                  12</span><br><span class=\"line\">hr                     3</span><br><span class=\"line\">holiday                0</span><br><span class=\"line\">weekday                0</span><br><span class=\"line\">workingday             0</span><br><span class=\"line\">weathersit             2</span><br><span class=\"line\">temp                0.34</span><br><span class=\"line\">atemp             0.3333</span><br><span class=\"line\">hum                 0.87</span><br><span class=\"line\">windspeed          0.194</span><br><span class=\"line\">casual                 1</span><br><span class=\"line\">registered            37</span><br><span class=\"line\">cnt                   38</span><br><span class=\"line\">Name: 17000, dtype: object</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据库读取\"><a href=\"#数据库读取\" class=\"headerlink\" title=\"数据库读取\"></a>数据库读取</h4><p>上面的是直接从文件中读取的数据，但是数据也可能存在数据库中，因为通过数据库不仅能够有效进行增删查改等操作，而且通过 Database Normalization 能够在不丢失信息的基础上减少数据冗余性。</p>\n<p>假设上面的数据已经存储在 SQLite 数据库中，则流式读取的方法如下(<strong>未验证</strong>)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import sqlite3</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\"></span><br><span class=\"line\">DB_NAME = &apos;bikesharing.sqlite&apos;</span><br><span class=\"line\">CHUNK_SIZE = 2500</span><br><span class=\"line\"></span><br><span class=\"line\">conn = sqlite3.connect(DB_NAME)</span><br><span class=\"line\">conn.text_factory = str  # allows utf-8 data to be stored     </span><br><span class=\"line\">sql = &quot;SELECT H.*, D.cnt AS day_cnt FROM hour AS H INNER JOIN day as D ON (H.dteday = D.dteday)&quot;</span><br><span class=\"line\">DB_stream = pd.io.sql.read_sql(sql, conn, chunksize=CHUNK_SIZE)</span><br><span class=\"line\">for j,data_chunk in enumerate(DB_stream):</span><br><span class=\"line\">    print (&apos;Chunk %i -&apos; % (j+1)),</span><br><span class=\"line\">    print (&apos;Size of uploaded chunk: %i istances, %i features&apos; % (data_chunk.shape))</span><br><span class=\"line\">    # data processing</span><br><span class=\"line\">    # training</span><br><span class=\"line\">    pass</span><br></pre></td></tr></table></figure>\n<p>输出为</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Chunk 1 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 2 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 3 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 4 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 5 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 6 - Size of uploaded chunk: 2500 istances, 18 features</span><br><span class=\"line\">Chunk 7 - Size of uploaded chunk: 2379 istances, 18 features</span><br></pre></td></tr></table></figure>\n<h4 id=\"样本的读取顺序\"><a href=\"#样本的读取顺序\" class=\"headerlink\" title=\"样本的读取顺序\"></a>样本的读取顺序</h4><p>上面简单提到了通过 SGD 训练模型的时候，需要注意样本的顺序必须要是随机打乱的。</p>\n<p>假如给定一批样本，然后用整批的样本来更新，那么就不存在样本的读取顺序问题；但是由于像 SGD 这种 online learning 的训练模式，越是后面才读取的样本，模型一般会拟合得更好，因为这是模型最近看到了这些样本且针对这些样本进行了调整。</p>\n<p>这样的特性有其好处，如处理时间序列的数据时，由于对最近时间的数据拟合得更好，因此不会受到时间太久远的数据的影响，但是在更多的情况下，这种由样本顺序带来的是有弊无益的，如上面提到的先用全部的正样本训练，再用全部的负样本训练。因此有必要对数据先进行 shuffle ，然后再通过 SGD 来进行训练。</p>\n<p>假如内存能够容纳这些数据，那么所有的数据可以在内存中进行一次 shuffle；假如无法容纳，则可以将整个大的数据文件分为若干个小的文件，分别进行 shuffle ，然后再拼接起来，拼接时也不按照原来的顺序，而是进行 shuffle 后再拼接，下面是这两种 shuffle 方法的实现代码。</p>\n<p>在<strong>内存</strong>中进行 shuffle 之前可以通过 zlib 对样本先进行压缩，从而让内存可以容纳更多的样本，实现代码如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import zlib</span><br><span class=\"line\">from random import shuffle</span><br><span class=\"line\"></span><br><span class=\"line\">def ram_shuffle(filename_in, filename_out, header=True):</span><br><span class=\"line\">    with open(filename_in, &apos;r&apos;) as f:</span><br><span class=\"line\">        zlines = [zlib.compress(line, 9) for line in f]</span><br><span class=\"line\">        if header:</span><br><span class=\"line\">            first_row = zlines.pop(0)</span><br><span class=\"line\">    shuffle(zlines)</span><br><span class=\"line\">    with open(filename_out, &apos;w&apos;) as f:</span><br><span class=\"line\">        if header:</span><br><span class=\"line\">            f.write(zlib.decompress(first_row))</span><br><span class=\"line\">        for zline in zlines:</span><br><span class=\"line\">            f.write(zlib.decompress(zline))</span><br></pre></td></tr></table></figure>\n<p>基于<strong>磁盘</strong>的 shuffle 方法首先将整个文件划分为若干个小文件，然后再进行 shuffle， 为了能够实现整个数据集更彻底的 shuffle ，可以将上面的过程重复几遍，同时每次都改变划分的文件的大小，实现的代码如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from random import shuffle</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">def disk_shuffle(filename_in, filename_out, header=True, iterations = 3, CHUNK_SIZE = 2500, SEP=&apos;,&apos;):</span><br><span class=\"line\">    for i in range(iterations):</span><br><span class=\"line\">        with open(filename_in, &apos;rb&apos;) as R:</span><br><span class=\"line\">            iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">            for n, df in enumerate(iterator):</span><br><span class=\"line\">                if n==0 and header:</span><br><span class=\"line\">                    header_cols =SEP.join(df.columns)+&apos;\\n&apos;</span><br><span class=\"line\">                df.iloc[np.random.permutation(len(df))].to_csv(str(n)+&apos;_chunk.csv&apos;, index=False, header=False, sep=SEP)</span><br><span class=\"line\">        ordering = list(range(0,n+1))</span><br><span class=\"line\">        shuffle(ordering)</span><br><span class=\"line\">        with open(filename_out, &apos;wb&apos;) as W:</span><br><span class=\"line\">            if header:</span><br><span class=\"line\">                W.write(header_cols)</span><br><span class=\"line\">            for f in ordering:</span><br><span class=\"line\">                with open(str(f)+&apos;_chunk.csv&apos;, &apos;r&apos;) as R:</span><br><span class=\"line\">                    for line in R:</span><br><span class=\"line\">                        W.write(line)</span><br><span class=\"line\">                os.remove(str(f)+&apos;_chunk.csv&apos;)</span><br><span class=\"line\">        filename_in = filename_out</span><br><span class=\"line\">        CHUNK_SIZE = int(CHUNK_SIZE / 2)</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"sklearn-中的-SGD\"><a href=\"#sklearn-中的-SGD\" class=\"headerlink\" title=\"sklearn 中的 SGD\"></a>sklearn 中的 SGD</h3><p>通过前面的步骤可以将数据以流式输入，下面接着就是要通过 SGD 进行训练，在 sklearn 中， sklearn.linear_model.SGDClassifier 和 sklearn.linear_model.SGDRegressor 均是通过 SGD 实现，只是一个用于分类，一个用于回归。下面以 sklearn.linear_model.SGDClassifier 为例进行简单说明，更详细的内容可参考其官方文档。这里仅对其几个参数和方法进行简单的讲解。</p>\n<p>需要注意的参数有：</p>\n<ul>\n<li>loss : 表示具体的分类器，可选的值为 hinge、log、modified_huber、squared_hinge、perceptron；如 hinge 表示 SVM 分类器，log 表示logistics regression等</li>\n<li>penalty：正则项，用于防止过拟合(默认为 L2 正则项)</li>\n<li>learning_rate: 表示选择哪种学习速率方案，共有三种：constant、optimal、invscaling，各种详细含义可参考官方文档</li>\n<li>需要注意的方法主要就是 partial_fit(X, y, classes), X 和 y 是每次流式输入的数据，而 classes 则是具体的分类数目, 若 classes 数目大于2，则会根据 one-vs-rest 规则训练多个分类器。</li>\n</ul>\n<p>需要注意的是 partial_fit 只会对数据遍历一次，需要自己显式指定遍历的次数，如下是使用 sklearn 中的 SGDClassfier 的一个简单例子。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn import linear_model</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\"></span><br><span class=\"line\">CHUNK_SIZE = 1000</span><br><span class=\"line\">n_iter = 10  # number of iteration over the whole dataset</span><br><span class=\"line\">n_class = 7</span><br><span class=\"line\"></span><br><span class=\"line\">model = linear_model.SGDClassifier(loss = &apos;hinge&apos;, penalty =&apos;l1&apos;,)</span><br><span class=\"line\">for _ in range(n_iter): </span><br><span class=\"line\">    with open(source_file, &apos;rb&apos;) as R:</span><br><span class=\"line\">        iterator = pd.read_csv(R, chunksize=CHUNK_SIZE) </span><br><span class=\"line\">        for n, data_chunk in enumerate(iterator):</span><br><span class=\"line\">            model.partial_fit(data_chunk.x, data_chunk.y, classes = np.array(range(0, n_class)))</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"流式数据中的特征工程\"><a href=\"#流式数据中的特征工程\" class=\"headerlink\" title=\"流式数据中的特征工程\"></a>流式数据中的特征工程</h3><h4 id=\"feature-scaling\"><a href=\"#feature-scaling\" class=\"headerlink\" title=\"feature scaling\"></a>feature scaling</h4><p>对于 SGD 算法，特征的 scaling 会影响其优化过程，也就是只有将特征标准化（均值为0，方差为1）或归一化（处于 [0,1] 内）才能加快算法收敛的速度，但是由于数据不能一次读入内存，如果需要标准化或归一化，需要对数据遍历 2 次，第一次遍历是为了求特征的均值和方差（标准化需要）或最大最小值（归一化需要），第二次遍历便可以用上面的均值、方差、最大值，最小值等值进行标准化。</p>\n<p>由于数据是流式输入的，求解均值、最大值、最小值都没有什么问题，但是求解方差的公式为(\\(\\mu\\)为均值)：\n\\[\n\\sigma ^2= \\frac{1}{n} \\sum_x (x−μ)^2\n\\]</p>\n<p>只有知道均值才能求解，这意味着只有遍历一次求得\\(\\mu\\)后才能求\\(\\sigma^2\\)，这无疑会增加求解的时间，下面对这个公式进行简单的变换，使得\\(\\mu和 \\sigma^2 \\)能够同时求出。假如当前有 n 个样本，当前的均值 \\( \\mu’ \\) 可以简单求出，而当前的方差 \\( \\sigma’^2 \\) 可通以下公式求解\n\\[\n\\sigma′^2 = \\frac{1}{n} \\sum_x (x^2 − 2 x \\mu′+\\mu′^2 ) = \\frac{1}{n} \\sum_x (x^2) − \\frac{1}{n} (2n\\mu′^2 − n \\mu′^2)=\\frac{1}{n}\\sum_x (x^2) − \\mu′^2\n\\]</p>\n<p>通过这个公式，可以遍历一次便求出任意个样本的方差，下面通过这个公式求解均值和方差随着样本数量变化而变化的情况，并比较进行 shuffle 前后两者在均值和方差上的区别。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># calculate the running mean,standard deviation, and range reporting the final result</span><br><span class=\"line\">import os, csv</span><br><span class=\"line\">raw_source = &apos;bikesharing/hour.csv&apos; # unshuffle</span><br><span class=\"line\">shuffle_source = &apos;bikesharing/shuffled_hour.csv&apos;</span><br><span class=\"line\"></span><br><span class=\"line\">def running_statistic(source):</span><br><span class=\"line\">    SEP=&apos;,&apos;</span><br><span class=\"line\">    running_mean = list()</span><br><span class=\"line\">    running_std = list()</span><br><span class=\"line\">    with open(local_path+&apos;/&apos;+source, &apos;rb&apos;) as R:</span><br><span class=\"line\">        iterator = csv.DictReader(R, delimiter=SEP)</span><br><span class=\"line\">        x = 0.0</span><br><span class=\"line\">        x_squared = 0.0</span><br><span class=\"line\">        for n, row in enumerate(iterator):</span><br><span class=\"line\">            temp = float(row[&apos;temp&apos;])</span><br><span class=\"line\">            if n == 0:</span><br><span class=\"line\">                max_x, min_x = temp, temp</span><br><span class=\"line\">            else:</span><br><span class=\"line\">                max_x, min_x = max(temp, max_x),min(temp, min_x)</span><br><span class=\"line\">            x += temp</span><br><span class=\"line\">            x_squared += temp**2</span><br><span class=\"line\">            running_mean.append(x / (n+1))</span><br><span class=\"line\">            running_std.append(((x_squared - (x**2)/(n+1))/(n+1))**0.5)</span><br><span class=\"line\">            # DATA PROCESSING placeholder</span><br><span class=\"line\">            # MACHINE LEARNING placeholder</span><br><span class=\"line\">            pass</span><br><span class=\"line\">        print (&apos;Total rows: %i&apos; % (n+1))</span><br><span class=\"line\">        print (&apos;Feature \\&apos;temp\\&apos;: mean=%0.3f, max=%0.3f, min=%0.3f,sd=%0.3f&apos; \\</span><br><span class=\"line\">               % (running_mean[-1], max_x, min_x, running_std[-1]))</span><br><span class=\"line\">        return running_mean, running_std</span><br><span class=\"line\">print &apos;===========raw data file===========&apos;</span><br><span class=\"line\">raw_running_mean, raw_running_std = running_statistic(raw_source)</span><br><span class=\"line\">print &apos;===========shuffle data file===========&apos;</span><br><span class=\"line\">shuffle_running_mean, shuffle_running_std = running_statistic(shuffle_source)</span><br></pre></td></tr></table></figure>\n<p>输出如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">===========raw data file===========</span><br><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193</span><br><span class=\"line\">===========shuffle data file===========</span><br><span class=\"line\">Total rows: 17379</span><br><span class=\"line\">Feature &apos;temp&apos;: mean=0.497, max=1.000, min=0.020,sd=0.193</span><br></pre></td></tr></table></figure>\n<p>两者的统计数据一致，符合要求，下面再看看两者的均值和方差随着时间如何变化，也就是将上面得到的 running_mean 和 running_std 进行可视化</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># plot how such stats changed as data was streamed from disk</span><br><span class=\"line\"># get an idea about how many instances are required before getting a stable mean and standard deviation estimate</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">for mean, std in ((raw_running_mean, raw_running_std), (shuffle_running_mean, shuffle_running_std)):</span><br><span class=\"line\">    plt.plot(mean,&apos;r-&apos;, label=&apos;mean&apos;)</span><br><span class=\"line\">    plt.plot(std,&apos;b-&apos;, label=&apos;standard deviation&apos;)</span><br><span class=\"line\">    plt.ylim(0.0,0.6)</span><br><span class=\"line\">    plt.xlabel(&apos;Number of training examples&apos;)</span><br><span class=\"line\">    plt.ylabel(&apos;Value&apos;) </span><br><span class=\"line\">    plt.legend(loc=&apos;lower right&apos;, numpoints= 1)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"># The difference in the two charts reminds us of the importance of randomizing the order of the observations.</span><br></pre></td></tr></table></figure>\n<p>得到的结果如下</p>\n<p>原始的文件</p>\n<p><img src=\"/posts_res/2018-04-19-bigscaledataml/1.png\" alt=\"original\"></p>\n<p>shuffle 后的文件</p>\n<p><img src=\"/posts_res/2018-04-19-bigscaledataml/2.png\" alt=\"after\"></p>\n<p>可以看到，经过 shuffle 后的数据的均值和方差很快就达到了稳定的状态，可以让 SGD 算法更快地收敛，这也从另一个角度验证了 shuffle 的必要性。</p>\n<h4 id=\"hasing-trick\"><a href=\"#hasing-trick\" class=\"headerlink\" title=\"hasing trick\"></a>hasing trick</h4><p>对于 categorial feature， 往往要对其进行 one-hot 编码，但是进行 one-hot 编码需要知道这个 feature 所有可能的取值的数量，对于流式输入的数据，可以先遍历一遍数据得到 categorial feature 所有可能取值的数目。除此之外，还可以利用接下来要讲的 hashing trick 对categorical feature 进行 one-hot 编码，这种方法对只能遍历一遍的数据有效。</p>\n<p>hahsing trick 利用了 hash 函数，通过hash后取模，将样本的值映射到预先定义好的固定长度的槽列中的某个槽中，这种方法需要对 categorical feature 的所有可能取值有大概的估计，而且可能会出现冲突的情况，但是如果对categorical feature 的所有可能取值有较准确的估计时，冲突的概率会比较低。下面是利用 sklearn 中的 HashingVectorizer 进行这种编码的一个例子</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.feature_extraction.text import HashingVectorizer</span><br><span class=\"line\">h = HashingVectorizer(n_features=1000, binary=True, norm=None)</span><br><span class=\"line\">sparse_vector = h.transform([&apos;A simple toy example will make clear how it works.&apos;])</span><br><span class=\"line\">print(sparse_vector)</span><br></pre></td></tr></table></figure>\n<p>输出如下</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(0, 61)\t1.0</span><br><span class=\"line\">(0, 271)\t1.0</span><br><span class=\"line\">(0, 287)\t1.0</span><br><span class=\"line\">(0, 452)\t1.0</span><br><span class=\"line\">(0, 462)\t1.0</span><br><span class=\"line\">(0, 539)\t1.0</span><br><span class=\"line\">(0, 605)\t1.0</span><br><span class=\"line\">(0, 726)\t1.0</span><br><span class=\"line\">(0, 918)\t1.0</span><br></pre></td></tr></table></figure>\n<p>这里定义的槽列的长度为1000，即假设字典中的单词数目为 1000， 然后将文本映射到这个槽列中，1 表示有这个单词，0表示没有。</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>本文主要介绍了如何进行 out-of-core learning，主要思想就是将数据以流式方式读入，然后通过 SGD 算法进行更新，\n在读入数据之前，首先需要对数据进行 shuffle 操作，消除数据本来的顺序信息等，同时可以让样本的特征的方差和均值更快达到稳定状态。</p>\n"},{"layout":"post","title":"EM算法","date":"2018-04-28T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n目录\n\n* 最大似然\n* Jensen不等式\n* 算法推导\n* 算法收敛性\n* 算法举例\n* 代码\n\n-----------------\n\n### 最大似然(极大似然)\n\n最大似然估计是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。\n最大似然估计是建立在这样的思想上:已知某个参数能使这个样本出现的概率最大，当然不会再去选择其他小概率的样本，所以就把这个参数作为估计的真实值。\n\n假设需要调查学校的男生和女生的身高分布。肯定是抽样解决，假设你在校园里随便地询问100个男生和100个女生。他们共200个人(也就是200个身高的样本数据，为了方便表示，下面说“人”的意思即对应其身高)都在教室里面了。那之后怎么办？通过某种办法将男女生分开，然后你就先统计抽样得到的100个男生的身高。\n假设他们的身高是服从高斯分布的，但是这个分布的均值 \\\\( \\mu \\\\) 和方差 \\\\( \\sigma^2 \\\\) 不知道，这两个参数是需要估计的。\n记作\\\\( \\theta=[\\mu, \\sigma]^T \\\\)。\n\n用数学的语言来说：在学校那么多男生（身高）中，我们独立地按照概率密度 \\\\( p(x\\|\\theta) \\\\) 抽取100个（身高）组成样本集\\\\( X \\\\).\n我们想通过样本集\\\\( X \\\\) 来估计出未知参数\\\\( \\theta \\\\)。已知概率密度 \\\\( p(x\\|\\theta) \\\\) 是高斯分布\\\\( N(\\mu, \\sigma) \\\\)的形式，其中的未知参数是\\\\( \\theta=[\\mu, \\sigma]^T \\\\)。抽到的样本集是\\\\( X = \\lbrace x\\_1, x\\_2, … , x\\_N \\rbrace \\\\)，其中\\\\( x\\_i \\\\)表示抽到的第\\\\(i\\\\)个人的身高，这里\\\\(N\\\\)就是100，表示抽到的样本个数。\n\n由于每个样本都是独立地从\\\\( p(x\\| \\theta) \\\\)中抽取的，且可以认为这些男生之间是没有关系的。\n那么从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？\n因为这些男生（的身高）是服从同一个高斯分布\\\\( p(x\\|\\theta) \\\\)的。那么抽到男生A（的身高）的概率是\\\\( p(x\\_A \\| \\theta) \\\\)，抽到男生B的概率是\\\\( p(x\\_B \\| \\theta) \\\\)。又因为它们是独立的，所以同时抽到男生A和男生B的概率是\\\\( p(x\\_A \\| \\theta) \\cdot p(x\\_B \\| \\theta) \\\\)；\n同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。换句话就是从分布是\\\\( p(x \\| \\theta) \\\\)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：\n\n\\\\[\nL(\\theta) = L(x\\_1,...,x\\_N; \\theta) = \\mathop{\\prod\\_{i=1}^N} p(x\\_i;\\theta), \\quad \\theta \\in \\Theta\n\\\\]\n\n这个概率反映了，在概率密度函数的参数是\\\\(\\theta\\\\)时，得到\\\\( X\\\\)这组样本的概率。因为这里\\\\(X\\\\)是已知的，也就是说抽取到的这100个人的身高可以测出来，也就是已知的了。而\\\\(\\theta\\\\)是未知的，则上面这个公式只有\\\\(\\theta\\\\)是未知数，所以它是\\\\(\\theta\\\\)的函数。这个函数放映的是在不同的参数\\\\(\\theta\\\\)取值下，取得当前这个样本集的可能性，因此称为参数\\\\( \\theta\\\\)相对于样本集\\\\(X\\\\)的似然函数（likehood function）。\n记为\\\\( L(\\theta)\\\\)。\n\n在学校那么男生中，抽到这100个男生（表示身高）而不是其他人，则表示在整个学校中，这100个人（的身高）出现的概率最大。那么这个概率就可以用上面那个似然函数\\\\( L(\\theta) \\\\)来表示。只需要找到一个参数\\\\( \\theta \\\\)，其对应的似然函数\\\\( L(\\theta) \\\\)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做\\\\( \\theta \\\\)的最大似然估计量，记为：\n\n\\\\[\n\\hat{\\theta} = \\mathop{\\arg\\max}\\_{\\theta} L(\\theta)\n\\\\]\n\n**因此，当知道每个样例属于的分布时，求解分布的参数直接利用最大似然估计或贝叶斯估计法即可。但是对于含有隐变量时，就不能直接使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。**\n\n\n---------\n\n### Jensen不等式\n\n设\\\\(f\\\\)是定义域为实数的函数，如果对于所有的实数\\\\(x\\\\)，\\\\( f^{\\prime \\prime}(x) \\geq 0 \\\\)，那么\\\\(f\\\\)是凸函数；\n当\\\\(x\\\\)是向量时，如果其hessian矩阵H是半正定的(\\\\( H \\geq 0 \\\\))，那么\\\\( f\\\\)是凸函数。\n如果\\\\( f^{\\prime \\prime}(x) > 0 \\\\)或者\\\\( H > 0 \\\\)，那么称\\\\(f\\\\)是严格凸函数。\n\nJensen不等式表述如下：\n\n如果\\\\(f\\\\)是凸函数，\\\\(X\\\\)是随机变量，那么\\\\( E[f(X)] \\geq f(EX) \\\\)。\n特别地，如果\\\\(f\\\\)是严格凸函数，那么\\\\( E[f(X)]=f(EX) \\\\)当且仅当\\\\( p(X=EX)=1 \\\\)，也就是说\\\\( X \\\\)是常量。\n\n下图表示会更清晰：\n\n![jensen](/posts_res/2018-04-28-emalgorithm/1.jpg)\n\n图中，实线\\\\( f \\\\)是凸函数，\\\\(X\\\\)是随机变量，有0.5的概率是a，有0.5的概率是b。\\\\(X\\\\)的期望值就是a和b的中值了。\n图中可以看到\\\\( E[f(x)] \\geq f(EX) \\\\)成立。\n\n当f是（严格）凹函数当且仅当-f是（严格）凸函数。Jensen不等式应用于凹函数时，不等号方向反向，即\\\\( E[f(X)] \\leq f(EX) \\\\)。\n\n\n----------\n\n### 算法推导\n\n给定的训练样本是\\\\( \\lbrace x^{(1)},...,x^{(m)} \\rbrace \\\\)，样例间独立，想找到每个样例隐含的类别\\\\( z \\\\)，能使得\\\\( p(x,z) \\\\)最大。\n\\\\( p(x,z) \\\\)的最大似然估计如下：\n\n\\\\[\nL(\\theta) = \\mathop{\\sum\\_{i=1}^m} log p(x^{(i)};\\theta) = \\mathop{\\sum\\_{i=1}^m} log \\sum\\_z p(x^{(i)},z;\\theta)\n\\\\]\n\n前一步是对极大似然取对数，后一步是对每个样例的每个可能类别\\\\(z\\\\)求联合分布概率和。\n\n对于每一个样例\\\\( x^{(i)}\\\\)，让\\\\( Q\\_i \\\\)表示该样例隐含变量\\\\( z \\\\)的某种分布，\\\\( Q\\_i\\\\)满足的条件是\\\\( \\sum\\_z Q\\_i(z) = 1, Q\\_i(z) \\geq 0 \\\\)。\n（如果\\\\(z\\\\)是连续性的，那么\\\\(Q\\_i\\\\)是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量\\\\( z \\\\)是身高，那么就是连续的高斯分布。\n如果按照隐藏变量是男女，那么就是伯努利分布了。\n\n可以由前面阐述的内容得到下面的公式：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum\\_i log p(x^{(i)}; \\theta)\n& = \\sum\\_i log \\sum\\_{z} p(x^{(i)}, z; \\theta) \\\\\\\n& = \\sum\\_i log \\sum\\_{z} Q\\_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\\\\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)}\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n*第二个等号比较直接，就是分子分母同乘以一个相等的函数；最后不等号利用了Jensen不等式，考虑到 log(x) 是凹函数(二阶导数小于0)。而且*\n\\\\( \\sum\\_{z} Q\\_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\)\n*就是*\\\\( \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\)*的期望*\n\n这个过程可以看作是对\\\\( L(\\theta) \\\\)求了下界。对于\\\\( Q\\_i\\\\)的选择，有多种可能，哪种更好的？\n假设\\\\( \\theta \\\\)已经给定，那么\\\\(L(\\theta)\\\\)的值就决定于\\\\( Q\\_i(z) \\\\)和\\\\( p(x^{(i)}, z) \\\\)了。\n可以通过调整这两个概率使下界不断上升，以逼近\\\\( L(\\theta) \\\\)的真实值，什么时候算是调整好了呢？\n当不等式变成等式时，说明调整后的概率能够等价于\\\\( L(\\theta) \\\\)了。按照这个思路，根据Jensen不等式，等式成立当且仅当随机变量为常数值，即：\n\n\\\\[\n\\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} = c\n\\\\]\n*c为常数，不依赖于\\\\(z\\\\)；*\n\n对此式子做进一步推导，知道\\\\( \\sum\\_z Q\\_i(z)=1\\\\)，那么也就有\\\\( \\sum\\_z p(x^{(i)}, z; \\theta) = c \\\\)，那么有下式：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nQ\\_i(z)\n& = \\frac{p(x^{(i)}, z; \\theta)}{\\sum\\_z p(x^{(i)}, z; \\theta)} \\\\\\\n& = \\frac{p(x^{(i)}, z; \\theta)}{p(x^{(i)}; \\theta)} \\\\\\\n& = p(z \\| x^{(i)}; \\theta)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n至此，推出了在固定其他参数\\\\(\\theta\\\\)后，\\\\( Q\\_i(z) \\\\)的计算公式就是后验概率，解决了\\\\( Q\\_i(z) \\\\)如何选择的问题。\n这一步就是E步，建立\\\\( L(\\theta) \\\\)的下界。接下来的M步，就是在给定\\\\( Q\\_i(z) \\\\)后，调整\\\\( \\theta\\\\)，极大化\\\\(L(\\theta) \\\\)的下界（在固定\\\\( Q\\_i(z) \\\\)后，下界还可以调整的更大）。\n\n**那么一般的EM算法的步骤如下：**\n\n1. 选择参数的初始值\\\\(\\theta^{(0)}\\\\)，开始迭代；\n2. (E步)对于每一个 \\\\( i\\\\)，计算 \\\\( Q\\_i(z) = p(z \\| x^{(i)}; \\theta^{(t)}) \\\\)；\n3. (M步)计算 \\\\( \\theta^{(t+1)} = \\arg\\max\\_{\\theta^{(t)}} \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)} \\\\)；\n4. 重复2-3步，直到收敛。\n\n\n--------\n\n### 算法收敛性\n\n假定\\\\( \\theta^{(t)} \\\\)和\\\\( \\theta^{(t+1)} \\\\)是EM第 t 次和 t+1 次迭代后的结果。如果我们证明了\\\\( L(\\theta^{(t)}) \\leq L(\\theta^{(t+1)})\\\\)，也就是说最大似然估计单调增加，那么最终会到达最大似然估计的最大值。下面来证明，选定\\\\( \\theta^{(t)} \\\\)后，我们得到E步\n\n\\\\[\nQ\\_i(z) = p(z \\| x^{(i)}; \\theta^{(t)})\n\\\\]\n\n这一步保证了在给定\\\\( \\theta^{(t)} \\\\)时，Jensen不等式中的等式成立，也就是\n\n\\\\[\nL(\\theta) = \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)}\n\\\\]\n\n然后进行M步，固定\\\\( Q\\_i^{(t)}(z) \\\\)，并将\\\\( \\theta^{(t)} \\\\)视作变量，对上面的\\\\( L(\\theta^{(t)}) \\\\)求导后，得到\\\\( L(\\theta^{(t+1)}) \\\\)，这样经过一些推导会有以下式子成立：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(\\theta^{(t+1)})\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t+1)})}{Q\\_i(z)} \\\\\\\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)} \\\\\\\n& = L(\\theta^{(t)})\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n*第一个不等号，得到\\\\( \\theta^{(t+1)} \\\\)时，只是最大化\\\\( L(\\theta^{(t)}) \\\\)，也就是\\\\( L(\\theta^{(t+1)}) \\\\)的下界，而没有使等式成立，*\n*等式成立只有是在固定\\\\(\\theta\\\\)，并按E步得到\\\\( Q\\_i \\\\)时才能成立。*\n\n*况且根据前面得到的下式，对于所有的\\\\(\\theta\\\\)和\\\\( Q\\_i \\\\)都成立*\n\n\\\\[\nL(\\theta) \\geq \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)}\n\\\\]\n\n*第二个不等号利用了M步的定义，M步就是将\\\\( \\theta^{(t)} \\\\)调整到\\\\( \\theta^{(t+1)} \\\\)，使得下界最大化。*\n\n这样就证明了\\\\( L(\\theta) \\\\)会单调增加。一种收敛方法是\\\\( L(\\theta) \\\\)不再变化，还有一种就是变化幅度很小。\n\n\n----------\n\n### 算法举例\n\n假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H；(H代表证明朝上)。\na是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？\n\n![example](/posts_res/2018-04-28-emalgorithm/2.png)\n\n针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的\\\\( \\hat{\\theta}\\_A \\\\)，这其实也是最大似然求导求出来的。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathop{\\arg\\max}\\_{\\theta} log P(Y \\| \\theta)\n& = log((\\theta\\_B^5(1-\\theta\\_B)^5) (\\theta\\_A^9(1-\\theta\\_A))(\\theta\\_A^8(1-\\theta\\_A)^2) (\\theta\\_B^4(1-\\theta\\_B)^6) (\\theta\\_A^7(1-\\theta\\_A)^3) ) \\\\\\ \n& = log((\\theta\\_A^{24}(1-\\theta\\_A)^6) (\\theta\\_B^9(1-\\theta\\_B)^{11}))\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n上面这个式子求导之后就能得出\\\\( \\hat{\\theta}\\_A = \\frac{24}{24+6} = 0.80 以及 \\hat{\\theta}\\_B = \\frac{9}{9+11} = 0.45 \\\\)。\n\n<br>\n\n针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。\n\n**E步**:计算在给定的\\\\( \\hat{\\theta}\\_A^{(0)} \\\\)和\\\\( \\hat{\\theta}\\_B^{(0)} \\\\)下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)\n\n\\\\[\nP(z=A \\| y\\_1, \\theta) = \\frac{P(z=A, y\\_1 \\| \\theta)}{P(z=A,y\\_1 \\| \\theta) + P(z=B, y\\_1 \\| \\theta)} = \\frac{0.6^5 \\cdot 0.4^5}{0.6^5 \\cdot 0.4^5 + 0.5^{10}} = 0.45\n\\\\]\n\n**M步**:针对Q函数求导，在本题中Q函数形式如下，这里的\\\\( y\\_j\\\\)代表的是每次正面朝上的个数。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nQ(\\theta, \\theta^{(i)})\n& = \\sum\\_{j=1}^N \\sum\\_z P(z\\| y\\_j, \\theta^{(i)}) log P(y\\_j, z \\| \\theta) \\\\\\\n& = \\sum\\_{j=1}^N \\mu\\_j log(\\theta\\_A^{y\\_j} (1-\\theta\\_A)^{10-y\\_j}) + (1-\\mu\\_j) log(\\theta\\_B^{y\\_j} (1-\\theta\\_B)^{10-y\\_j})\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n从而针对这个式子来对参数求导，例如对\\\\(\\theta\\_A\\\\)求导\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial Q}{\\partial \\theta\\_A}\n& = \\mu\\_1 (\\frac{y\\_1}{\\theta\\_A} - \\frac{10-y\\_1}{1-\\theta\\_A}) + ... + \\mu\\_5 ( \\frac{y\\_5}{\\theta\\_A} - \\frac{10-y\\_5}{1-\\theta\\_5} ) \\\\\\\n& = \\mu\\_1 ( \\frac{y\\_1 - 10 \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} ) + ... + \\mu\\_5 ( \\frac{y\\_5 - 10 \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} ) \\\\\\\n& = \\frac{\\sum\\_{j=1}^5 \\mu\\_j y\\_j - \\sum\\_{j=1}^5 10 \\mu\\_j \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n求导等于0之后就可得到图中的第一次迭代之后的参数值\\\\( \\hat{\\theta}\\_A^{(1)} = 0.71 和 \\hat{\\theta}\\_B^{(1)} = 0.58 \\\\)。\n\n这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。\n\n\n----------\n\n### 代码\n\n[https://blog.csdn.net/u010866505/article/details/77877345](https://blog.csdn.net/u010866505/article/details/77877345)\n\n\n----------\n\n### 参考\n\n>\n1. [EM算法 The EM Algorithm](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)\n2. [机器学习算法系列之一 EM算法实例分析](https://chenrudan.github.io/blog/2015/12/02/emexample.html)\n3. [从最大似然到EM算法浅解](https://blog.csdn.net/zouxy09/article/details/8537620/)\n4. [EM算法原理和python简单实现](https://blog.csdn.net/u010866505/article/details/77877345)\n5. [What is the expectation maximization algorithm?](http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf)\n\n","source":"_posts/2018-04-28-emalgorithm.md","raw":"---\nlayout: post\ntitle: EM算法\ndate: 2018-04-28 21:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n\n目录\n\n* 最大似然\n* Jensen不等式\n* 算法推导\n* 算法收敛性\n* 算法举例\n* 代码\n\n-----------------\n\n### 最大似然(极大似然)\n\n最大似然估计是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。\n最大似然估计是建立在这样的思想上:已知某个参数能使这个样本出现的概率最大，当然不会再去选择其他小概率的样本，所以就把这个参数作为估计的真实值。\n\n假设需要调查学校的男生和女生的身高分布。肯定是抽样解决，假设你在校园里随便地询问100个男生和100个女生。他们共200个人(也就是200个身高的样本数据，为了方便表示，下面说“人”的意思即对应其身高)都在教室里面了。那之后怎么办？通过某种办法将男女生分开，然后你就先统计抽样得到的100个男生的身高。\n假设他们的身高是服从高斯分布的，但是这个分布的均值 \\\\( \\mu \\\\) 和方差 \\\\( \\sigma^2 \\\\) 不知道，这两个参数是需要估计的。\n记作\\\\( \\theta=[\\mu, \\sigma]^T \\\\)。\n\n用数学的语言来说：在学校那么多男生（身高）中，我们独立地按照概率密度 \\\\( p(x\\|\\theta) \\\\) 抽取100个（身高）组成样本集\\\\( X \\\\).\n我们想通过样本集\\\\( X \\\\) 来估计出未知参数\\\\( \\theta \\\\)。已知概率密度 \\\\( p(x\\|\\theta) \\\\) 是高斯分布\\\\( N(\\mu, \\sigma) \\\\)的形式，其中的未知参数是\\\\( \\theta=[\\mu, \\sigma]^T \\\\)。抽到的样本集是\\\\( X = \\lbrace x\\_1, x\\_2, … , x\\_N \\rbrace \\\\)，其中\\\\( x\\_i \\\\)表示抽到的第\\\\(i\\\\)个人的身高，这里\\\\(N\\\\)就是100，表示抽到的样本个数。\n\n由于每个样本都是独立地从\\\\( p(x\\| \\theta) \\\\)中抽取的，且可以认为这些男生之间是没有关系的。\n那么从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？\n因为这些男生（的身高）是服从同一个高斯分布\\\\( p(x\\|\\theta) \\\\)的。那么抽到男生A（的身高）的概率是\\\\( p(x\\_A \\| \\theta) \\\\)，抽到男生B的概率是\\\\( p(x\\_B \\| \\theta) \\\\)。又因为它们是独立的，所以同时抽到男生A和男生B的概率是\\\\( p(x\\_A \\| \\theta) \\cdot p(x\\_B \\| \\theta) \\\\)；\n同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。换句话就是从分布是\\\\( p(x \\| \\theta) \\\\)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：\n\n\\\\[\nL(\\theta) = L(x\\_1,...,x\\_N; \\theta) = \\mathop{\\prod\\_{i=1}^N} p(x\\_i;\\theta), \\quad \\theta \\in \\Theta\n\\\\]\n\n这个概率反映了，在概率密度函数的参数是\\\\(\\theta\\\\)时，得到\\\\( X\\\\)这组样本的概率。因为这里\\\\(X\\\\)是已知的，也就是说抽取到的这100个人的身高可以测出来，也就是已知的了。而\\\\(\\theta\\\\)是未知的，则上面这个公式只有\\\\(\\theta\\\\)是未知数，所以它是\\\\(\\theta\\\\)的函数。这个函数放映的是在不同的参数\\\\(\\theta\\\\)取值下，取得当前这个样本集的可能性，因此称为参数\\\\( \\theta\\\\)相对于样本集\\\\(X\\\\)的似然函数（likehood function）。\n记为\\\\( L(\\theta)\\\\)。\n\n在学校那么男生中，抽到这100个男生（表示身高）而不是其他人，则表示在整个学校中，这100个人（的身高）出现的概率最大。那么这个概率就可以用上面那个似然函数\\\\( L(\\theta) \\\\)来表示。只需要找到一个参数\\\\( \\theta \\\\)，其对应的似然函数\\\\( L(\\theta) \\\\)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做\\\\( \\theta \\\\)的最大似然估计量，记为：\n\n\\\\[\n\\hat{\\theta} = \\mathop{\\arg\\max}\\_{\\theta} L(\\theta)\n\\\\]\n\n**因此，当知道每个样例属于的分布时，求解分布的参数直接利用最大似然估计或贝叶斯估计法即可。但是对于含有隐变量时，就不能直接使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。**\n\n\n---------\n\n### Jensen不等式\n\n设\\\\(f\\\\)是定义域为实数的函数，如果对于所有的实数\\\\(x\\\\)，\\\\( f^{\\prime \\prime}(x) \\geq 0 \\\\)，那么\\\\(f\\\\)是凸函数；\n当\\\\(x\\\\)是向量时，如果其hessian矩阵H是半正定的(\\\\( H \\geq 0 \\\\))，那么\\\\( f\\\\)是凸函数。\n如果\\\\( f^{\\prime \\prime}(x) > 0 \\\\)或者\\\\( H > 0 \\\\)，那么称\\\\(f\\\\)是严格凸函数。\n\nJensen不等式表述如下：\n\n如果\\\\(f\\\\)是凸函数，\\\\(X\\\\)是随机变量，那么\\\\( E[f(X)] \\geq f(EX) \\\\)。\n特别地，如果\\\\(f\\\\)是严格凸函数，那么\\\\( E[f(X)]=f(EX) \\\\)当且仅当\\\\( p(X=EX)=1 \\\\)，也就是说\\\\( X \\\\)是常量。\n\n下图表示会更清晰：\n\n![jensen](/posts_res/2018-04-28-emalgorithm/1.jpg)\n\n图中，实线\\\\( f \\\\)是凸函数，\\\\(X\\\\)是随机变量，有0.5的概率是a，有0.5的概率是b。\\\\(X\\\\)的期望值就是a和b的中值了。\n图中可以看到\\\\( E[f(x)] \\geq f(EX) \\\\)成立。\n\n当f是（严格）凹函数当且仅当-f是（严格）凸函数。Jensen不等式应用于凹函数时，不等号方向反向，即\\\\( E[f(X)] \\leq f(EX) \\\\)。\n\n\n----------\n\n### 算法推导\n\n给定的训练样本是\\\\( \\lbrace x^{(1)},...,x^{(m)} \\rbrace \\\\)，样例间独立，想找到每个样例隐含的类别\\\\( z \\\\)，能使得\\\\( p(x,z) \\\\)最大。\n\\\\( p(x,z) \\\\)的最大似然估计如下：\n\n\\\\[\nL(\\theta) = \\mathop{\\sum\\_{i=1}^m} log p(x^{(i)};\\theta) = \\mathop{\\sum\\_{i=1}^m} log \\sum\\_z p(x^{(i)},z;\\theta)\n\\\\]\n\n前一步是对极大似然取对数，后一步是对每个样例的每个可能类别\\\\(z\\\\)求联合分布概率和。\n\n对于每一个样例\\\\( x^{(i)}\\\\)，让\\\\( Q\\_i \\\\)表示该样例隐含变量\\\\( z \\\\)的某种分布，\\\\( Q\\_i\\\\)满足的条件是\\\\( \\sum\\_z Q\\_i(z) = 1, Q\\_i(z) \\geq 0 \\\\)。\n（如果\\\\(z\\\\)是连续性的，那么\\\\(Q\\_i\\\\)是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量\\\\( z \\\\)是身高，那么就是连续的高斯分布。\n如果按照隐藏变量是男女，那么就是伯努利分布了。\n\n可以由前面阐述的内容得到下面的公式：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum\\_i log p(x^{(i)}; \\theta)\n& = \\sum\\_i log \\sum\\_{z} p(x^{(i)}, z; \\theta) \\\\\\\n& = \\sum\\_i log \\sum\\_{z} Q\\_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\\\\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)}\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n*第二个等号比较直接，就是分子分母同乘以一个相等的函数；最后不等号利用了Jensen不等式，考虑到 log(x) 是凹函数(二阶导数小于0)。而且*\n\\\\( \\sum\\_{z} Q\\_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\)\n*就是*\\\\( \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} \\\\)*的期望*\n\n这个过程可以看作是对\\\\( L(\\theta) \\\\)求了下界。对于\\\\( Q\\_i\\\\)的选择，有多种可能，哪种更好的？\n假设\\\\( \\theta \\\\)已经给定，那么\\\\(L(\\theta)\\\\)的值就决定于\\\\( Q\\_i(z) \\\\)和\\\\( p(x^{(i)}, z) \\\\)了。\n可以通过调整这两个概率使下界不断上升，以逼近\\\\( L(\\theta) \\\\)的真实值，什么时候算是调整好了呢？\n当不等式变成等式时，说明调整后的概率能够等价于\\\\( L(\\theta) \\\\)了。按照这个思路，根据Jensen不等式，等式成立当且仅当随机变量为常数值，即：\n\n\\\\[\n\\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)} = c\n\\\\]\n*c为常数，不依赖于\\\\(z\\\\)；*\n\n对此式子做进一步推导，知道\\\\( \\sum\\_z Q\\_i(z)=1\\\\)，那么也就有\\\\( \\sum\\_z p(x^{(i)}, z; \\theta) = c \\\\)，那么有下式：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nQ\\_i(z)\n& = \\frac{p(x^{(i)}, z; \\theta)}{\\sum\\_z p(x^{(i)}, z; \\theta)} \\\\\\\n& = \\frac{p(x^{(i)}, z; \\theta)}{p(x^{(i)}; \\theta)} \\\\\\\n& = p(z \\| x^{(i)}; \\theta)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n至此，推出了在固定其他参数\\\\(\\theta\\\\)后，\\\\( Q\\_i(z) \\\\)的计算公式就是后验概率，解决了\\\\( Q\\_i(z) \\\\)如何选择的问题。\n这一步就是E步，建立\\\\( L(\\theta) \\\\)的下界。接下来的M步，就是在给定\\\\( Q\\_i(z) \\\\)后，调整\\\\( \\theta\\\\)，极大化\\\\(L(\\theta) \\\\)的下界（在固定\\\\( Q\\_i(z) \\\\)后，下界还可以调整的更大）。\n\n**那么一般的EM算法的步骤如下：**\n\n1. 选择参数的初始值\\\\(\\theta^{(0)}\\\\)，开始迭代；\n2. (E步)对于每一个 \\\\( i\\\\)，计算 \\\\( Q\\_i(z) = p(z \\| x^{(i)}; \\theta^{(t)}) \\\\)；\n3. (M步)计算 \\\\( \\theta^{(t+1)} = \\arg\\max\\_{\\theta^{(t)}} \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)} \\\\)；\n4. 重复2-3步，直到收敛。\n\n\n--------\n\n### 算法收敛性\n\n假定\\\\( \\theta^{(t)} \\\\)和\\\\( \\theta^{(t+1)} \\\\)是EM第 t 次和 t+1 次迭代后的结果。如果我们证明了\\\\( L(\\theta^{(t)}) \\leq L(\\theta^{(t+1)})\\\\)，也就是说最大似然估计单调增加，那么最终会到达最大似然估计的最大值。下面来证明，选定\\\\( \\theta^{(t)} \\\\)后，我们得到E步\n\n\\\\[\nQ\\_i(z) = p(z \\| x^{(i)}; \\theta^{(t)})\n\\\\]\n\n这一步保证了在给定\\\\( \\theta^{(t)} \\\\)时，Jensen不等式中的等式成立，也就是\n\n\\\\[\nL(\\theta) = \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)}\n\\\\]\n\n然后进行M步，固定\\\\( Q\\_i^{(t)}(z) \\\\)，并将\\\\( \\theta^{(t)} \\\\)视作变量，对上面的\\\\( L(\\theta^{(t)}) \\\\)求导后，得到\\\\( L(\\theta^{(t+1)}) \\\\)，这样经过一些推导会有以下式子成立：\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nL(\\theta^{(t+1)})\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t+1)})}{Q\\_i(z)} \\\\\\\n& \\geq \\sum\\_i \\sum\\_{z} Q\\_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q\\_i(z)} \\\\\\\n& = L(\\theta^{(t)})\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n*第一个不等号，得到\\\\( \\theta^{(t+1)} \\\\)时，只是最大化\\\\( L(\\theta^{(t)}) \\\\)，也就是\\\\( L(\\theta^{(t+1)}) \\\\)的下界，而没有使等式成立，*\n*等式成立只有是在固定\\\\(\\theta\\\\)，并按E步得到\\\\( Q\\_i \\\\)时才能成立。*\n\n*况且根据前面得到的下式，对于所有的\\\\(\\theta\\\\)和\\\\( Q\\_i \\\\)都成立*\n\n\\\\[\nL(\\theta) \\geq \\sum\\_i \\sum\\_{z} Q\\_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q\\_i(z)}\n\\\\]\n\n*第二个不等号利用了M步的定义，M步就是将\\\\( \\theta^{(t)} \\\\)调整到\\\\( \\theta^{(t+1)} \\\\)，使得下界最大化。*\n\n这样就证明了\\\\( L(\\theta) \\\\)会单调增加。一种收敛方法是\\\\( L(\\theta) \\\\)不再变化，还有一种就是变化幅度很小。\n\n\n----------\n\n### 算法举例\n\n假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H；(H代表证明朝上)。\na是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？\n\n![example](/posts_res/2018-04-28-emalgorithm/2.png)\n\n针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的\\\\( \\hat{\\theta}\\_A \\\\)，这其实也是最大似然求导求出来的。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathop{\\arg\\max}\\_{\\theta} log P(Y \\| \\theta)\n& = log((\\theta\\_B^5(1-\\theta\\_B)^5) (\\theta\\_A^9(1-\\theta\\_A))(\\theta\\_A^8(1-\\theta\\_A)^2) (\\theta\\_B^4(1-\\theta\\_B)^6) (\\theta\\_A^7(1-\\theta\\_A)^3) ) \\\\\\ \n& = log((\\theta\\_A^{24}(1-\\theta\\_A)^6) (\\theta\\_B^9(1-\\theta\\_B)^{11}))\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n上面这个式子求导之后就能得出\\\\( \\hat{\\theta}\\_A = \\frac{24}{24+6} = 0.80 以及 \\hat{\\theta}\\_B = \\frac{9}{9+11} = 0.45 \\\\)。\n\n<br>\n\n针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。\n\n**E步**:计算在给定的\\\\( \\hat{\\theta}\\_A^{(0)} \\\\)和\\\\( \\hat{\\theta}\\_B^{(0)} \\\\)下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)\n\n\\\\[\nP(z=A \\| y\\_1, \\theta) = \\frac{P(z=A, y\\_1 \\| \\theta)}{P(z=A,y\\_1 \\| \\theta) + P(z=B, y\\_1 \\| \\theta)} = \\frac{0.6^5 \\cdot 0.4^5}{0.6^5 \\cdot 0.4^5 + 0.5^{10}} = 0.45\n\\\\]\n\n**M步**:针对Q函数求导，在本题中Q函数形式如下，这里的\\\\( y\\_j\\\\)代表的是每次正面朝上的个数。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nQ(\\theta, \\theta^{(i)})\n& = \\sum\\_{j=1}^N \\sum\\_z P(z\\| y\\_j, \\theta^{(i)}) log P(y\\_j, z \\| \\theta) \\\\\\\n& = \\sum\\_{j=1}^N \\mu\\_j log(\\theta\\_A^{y\\_j} (1-\\theta\\_A)^{10-y\\_j}) + (1-\\mu\\_j) log(\\theta\\_B^{y\\_j} (1-\\theta\\_B)^{10-y\\_j})\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n从而针对这个式子来对参数求导，例如对\\\\(\\theta\\_A\\\\)求导\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial Q}{\\partial \\theta\\_A}\n& = \\mu\\_1 (\\frac{y\\_1}{\\theta\\_A} - \\frac{10-y\\_1}{1-\\theta\\_A}) + ... + \\mu\\_5 ( \\frac{y\\_5}{\\theta\\_A} - \\frac{10-y\\_5}{1-\\theta\\_5} ) \\\\\\\n& = \\mu\\_1 ( \\frac{y\\_1 - 10 \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} ) + ... + \\mu\\_5 ( \\frac{y\\_5 - 10 \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} ) \\\\\\\n& = \\frac{\\sum\\_{j=1}^5 \\mu\\_j y\\_j - \\sum\\_{j=1}^5 10 \\mu\\_j \\theta\\_A}{\\theta\\_A (1-\\theta\\_A)} \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n求导等于0之后就可得到图中的第一次迭代之后的参数值\\\\( \\hat{\\theta}\\_A^{(1)} = 0.71 和 \\hat{\\theta}\\_B^{(1)} = 0.58 \\\\)。\n\n这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。\n\n\n----------\n\n### 代码\n\n[https://blog.csdn.net/u010866505/article/details/77877345](https://blog.csdn.net/u010866505/article/details/77877345)\n\n\n----------\n\n### 参考\n\n>\n1. [EM算法 The EM Algorithm](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)\n2. [机器学习算法系列之一 EM算法实例分析](https://chenrudan.github.io/blog/2015/12/02/emexample.html)\n3. [从最大似然到EM算法浅解](https://blog.csdn.net/zouxy09/article/details/8537620/)\n4. [EM算法原理和python简单实现](https://blog.csdn.net/u010866505/article/details/77877345)\n5. [What is the expectation maximization algorithm?](http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf)\n\n","slug":"emalgorithm","published":1,"updated":"2019-08-17T09:35:55.251Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvr001s2qwpy0kyg8vb","content":"<p>目录</p><ul>\n<li>最大似然</li>\n<li>Jensen不等式</li>\n<li>算法推导</li>\n<li>算法收敛性</li>\n<li>算法举例</li>\n<li>代码</li>\n</ul><hr><h3 id=\"最大似然-极大似然\"><a href=\"#最大似然-极大似然\" class=\"headerlink\" title=\"最大似然(极大似然)\"></a>最大似然(极大似然)</h3><p>最大似然估计是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。\n最大似然估计是建立在这样的思想上:已知某个参数能使这个样本出现的概率最大，当然不会再去选择其他小概率的样本，所以就把这个参数作为估计的真实值。</p><p>假设需要调查学校的男生和女生的身高分布。肯定是抽样解决，假设你在校园里随便地询问100个男生和100个女生。他们共200个人(也就是200个身高的样本数据，为了方便表示，下面说“人”的意思即对应其身高)都在教室里面了。那之后怎么办？通过某种办法将男女生分开，然后你就先统计抽样得到的100个男生的身高。\n假设他们的身高是服从高斯分布的，但是这个分布的均值 \\( \\mu \\) 和方差 \\( \\sigma^2 \\) 不知道，这两个参数是需要估计的。\n记作\\( \\theta=[\\mu, \\sigma]^T \\)。</p><a id=\"more\"></a>\n\n\n\n\n<p>用数学的语言来说：在学校那么多男生（身高）中，我们独立地按照概率密度 \\( p(x|\\theta) \\) 抽取100个（身高）组成样本集\\( X \\).\n我们想通过样本集\\( X \\) 来估计出未知参数\\( \\theta \\)。已知概率密度 \\( p(x|\\theta) \\) 是高斯分布\\( N(\\mu, \\sigma) \\)的形式，其中的未知参数是\\( \\theta=[\\mu, \\sigma]^T \\)。抽到的样本集是\\( X = \\lbrace x_1, x_2, … , x_N \\rbrace \\)，其中\\( x_i \\)表示抽到的第\\(i\\)个人的身高，这里\\(N\\)就是100，表示抽到的样本个数。</p>\n<p>由于每个样本都是独立地从\\( p(x| \\theta) \\)中抽取的，且可以认为这些男生之间是没有关系的。\n那么从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？\n因为这些男生（的身高）是服从同一个高斯分布\\( p(x|\\theta) \\)的。那么抽到男生A（的身高）的概率是\\( p(x_A | \\theta) \\)，抽到男生B的概率是\\( p(x_B | \\theta) \\)。又因为它们是独立的，所以同时抽到男生A和男生B的概率是\\( p(x_A | \\theta) \\cdot p(x_B | \\theta) \\)；\n同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。换句话就是从分布是\\( p(x | \\theta) \\)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：</p>\n<p>\\[\nL(\\theta) = L(x_1,…,x_N; \\theta) = \\mathop{\\prod_{i=1}^N} p(x_i;\\theta), \\quad \\theta \\in \\Theta\n\\]</p>\n<p>这个概率反映了，在概率密度函数的参数是\\(\\theta\\)时，得到\\( X\\)这组样本的概率。因为这里\\(X\\)是已知的，也就是说抽取到的这100个人的身高可以测出来，也就是已知的了。而\\(\\theta\\)是未知的，则上面这个公式只有\\(\\theta\\)是未知数，所以它是\\(\\theta\\)的函数。这个函数放映的是在不同的参数\\(\\theta\\)取值下，取得当前这个样本集的可能性，因此称为参数\\( \\theta\\)相对于样本集\\(X\\)的似然函数（likehood function）。\n记为\\( L(\\theta)\\)。</p>\n<p>在学校那么男生中，抽到这100个男生（表示身高）而不是其他人，则表示在整个学校中，这100个人（的身高）出现的概率最大。那么这个概率就可以用上面那个似然函数\\( L(\\theta) \\)来表示。只需要找到一个参数\\( \\theta \\)，其对应的似然函数\\( L(\\theta) \\)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做\\( \\theta \\)的最大似然估计量，记为：</p>\n<p>\\[\n\\hat{\\theta} = \\mathop{\\arg\\max}_{\\theta} L(\\theta)\n\\]</p>\n<p><strong>因此，当知道每个样例属于的分布时，求解分布的参数直接利用最大似然估计或贝叶斯估计法即可。但是对于含有隐变量时，就不能直接使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。</strong></p>\n<hr>\n<h3 id=\"Jensen不等式\"><a href=\"#Jensen不等式\" class=\"headerlink\" title=\"Jensen不等式\"></a>Jensen不等式</h3><p>设\\(f\\)是定义域为实数的函数，如果对于所有的实数\\(x\\)，\\( f^{\\prime \\prime}(x) \\geq 0 \\)，那么\\(f\\)是凸函数；\n当\\(x\\)是向量时，如果其hessian矩阵H是半正定的(\\( H \\geq 0 \\))，那么\\( f\\)是凸函数。\n如果\\( f^{\\prime \\prime}(x) &gt; 0 \\)或者\\( H &gt; 0 \\)，那么称\\(f\\)是严格凸函数。</p>\n<p>Jensen不等式表述如下：</p>\n<p>如果\\(f\\)是凸函数，\\(X\\)是随机变量，那么\\( E[f(X)] \\geq f(EX) \\)。\n特别地，如果\\(f\\)是严格凸函数，那么\\( E[f(X)]=f(EX) \\)当且仅当\\( p(X=EX)=1 \\)，也就是说\\( X \\)是常量。</p>\n<p>下图表示会更清晰：</p>\n<p><img src=\"/posts_res/2018-04-28-emalgorithm/1.jpg\" alt=\"jensen\"></p>\n<p>图中，实线\\( f \\)是凸函数，\\(X\\)是随机变量，有0.5的概率是a，有0.5的概率是b。\\(X\\)的期望值就是a和b的中值了。\n图中可以看到\\( E[f(x)] \\geq f(EX) \\)成立。</p>\n<p>当f是（严格）凹函数当且仅当-f是（严格）凸函数。Jensen不等式应用于凹函数时，不等号方向反向，即\\( E[f(X)] \\leq f(EX) \\)。</p>\n<hr>\n<h3 id=\"算法推导\"><a href=\"#算法推导\" class=\"headerlink\" title=\"算法推导\"></a>算法推导</h3><p>给定的训练样本是\\( \\lbrace x^{(1)},…,x^{(m)} \\rbrace \\)，样例间独立，想找到每个样例隐含的类别\\( z \\)，能使得\\( p(x,z) \\)最大。\n\\( p(x,z) \\)的最大似然估计如下：</p>\n<p>\\[\nL(\\theta) = \\mathop{\\sum_{i=1}^m} log p(x^{(i)};\\theta) = \\mathop{\\sum_{i=1}^m} log \\sum_z p(x^{(i)},z;\\theta)\n\\]</p>\n<p>前一步是对极大似然取对数，后一步是对每个样例的每个可能类别\\(z\\)求联合分布概率和。</p>\n<p>对于每一个样例\\( x^{(i)}\\)，让\\( Q_i \\)表示该样例隐含变量\\( z \\)的某种分布，\\( Q_i\\)满足的条件是\\( \\sum_z Q_i(z) = 1, Q_i(z) \\geq 0 \\)。\n（如果\\(z\\)是连续性的，那么\\(Q_i\\)是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量\\( z \\)是身高，那么就是连续的高斯分布。\n如果按照隐藏变量是男女，那么就是伯努利分布了。</p>\n<p>可以由前面阐述的内容得到下面的公式：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum_i log p(x^{(i)}; \\theta)\n&amp; = \\sum_i log \\sum_{z} p(x^{(i)}, z; \\theta) \\\\\n&amp; = \\sum_i log \\sum_{z} Q_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\\\\n&amp; \\geq \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)}\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><em>第二个等号比较直接，就是分子分母同乘以一个相等的函数；最后不等号利用了Jensen不等式，考虑到 log(x) 是凹函数(二阶导数小于0)。而且</em>\n\\( \\sum_{z} Q_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\)\n<em>就是</em>\\( \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\)<em>的期望</em></p>\n<p>这个过程可以看作是对\\( L(\\theta) \\)求了下界。对于\\( Q_i\\)的选择，有多种可能，哪种更好的？\n假设\\( \\theta \\)已经给定，那么\\(L(\\theta)\\)的值就决定于\\( Q_i(z) \\)和\\( p(x^{(i)}, z) \\)了。\n可以通过调整这两个概率使下界不断上升，以逼近\\( L(\\theta) \\)的真实值，什么时候算是调整好了呢？\n当不等式变成等式时，说明调整后的概率能够等价于\\( L(\\theta) \\)了。按照这个思路，根据Jensen不等式，等式成立当且仅当随机变量为常数值，即：</p>\n<p>\\[\n\\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} = c\n\\]\n<em>c为常数，不依赖于\\(z\\)；</em></p>\n<p>对此式子做进一步推导，知道\\( \\sum_z Q_i(z)=1\\)，那么也就有\\( \\sum_z p(x^{(i)}, z; \\theta) = c \\)，那么有下式：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nQ_i(z)\n&amp; = \\frac{p(x^{(i)}, z; \\theta)}{\\sum_z p(x^{(i)}, z; \\theta)} \\\\\n&amp; = \\frac{p(x^{(i)}, z; \\theta)}{p(x^{(i)}; \\theta)} \\\\\n&amp; = p(z | x^{(i)}; \\theta)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>至此，推出了在固定其他参数\\(\\theta\\)后，\\( Q_i(z) \\)的计算公式就是后验概率，解决了\\( Q_i(z) \\)如何选择的问题。\n这一步就是E步，建立\\( L(\\theta) \\)的下界。接下来的M步，就是在给定\\( Q_i(z) \\)后，调整\\( \\theta\\)，极大化\\(L(\\theta) \\)的下界（在固定\\( Q_i(z) \\)后，下界还可以调整的更大）。</p>\n<p><strong>那么一般的EM算法的步骤如下：</strong></p>\n<ol>\n<li>选择参数的初始值\\(\\theta^{(0)}\\)，开始迭代；</li>\n<li>(E步)对于每一个 \\( i\\)，计算 \\( Q_i(z) = p(z | x^{(i)}; \\theta^{(t)}) \\)；</li>\n<li>(M步)计算 \\( \\theta^{(t+1)} = \\arg\\max_{\\theta^{(t)}} \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)} \\)；</li>\n<li>重复2-3步，直到收敛。</li>\n</ol>\n<hr>\n<h3 id=\"算法收敛性\"><a href=\"#算法收敛性\" class=\"headerlink\" title=\"算法收敛性\"></a>算法收敛性</h3><p>假定\\( \\theta^{(t)} \\)和\\( \\theta^{(t+1)} \\)是EM第 t 次和 t+1 次迭代后的结果。如果我们证明了\\( L(\\theta^{(t)}) \\leq L(\\theta^{(t+1)})\\)，也就是说最大似然估计单调增加，那么最终会到达最大似然估计的最大值。下面来证明，选定\\( \\theta^{(t)} \\)后，我们得到E步</p>\n<p>\\[\nQ_i(z) = p(z | x^{(i)}; \\theta^{(t)})\n\\]</p>\n<p>这一步保证了在给定\\( \\theta^{(t)} \\)时，Jensen不等式中的等式成立，也就是</p>\n<p>\\[\nL(\\theta) = \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)}\n\\]</p>\n<p>然后进行M步，固定\\( Q_i^{(t)}(z) \\)，并将\\( \\theta^{(t)} \\)视作变量，对上面的\\( L(\\theta^{(t)}) \\)求导后，得到\\( L(\\theta^{(t+1)}) \\)，这样经过一些推导会有以下式子成立：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nL(\\theta^{(t+1)})\n&amp; \\geq \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t+1)})}{Q_i(z)} \\\\\n&amp; \\geq \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)} \\\\\n&amp; = L(\\theta^{(t)})\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><em>第一个不等号，得到\\( \\theta^{(t+1)} \\)时，只是最大化\\( L(\\theta^{(t)}) \\)，也就是\\( L(\\theta^{(t+1)}) \\)的下界，而没有使等式成立，</em>\n<em>等式成立只有是在固定\\(\\theta\\)，并按E步得到\\( Q_i \\)时才能成立。</em></p>\n<p><em>况且根据前面得到的下式，对于所有的\\(\\theta\\)和\\( Q_i \\)都成立</em></p>\n<p>\\[\nL(\\theta) \\geq \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)}\n\\]</p>\n<p><em>第二个不等号利用了M步的定义，M步就是将\\( \\theta^{(t)} \\)调整到\\( \\theta^{(t+1)} \\)，使得下界最大化。</em></p>\n<p>这样就证明了\\( L(\\theta) \\)会单调增加。一种收敛方法是\\( L(\\theta) \\)不再变化，还有一种就是变化幅度很小。</p>\n<hr>\n<h3 id=\"算法举例\"><a href=\"#算法举例\" class=\"headerlink\" title=\"算法举例\"></a>算法举例</h3><p>假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H；(H代表证明朝上)。\na是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？</p>\n<p><img src=\"/posts_res/2018-04-28-emalgorithm/2.png\" alt=\"example\"></p>\n<p>针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的\\( \\hat{\\theta}_A \\)，这其实也是最大似然求导求出来的。</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathop{\\arg\\max}_{\\theta} log P(Y | \\theta)\n&amp; = log((\\theta_B^5(1-\\theta_B)^5) (\\theta_A^9(1-\\theta_A))(\\theta_A^8(1-\\theta_A)^2) (\\theta_B^4(1-\\theta_B)^6) (\\theta_A^7(1-\\theta_A)^3) ) \\\\ \n&amp; = log((\\theta_A^{24}(1-\\theta_A)^6) (\\theta_B^9(1-\\theta_B)^{11}))\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>上面这个式子求导之后就能得出\\( \\hat{\\theta}_A = \\frac{24}{24+6} = 0.80 以及 \\hat{\\theta}_B = \\frac{9}{9+11} = 0.45 \\)。</p>\n<p><br></p>\n<p>针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。</p>\n<p><strong>E步</strong>:计算在给定的\\( \\hat{\\theta}_A^{(0)} \\)和\\( \\hat{\\theta}_B^{(0)} \\)下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)</p>\n<p>\\[\nP(z=A | y_1, \\theta) = \\frac{P(z=A, y_1 | \\theta)}{P(z=A,y_1 | \\theta) + P(z=B, y_1 | \\theta)} = \\frac{0.6^5 \\cdot 0.4^5}{0.6^5 \\cdot 0.4^5 + 0.5^{10}} = 0.45\n\\]</p>\n<p><strong>M步</strong>:针对Q函数求导，在本题中Q函数形式如下，这里的\\( y_j\\)代表的是每次正面朝上的个数。</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nQ(\\theta, \\theta^{(i)})\n&amp; = \\sum_{j=1}^N \\sum_z P(z| y_j, \\theta^{(i)}) log P(y_j, z | \\theta) \\\\\n&amp; = \\sum_{j=1}^N \\mu_j log(\\theta_A^{y_j} (1-\\theta_A)^{10-y_j}) + (1-\\mu_j) log(\\theta_B^{y_j} (1-\\theta_B)^{10-y_j})\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>从而针对这个式子来对参数求导，例如对\\(\\theta_A\\)求导</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial Q}{\\partial \\theta_A}\n&amp; = \\mu_1 (\\frac{y_1}{\\theta_A} - \\frac{10-y_1}{1-\\theta_A}) + … + \\mu_5 ( \\frac{y_5}{\\theta_A} - \\frac{10-y_5}{1-\\theta_5} ) \\\\\n&amp; = \\mu_1 ( \\frac{y_1 - 10 \\theta_A}{\\theta_A (1-\\theta_A)} ) + … + \\mu_5 ( \\frac{y_5 - 10 \\theta_A}{\\theta_A (1-\\theta_A)} ) \\\\\n&amp; = \\frac{\\sum_{j=1}^5 \\mu_j y_j - \\sum_{j=1}^5 10 \\mu_j \\theta_A}{\\theta_A (1-\\theta_A)} \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>求导等于0之后就可得到图中的第一次迭代之后的参数值\\( \\hat{\\theta}_A^{(1)} = 0.71 和 \\hat{\\theta}_B^{(1)} = 0.58 \\)。</p>\n<p>这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。</p>\n<hr>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><p><a href=\"https://blog.csdn.net/u010866505/article/details/77877345\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/u010866505/article/details/77877345</a></p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html\" target=\"_blank\" rel=\"noopener\">EM算法 The EM Algorithm</a></li>\n<li><a href=\"https://chenrudan.github.io/blog/2015/12/02/emexample.html\" target=\"_blank\" rel=\"noopener\">机器学习算法系列之一 EM算法实例分析</a></li>\n<li><a href=\"https://blog.csdn.net/zouxy09/article/details/8537620/\" target=\"_blank\" rel=\"noopener\">从最大似然到EM算法浅解</a></li>\n<li><a href=\"https://blog.csdn.net/u010866505/article/details/77877345\" target=\"_blank\" rel=\"noopener\">EM算法原理和python简单实现</a></li>\n<li><a href=\"http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf\" target=\"_blank\" rel=\"noopener\">What is the expectation maximization algorithm?</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>最大似然</li>\n<li>Jensen不等式</li>\n<li>算法推导</li>\n<li>算法收敛性</li>\n<li>算法举例</li>\n<li>代码</li>\n</ul><hr><h3 id=\"最大似然-极大似然\"><a href=\"#最大似然-极大似然\" class=\"headerlink\" title=\"最大似然(极大似然)\"></a>最大似然(极大似然)</h3><p>最大似然估计是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。\n最大似然估计是建立在这样的思想上:已知某个参数能使这个样本出现的概率最大，当然不会再去选择其他小概率的样本，所以就把这个参数作为估计的真实值。</p><p>假设需要调查学校的男生和女生的身高分布。肯定是抽样解决，假设你在校园里随便地询问100个男生和100个女生。他们共200个人(也就是200个身高的样本数据，为了方便表示，下面说“人”的意思即对应其身高)都在教室里面了。那之后怎么办？通过某种办法将男女生分开，然后你就先统计抽样得到的100个男生的身高。\n假设他们的身高是服从高斯分布的，但是这个分布的均值 \\( \\mu \\) 和方差 \\( \\sigma^2 \\) 不知道，这两个参数是需要估计的。\n记作\\( \\theta=[\\mu, \\sigma]^T \\)。</p>","more":"\n\n\n\n\n<p>用数学的语言来说：在学校那么多男生（身高）中，我们独立地按照概率密度 \\( p(x|\\theta) \\) 抽取100个（身高）组成样本集\\( X \\).\n我们想通过样本集\\( X \\) 来估计出未知参数\\( \\theta \\)。已知概率密度 \\( p(x|\\theta) \\) 是高斯分布\\( N(\\mu, \\sigma) \\)的形式，其中的未知参数是\\( \\theta=[\\mu, \\sigma]^T \\)。抽到的样本集是\\( X = \\lbrace x_1, x_2, … , x_N \\rbrace \\)，其中\\( x_i \\)表示抽到的第\\(i\\)个人的身高，这里\\(N\\)就是100，表示抽到的样本个数。</p>\n<p>由于每个样本都是独立地从\\( p(x| \\theta) \\)中抽取的，且可以认为这些男生之间是没有关系的。\n那么从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？\n因为这些男生（的身高）是服从同一个高斯分布\\( p(x|\\theta) \\)的。那么抽到男生A（的身高）的概率是\\( p(x_A | \\theta) \\)，抽到男生B的概率是\\( p(x_B | \\theta) \\)。又因为它们是独立的，所以同时抽到男生A和男生B的概率是\\( p(x_A | \\theta) \\cdot p(x_B | \\theta) \\)；\n同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。换句话就是从分布是\\( p(x | \\theta) \\)的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：</p>\n<p>\\[\nL(\\theta) = L(x_1,…,x_N; \\theta) = \\mathop{\\prod_{i=1}^N} p(x_i;\\theta), \\quad \\theta \\in \\Theta\n\\]</p>\n<p>这个概率反映了，在概率密度函数的参数是\\(\\theta\\)时，得到\\( X\\)这组样本的概率。因为这里\\(X\\)是已知的，也就是说抽取到的这100个人的身高可以测出来，也就是已知的了。而\\(\\theta\\)是未知的，则上面这个公式只有\\(\\theta\\)是未知数，所以它是\\(\\theta\\)的函数。这个函数放映的是在不同的参数\\(\\theta\\)取值下，取得当前这个样本集的可能性，因此称为参数\\( \\theta\\)相对于样本集\\(X\\)的似然函数（likehood function）。\n记为\\( L(\\theta)\\)。</p>\n<p>在学校那么男生中，抽到这100个男生（表示身高）而不是其他人，则表示在整个学校中，这100个人（的身高）出现的概率最大。那么这个概率就可以用上面那个似然函数\\( L(\\theta) \\)来表示。只需要找到一个参数\\( \\theta \\)，其对应的似然函数\\( L(\\theta) \\)最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做\\( \\theta \\)的最大似然估计量，记为：</p>\n<p>\\[\n\\hat{\\theta} = \\mathop{\\arg\\max}_{\\theta} L(\\theta)\n\\]</p>\n<p><strong>因此，当知道每个样例属于的分布时，求解分布的参数直接利用最大似然估计或贝叶斯估计法即可。但是对于含有隐变量时，就不能直接使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。</strong></p>\n<hr>\n<h3 id=\"Jensen不等式\"><a href=\"#Jensen不等式\" class=\"headerlink\" title=\"Jensen不等式\"></a>Jensen不等式</h3><p>设\\(f\\)是定义域为实数的函数，如果对于所有的实数\\(x\\)，\\( f^{\\prime \\prime}(x) \\geq 0 \\)，那么\\(f\\)是凸函数；\n当\\(x\\)是向量时，如果其hessian矩阵H是半正定的(\\( H \\geq 0 \\))，那么\\( f\\)是凸函数。\n如果\\( f^{\\prime \\prime}(x) &gt; 0 \\)或者\\( H &gt; 0 \\)，那么称\\(f\\)是严格凸函数。</p>\n<p>Jensen不等式表述如下：</p>\n<p>如果\\(f\\)是凸函数，\\(X\\)是随机变量，那么\\( E[f(X)] \\geq f(EX) \\)。\n特别地，如果\\(f\\)是严格凸函数，那么\\( E[f(X)]=f(EX) \\)当且仅当\\( p(X=EX)=1 \\)，也就是说\\( X \\)是常量。</p>\n<p>下图表示会更清晰：</p>\n<p><img src=\"/posts_res/2018-04-28-emalgorithm/1.jpg\" alt=\"jensen\"></p>\n<p>图中，实线\\( f \\)是凸函数，\\(X\\)是随机变量，有0.5的概率是a，有0.5的概率是b。\\(X\\)的期望值就是a和b的中值了。\n图中可以看到\\( E[f(x)] \\geq f(EX) \\)成立。</p>\n<p>当f是（严格）凹函数当且仅当-f是（严格）凸函数。Jensen不等式应用于凹函数时，不等号方向反向，即\\( E[f(X)] \\leq f(EX) \\)。</p>\n<hr>\n<h3 id=\"算法推导\"><a href=\"#算法推导\" class=\"headerlink\" title=\"算法推导\"></a>算法推导</h3><p>给定的训练样本是\\( \\lbrace x^{(1)},…,x^{(m)} \\rbrace \\)，样例间独立，想找到每个样例隐含的类别\\( z \\)，能使得\\( p(x,z) \\)最大。\n\\( p(x,z) \\)的最大似然估计如下：</p>\n<p>\\[\nL(\\theta) = \\mathop{\\sum_{i=1}^m} log p(x^{(i)};\\theta) = \\mathop{\\sum_{i=1}^m} log \\sum_z p(x^{(i)},z;\\theta)\n\\]</p>\n<p>前一步是对极大似然取对数，后一步是对每个样例的每个可能类别\\(z\\)求联合分布概率和。</p>\n<p>对于每一个样例\\( x^{(i)}\\)，让\\( Q_i \\)表示该样例隐含变量\\( z \\)的某种分布，\\( Q_i\\)满足的条件是\\( \\sum_z Q_i(z) = 1, Q_i(z) \\geq 0 \\)。\n（如果\\(z\\)是连续性的，那么\\(Q_i\\)是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量\\( z \\)是身高，那么就是连续的高斯分布。\n如果按照隐藏变量是男女，那么就是伯努利分布了。</p>\n<p>可以由前面阐述的内容得到下面的公式：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum_i log p(x^{(i)}; \\theta)\n&amp; = \\sum_i log \\sum_{z} p(x^{(i)}, z; \\theta) \\\\\n&amp; = \\sum_i log \\sum_{z} Q_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\\\\n&amp; \\geq \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)}\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><em>第二个等号比较直接，就是分子分母同乘以一个相等的函数；最后不等号利用了Jensen不等式，考虑到 log(x) 是凹函数(二阶导数小于0)。而且</em>\n\\( \\sum_{z} Q_i(z) \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\)\n<em>就是</em>\\( \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} \\)<em>的期望</em></p>\n<p>这个过程可以看作是对\\( L(\\theta) \\)求了下界。对于\\( Q_i\\)的选择，有多种可能，哪种更好的？\n假设\\( \\theta \\)已经给定，那么\\(L(\\theta)\\)的值就决定于\\( Q_i(z) \\)和\\( p(x^{(i)}, z) \\)了。\n可以通过调整这两个概率使下界不断上升，以逼近\\( L(\\theta) \\)的真实值，什么时候算是调整好了呢？\n当不等式变成等式时，说明调整后的概率能够等价于\\( L(\\theta) \\)了。按照这个思路，根据Jensen不等式，等式成立当且仅当随机变量为常数值，即：</p>\n<p>\\[\n\\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)} = c\n\\]\n<em>c为常数，不依赖于\\(z\\)；</em></p>\n<p>对此式子做进一步推导，知道\\( \\sum_z Q_i(z)=1\\)，那么也就有\\( \\sum_z p(x^{(i)}, z; \\theta) = c \\)，那么有下式：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nQ_i(z)\n&amp; = \\frac{p(x^{(i)}, z; \\theta)}{\\sum_z p(x^{(i)}, z; \\theta)} \\\\\n&amp; = \\frac{p(x^{(i)}, z; \\theta)}{p(x^{(i)}; \\theta)} \\\\\n&amp; = p(z | x^{(i)}; \\theta)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>至此，推出了在固定其他参数\\(\\theta\\)后，\\( Q_i(z) \\)的计算公式就是后验概率，解决了\\( Q_i(z) \\)如何选择的问题。\n这一步就是E步，建立\\( L(\\theta) \\)的下界。接下来的M步，就是在给定\\( Q_i(z) \\)后，调整\\( \\theta\\)，极大化\\(L(\\theta) \\)的下界（在固定\\( Q_i(z) \\)后，下界还可以调整的更大）。</p>\n<p><strong>那么一般的EM算法的步骤如下：</strong></p>\n<ol>\n<li>选择参数的初始值\\(\\theta^{(0)}\\)，开始迭代；</li>\n<li>(E步)对于每一个 \\( i\\)，计算 \\( Q_i(z) = p(z | x^{(i)}; \\theta^{(t)}) \\)；</li>\n<li>(M步)计算 \\( \\theta^{(t+1)} = \\arg\\max_{\\theta^{(t)}} \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)} \\)；</li>\n<li>重复2-3步，直到收敛。</li>\n</ol>\n<hr>\n<h3 id=\"算法收敛性\"><a href=\"#算法收敛性\" class=\"headerlink\" title=\"算法收敛性\"></a>算法收敛性</h3><p>假定\\( \\theta^{(t)} \\)和\\( \\theta^{(t+1)} \\)是EM第 t 次和 t+1 次迭代后的结果。如果我们证明了\\( L(\\theta^{(t)}) \\leq L(\\theta^{(t+1)})\\)，也就是说最大似然估计单调增加，那么最终会到达最大似然估计的最大值。下面来证明，选定\\( \\theta^{(t)} \\)后，我们得到E步</p>\n<p>\\[\nQ_i(z) = p(z | x^{(i)}; \\theta^{(t)})\n\\]</p>\n<p>这一步保证了在给定\\( \\theta^{(t)} \\)时，Jensen不等式中的等式成立，也就是</p>\n<p>\\[\nL(\\theta) = \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)}\n\\]</p>\n<p>然后进行M步，固定\\( Q_i^{(t)}(z) \\)，并将\\( \\theta^{(t)} \\)视作变量，对上面的\\( L(\\theta^{(t)}) \\)求导后，得到\\( L(\\theta^{(t+1)}) \\)，这样经过一些推导会有以下式子成立：</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nL(\\theta^{(t+1)})\n&amp; \\geq \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t+1)})}{Q_i(z)} \\\\\n&amp; \\geq \\sum_i \\sum_{z} Q_i^{(t)}(z) log \\frac{p(x^{(i)}, z; \\theta^{(t)})}{Q_i(z)} \\\\\n&amp; = L(\\theta^{(t)})\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><em>第一个不等号，得到\\( \\theta^{(t+1)} \\)时，只是最大化\\( L(\\theta^{(t)}) \\)，也就是\\( L(\\theta^{(t+1)}) \\)的下界，而没有使等式成立，</em>\n<em>等式成立只有是在固定\\(\\theta\\)，并按E步得到\\( Q_i \\)时才能成立。</em></p>\n<p><em>况且根据前面得到的下式，对于所有的\\(\\theta\\)和\\( Q_i \\)都成立</em></p>\n<p>\\[\nL(\\theta) \\geq \\sum_i \\sum_{z} Q_i(z) log \\frac{p(x^{(i)}, z; \\theta)}{Q_i(z)}\n\\]</p>\n<p><em>第二个不等号利用了M步的定义，M步就是将\\( \\theta^{(t)} \\)调整到\\( \\theta^{(t+1)} \\)，使得下界最大化。</em></p>\n<p>这样就证明了\\( L(\\theta) \\)会单调增加。一种收敛方法是\\( L(\\theta) \\)不再变化，还有一种就是变化幅度很小。</p>\n<hr>\n<h3 id=\"算法举例\"><a href=\"#算法举例\" class=\"headerlink\" title=\"算法举例\"></a>算法举例</h3><p>假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H；(H代表证明朝上)。\na是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？</p>\n<p><img src=\"/posts_res/2018-04-28-emalgorithm/2.png\" alt=\"example\"></p>\n<p>针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的\\( \\hat{\\theta}_A \\)，这其实也是最大似然求导求出来的。</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\mathop{\\arg\\max}_{\\theta} log P(Y | \\theta)\n&amp; = log((\\theta_B^5(1-\\theta_B)^5) (\\theta_A^9(1-\\theta_A))(\\theta_A^8(1-\\theta_A)^2) (\\theta_B^4(1-\\theta_B)^6) (\\theta_A^7(1-\\theta_A)^3) ) \\\\ \n&amp; = log((\\theta_A^{24}(1-\\theta_A)^6) (\\theta_B^9(1-\\theta_B)^{11}))\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>上面这个式子求导之后就能得出\\( \\hat{\\theta}_A = \\frac{24}{24+6} = 0.80 以及 \\hat{\\theta}_B = \\frac{9}{9+11} = 0.45 \\)。</p>\n<p><br></p>\n<p>针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。</p>\n<p><strong>E步</strong>:计算在给定的\\( \\hat{\\theta}_A^{(0)} \\)和\\( \\hat{\\theta}_B^{(0)} \\)下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)</p>\n<p>\\[\nP(z=A | y_1, \\theta) = \\frac{P(z=A, y_1 | \\theta)}{P(z=A,y_1 | \\theta) + P(z=B, y_1 | \\theta)} = \\frac{0.6^5 \\cdot 0.4^5}{0.6^5 \\cdot 0.4^5 + 0.5^{10}} = 0.45\n\\]</p>\n<p><strong>M步</strong>:针对Q函数求导，在本题中Q函数形式如下，这里的\\( y_j\\)代表的是每次正面朝上的个数。</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nQ(\\theta, \\theta^{(i)})\n&amp; = \\sum_{j=1}^N \\sum_z P(z| y_j, \\theta^{(i)}) log P(y_j, z | \\theta) \\\\\n&amp; = \\sum_{j=1}^N \\mu_j log(\\theta_A^{y_j} (1-\\theta_A)^{10-y_j}) + (1-\\mu_j) log(\\theta_B^{y_j} (1-\\theta_B)^{10-y_j})\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>从而针对这个式子来对参数求导，例如对\\(\\theta_A\\)求导</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial Q}{\\partial \\theta_A}\n&amp; = \\mu_1 (\\frac{y_1}{\\theta_A} - \\frac{10-y_1}{1-\\theta_A}) + … + \\mu_5 ( \\frac{y_5}{\\theta_A} - \\frac{10-y_5}{1-\\theta_5} ) \\\\\n&amp; = \\mu_1 ( \\frac{y_1 - 10 \\theta_A}{\\theta_A (1-\\theta_A)} ) + … + \\mu_5 ( \\frac{y_5 - 10 \\theta_A}{\\theta_A (1-\\theta_A)} ) \\\\\n&amp; = \\frac{\\sum_{j=1}^5 \\mu_j y_j - \\sum_{j=1}^5 10 \\mu_j \\theta_A}{\\theta_A (1-\\theta_A)} \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>求导等于0之后就可得到图中的第一次迭代之后的参数值\\( \\hat{\\theta}_A^{(1)} = 0.71 和 \\hat{\\theta}_B^{(1)} = 0.58 \\)。</p>\n<p>这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。</p>\n<hr>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><p><a href=\"https://blog.csdn.net/u010866505/article/details/77877345\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/u010866505/article/details/77877345</a></p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html\" target=\"_blank\" rel=\"noopener\">EM算法 The EM Algorithm</a></li>\n<li><a href=\"https://chenrudan.github.io/blog/2015/12/02/emexample.html\" target=\"_blank\" rel=\"noopener\">机器学习算法系列之一 EM算法实例分析</a></li>\n<li><a href=\"https://blog.csdn.net/zouxy09/article/details/8537620/\" target=\"_blank\" rel=\"noopener\">从最大似然到EM算法浅解</a></li>\n<li><a href=\"https://blog.csdn.net/u010866505/article/details/77877345\" target=\"_blank\" rel=\"noopener\">EM算法原理和python简单实现</a></li>\n<li><a href=\"http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf\" target=\"_blank\" rel=\"noopener\">What is the expectation maximization algorithm?</a></li>\n</ol>\n"},{"layout":"post","title":"sklearn-GridSearchCV & hyperopt & hyperopt-sklearn 调参","date":"2018-04-23T13:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n**目录**\n\n* sklearn-GridSearchCV\n* hyperopt\n* hyperopt-sklearn\n\n---------\n\n### sklearn-GridSearchCV\n\n#### 常用参数\n\n[sklearn.model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n\n<table>\n <tr>\n  <th>参数</th> <th>含义</th> <th>其他</th>\n </tr>\n <tr>\n  <td>estimator</td> <td>所使用的模型</td> <td>假定这是scikit-learn中模型接口。该模型可以提供score方法或scoring参数</td>\n </tr>\n <tr>\n  <td>param_grid</td> <td>dict或list</td> <td>带有参数名称作为键的字典,例如param_grid=param_test, param_test={'n_estimators': range(1, 6)}</td>\n </tr>\n <tr>\n  <td>scoring</td> <td>评价标准，默认为None</td> <td>字符串，或是可调用对象，需要其函数形式如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数</td>\n </tr>\n <tr>\n  <td>cv</td> <td>交叉验证参数，默认为None，使用三折交叉验证</td> <td>整数指定交叉验证折数，也可以是交叉验证生成器</td>\n </tr>\n <tr>\n  <td>refit</td> <td>默认为True</td> <td>在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集</td>\n </tr>\n <tr>\n  <td>iid</td> <td>默认为True</td> <td>默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均</td>\n </tr>\n <tr>\n  <td>verbose</td> <td>默认为0</td> <td>日志冗长度。0：不输出训练过程；1：偶尔输出；>1：对每个子模型都输出</td>\n </tr>\n  <tr>\n  <td>n_jobs</td> <td>并行数，int类型</td> <td>-1：跟CPU核数一致； 1:默认值</td>\n </tr>\n <tr>\n  <td>pre_dispatch</td> <td>指定总共分发的并行任务数</td> <td>当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次</td>\n </tr>\n</table>\n\nscikit-learn内置可用评价标准如下：[scikit-learn model_evalution](http://scikit-learn.org/stable/modules/model_evaluation.html)\n\n![img](/posts_res/2018-04-23-tuneparameters/1.png)\n\n----------\n\n#### 常用方法\n\n<table>\n <tr>\n  <th>方法</th> <th>含义</th>\n </tr>\n <tr>\n  <td>grid.fit()</td> <td>运行网格搜索</td>\n </tr>\n <tr>\n  <td>grid.grid_scores_</td> <td>给出不同参数情况下的评价结果</td>\n </tr>\n <tr>\n  <td>grid.best_params_</td> <td>已取得最佳结果的参数的组合</td>\n </tr>\n <tr>\n  <td>grid.best_score_</td> <td>优化过程期间观察到的最好的评分</td>\n </tr>\n</table>\n\n---------\n\n#### 代码\n\n```python3\n# coding:utf-8\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\ndef grid_search(params, X, y):\n    svc = svm.SVC()\n    grid = GridSearchCV(estimator=svc,\n                        param_grid=param_test,\n                        scoring=\"accuracy\",\n                        cv=3,\n                        refit=True,\n                        iid=True,\n                        verbose=True)\n    grid.fit(X, y)\n    return grid\n\nif __name__ == '__main__':\n    iris = datasets.load_iris()\n    param_test = {'kernel':['linear', 'rbf'], 'C':[1, 10]}\n    estimator = grid_search(param_test, iris.data, iris.target)\n\n    print(\"grid_scores_:\", estimator.grid_scores_)\n    print(\"best_params_\", estimator.best_params_)\n    print(\"best_score_\", estimator.best_score_)\n    # Fitting 3 folds for each of 4 candidates, totalling 12 fits\n    # [Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n    # grid_scores_: [mean: 0.98000, std: 0.01602, params: {'kernel': 'linear', 'C': 1}, \n                     mean: 0.97333, std: 0.00897, params: {'kernel': 'rbf', 'C': 1}, \n                     mean: 0.97333, std: 0.03697, params: {'kernel': 'linear', 'C': 10}, \n                     mean: 0.98000, std: 0.01601, params: {'kernel': 'rbf', 'C': 10}]\n    # best_params_ {'kernel': 'linear', 'C': 1}\n    # best_score_ 0.98\n```\n\n---------\n\n### hyperopt\n\n[Hyperopt-Github](https://github.com/hyperopt/hyperopt)\n\nHyheropt四个重要的因素：\n* 指定需要最小化的函数(the objective function to minimize)；\n* 搜索的空间(the space over which to search)；\n* 采样的数据集(trails database)(可选)；\n* 搜索的算法(可选)\n\n**目标函数**，指定最小化的函数，比如要最小化函数\\\\( q(x,y) = x^2 + y^2 \\\\)\n\n**搜索的算法**，即hyperopt的fmin函数的algo参数的取值。\n当前支持的算法有随机搜索(hyperopt.rand.suggest)，模拟退火(hyperopt.anneal.suggest)，TPE算法。\n\n**搜索(参数)空间设置**，例如优化函数q，输入 fmin(q，space=hp.uniform(‘a’，0，1))。\nhp.uniform函数的第一个参数是标签，每个超参数在参数空间内必须具有独一无二的标签。\nhp.uniform指定了参数的分布。\n\n其他的参数分布：\n* hp.choice返回一个选项，选项可以是list或者tuple.options可以是嵌套的表达式，用于组成条件参数。 \n* hp.pchoice(label,p_options)以一定的概率返回一个p_options的一个选项。这个选项使得函数在搜索过程中对每个选项的可能性不均匀。 \n* hp.uniform(label,low,high)参数在low和high之间均匀分布。 \n* hp.quniform(label,low,high,q),参数的取值是round(uniform(low,high)/q)*q，适用于那些离散的取值。 \n* hp.loguniform(label,low,high)绘制exp(uniform(low,high)),变量的取值范围是[exp(low),exp(high)] \n* hp.randint(label,upper) 返回一个在[0,upper)前闭后开的区间内的随机整数。 \n\n搜索空间可以含有list和dictionary。\n```python3\nfrom hyperopt import hp\n\nlist_space = [hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1)]\ntuple_space = (hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1))\ndict_space = {'a': hp.uniform('a', 0, 1), 'b': hp.loguniform('b', 0, 1)}\n```\n\n**sample函数在参数空间内采样**\n\n```python3\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\nlist_space = [hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1), hp.randint('c', 8)]\nprint(sample(list_space))\n# (0.7802721043817558, 1.6616883461586371, array(4))\n```\n\n---------\n\n#### 简单的例子\n\n```python3\nfrom hyperopt import fmin, hp, tpe\ndef objective(args):\n    x, y = args\n    return x**2 + y**2 + 1\nspace = [hp.randint('x', 5), hp.randint('y', 5)]\nbest = fmin(objective, space=space, algo=tpe.suggest, max_evals=100)\nprint(best)\n# {'y': 0, 'x': 0}\n```\n\n--------\n\n#### Perceptron鸢尾花例子\n\n使用感知器判别鸢尾花，使用的学习率是0.1，迭代40次得到了一个测试集上正确率为82%的结果；使用hyperopt优化参数，将正确率提升到了91%。\n\n```python3\n# coding:utf-8\n\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom hyperopt import fmin, tpe, hp, partial\n\n# load data set\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# data standard\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n# create perceptron model\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\nppn.fit(X_train_std, y_train)\n\n# show result before tune parameters\ny_pred = ppn.predict(X_test_std)\nprint(\"=\"*15, \"before tune\", \"=\"*15)\nprint(accuracy_score(y_test, y_pred))\n\n# =============================================================================================\n\n# define object which hyperopt use\ndef percept(args):\n    global X_train_std, y_train, y_test\n    ppn = Perceptron(n_iter=int(args[\"n_iter\"]), eta0=args[\"eta\"] * 0.01, random_state=0)\n    ppn.fit(X_train_std, y_train)\n    y_pred = ppn.predict(X_test_std)\n    return -accuracy_score(y_test, y_pred)\n\n# define search space\nspace = {\"n_iter\": hp.choice(\"n_iter\", range(30, 50)), \"eta\": hp.uniform(\"eta\", 0.05, 0.5)}\n# define search algorithm\nalgo = partial(tpe.suggest, n_startup_jobs=10)\n\nbest = fmin(percept, space, algo=algo, max_evals=100)\nprint(\"=\"*15, \"after tune\", \"=\"*15)\nprint(best)\nprint(percept(best))\n```\n\n**结果**\n\n由于使用tpe搜索算法，每次搜索的结果都不一样，不稳定。\n\n```text\n=============== before tune ===============\n0.822222222222\n=============== after tune ===============\n{'n_iter': 14, 'eta': 0.12949436553904228}\n-0.911111111111\n```\n\n--------------\n\n#### xgboost癌症例子\n\n```python3\n# coding:utf-8\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.datasets import load_breast_cancer\nfrom hyperopt import fmin, tpe, hp, partial\n\n\ndef loadData():\n    d = load_breast_cancer()\n    data, target = d.data, d.target\n    minmaxscaler = MinMaxScaler()\n    data_scal = minmaxscaler.fit_transform(data)\n\n    indices = np.random.permutation(target.shape[0])\n    train_indices = indices[: int(target.shape[0] * 0.8)]\n    test_indices = indices[int(target.shape[0] * 0.8):]\n    train_x = data[train_indices]\n    test_x = data[test_indices]\n    train_y = target[train_indices]\n    test_y = target[test_indices]\n\n    return train_x, test_x, train_y, test_y\n\ntrain_x, test_x, train_y, test_y = loadData()\nprint(\"train_x:\", train_x.shape, \" test_x:\", test_x.shape, \" train_y:\", train_y.shape, \" test_y:\", test_y.shape)\n\n\n# define object function which need to minimize\ndef GBM(argsDict):\n    max_depth = argsDict[\"max_depth\"] + 5\n    n_estimators = argsDict['n_estimators'] * 5 + 50\n    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n    min_child_weight = argsDict[\"min_child_weight\"] + 1\n    print(\"=\"*20)\n    print(\"max_depth:\" + str(max_depth))\n    print(\"n_estimator:\" + str(n_estimators))\n    print(\"learning_rate:\" + str(learning_rate))\n    print(\"subsample:\" + str(subsample))\n    print(\"min_child_weight:\" + str(min_child_weight))\n    global train_x, train_y\n\n    gbm = xgb.XGBClassifier(nthread=4,  # 进程数\n                            max_depth=max_depth,  # 最大深度\n                            n_estimators=n_estimators,  # 树的数量\n                            learning_rate=learning_rate,  # 学习率\n                            subsample=subsample,  # 采样数\n                            min_child_weight=min_child_weight,  # 孩子数\n                            max_delta_step=10,  # 10步不降则停止\n                            objective=\"binary:logistic\")\n    metric = cross_val_score(gbm, train_x, train_y, cv=5, scoring=\"roc_auc\").mean()\n    print(\"cross_val_score:\", metric, \"\\n\")\n    return -metric\n\n\n# define search space\nspace = {\"max_depth\": hp.randint(\"max_depth\", 15),\n         \"n_estimators\": hp.randint(\"n_estimators\", 10),  # [0,1,2,3,4,5] -> [50,]\n         \"learning_rate\": hp.randint(\"learning_rate\", 6),  # [0,1,2,3,4,5] -> 0.05,0.06\n         \"subsample\": hp.randint(\"subsample\", 4),  # [0,1,2,3] -> [0.7,0.8,0.9,1.0]\n         \"min_child_weight\": hp.randint(\"min_child_weight\", 5),  # [0,1,2,3,4] -> [1,2,3,4,5]\n         }\n# define search algorithm\nalgo = partial(tpe.suggest, n_startup_jobs=1)\n# search best parameters\nbest = fmin(GBM, space, algo=algo, max_evals=4)\n\nprint(\"best param:\", best) # output best parameters\nprint(\"best result:\", GBM(best)) # output model result in best parameters\n```\n\n**结果**\n\n```text\ntrain_x: (455, 30)  test_x: (114, 30)  train_y: (455,)  test_y: (114,)\n====================\nmax_depth:16\nn_estimator:60\nlearning_rate:0.07\nsubsample:0.7999999999999999\nmin_child_weight:5\ncross_val_score: 0.988504345257 \n\n====================\nmax_depth:6\nn_estimator:60\nlearning_rate:0.15000000000000002\nsubsample:0.7\nmin_child_weight:5\ncross_val_score: 0.990053680824 \n\n====================\nmax_depth:8\nn_estimator:95\nlearning_rate:0.13\nsubsample:0.8999999999999999\nmin_child_weight:5\ncross_val_score: 0.988304093567 \n\n====================\nmax_depth:12\nn_estimator:60\nlearning_rate:0.09\nsubsample:0.7\nmin_child_weight:3\ncross_val_score: 0.990361013789 \n\nbest param: {'n_estimators': 2, 'max_depth': 7, 'subsample': 0, 'learning_rate': 2, 'min_child_weight': 2}\n====================\nmax_depth:12\nn_estimator:60\nlearning_rate:0.09\nsubsample:0.7\nmin_child_weight:3\ncross_val_score: 0.990361013789 \n\nbest result: -0.990361013789\n```\n\n\n-----------\n\n### hyperopt-sklearn\n\n[hyperopt-sklearn-Github](https://github.com/hyperopt/hyperopt-sklearn/tree/master)\n\n目前hyperopt-sklearn只支持部分Classifiers\\Regressors\\Preprocessing，具体请见上面hyperopt-sklearn的Github主页。\n\n#### 安装\n\n```shell\ngit clone git@github.com:hyperopt/hyperopt-sklearn.git\n(cd hyperopt-sklearn && pip install -e .)\n```\n\n#### 使用模版\n\n```python3\nfrom hpsklearn import HyperoptEstimator, svc\nfrom sklearn import svm\n\n# Load Data\n# ...\n\nif use_hpsklearn:\n    estim = HyperoptEstimator(classifier=svc('mySVC'))\nelse:\n    estim = svm.SVC()\n\nestim.fit(X_train, y_train)\n\nprint(estim.score(X_test, y_test))\n```\n\n#### 鸢尾花例子\n\n```python3\n# coding:utf-8\n\nfrom hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\nfrom sklearn.datasets import load_iris\nfrom hyperopt import tpe\nimport numpy as np\n\n# Download the data and split into training and test sets\niris = load_iris()\nX = iris.data\ny = iris.target\ntest_size = int(0.2 * len(y))\nnp.random.seed(13)\nindices = np.random.permutation(len(X))\ntrain_x = X[indices[:-test_size]]\ntrain_y = y[indices[:-test_size]]\ntest_x = X[indices[-test_size:]]\ntest_y = y[indices[-test_size:]]\n\n# Instantiate a HyperoptEstimator with the search space and number of evaluations\nestim = HyperoptEstimator(classifier=any_classifier('my_clf'),\n                          preprocessing=any_preprocessing('my_pre'),\n                          algo=tpe.suggest,\n                          max_evals=100,\n                          trial_timeout=120)\n\n# Search the hyperparameter space based on the data\nestim.fit(train_x, train_x)\n\n# Show the results\nprint(estim.score(test_x, test_y))\n# 1.0\nprint(estim.best_model())\n# {'learner': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n#           max_depth=3, max_features='log2', max_leaf_nodes=None,\n#           min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=1, min_samples_split=2,\n#           min_weight_fraction_leaf=0.0, n_estimators=13, n_jobs=1,\n#           oob_score=False, random_state=1, verbose=False,\n#           warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n```\n\n#### MNIST例子\n\n```python3\n# coding:utf-8\n\nfrom hpsklearn import HyperoptEstimator, extra_trees\nfrom sklearn.datasets import fetch_mldata\nfrom hyperopt import tpe\nimport numpy as np\n\n# Download the data and split into training and test sets\ndigits = fetch_mldata('MNIST original')\n\nX = digits.data\ny = digits.target\ntest_size = int(0.2 * len(y))\nnp.random.seed(13)\nindices = np.random.permutation(len(X))\nX_train = X[indices[:-test_size]]\ny_train = y[indices[:-test_size]]\nX_test = X[indices[-test_size:]]\ny_test = y[indices[-test_size:]]\n\n# Instantiate a HyperoptEstimator with the search space and number of evaluations\nestim = HyperoptEstimator(classifier=extra_trees('my_clf'),\n                          preprocessing=[],\n                          algo=tpe.suggest,\n                          max_evals=10,\n                          trial_timeout=300)\n\n# Search the hyperparameter space based on the data\nestim.fit(X_train, y_train)\n\n# Show the results\nprint(estim.score(X_test, y_test))\n# 0.962785714286\nprint(estim.best_model())\n# {'learner': ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n#           max_depth=None, max_features=0.959202875857,\n#           max_leaf_nodes=None, min_impurity_decrease=0.0,\n#           min_impurity_split=None, min_samples_leaf=1,\n#           min_samples_split=2, min_weight_fraction_leaf=0.0,\n#           n_estimators=20, n_jobs=1, oob_score=False, random_state=3,\n#           verbose=False, warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n```\n\n\n---------\n\n### 参考\n\n>\n1. [sklearn-GridSearchCV,CV调节超参使用方法](https://blog.csdn.net/u012969412/article/details/72973055)\n2. [python调参神器hyperopt-简略](http://www.cnblogs.com/gczr/p/7156270.html)\n3. [python调参神器hyperopt-详细](https://blog.csdn.net/qq_34139222/article/details/60322995)\n\n","source":"_posts/2018-04-23-tuneparameters.md","raw":"---\nlayout: post\ntitle: sklearn-GridSearchCV & hyperopt & hyperopt-sklearn 调参\ndate: 2018-04-23 21:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n\n**目录**\n\n* sklearn-GridSearchCV\n* hyperopt\n* hyperopt-sklearn\n\n---------\n\n### sklearn-GridSearchCV\n\n#### 常用参数\n\n[sklearn.model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n\n<table>\n <tr>\n  <th>参数</th> <th>含义</th> <th>其他</th>\n </tr>\n <tr>\n  <td>estimator</td> <td>所使用的模型</td> <td>假定这是scikit-learn中模型接口。该模型可以提供score方法或scoring参数</td>\n </tr>\n <tr>\n  <td>param_grid</td> <td>dict或list</td> <td>带有参数名称作为键的字典,例如param_grid=param_test, param_test={'n_estimators': range(1, 6)}</td>\n </tr>\n <tr>\n  <td>scoring</td> <td>评价标准，默认为None</td> <td>字符串，或是可调用对象，需要其函数形式如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数</td>\n </tr>\n <tr>\n  <td>cv</td> <td>交叉验证参数，默认为None，使用三折交叉验证</td> <td>整数指定交叉验证折数，也可以是交叉验证生成器</td>\n </tr>\n <tr>\n  <td>refit</td> <td>默认为True</td> <td>在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集</td>\n </tr>\n <tr>\n  <td>iid</td> <td>默认为True</td> <td>默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均</td>\n </tr>\n <tr>\n  <td>verbose</td> <td>默认为0</td> <td>日志冗长度。0：不输出训练过程；1：偶尔输出；>1：对每个子模型都输出</td>\n </tr>\n  <tr>\n  <td>n_jobs</td> <td>并行数，int类型</td> <td>-1：跟CPU核数一致； 1:默认值</td>\n </tr>\n <tr>\n  <td>pre_dispatch</td> <td>指定总共分发的并行任务数</td> <td>当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次</td>\n </tr>\n</table>\n\nscikit-learn内置可用评价标准如下：[scikit-learn model_evalution](http://scikit-learn.org/stable/modules/model_evaluation.html)\n\n![img](/posts_res/2018-04-23-tuneparameters/1.png)\n\n----------\n\n#### 常用方法\n\n<table>\n <tr>\n  <th>方法</th> <th>含义</th>\n </tr>\n <tr>\n  <td>grid.fit()</td> <td>运行网格搜索</td>\n </tr>\n <tr>\n  <td>grid.grid_scores_</td> <td>给出不同参数情况下的评价结果</td>\n </tr>\n <tr>\n  <td>grid.best_params_</td> <td>已取得最佳结果的参数的组合</td>\n </tr>\n <tr>\n  <td>grid.best_score_</td> <td>优化过程期间观察到的最好的评分</td>\n </tr>\n</table>\n\n---------\n\n#### 代码\n\n```python3\n# coding:utf-8\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\ndef grid_search(params, X, y):\n    svc = svm.SVC()\n    grid = GridSearchCV(estimator=svc,\n                        param_grid=param_test,\n                        scoring=\"accuracy\",\n                        cv=3,\n                        refit=True,\n                        iid=True,\n                        verbose=True)\n    grid.fit(X, y)\n    return grid\n\nif __name__ == '__main__':\n    iris = datasets.load_iris()\n    param_test = {'kernel':['linear', 'rbf'], 'C':[1, 10]}\n    estimator = grid_search(param_test, iris.data, iris.target)\n\n    print(\"grid_scores_:\", estimator.grid_scores_)\n    print(\"best_params_\", estimator.best_params_)\n    print(\"best_score_\", estimator.best_score_)\n    # Fitting 3 folds for each of 4 candidates, totalling 12 fits\n    # [Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n    # grid_scores_: [mean: 0.98000, std: 0.01602, params: {'kernel': 'linear', 'C': 1}, \n                     mean: 0.97333, std: 0.00897, params: {'kernel': 'rbf', 'C': 1}, \n                     mean: 0.97333, std: 0.03697, params: {'kernel': 'linear', 'C': 10}, \n                     mean: 0.98000, std: 0.01601, params: {'kernel': 'rbf', 'C': 10}]\n    # best_params_ {'kernel': 'linear', 'C': 1}\n    # best_score_ 0.98\n```\n\n---------\n\n### hyperopt\n\n[Hyperopt-Github](https://github.com/hyperopt/hyperopt)\n\nHyheropt四个重要的因素：\n* 指定需要最小化的函数(the objective function to minimize)；\n* 搜索的空间(the space over which to search)；\n* 采样的数据集(trails database)(可选)；\n* 搜索的算法(可选)\n\n**目标函数**，指定最小化的函数，比如要最小化函数\\\\( q(x,y) = x^2 + y^2 \\\\)\n\n**搜索的算法**，即hyperopt的fmin函数的algo参数的取值。\n当前支持的算法有随机搜索(hyperopt.rand.suggest)，模拟退火(hyperopt.anneal.suggest)，TPE算法。\n\n**搜索(参数)空间设置**，例如优化函数q，输入 fmin(q，space=hp.uniform(‘a’，0，1))。\nhp.uniform函数的第一个参数是标签，每个超参数在参数空间内必须具有独一无二的标签。\nhp.uniform指定了参数的分布。\n\n其他的参数分布：\n* hp.choice返回一个选项，选项可以是list或者tuple.options可以是嵌套的表达式，用于组成条件参数。 \n* hp.pchoice(label,p_options)以一定的概率返回一个p_options的一个选项。这个选项使得函数在搜索过程中对每个选项的可能性不均匀。 \n* hp.uniform(label,low,high)参数在low和high之间均匀分布。 \n* hp.quniform(label,low,high,q),参数的取值是round(uniform(low,high)/q)*q，适用于那些离散的取值。 \n* hp.loguniform(label,low,high)绘制exp(uniform(low,high)),变量的取值范围是[exp(low),exp(high)] \n* hp.randint(label,upper) 返回一个在[0,upper)前闭后开的区间内的随机整数。 \n\n搜索空间可以含有list和dictionary。\n```python3\nfrom hyperopt import hp\n\nlist_space = [hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1)]\ntuple_space = (hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1))\ndict_space = {'a': hp.uniform('a', 0, 1), 'b': hp.loguniform('b', 0, 1)}\n```\n\n**sample函数在参数空间内采样**\n\n```python3\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\nlist_space = [hp.uniform('a', 0, 1), hp.loguniform('b', 0, 1), hp.randint('c', 8)]\nprint(sample(list_space))\n# (0.7802721043817558, 1.6616883461586371, array(4))\n```\n\n---------\n\n#### 简单的例子\n\n```python3\nfrom hyperopt import fmin, hp, tpe\ndef objective(args):\n    x, y = args\n    return x**2 + y**2 + 1\nspace = [hp.randint('x', 5), hp.randint('y', 5)]\nbest = fmin(objective, space=space, algo=tpe.suggest, max_evals=100)\nprint(best)\n# {'y': 0, 'x': 0}\n```\n\n--------\n\n#### Perceptron鸢尾花例子\n\n使用感知器判别鸢尾花，使用的学习率是0.1，迭代40次得到了一个测试集上正确率为82%的结果；使用hyperopt优化参数，将正确率提升到了91%。\n\n```python3\n# coding:utf-8\n\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Perceptron\nfrom hyperopt import fmin, tpe, hp, partial\n\n# load data set\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# data standard\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n# create perceptron model\nppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)\nppn.fit(X_train_std, y_train)\n\n# show result before tune parameters\ny_pred = ppn.predict(X_test_std)\nprint(\"=\"*15, \"before tune\", \"=\"*15)\nprint(accuracy_score(y_test, y_pred))\n\n# =============================================================================================\n\n# define object which hyperopt use\ndef percept(args):\n    global X_train_std, y_train, y_test\n    ppn = Perceptron(n_iter=int(args[\"n_iter\"]), eta0=args[\"eta\"] * 0.01, random_state=0)\n    ppn.fit(X_train_std, y_train)\n    y_pred = ppn.predict(X_test_std)\n    return -accuracy_score(y_test, y_pred)\n\n# define search space\nspace = {\"n_iter\": hp.choice(\"n_iter\", range(30, 50)), \"eta\": hp.uniform(\"eta\", 0.05, 0.5)}\n# define search algorithm\nalgo = partial(tpe.suggest, n_startup_jobs=10)\n\nbest = fmin(percept, space, algo=algo, max_evals=100)\nprint(\"=\"*15, \"after tune\", \"=\"*15)\nprint(best)\nprint(percept(best))\n```\n\n**结果**\n\n由于使用tpe搜索算法，每次搜索的结果都不一样，不稳定。\n\n```text\n=============== before tune ===============\n0.822222222222\n=============== after tune ===============\n{'n_iter': 14, 'eta': 0.12949436553904228}\n-0.911111111111\n```\n\n--------------\n\n#### xgboost癌症例子\n\n```python3\n# coding:utf-8\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.datasets import load_breast_cancer\nfrom hyperopt import fmin, tpe, hp, partial\n\n\ndef loadData():\n    d = load_breast_cancer()\n    data, target = d.data, d.target\n    minmaxscaler = MinMaxScaler()\n    data_scal = minmaxscaler.fit_transform(data)\n\n    indices = np.random.permutation(target.shape[0])\n    train_indices = indices[: int(target.shape[0] * 0.8)]\n    test_indices = indices[int(target.shape[0] * 0.8):]\n    train_x = data[train_indices]\n    test_x = data[test_indices]\n    train_y = target[train_indices]\n    test_y = target[test_indices]\n\n    return train_x, test_x, train_y, test_y\n\ntrain_x, test_x, train_y, test_y = loadData()\nprint(\"train_x:\", train_x.shape, \" test_x:\", test_x.shape, \" train_y:\", train_y.shape, \" test_y:\", test_y.shape)\n\n\n# define object function which need to minimize\ndef GBM(argsDict):\n    max_depth = argsDict[\"max_depth\"] + 5\n    n_estimators = argsDict['n_estimators'] * 5 + 50\n    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n    min_child_weight = argsDict[\"min_child_weight\"] + 1\n    print(\"=\"*20)\n    print(\"max_depth:\" + str(max_depth))\n    print(\"n_estimator:\" + str(n_estimators))\n    print(\"learning_rate:\" + str(learning_rate))\n    print(\"subsample:\" + str(subsample))\n    print(\"min_child_weight:\" + str(min_child_weight))\n    global train_x, train_y\n\n    gbm = xgb.XGBClassifier(nthread=4,  # 进程数\n                            max_depth=max_depth,  # 最大深度\n                            n_estimators=n_estimators,  # 树的数量\n                            learning_rate=learning_rate,  # 学习率\n                            subsample=subsample,  # 采样数\n                            min_child_weight=min_child_weight,  # 孩子数\n                            max_delta_step=10,  # 10步不降则停止\n                            objective=\"binary:logistic\")\n    metric = cross_val_score(gbm, train_x, train_y, cv=5, scoring=\"roc_auc\").mean()\n    print(\"cross_val_score:\", metric, \"\\n\")\n    return -metric\n\n\n# define search space\nspace = {\"max_depth\": hp.randint(\"max_depth\", 15),\n         \"n_estimators\": hp.randint(\"n_estimators\", 10),  # [0,1,2,3,4,5] -> [50,]\n         \"learning_rate\": hp.randint(\"learning_rate\", 6),  # [0,1,2,3,4,5] -> 0.05,0.06\n         \"subsample\": hp.randint(\"subsample\", 4),  # [0,1,2,3] -> [0.7,0.8,0.9,1.0]\n         \"min_child_weight\": hp.randint(\"min_child_weight\", 5),  # [0,1,2,3,4] -> [1,2,3,4,5]\n         }\n# define search algorithm\nalgo = partial(tpe.suggest, n_startup_jobs=1)\n# search best parameters\nbest = fmin(GBM, space, algo=algo, max_evals=4)\n\nprint(\"best param:\", best) # output best parameters\nprint(\"best result:\", GBM(best)) # output model result in best parameters\n```\n\n**结果**\n\n```text\ntrain_x: (455, 30)  test_x: (114, 30)  train_y: (455,)  test_y: (114,)\n====================\nmax_depth:16\nn_estimator:60\nlearning_rate:0.07\nsubsample:0.7999999999999999\nmin_child_weight:5\ncross_val_score: 0.988504345257 \n\n====================\nmax_depth:6\nn_estimator:60\nlearning_rate:0.15000000000000002\nsubsample:0.7\nmin_child_weight:5\ncross_val_score: 0.990053680824 \n\n====================\nmax_depth:8\nn_estimator:95\nlearning_rate:0.13\nsubsample:0.8999999999999999\nmin_child_weight:5\ncross_val_score: 0.988304093567 \n\n====================\nmax_depth:12\nn_estimator:60\nlearning_rate:0.09\nsubsample:0.7\nmin_child_weight:3\ncross_val_score: 0.990361013789 \n\nbest param: {'n_estimators': 2, 'max_depth': 7, 'subsample': 0, 'learning_rate': 2, 'min_child_weight': 2}\n====================\nmax_depth:12\nn_estimator:60\nlearning_rate:0.09\nsubsample:0.7\nmin_child_weight:3\ncross_val_score: 0.990361013789 \n\nbest result: -0.990361013789\n```\n\n\n-----------\n\n### hyperopt-sklearn\n\n[hyperopt-sklearn-Github](https://github.com/hyperopt/hyperopt-sklearn/tree/master)\n\n目前hyperopt-sklearn只支持部分Classifiers\\Regressors\\Preprocessing，具体请见上面hyperopt-sklearn的Github主页。\n\n#### 安装\n\n```shell\ngit clone git@github.com:hyperopt/hyperopt-sklearn.git\n(cd hyperopt-sklearn && pip install -e .)\n```\n\n#### 使用模版\n\n```python3\nfrom hpsklearn import HyperoptEstimator, svc\nfrom sklearn import svm\n\n# Load Data\n# ...\n\nif use_hpsklearn:\n    estim = HyperoptEstimator(classifier=svc('mySVC'))\nelse:\n    estim = svm.SVC()\n\nestim.fit(X_train, y_train)\n\nprint(estim.score(X_test, y_test))\n```\n\n#### 鸢尾花例子\n\n```python3\n# coding:utf-8\n\nfrom hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\nfrom sklearn.datasets import load_iris\nfrom hyperopt import tpe\nimport numpy as np\n\n# Download the data and split into training and test sets\niris = load_iris()\nX = iris.data\ny = iris.target\ntest_size = int(0.2 * len(y))\nnp.random.seed(13)\nindices = np.random.permutation(len(X))\ntrain_x = X[indices[:-test_size]]\ntrain_y = y[indices[:-test_size]]\ntest_x = X[indices[-test_size:]]\ntest_y = y[indices[-test_size:]]\n\n# Instantiate a HyperoptEstimator with the search space and number of evaluations\nestim = HyperoptEstimator(classifier=any_classifier('my_clf'),\n                          preprocessing=any_preprocessing('my_pre'),\n                          algo=tpe.suggest,\n                          max_evals=100,\n                          trial_timeout=120)\n\n# Search the hyperparameter space based on the data\nestim.fit(train_x, train_x)\n\n# Show the results\nprint(estim.score(test_x, test_y))\n# 1.0\nprint(estim.best_model())\n# {'learner': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n#           max_depth=3, max_features='log2', max_leaf_nodes=None,\n#           min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=1, min_samples_split=2,\n#           min_weight_fraction_leaf=0.0, n_estimators=13, n_jobs=1,\n#           oob_score=False, random_state=1, verbose=False,\n#           warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n```\n\n#### MNIST例子\n\n```python3\n# coding:utf-8\n\nfrom hpsklearn import HyperoptEstimator, extra_trees\nfrom sklearn.datasets import fetch_mldata\nfrom hyperopt import tpe\nimport numpy as np\n\n# Download the data and split into training and test sets\ndigits = fetch_mldata('MNIST original')\n\nX = digits.data\ny = digits.target\ntest_size = int(0.2 * len(y))\nnp.random.seed(13)\nindices = np.random.permutation(len(X))\nX_train = X[indices[:-test_size]]\ny_train = y[indices[:-test_size]]\nX_test = X[indices[-test_size:]]\ny_test = y[indices[-test_size:]]\n\n# Instantiate a HyperoptEstimator with the search space and number of evaluations\nestim = HyperoptEstimator(classifier=extra_trees('my_clf'),\n                          preprocessing=[],\n                          algo=tpe.suggest,\n                          max_evals=10,\n                          trial_timeout=300)\n\n# Search the hyperparameter space based on the data\nestim.fit(X_train, y_train)\n\n# Show the results\nprint(estim.score(X_test, y_test))\n# 0.962785714286\nprint(estim.best_model())\n# {'learner': ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n#           max_depth=None, max_features=0.959202875857,\n#           max_leaf_nodes=None, min_impurity_decrease=0.0,\n#           min_impurity_split=None, min_samples_leaf=1,\n#           min_samples_split=2, min_weight_fraction_leaf=0.0,\n#           n_estimators=20, n_jobs=1, oob_score=False, random_state=3,\n#           verbose=False, warm_start=False), 'preprocs': (), 'ex_preprocs': ()}\n```\n\n\n---------\n\n### 参考\n\n>\n1. [sklearn-GridSearchCV,CV调节超参使用方法](https://blog.csdn.net/u012969412/article/details/72973055)\n2. [python调参神器hyperopt-简略](http://www.cnblogs.com/gczr/p/7156270.html)\n3. [python调参神器hyperopt-详细](https://blog.csdn.net/qq_34139222/article/details/60322995)\n\n","slug":"tuneparameters","published":1,"updated":"2019-08-17T09:35:44.502Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvt001v2qwpwk5omud2","content":"<p><strong>目录</strong></p><ul>\n<li>sklearn-GridSearchCV</li>\n<li>hyperopt</li>\n<li>hyperopt-sklearn</li>\n</ul><hr><h3 id=\"sklearn-GridSearchCV\"><a href=\"#sklearn-GridSearchCV\" class=\"headerlink\" title=\"sklearn-GridSearchCV\"></a>sklearn-GridSearchCV</h3><h4 id=\"常用参数\"><a href=\"#常用参数\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\" rel=\"noopener\">sklearn.model_selection.GridSearchCV</a></p><table>\n <tr>\n  <th>参数</th> <th>含义</th> <th>其他</th>\n </tr>\n <tr>\n  <td>estimator</td> <td>所使用的模型</td> <td>假定这是scikit-learn中模型接口。该模型可以提供score方法或scoring参数</td>\n </tr>\n <tr>\n  <td>param_grid</td> <td>dict或list</td> <td>带有参数名称作为键的字典,例如param_grid=param_test, param_test={'n_estimators': range(1, 6)}</td>\n </tr>\n <tr>\n  <td>scoring</td> <td>评价标准，默认为None</td> <td>字符串，或是可调用对象，需要其函数形式如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数</td>\n </tr>\n <tr>\n  <td>cv</td> <td>交叉验证参数，默认为None，使用三折交叉验证</td> <td>整数指定交叉验证折数，也可以是交叉验证生成器</td>\n </tr>\n <tr>\n  <td>refit</td> <td>默认为True</td> <td>在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集</td>\n </tr>\n <tr>\n  <td>iid</td> <td>默认为True</td> <td>默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均</td>\n </tr>\n <tr>\n  <td>verbose</td> <td>默认为0</td> <td>日志冗长度。0：不输出训练过程；1：偶尔输出；>1：对每个子模型都输出</td>\n </tr>\n  <tr>\n  <td>n_jobs</td> <td>并行数，int类型</td> <td>-1：跟CPU核数一致； 1:默认值</td>\n </tr>\n <tr>\n  <td>pre_dispatch</td> <td>指定总共分发的并行任务数</td> <td>当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次</td>\n </tr>\n</table><a id=\"more\"></a>\n\n\n\n\n\n<p>scikit-learn内置可用评价标准如下：<a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" target=\"_blank\" rel=\"noopener\">scikit-learn model_evalution</a></p>\n<p><img src=\"/posts_res/2018-04-23-tuneparameters/1.png\" alt=\"img\"></p>\n<hr>\n<h4 id=\"常用方法\"><a href=\"#常用方法\" class=\"headerlink\" title=\"常用方法\"></a>常用方法</h4><table>\n <tr>\n  <th>方法</th> <th>含义</th>\n </tr>\n <tr>\n  <td>grid.fit()</td> <td>运行网格搜索</td>\n </tr>\n <tr>\n  <td>grid.grid_scores_</td> <td>给出不同参数情况下的评价结果</td>\n </tr>\n <tr>\n  <td>grid.best_params_</td> <td>已取得最佳结果的参数的组合</td>\n </tr>\n <tr>\n  <td>grid.best_score_</td> <td>优化过程期间观察到的最好的评分</td>\n </tr>\n</table>\n\n<hr>\n<h4 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn import svm, datasets</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">def grid_search(params, X, y):</span><br><span class=\"line\">    svc = svm.SVC()</span><br><span class=\"line\">    grid = GridSearchCV(estimator=svc,</span><br><span class=\"line\">                        param_grid=param_test,</span><br><span class=\"line\">                        scoring=&quot;accuracy&quot;,</span><br><span class=\"line\">                        cv=3,</span><br><span class=\"line\">                        refit=True,</span><br><span class=\"line\">                        iid=True,</span><br><span class=\"line\">                        verbose=True)</span><br><span class=\"line\">    grid.fit(X, y)</span><br><span class=\"line\">    return grid</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    iris = datasets.load_iris()</span><br><span class=\"line\">    param_test = &#123;&apos;kernel&apos;:[&apos;linear&apos;, &apos;rbf&apos;], &apos;C&apos;:[1, 10]&#125;</span><br><span class=\"line\">    estimator = grid_search(param_test, iris.data, iris.target)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(&quot;grid_scores_:&quot;, estimator.grid_scores_)</span><br><span class=\"line\">    print(&quot;best_params_&quot;, estimator.best_params_)</span><br><span class=\"line\">    print(&quot;best_score_&quot;, estimator.best_score_)</span><br><span class=\"line\">    # Fitting 3 folds for each of 4 candidates, totalling 12 fits</span><br><span class=\"line\">    # [Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished</span><br><span class=\"line\">    # grid_scores_: [mean: 0.98000, std: 0.01602, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125;, </span><br><span class=\"line\">                     mean: 0.97333, std: 0.00897, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 1&#125;, </span><br><span class=\"line\">                     mean: 0.97333, std: 0.03697, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 10&#125;, </span><br><span class=\"line\">                     mean: 0.98000, std: 0.01601, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 10&#125;]</span><br><span class=\"line\">    # best_params_ &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125;</span><br><span class=\"line\">    # best_score_ 0.98</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"hyperopt\"><a href=\"#hyperopt\" class=\"headerlink\" title=\"hyperopt\"></a>hyperopt</h3><p><a href=\"https://github.com/hyperopt/hyperopt\" target=\"_blank\" rel=\"noopener\">Hyperopt-Github</a></p>\n<p>Hyheropt四个重要的因素：</p>\n<ul>\n<li>指定需要最小化的函数(the objective function to minimize)；</li>\n<li>搜索的空间(the space over which to search)；</li>\n<li>采样的数据集(trails database)(可选)；</li>\n<li>搜索的算法(可选)</li>\n</ul>\n<p><strong>目标函数</strong>，指定最小化的函数，比如要最小化函数\\( q(x,y) = x^2 + y^2 \\)</p>\n<p><strong>搜索的算法</strong>，即hyperopt的fmin函数的algo参数的取值。\n当前支持的算法有随机搜索(hyperopt.rand.suggest)，模拟退火(hyperopt.anneal.suggest)，TPE算法。</p>\n<p><strong>搜索(参数)空间设置</strong>，例如优化函数q，输入 fmin(q，space=hp.uniform(‘a’，0，1))。\nhp.uniform函数的第一个参数是标签，每个超参数在参数空间内必须具有独一无二的标签。\nhp.uniform指定了参数的分布。</p>\n<p>其他的参数分布：</p>\n<ul>\n<li>hp.choice返回一个选项，选项可以是list或者tuple.options可以是嵌套的表达式，用于组成条件参数。 </li>\n<li>hp.pchoice(label,p_options)以一定的概率返回一个p_options的一个选项。这个选项使得函数在搜索过程中对每个选项的可能性不均匀。 </li>\n<li>hp.uniform(label,low,high)参数在low和high之间均匀分布。 </li>\n<li>hp.quniform(label,low,high,q),参数的取值是round(uniform(low,high)/q)*q，适用于那些离散的取值。 </li>\n<li>hp.loguniform(label,low,high)绘制exp(uniform(low,high)),变量的取值范围是[exp(low),exp(high)] </li>\n<li>hp.randint(label,upper) 返回一个在[0,upper)前闭后开的区间内的随机整数。 </li>\n</ul>\n<p>搜索空间可以含有list和dictionary。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import hp</span><br><span class=\"line\"></span><br><span class=\"line\">list_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1)]</span><br><span class=\"line\">tuple_space = (hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1))</span><br><span class=\"line\">dict_space = &#123;&apos;a&apos;: hp.uniform(&apos;a&apos;, 0, 1), &apos;b&apos;: hp.loguniform(&apos;b&apos;, 0, 1)&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>sample函数在参数空间内采样</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import hp</span><br><span class=\"line\">from hyperopt.pyll.stochastic import sample</span><br><span class=\"line\">list_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1), hp.randint(&apos;c&apos;, 8)]</span><br><span class=\"line\">print(sample(list_space))</span><br><span class=\"line\"># (0.7802721043817558, 1.6616883461586371, array(4))</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"简单的例子\"><a href=\"#简单的例子\" class=\"headerlink\" title=\"简单的例子\"></a>简单的例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import fmin, hp, tpe</span><br><span class=\"line\">def objective(args):</span><br><span class=\"line\">    x, y = args</span><br><span class=\"line\">    return x**2 + y**2 + 1</span><br><span class=\"line\">space = [hp.randint(&apos;x&apos;, 5), hp.randint(&apos;y&apos;, 5)]</span><br><span class=\"line\">best = fmin(objective, space=space, algo=tpe.suggest, max_evals=100)</span><br><span class=\"line\">print(best)</span><br><span class=\"line\"># &#123;&apos;y&apos;: 0, &apos;x&apos;: 0&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"Perceptron鸢尾花例子\"><a href=\"#Perceptron鸢尾花例子\" class=\"headerlink\" title=\"Perceptron鸢尾花例子\"></a>Perceptron鸢尾花例子</h4><p>使用感知器判别鸢尾花，使用的学习率是0.1，迭代40次得到了一个测试集上正确率为82%的结果；使用hyperopt优化参数，将正确率提升到了91%。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn import datasets</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">from sklearn.cross_validation import train_test_split</span><br><span class=\"line\">from sklearn.metrics import accuracy_score</span><br><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\">from sklearn.linear_model import Perceptron</span><br><span class=\"line\">from hyperopt import fmin, tpe, hp, partial</span><br><span class=\"line\"></span><br><span class=\"line\"># load data set</span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><br><span class=\"line\"></span><br><span class=\"line\"># data standard</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)</span><br><span class=\"line\">X_train_std = sc.transform(X_train)</span><br><span class=\"line\">X_test_std = sc.transform(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\"># create perceptron model</span><br><span class=\"line\">ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)</span><br><span class=\"line\">ppn.fit(X_train_std, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># show result before tune parameters</span><br><span class=\"line\">y_pred = ppn.predict(X_test_std)</span><br><span class=\"line\">print(&quot;=&quot;*15, &quot;before tune&quot;, &quot;=&quot;*15)</span><br><span class=\"line\">print(accuracy_score(y_test, y_pred))</span><br><span class=\"line\"></span><br><span class=\"line\"># =============================================================================================</span><br><span class=\"line\"></span><br><span class=\"line\"># define object which hyperopt use</span><br><span class=\"line\">def percept(args):</span><br><span class=\"line\">    global X_train_std, y_train, y_test</span><br><span class=\"line\">    ppn = Perceptron(n_iter=int(args[&quot;n_iter&quot;]), eta0=args[&quot;eta&quot;] * 0.01, random_state=0)</span><br><span class=\"line\">    ppn.fit(X_train_std, y_train)</span><br><span class=\"line\">    y_pred = ppn.predict(X_test_std)</span><br><span class=\"line\">    return -accuracy_score(y_test, y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\"># define search space</span><br><span class=\"line\">space = &#123;&quot;n_iter&quot;: hp.choice(&quot;n_iter&quot;, range(30, 50)), &quot;eta&quot;: hp.uniform(&quot;eta&quot;, 0.05, 0.5)&#125;</span><br><span class=\"line\"># define search algorithm</span><br><span class=\"line\">algo = partial(tpe.suggest, n_startup_jobs=10)</span><br><span class=\"line\"></span><br><span class=\"line\">best = fmin(percept, space, algo=algo, max_evals=100)</span><br><span class=\"line\">print(&quot;=&quot;*15, &quot;after tune&quot;, &quot;=&quot;*15)</span><br><span class=\"line\">print(best)</span><br><span class=\"line\">print(percept(best))</span><br></pre></td></tr></table></figure>\n<p><strong>结果</strong></p>\n<p>由于使用tpe搜索算法，每次搜索的结果都不一样，不稳定。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=============== before tune ===============</span><br><span class=\"line\">0.822222222222</span><br><span class=\"line\">=============== after tune ===============</span><br><span class=\"line\">&#123;&apos;n_iter&apos;: 14, &apos;eta&apos;: 0.12949436553904228&#125;</span><br><span class=\"line\">-0.911111111111</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"xgboost癌症例子\"><a href=\"#xgboost癌症例子\" class=\"headerlink\" title=\"xgboost癌症例子\"></a>xgboost癌症例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">from sklearn.preprocessing import MinMaxScaler</span><br><span class=\"line\">import xgboost as xgb</span><br><span class=\"line\">from sklearn.cross_validation import cross_val_score</span><br><span class=\"line\">from sklearn.datasets import load_breast_cancer</span><br><span class=\"line\">from hyperopt import fmin, tpe, hp, partial</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def loadData():</span><br><span class=\"line\">    d = load_breast_cancer()</span><br><span class=\"line\">    data, target = d.data, d.target</span><br><span class=\"line\">    minmaxscaler = MinMaxScaler()</span><br><span class=\"line\">    data_scal = minmaxscaler.fit_transform(data)</span><br><span class=\"line\"></span><br><span class=\"line\">    indices = np.random.permutation(target.shape[0])</span><br><span class=\"line\">    train_indices = indices[: int(target.shape[0] * 0.8)]</span><br><span class=\"line\">    test_indices = indices[int(target.shape[0] * 0.8):]</span><br><span class=\"line\">    train_x = data[train_indices]</span><br><span class=\"line\">    test_x = data[test_indices]</span><br><span class=\"line\">    train_y = target[train_indices]</span><br><span class=\"line\">    test_y = target[test_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">    return train_x, test_x, train_y, test_y</span><br><span class=\"line\"></span><br><span class=\"line\">train_x, test_x, train_y, test_y = loadData()</span><br><span class=\"line\">print(&quot;train_x:&quot;, train_x.shape, &quot; test_x:&quot;, test_x.shape, &quot; train_y:&quot;, train_y.shape, &quot; test_y:&quot;, test_y.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># define object function which need to minimize</span><br><span class=\"line\">def GBM(argsDict):</span><br><span class=\"line\">    max_depth = argsDict[&quot;max_depth&quot;] + 5</span><br><span class=\"line\">    n_estimators = argsDict[&apos;n_estimators&apos;] * 5 + 50</span><br><span class=\"line\">    learning_rate = argsDict[&quot;learning_rate&quot;] * 0.02 + 0.05</span><br><span class=\"line\">    subsample = argsDict[&quot;subsample&quot;] * 0.1 + 0.7</span><br><span class=\"line\">    min_child_weight = argsDict[&quot;min_child_weight&quot;] + 1</span><br><span class=\"line\">    print(&quot;=&quot;*20)</span><br><span class=\"line\">    print(&quot;max_depth:&quot; + str(max_depth))</span><br><span class=\"line\">    print(&quot;n_estimator:&quot; + str(n_estimators))</span><br><span class=\"line\">    print(&quot;learning_rate:&quot; + str(learning_rate))</span><br><span class=\"line\">    print(&quot;subsample:&quot; + str(subsample))</span><br><span class=\"line\">    print(&quot;min_child_weight:&quot; + str(min_child_weight))</span><br><span class=\"line\">    global train_x, train_y</span><br><span class=\"line\"></span><br><span class=\"line\">    gbm = xgb.XGBClassifier(nthread=4,  # 进程数</span><br><span class=\"line\">                            max_depth=max_depth,  # 最大深度</span><br><span class=\"line\">                            n_estimators=n_estimators,  # 树的数量</span><br><span class=\"line\">                            learning_rate=learning_rate,  # 学习率</span><br><span class=\"line\">                            subsample=subsample,  # 采样数</span><br><span class=\"line\">                            min_child_weight=min_child_weight,  # 孩子数</span><br><span class=\"line\">                            max_delta_step=10,  # 10步不降则停止</span><br><span class=\"line\">                            objective=&quot;binary:logistic&quot;)</span><br><span class=\"line\">    metric = cross_val_score(gbm, train_x, train_y, cv=5, scoring=&quot;roc_auc&quot;).mean()</span><br><span class=\"line\">    print(&quot;cross_val_score:&quot;, metric, &quot;\\n&quot;)</span><br><span class=\"line\">    return -metric</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># define search space</span><br><span class=\"line\">space = &#123;&quot;max_depth&quot;: hp.randint(&quot;max_depth&quot;, 15),</span><br><span class=\"line\">         &quot;n_estimators&quot;: hp.randint(&quot;n_estimators&quot;, 10),  # [0,1,2,3,4,5] -&gt; [50,]</span><br><span class=\"line\">         &quot;learning_rate&quot;: hp.randint(&quot;learning_rate&quot;, 6),  # [0,1,2,3,4,5] -&gt; 0.05,0.06</span><br><span class=\"line\">         &quot;subsample&quot;: hp.randint(&quot;subsample&quot;, 4),  # [0,1,2,3] -&gt; [0.7,0.8,0.9,1.0]</span><br><span class=\"line\">         &quot;min_child_weight&quot;: hp.randint(&quot;min_child_weight&quot;, 5),  # [0,1,2,3,4] -&gt; [1,2,3,4,5]</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"># define search algorithm</span><br><span class=\"line\">algo = partial(tpe.suggest, n_startup_jobs=1)</span><br><span class=\"line\"># search best parameters</span><br><span class=\"line\">best = fmin(GBM, space, algo=algo, max_evals=4)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;best param:&quot;, best) # output best parameters</span><br><span class=\"line\">print(&quot;best result:&quot;, GBM(best)) # output model result in best parameters</span><br></pre></td></tr></table></figure>\n<p><strong>结果</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x: (455, 30)  test_x: (114, 30)  train_y: (455,)  test_y: (114,)</span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:16</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.07</span><br><span class=\"line\">subsample:0.7999999999999999</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.988504345257 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:6</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.15000000000000002</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.990053680824 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:8</span><br><span class=\"line\">n_estimator:95</span><br><span class=\"line\">learning_rate:0.13</span><br><span class=\"line\">subsample:0.8999999999999999</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.988304093567 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:12</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.09</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:3</span><br><span class=\"line\">cross_val_score: 0.990361013789 </span><br><span class=\"line\"></span><br><span class=\"line\">best param: &#123;&apos;n_estimators&apos;: 2, &apos;max_depth&apos;: 7, &apos;subsample&apos;: 0, &apos;learning_rate&apos;: 2, &apos;min_child_weight&apos;: 2&#125;</span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:12</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.09</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:3</span><br><span class=\"line\">cross_val_score: 0.990361013789 </span><br><span class=\"line\"></span><br><span class=\"line\">best result: -0.990361013789</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"hyperopt-sklearn\"><a href=\"#hyperopt-sklearn\" class=\"headerlink\" title=\"hyperopt-sklearn\"></a>hyperopt-sklearn</h3><p><a href=\"https://github.com/hyperopt/hyperopt-sklearn/tree/master\" target=\"_blank\" rel=\"noopener\">hyperopt-sklearn-Github</a></p>\n<p>目前hyperopt-sklearn只支持部分Classifiers\\Regressors\\Preprocessing，具体请见上面hyperopt-sklearn的Github主页。</p>\n<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone git@github.com:hyperopt/hyperopt-sklearn.git</span><br><span class=\"line\">(cd hyperopt-sklearn &amp;&amp; pip install -e .)</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用模版\"><a href=\"#使用模版\" class=\"headerlink\" title=\"使用模版\"></a>使用模版</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hpsklearn import HyperoptEstimator, svc</span><br><span class=\"line\">from sklearn import svm</span><br><span class=\"line\"></span><br><span class=\"line\"># Load Data</span><br><span class=\"line\"># ...</span><br><span class=\"line\"></span><br><span class=\"line\">if use_hpsklearn:</span><br><span class=\"line\">    estim = HyperoptEstimator(classifier=svc(&apos;mySVC&apos;))</span><br><span class=\"line\">else:</span><br><span class=\"line\">    estim = svm.SVC()</span><br><span class=\"line\"></span><br><span class=\"line\">estim.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(estim.score(X_test, y_test))</span><br></pre></td></tr></table></figure>\n<h4 id=\"鸢尾花例子\"><a href=\"#鸢尾花例子\" class=\"headerlink\" title=\"鸢尾花例子\"></a>鸢尾花例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing</span><br><span class=\"line\">from sklearn.datasets import load_iris</span><br><span class=\"line\">from hyperopt import tpe</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># Download the data and split into training and test sets</span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">test_size = int(0.2 * len(y))</span><br><span class=\"line\">np.random.seed(13)</span><br><span class=\"line\">indices = np.random.permutation(len(X))</span><br><span class=\"line\">train_x = X[indices[:-test_size]]</span><br><span class=\"line\">train_y = y[indices[:-test_size]]</span><br><span class=\"line\">test_x = X[indices[-test_size:]]</span><br><span class=\"line\">test_y = y[indices[-test_size:]]</span><br><span class=\"line\"></span><br><span class=\"line\"># Instantiate a HyperoptEstimator with the search space and number of evaluations</span><br><span class=\"line\">estim = HyperoptEstimator(classifier=any_classifier(&apos;my_clf&apos;),</span><br><span class=\"line\">                          preprocessing=any_preprocessing(&apos;my_pre&apos;),</span><br><span class=\"line\">                          algo=tpe.suggest,</span><br><span class=\"line\">                          max_evals=100,</span><br><span class=\"line\">                          trial_timeout=120)</span><br><span class=\"line\"></span><br><span class=\"line\"># Search the hyperparameter space based on the data</span><br><span class=\"line\">estim.fit(train_x, train_x)</span><br><span class=\"line\"></span><br><span class=\"line\"># Show the results</span><br><span class=\"line\">print(estim.score(test_x, test_y))</span><br><span class=\"line\"># 1.0</span><br><span class=\"line\">print(estim.best_model())</span><br><span class=\"line\"># &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&apos;gini&apos;,</span><br><span class=\"line\">#           max_depth=3, max_features=&apos;log2&apos;, max_leaf_nodes=None,</span><br><span class=\"line\">#           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class=\"line\">#           min_samples_leaf=1, min_samples_split=2,</span><br><span class=\"line\">#           min_weight_fraction_leaf=0.0, n_estimators=13, n_jobs=1,</span><br><span class=\"line\">#           oob_score=False, random_state=1, verbose=False,</span><br><span class=\"line\">#           warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"MNIST例子\"><a href=\"#MNIST例子\" class=\"headerlink\" title=\"MNIST例子\"></a>MNIST例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from hpsklearn import HyperoptEstimator, extra_trees</span><br><span class=\"line\">from sklearn.datasets import fetch_mldata</span><br><span class=\"line\">from hyperopt import tpe</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># Download the data and split into training and test sets</span><br><span class=\"line\">digits = fetch_mldata(&apos;MNIST original&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">X = digits.data</span><br><span class=\"line\">y = digits.target</span><br><span class=\"line\">test_size = int(0.2 * len(y))</span><br><span class=\"line\">np.random.seed(13)</span><br><span class=\"line\">indices = np.random.permutation(len(X))</span><br><span class=\"line\">X_train = X[indices[:-test_size]]</span><br><span class=\"line\">y_train = y[indices[:-test_size]]</span><br><span class=\"line\">X_test = X[indices[-test_size:]]</span><br><span class=\"line\">y_test = y[indices[-test_size:]]</span><br><span class=\"line\"></span><br><span class=\"line\"># Instantiate a HyperoptEstimator with the search space and number of evaluations</span><br><span class=\"line\">estim = HyperoptEstimator(classifier=extra_trees(&apos;my_clf&apos;),</span><br><span class=\"line\">                          preprocessing=[],</span><br><span class=\"line\">                          algo=tpe.suggest,</span><br><span class=\"line\">                          max_evals=10,</span><br><span class=\"line\">                          trial_timeout=300)</span><br><span class=\"line\"></span><br><span class=\"line\"># Search the hyperparameter space based on the data</span><br><span class=\"line\">estim.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># Show the results</span><br><span class=\"line\">print(estim.score(X_test, y_test))</span><br><span class=\"line\"># 0.962785714286</span><br><span class=\"line\">print(estim.best_model())</span><br><span class=\"line\"># &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion=&apos;entropy&apos;,</span><br><span class=\"line\">#           max_depth=None, max_features=0.959202875857,</span><br><span class=\"line\">#           max_leaf_nodes=None, min_impurity_decrease=0.0,</span><br><span class=\"line\">#           min_impurity_split=None, min_samples_leaf=1,</span><br><span class=\"line\">#           min_samples_split=2, min_weight_fraction_leaf=0.0,</span><br><span class=\"line\">#           n_estimators=20, n_jobs=1, oob_score=False, random_state=3,</span><br><span class=\"line\">#           verbose=False, warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://blog.csdn.net/u012969412/article/details/72973055\" target=\"_blank\" rel=\"noopener\">sklearn-GridSearchCV,CV调节超参使用方法</a></li>\n<li><a href=\"http://www.cnblogs.com/gczr/p/7156270.html\" target=\"_blank\" rel=\"noopener\">python调参神器hyperopt-简略</a></li>\n<li><a href=\"https://blog.csdn.net/qq_34139222/article/details/60322995\" target=\"_blank\" rel=\"noopener\">python调参神器hyperopt-详细</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p><strong>目录</strong></p><ul>\n<li>sklearn-GridSearchCV</li>\n<li>hyperopt</li>\n<li>hyperopt-sklearn</li>\n</ul><hr><h3 id=\"sklearn-GridSearchCV\"><a href=\"#sklearn-GridSearchCV\" class=\"headerlink\" title=\"sklearn-GridSearchCV\"></a>sklearn-GridSearchCV</h3><h4 id=\"常用参数\"><a href=\"#常用参数\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><p><a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\" rel=\"noopener\">sklearn.model_selection.GridSearchCV</a></p><table>\n <tr>\n  <th>参数</th> <th>含义</th> <th>其他</th>\n </tr>\n <tr>\n  <td>estimator</td> <td>所使用的模型</td> <td>假定这是scikit-learn中模型接口。该模型可以提供score方法或scoring参数</td>\n </tr>\n <tr>\n  <td>param_grid</td> <td>dict或list</td> <td>带有参数名称作为键的字典,例如param_grid=param_test, param_test={'n_estimators': range(1, 6)}</td>\n </tr>\n <tr>\n  <td>scoring</td> <td>评价标准，默认为None</td> <td>字符串，或是可调用对象，需要其函数形式如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数</td>\n </tr>\n <tr>\n  <td>cv</td> <td>交叉验证参数，默认为None，使用三折交叉验证</td> <td>整数指定交叉验证折数，也可以是交叉验证生成器</td>\n </tr>\n <tr>\n  <td>refit</td> <td>默认为True</td> <td>在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集</td>\n </tr>\n <tr>\n  <td>iid</td> <td>默认为True</td> <td>默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均</td>\n </tr>\n <tr>\n  <td>verbose</td> <td>默认为0</td> <td>日志冗长度。0：不输出训练过程；1：偶尔输出；>1：对每个子模型都输出</td>\n </tr>\n  <tr>\n  <td>n_jobs</td> <td>并行数，int类型</td> <td>-1：跟CPU核数一致； 1:默认值</td>\n </tr>\n <tr>\n  <td>pre_dispatch</td> <td>指定总共分发的并行任务数</td> <td>当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次</td>\n </tr>\n</table>","more":"\n\n\n\n\n\n<p>scikit-learn内置可用评价标准如下：<a href=\"http://scikit-learn.org/stable/modules/model_evaluation.html\" target=\"_blank\" rel=\"noopener\">scikit-learn model_evalution</a></p>\n<p><img src=\"/posts_res/2018-04-23-tuneparameters/1.png\" alt=\"img\"></p>\n<hr>\n<h4 id=\"常用方法\"><a href=\"#常用方法\" class=\"headerlink\" title=\"常用方法\"></a>常用方法</h4><table>\n <tr>\n  <th>方法</th> <th>含义</th>\n </tr>\n <tr>\n  <td>grid.fit()</td> <td>运行网格搜索</td>\n </tr>\n <tr>\n  <td>grid.grid_scores_</td> <td>给出不同参数情况下的评价结果</td>\n </tr>\n <tr>\n  <td>grid.best_params_</td> <td>已取得最佳结果的参数的组合</td>\n </tr>\n <tr>\n  <td>grid.best_score_</td> <td>优化过程期间观察到的最好的评分</td>\n </tr>\n</table>\n\n<hr>\n<h4 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn import svm, datasets</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">def grid_search(params, X, y):</span><br><span class=\"line\">    svc = svm.SVC()</span><br><span class=\"line\">    grid = GridSearchCV(estimator=svc,</span><br><span class=\"line\">                        param_grid=param_test,</span><br><span class=\"line\">                        scoring=&quot;accuracy&quot;,</span><br><span class=\"line\">                        cv=3,</span><br><span class=\"line\">                        refit=True,</span><br><span class=\"line\">                        iid=True,</span><br><span class=\"line\">                        verbose=True)</span><br><span class=\"line\">    grid.fit(X, y)</span><br><span class=\"line\">    return grid</span><br><span class=\"line\"></span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    iris = datasets.load_iris()</span><br><span class=\"line\">    param_test = &#123;&apos;kernel&apos;:[&apos;linear&apos;, &apos;rbf&apos;], &apos;C&apos;:[1, 10]&#125;</span><br><span class=\"line\">    estimator = grid_search(param_test, iris.data, iris.target)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(&quot;grid_scores_:&quot;, estimator.grid_scores_)</span><br><span class=\"line\">    print(&quot;best_params_&quot;, estimator.best_params_)</span><br><span class=\"line\">    print(&quot;best_score_&quot;, estimator.best_score_)</span><br><span class=\"line\">    # Fitting 3 folds for each of 4 candidates, totalling 12 fits</span><br><span class=\"line\">    # [Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished</span><br><span class=\"line\">    # grid_scores_: [mean: 0.98000, std: 0.01602, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125;, </span><br><span class=\"line\">                     mean: 0.97333, std: 0.00897, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 1&#125;, </span><br><span class=\"line\">                     mean: 0.97333, std: 0.03697, params: &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 10&#125;, </span><br><span class=\"line\">                     mean: 0.98000, std: 0.01601, params: &#123;&apos;kernel&apos;: &apos;rbf&apos;, &apos;C&apos;: 10&#125;]</span><br><span class=\"line\">    # best_params_ &#123;&apos;kernel&apos;: &apos;linear&apos;, &apos;C&apos;: 1&#125;</span><br><span class=\"line\">    # best_score_ 0.98</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"hyperopt\"><a href=\"#hyperopt\" class=\"headerlink\" title=\"hyperopt\"></a>hyperopt</h3><p><a href=\"https://github.com/hyperopt/hyperopt\" target=\"_blank\" rel=\"noopener\">Hyperopt-Github</a></p>\n<p>Hyheropt四个重要的因素：</p>\n<ul>\n<li>指定需要最小化的函数(the objective function to minimize)；</li>\n<li>搜索的空间(the space over which to search)；</li>\n<li>采样的数据集(trails database)(可选)；</li>\n<li>搜索的算法(可选)</li>\n</ul>\n<p><strong>目标函数</strong>，指定最小化的函数，比如要最小化函数\\( q(x,y) = x^2 + y^2 \\)</p>\n<p><strong>搜索的算法</strong>，即hyperopt的fmin函数的algo参数的取值。\n当前支持的算法有随机搜索(hyperopt.rand.suggest)，模拟退火(hyperopt.anneal.suggest)，TPE算法。</p>\n<p><strong>搜索(参数)空间设置</strong>，例如优化函数q，输入 fmin(q，space=hp.uniform(‘a’，0，1))。\nhp.uniform函数的第一个参数是标签，每个超参数在参数空间内必须具有独一无二的标签。\nhp.uniform指定了参数的分布。</p>\n<p>其他的参数分布：</p>\n<ul>\n<li>hp.choice返回一个选项，选项可以是list或者tuple.options可以是嵌套的表达式，用于组成条件参数。 </li>\n<li>hp.pchoice(label,p_options)以一定的概率返回一个p_options的一个选项。这个选项使得函数在搜索过程中对每个选项的可能性不均匀。 </li>\n<li>hp.uniform(label,low,high)参数在low和high之间均匀分布。 </li>\n<li>hp.quniform(label,low,high,q),参数的取值是round(uniform(low,high)/q)*q，适用于那些离散的取值。 </li>\n<li>hp.loguniform(label,low,high)绘制exp(uniform(low,high)),变量的取值范围是[exp(low),exp(high)] </li>\n<li>hp.randint(label,upper) 返回一个在[0,upper)前闭后开的区间内的随机整数。 </li>\n</ul>\n<p>搜索空间可以含有list和dictionary。\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import hp</span><br><span class=\"line\"></span><br><span class=\"line\">list_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1)]</span><br><span class=\"line\">tuple_space = (hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1))</span><br><span class=\"line\">dict_space = &#123;&apos;a&apos;: hp.uniform(&apos;a&apos;, 0, 1), &apos;b&apos;: hp.loguniform(&apos;b&apos;, 0, 1)&#125;</span><br></pre></td></tr></table></figure></p>\n<p><strong>sample函数在参数空间内采样</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import hp</span><br><span class=\"line\">from hyperopt.pyll.stochastic import sample</span><br><span class=\"line\">list_space = [hp.uniform(&apos;a&apos;, 0, 1), hp.loguniform(&apos;b&apos;, 0, 1), hp.randint(&apos;c&apos;, 8)]</span><br><span class=\"line\">print(sample(list_space))</span><br><span class=\"line\"># (0.7802721043817558, 1.6616883461586371, array(4))</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"简单的例子\"><a href=\"#简单的例子\" class=\"headerlink\" title=\"简单的例子\"></a>简单的例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hyperopt import fmin, hp, tpe</span><br><span class=\"line\">def objective(args):</span><br><span class=\"line\">    x, y = args</span><br><span class=\"line\">    return x**2 + y**2 + 1</span><br><span class=\"line\">space = [hp.randint(&apos;x&apos;, 5), hp.randint(&apos;y&apos;, 5)]</span><br><span class=\"line\">best = fmin(objective, space=space, algo=tpe.suggest, max_evals=100)</span><br><span class=\"line\">print(best)</span><br><span class=\"line\"># &#123;&apos;y&apos;: 0, &apos;x&apos;: 0&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"Perceptron鸢尾花例子\"><a href=\"#Perceptron鸢尾花例子\" class=\"headerlink\" title=\"Perceptron鸢尾花例子\"></a>Perceptron鸢尾花例子</h4><p>使用感知器判别鸢尾花，使用的学习率是0.1，迭代40次得到了一个测试集上正确率为82%的结果；使用hyperopt优化参数，将正确率提升到了91%。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn import datasets</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">from sklearn.cross_validation import train_test_split</span><br><span class=\"line\">from sklearn.metrics import accuracy_score</span><br><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\">from sklearn.linear_model import Perceptron</span><br><span class=\"line\">from hyperopt import fmin, tpe, hp, partial</span><br><span class=\"line\"></span><br><span class=\"line\"># load data set</span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><br><span class=\"line\"></span><br><span class=\"line\"># data standard</span><br><span class=\"line\">sc = StandardScaler()</span><br><span class=\"line\">sc.fit(X_train)</span><br><span class=\"line\">X_train_std = sc.transform(X_train)</span><br><span class=\"line\">X_test_std = sc.transform(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\"># create perceptron model</span><br><span class=\"line\">ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0)</span><br><span class=\"line\">ppn.fit(X_train_std, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># show result before tune parameters</span><br><span class=\"line\">y_pred = ppn.predict(X_test_std)</span><br><span class=\"line\">print(&quot;=&quot;*15, &quot;before tune&quot;, &quot;=&quot;*15)</span><br><span class=\"line\">print(accuracy_score(y_test, y_pred))</span><br><span class=\"line\"></span><br><span class=\"line\"># =============================================================================================</span><br><span class=\"line\"></span><br><span class=\"line\"># define object which hyperopt use</span><br><span class=\"line\">def percept(args):</span><br><span class=\"line\">    global X_train_std, y_train, y_test</span><br><span class=\"line\">    ppn = Perceptron(n_iter=int(args[&quot;n_iter&quot;]), eta0=args[&quot;eta&quot;] * 0.01, random_state=0)</span><br><span class=\"line\">    ppn.fit(X_train_std, y_train)</span><br><span class=\"line\">    y_pred = ppn.predict(X_test_std)</span><br><span class=\"line\">    return -accuracy_score(y_test, y_pred)</span><br><span class=\"line\"></span><br><span class=\"line\"># define search space</span><br><span class=\"line\">space = &#123;&quot;n_iter&quot;: hp.choice(&quot;n_iter&quot;, range(30, 50)), &quot;eta&quot;: hp.uniform(&quot;eta&quot;, 0.05, 0.5)&#125;</span><br><span class=\"line\"># define search algorithm</span><br><span class=\"line\">algo = partial(tpe.suggest, n_startup_jobs=10)</span><br><span class=\"line\"></span><br><span class=\"line\">best = fmin(percept, space, algo=algo, max_evals=100)</span><br><span class=\"line\">print(&quot;=&quot;*15, &quot;after tune&quot;, &quot;=&quot;*15)</span><br><span class=\"line\">print(best)</span><br><span class=\"line\">print(percept(best))</span><br></pre></td></tr></table></figure>\n<p><strong>结果</strong></p>\n<p>由于使用tpe搜索算法，每次搜索的结果都不一样，不稳定。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">=============== before tune ===============</span><br><span class=\"line\">0.822222222222</span><br><span class=\"line\">=============== after tune ===============</span><br><span class=\"line\">&#123;&apos;n_iter&apos;: 14, &apos;eta&apos;: 0.12949436553904228&#125;</span><br><span class=\"line\">-0.911111111111</span><br></pre></td></tr></table></figure>\n<hr>\n<h4 id=\"xgboost癌症例子\"><a href=\"#xgboost癌症例子\" class=\"headerlink\" title=\"xgboost癌症例子\"></a>xgboost癌症例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">from sklearn.preprocessing import MinMaxScaler</span><br><span class=\"line\">import xgboost as xgb</span><br><span class=\"line\">from sklearn.cross_validation import cross_val_score</span><br><span class=\"line\">from sklearn.datasets import load_breast_cancer</span><br><span class=\"line\">from hyperopt import fmin, tpe, hp, partial</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">def loadData():</span><br><span class=\"line\">    d = load_breast_cancer()</span><br><span class=\"line\">    data, target = d.data, d.target</span><br><span class=\"line\">    minmaxscaler = MinMaxScaler()</span><br><span class=\"line\">    data_scal = minmaxscaler.fit_transform(data)</span><br><span class=\"line\"></span><br><span class=\"line\">    indices = np.random.permutation(target.shape[0])</span><br><span class=\"line\">    train_indices = indices[: int(target.shape[0] * 0.8)]</span><br><span class=\"line\">    test_indices = indices[int(target.shape[0] * 0.8):]</span><br><span class=\"line\">    train_x = data[train_indices]</span><br><span class=\"line\">    test_x = data[test_indices]</span><br><span class=\"line\">    train_y = target[train_indices]</span><br><span class=\"line\">    test_y = target[test_indices]</span><br><span class=\"line\"></span><br><span class=\"line\">    return train_x, test_x, train_y, test_y</span><br><span class=\"line\"></span><br><span class=\"line\">train_x, test_x, train_y, test_y = loadData()</span><br><span class=\"line\">print(&quot;train_x:&quot;, train_x.shape, &quot; test_x:&quot;, test_x.shape, &quot; train_y:&quot;, train_y.shape, &quot; test_y:&quot;, test_y.shape)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># define object function which need to minimize</span><br><span class=\"line\">def GBM(argsDict):</span><br><span class=\"line\">    max_depth = argsDict[&quot;max_depth&quot;] + 5</span><br><span class=\"line\">    n_estimators = argsDict[&apos;n_estimators&apos;] * 5 + 50</span><br><span class=\"line\">    learning_rate = argsDict[&quot;learning_rate&quot;] * 0.02 + 0.05</span><br><span class=\"line\">    subsample = argsDict[&quot;subsample&quot;] * 0.1 + 0.7</span><br><span class=\"line\">    min_child_weight = argsDict[&quot;min_child_weight&quot;] + 1</span><br><span class=\"line\">    print(&quot;=&quot;*20)</span><br><span class=\"line\">    print(&quot;max_depth:&quot; + str(max_depth))</span><br><span class=\"line\">    print(&quot;n_estimator:&quot; + str(n_estimators))</span><br><span class=\"line\">    print(&quot;learning_rate:&quot; + str(learning_rate))</span><br><span class=\"line\">    print(&quot;subsample:&quot; + str(subsample))</span><br><span class=\"line\">    print(&quot;min_child_weight:&quot; + str(min_child_weight))</span><br><span class=\"line\">    global train_x, train_y</span><br><span class=\"line\"></span><br><span class=\"line\">    gbm = xgb.XGBClassifier(nthread=4,  # 进程数</span><br><span class=\"line\">                            max_depth=max_depth,  # 最大深度</span><br><span class=\"line\">                            n_estimators=n_estimators,  # 树的数量</span><br><span class=\"line\">                            learning_rate=learning_rate,  # 学习率</span><br><span class=\"line\">                            subsample=subsample,  # 采样数</span><br><span class=\"line\">                            min_child_weight=min_child_weight,  # 孩子数</span><br><span class=\"line\">                            max_delta_step=10,  # 10步不降则停止</span><br><span class=\"line\">                            objective=&quot;binary:logistic&quot;)</span><br><span class=\"line\">    metric = cross_val_score(gbm, train_x, train_y, cv=5, scoring=&quot;roc_auc&quot;).mean()</span><br><span class=\"line\">    print(&quot;cross_val_score:&quot;, metric, &quot;\\n&quot;)</span><br><span class=\"line\">    return -metric</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"># define search space</span><br><span class=\"line\">space = &#123;&quot;max_depth&quot;: hp.randint(&quot;max_depth&quot;, 15),</span><br><span class=\"line\">         &quot;n_estimators&quot;: hp.randint(&quot;n_estimators&quot;, 10),  # [0,1,2,3,4,5] -&gt; [50,]</span><br><span class=\"line\">         &quot;learning_rate&quot;: hp.randint(&quot;learning_rate&quot;, 6),  # [0,1,2,3,4,5] -&gt; 0.05,0.06</span><br><span class=\"line\">         &quot;subsample&quot;: hp.randint(&quot;subsample&quot;, 4),  # [0,1,2,3] -&gt; [0.7,0.8,0.9,1.0]</span><br><span class=\"line\">         &quot;min_child_weight&quot;: hp.randint(&quot;min_child_weight&quot;, 5),  # [0,1,2,3,4] -&gt; [1,2,3,4,5]</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"># define search algorithm</span><br><span class=\"line\">algo = partial(tpe.suggest, n_startup_jobs=1)</span><br><span class=\"line\"># search best parameters</span><br><span class=\"line\">best = fmin(GBM, space, algo=algo, max_evals=4)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&quot;best param:&quot;, best) # output best parameters</span><br><span class=\"line\">print(&quot;best result:&quot;, GBM(best)) # output model result in best parameters</span><br></pre></td></tr></table></figure>\n<p><strong>结果</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x: (455, 30)  test_x: (114, 30)  train_y: (455,)  test_y: (114,)</span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:16</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.07</span><br><span class=\"line\">subsample:0.7999999999999999</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.988504345257 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:6</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.15000000000000002</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.990053680824 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:8</span><br><span class=\"line\">n_estimator:95</span><br><span class=\"line\">learning_rate:0.13</span><br><span class=\"line\">subsample:0.8999999999999999</span><br><span class=\"line\">min_child_weight:5</span><br><span class=\"line\">cross_val_score: 0.988304093567 </span><br><span class=\"line\"></span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:12</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.09</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:3</span><br><span class=\"line\">cross_val_score: 0.990361013789 </span><br><span class=\"line\"></span><br><span class=\"line\">best param: &#123;&apos;n_estimators&apos;: 2, &apos;max_depth&apos;: 7, &apos;subsample&apos;: 0, &apos;learning_rate&apos;: 2, &apos;min_child_weight&apos;: 2&#125;</span><br><span class=\"line\">====================</span><br><span class=\"line\">max_depth:12</span><br><span class=\"line\">n_estimator:60</span><br><span class=\"line\">learning_rate:0.09</span><br><span class=\"line\">subsample:0.7</span><br><span class=\"line\">min_child_weight:3</span><br><span class=\"line\">cross_val_score: 0.990361013789 </span><br><span class=\"line\"></span><br><span class=\"line\">best result: -0.990361013789</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"hyperopt-sklearn\"><a href=\"#hyperopt-sklearn\" class=\"headerlink\" title=\"hyperopt-sklearn\"></a>hyperopt-sklearn</h3><p><a href=\"https://github.com/hyperopt/hyperopt-sklearn/tree/master\" target=\"_blank\" rel=\"noopener\">hyperopt-sklearn-Github</a></p>\n<p>目前hyperopt-sklearn只支持部分Classifiers\\Regressors\\Preprocessing，具体请见上面hyperopt-sklearn的Github主页。</p>\n<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone git@github.com:hyperopt/hyperopt-sklearn.git</span><br><span class=\"line\">(cd hyperopt-sklearn &amp;&amp; pip install -e .)</span><br></pre></td></tr></table></figure>\n<h4 id=\"使用模版\"><a href=\"#使用模版\" class=\"headerlink\" title=\"使用模版\"></a>使用模版</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from hpsklearn import HyperoptEstimator, svc</span><br><span class=\"line\">from sklearn import svm</span><br><span class=\"line\"></span><br><span class=\"line\"># Load Data</span><br><span class=\"line\"># ...</span><br><span class=\"line\"></span><br><span class=\"line\">if use_hpsklearn:</span><br><span class=\"line\">    estim = HyperoptEstimator(classifier=svc(&apos;mySVC&apos;))</span><br><span class=\"line\">else:</span><br><span class=\"line\">    estim = svm.SVC()</span><br><span class=\"line\"></span><br><span class=\"line\">estim.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\">print(estim.score(X_test, y_test))</span><br></pre></td></tr></table></figure>\n<h4 id=\"鸢尾花例子\"><a href=\"#鸢尾花例子\" class=\"headerlink\" title=\"鸢尾花例子\"></a>鸢尾花例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing</span><br><span class=\"line\">from sklearn.datasets import load_iris</span><br><span class=\"line\">from hyperopt import tpe</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># Download the data and split into training and test sets</span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\">test_size = int(0.2 * len(y))</span><br><span class=\"line\">np.random.seed(13)</span><br><span class=\"line\">indices = np.random.permutation(len(X))</span><br><span class=\"line\">train_x = X[indices[:-test_size]]</span><br><span class=\"line\">train_y = y[indices[:-test_size]]</span><br><span class=\"line\">test_x = X[indices[-test_size:]]</span><br><span class=\"line\">test_y = y[indices[-test_size:]]</span><br><span class=\"line\"></span><br><span class=\"line\"># Instantiate a HyperoptEstimator with the search space and number of evaluations</span><br><span class=\"line\">estim = HyperoptEstimator(classifier=any_classifier(&apos;my_clf&apos;),</span><br><span class=\"line\">                          preprocessing=any_preprocessing(&apos;my_pre&apos;),</span><br><span class=\"line\">                          algo=tpe.suggest,</span><br><span class=\"line\">                          max_evals=100,</span><br><span class=\"line\">                          trial_timeout=120)</span><br><span class=\"line\"></span><br><span class=\"line\"># Search the hyperparameter space based on the data</span><br><span class=\"line\">estim.fit(train_x, train_x)</span><br><span class=\"line\"></span><br><span class=\"line\"># Show the results</span><br><span class=\"line\">print(estim.score(test_x, test_y))</span><br><span class=\"line\"># 1.0</span><br><span class=\"line\">print(estim.best_model())</span><br><span class=\"line\"># &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&apos;gini&apos;,</span><br><span class=\"line\">#           max_depth=3, max_features=&apos;log2&apos;, max_leaf_nodes=None,</span><br><span class=\"line\">#           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class=\"line\">#           min_samples_leaf=1, min_samples_split=2,</span><br><span class=\"line\">#           min_weight_fraction_leaf=0.0, n_estimators=13, n_jobs=1,</span><br><span class=\"line\">#           oob_score=False, random_state=1, verbose=False,</span><br><span class=\"line\">#           warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"MNIST例子\"><a href=\"#MNIST例子\" class=\"headerlink\" title=\"MNIST例子\"></a>MNIST例子</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from hpsklearn import HyperoptEstimator, extra_trees</span><br><span class=\"line\">from sklearn.datasets import fetch_mldata</span><br><span class=\"line\">from hyperopt import tpe</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\"># Download the data and split into training and test sets</span><br><span class=\"line\">digits = fetch_mldata(&apos;MNIST original&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">X = digits.data</span><br><span class=\"line\">y = digits.target</span><br><span class=\"line\">test_size = int(0.2 * len(y))</span><br><span class=\"line\">np.random.seed(13)</span><br><span class=\"line\">indices = np.random.permutation(len(X))</span><br><span class=\"line\">X_train = X[indices[:-test_size]]</span><br><span class=\"line\">y_train = y[indices[:-test_size]]</span><br><span class=\"line\">X_test = X[indices[-test_size:]]</span><br><span class=\"line\">y_test = y[indices[-test_size:]]</span><br><span class=\"line\"></span><br><span class=\"line\"># Instantiate a HyperoptEstimator with the search space and number of evaluations</span><br><span class=\"line\">estim = HyperoptEstimator(classifier=extra_trees(&apos;my_clf&apos;),</span><br><span class=\"line\">                          preprocessing=[],</span><br><span class=\"line\">                          algo=tpe.suggest,</span><br><span class=\"line\">                          max_evals=10,</span><br><span class=\"line\">                          trial_timeout=300)</span><br><span class=\"line\"></span><br><span class=\"line\"># Search the hyperparameter space based on the data</span><br><span class=\"line\">estim.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"># Show the results</span><br><span class=\"line\">print(estim.score(X_test, y_test))</span><br><span class=\"line\"># 0.962785714286</span><br><span class=\"line\">print(estim.best_model())</span><br><span class=\"line\"># &#123;&apos;learner&apos;: ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion=&apos;entropy&apos;,</span><br><span class=\"line\">#           max_depth=None, max_features=0.959202875857,</span><br><span class=\"line\">#           max_leaf_nodes=None, min_impurity_decrease=0.0,</span><br><span class=\"line\">#           min_impurity_split=None, min_samples_leaf=1,</span><br><span class=\"line\">#           min_samples_split=2, min_weight_fraction_leaf=0.0,</span><br><span class=\"line\">#           n_estimators=20, n_jobs=1, oob_score=False, random_state=3,</span><br><span class=\"line\">#           verbose=False, warm_start=False), &apos;preprocs&apos;: (), &apos;ex_preprocs&apos;: ()&#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://blog.csdn.net/u012969412/article/details/72973055\" target=\"_blank\" rel=\"noopener\">sklearn-GridSearchCV,CV调节超参使用方法</a></li>\n<li><a href=\"http://www.cnblogs.com/gczr/p/7156270.html\" target=\"_blank\" rel=\"noopener\">python调参神器hyperopt-简略</a></li>\n<li><a href=\"https://blog.csdn.net/qq_34139222/article/details/60322995\" target=\"_blank\" rel=\"noopener\">python调参神器hyperopt-详细</a></li>\n</ol>\n"},{"layout":"post","title":"随机森林 Random Forest","date":"2018-04-21T12:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n## <center> 随即森林 - Random Forest </center>\n\n目录\n* 基本概念\n* 袋外错误率(out of bag error, oob error)\n* 例子\n* 特点及细节\n\n> TODO 分布式实现原理 \n> https://www.jianshu.com/p/d90189008864 \n> https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/\n\n------\n\n### 基本概念\n\n随机森林是由多颗CART树组成的，具体决策树部分知识[在这里](/2018/03/decision-tree/)\n\n前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。\n打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。\n该动物到底是老鼠还是松鼠，要依据**投票情况**来确定，获得票数最多的类别就是森林的分类结果。\n森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。\n少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。\n将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想\n（bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）。\n\n有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？\n\n每棵树的按照如下规则生成：\n\n（1）如果训练集大小为N，对于每棵树而言，**随机且有放回**地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。\n\n（2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树(节点)进行分裂时，从这m个特征中选择最优的；\n*具体意思是：每次进行分支的时候，都重新随机选择m个特征，计算这m个特征里，分裂效果最好的那个特征进行分类*\n\n（3）每棵树都尽最大程度的生长，并且没有剪枝过程。\n\n一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性（采样数据、采样特征）。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。\n\n随机森林分类效果（错误率）与两个因素有关：\n* 森林中任意两棵树的相关性：相关性越大，错误率越大；\n* 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。\n\n\n-------\n\n### 袋外错误率(out of bag error, oob error)\n\n上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error。\n\n随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。\n它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。\n\n我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。\n所以对于每棵树而言（假设对于第k棵树），大约有1/3(\\\\( 1/e \\approx 0.368 \\\\))的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。\n\n而这样的采样特点就允许我们进行oob估计，它的计算方式如下：(note：以样本为单位)\n\n（1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；\n\n（2）然后以简单多数投票作为该样本的分类结果；\n\n（3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。\n\noob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。\n\n\n------\n\n### 例子\n\n描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。\n\n收入层次 :\n\n  Band 1 : < 40,000\n\n  Band 2: 40,000 – 150,000\n\n  Band 3: > 150,000\n\n随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。\n\nCART 1 : Variable Age\n\n![age](/posts_res/2018-04-21-randomforest/3-1.png)\n\nCART 2 : Variable Gender\n\n![gender](/posts_res/2018-04-21-randomforest/3-2.png)\n\nCART 3 : Variable Education\n\n![edu](/posts_res/2018-04-21-randomforest/3-3.png)\n\nCART 4 : Variable Residence\n\n![res](/posts_res/2018-04-21-randomforest/3-4.png)\n\nCART 5 : Variable Industry\n\n![industy](/posts_res/2018-04-21-randomforest/3-5.png)\n\n\n我们要预测的某个人的信息如下：\n\n1. Age : 35 years\n2. Gender : Male\n3. Highest Educational Qualification : Diploma holder\n4. Industry : Manufacturing\n5. Residence : Metro\n\n根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：\n\n![level](/posts_res/2018-04-21-randomforest/3-6.png)\n\n最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（< 40,000）。\n\n\n-------\n\n### 特点及细节\n\n* 在当前所有算法中，具有极好的准确率\n* 能够有效地运行在大数据集上\n* 能够处理具有高维特征的输入样本，而且不需要降维\n* 能够评估各个特征在分类问题上的重要性\n* 在生成过程中，能够获取到内部生成误差的一种无偏估计\n* 对于缺省值问题也能够获得很好得结果\n\n```\n为什么要随机抽样训练集？\n如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；\n\n为什么要有放回地抽样？\n如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是\"有偏的\"，都是\"片面的\"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是\"求同\"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是\"盲人摸象\"。\n```\n\n<br>\n\n```\n减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。\n所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。\n```\n\n<br>\n\n```\nRF特征选择\n首先特征选择的目标有两个：\n1：找到与分类结果高度相关的特征变量。\n2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。\n\n特征选择的步骤：\n（1）对于每一棵决策树，计算其oob error\n（2）随机的修改OOB中的每个特征xi的值，计算oob error_2，再次计算重要性\n（3）按照特征的重要性排序，然后剔除后面不重要的特征\n（4）然后重复以上步骤，直到选出m个特征。\n```\n\n<br>\n\n```\nRF特征重要性的度量方法\n（1）对于每一棵决策树，计算其oob error_0\n（2）选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1\n（3）特征的重要性 = ∑(oob error_1-oob error_0) / 随机森林中决策树的个数\n（4）对随机森林中的特征变量按照特征重要性降序排序。\n（5）然后重复以上步骤，直到选出m个特征。\n\n解释：\n用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。\n```\n\n\n-------\n\n### 参考\n\n> [随机森林原理篇](https://blog.csdn.net/a819825294/article/details/51177435)\n\n> [随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)\n\n> [Introduction to Random forest (博主：爱67)](http://www.cnblogs.com/Bfrican/p/4463292.html)\n\n> [随即森林原理-简书](https://www.jianshu.com/p/57e862d695f2)\n\n> [Introduction to Random forest – Simplified](https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/)","source":"_posts/2018-04-21-randomforest.md","raw":"---\nlayout: post\ntitle: 随机森林 Random Forest\ndate: 2018-04-21 20:10 +0800\ncategories: 机器学习\ntags:\n- 集成学习\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n## <center> 随即森林 - Random Forest </center>\n\n目录\n* 基本概念\n* 袋外错误率(out of bag error, oob error)\n* 例子\n* 特点及细节\n\n> TODO 分布式实现原理 \n> https://www.jianshu.com/p/d90189008864 \n> https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/\n\n------\n\n### 基本概念\n\n随机森林是由多颗CART树组成的，具体决策树部分知识[在这里](/2018/03/decision-tree/)\n\n前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。\n打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。\n该动物到底是老鼠还是松鼠，要依据**投票情况**来确定，获得票数最多的类别就是森林的分类结果。\n森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。\n少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。\n将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想\n（bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）。\n\n有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？\n\n每棵树的按照如下规则生成：\n\n（1）如果训练集大小为N，对于每棵树而言，**随机且有放回**地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。\n\n（2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树(节点)进行分裂时，从这m个特征中选择最优的；\n*具体意思是：每次进行分支的时候，都重新随机选择m个特征，计算这m个特征里，分裂效果最好的那个特征进行分类*\n\n（3）每棵树都尽最大程度的生长，并且没有剪枝过程。\n\n一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性（采样数据、采样特征）。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。\n\n随机森林分类效果（错误率）与两个因素有关：\n* 森林中任意两棵树的相关性：相关性越大，错误率越大；\n* 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。\n\n\n-------\n\n### 袋外错误率(out of bag error, oob error)\n\n上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error。\n\n随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。\n它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。\n\n我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。\n所以对于每棵树而言（假设对于第k棵树），大约有1/3(\\\\( 1/e \\approx 0.368 \\\\))的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。\n\n而这样的采样特点就允许我们进行oob估计，它的计算方式如下：(note：以样本为单位)\n\n（1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；\n\n（2）然后以简单多数投票作为该样本的分类结果；\n\n（3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。\n\noob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。\n\n\n------\n\n### 例子\n\n描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。\n\n收入层次 :\n\n  Band 1 : < 40,000\n\n  Band 2: 40,000 – 150,000\n\n  Band 3: > 150,000\n\n随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。\n\nCART 1 : Variable Age\n\n![age](/posts_res/2018-04-21-randomforest/3-1.png)\n\nCART 2 : Variable Gender\n\n![gender](/posts_res/2018-04-21-randomforest/3-2.png)\n\nCART 3 : Variable Education\n\n![edu](/posts_res/2018-04-21-randomforest/3-3.png)\n\nCART 4 : Variable Residence\n\n![res](/posts_res/2018-04-21-randomforest/3-4.png)\n\nCART 5 : Variable Industry\n\n![industy](/posts_res/2018-04-21-randomforest/3-5.png)\n\n\n我们要预测的某个人的信息如下：\n\n1. Age : 35 years\n2. Gender : Male\n3. Highest Educational Qualification : Diploma holder\n4. Industry : Manufacturing\n5. Residence : Metro\n\n根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：\n\n![level](/posts_res/2018-04-21-randomforest/3-6.png)\n\n最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（< 40,000）。\n\n\n-------\n\n### 特点及细节\n\n* 在当前所有算法中，具有极好的准确率\n* 能够有效地运行在大数据集上\n* 能够处理具有高维特征的输入样本，而且不需要降维\n* 能够评估各个特征在分类问题上的重要性\n* 在生成过程中，能够获取到内部生成误差的一种无偏估计\n* 对于缺省值问题也能够获得很好得结果\n\n```\n为什么要随机抽样训练集？\n如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；\n\n为什么要有放回地抽样？\n如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是\"有偏的\"，都是\"片面的\"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是\"求同\"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是\"盲人摸象\"。\n```\n\n<br>\n\n```\n减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。\n所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。\n```\n\n<br>\n\n```\nRF特征选择\n首先特征选择的目标有两个：\n1：找到与分类结果高度相关的特征变量。\n2：选择出数目较少的特征变量并且能够充分的预测应变量的结果。\n\n特征选择的步骤：\n（1）对于每一棵决策树，计算其oob error\n（2）随机的修改OOB中的每个特征xi的值，计算oob error_2，再次计算重要性\n（3）按照特征的重要性排序，然后剔除后面不重要的特征\n（4）然后重复以上步骤，直到选出m个特征。\n```\n\n<br>\n\n```\nRF特征重要性的度量方法\n（1）对于每一棵决策树，计算其oob error_0\n（2）选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1\n（3）特征的重要性 = ∑(oob error_1-oob error_0) / 随机森林中决策树的个数\n（4）对随机森林中的特征变量按照特征重要性降序排序。\n（5）然后重复以上步骤，直到选出m个特征。\n\n解释：\n用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。\n```\n\n\n-------\n\n### 参考\n\n> [随机森林原理篇](https://blog.csdn.net/a819825294/article/details/51177435)\n\n> [随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)\n\n> [Introduction to Random forest (博主：爱67)](http://www.cnblogs.com/Bfrican/p/4463292.html)\n\n> [随即森林原理-简书](https://www.jianshu.com/p/57e862d695f2)\n\n> [Introduction to Random forest – Simplified](https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/)","slug":"randomforest","published":1,"updated":"2019-08-17T09:35:19.050Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvw001z2qwpq5188fx7","content":"<h2 id=\"随即森林-Random-Forest\"><a href=\"#随即森林-Random-Forest\" class=\"headerlink\" title=\" 随即森林 - Random Forest \"></a><center> 随即森林 - Random Forest </center></h2><p>目录</p><ul>\n<li>基本概念</li>\n<li>袋外错误率(out of bag error, oob error)</li>\n<li>例子</li>\n<li>特点及细节</li>\n</ul><blockquote>\n<p>TODO 分布式实现原理 \n<a href=\"https://www.jianshu.com/p/d90189008864\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/d90189008864</a> \n<a href=\"https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/\" target=\"_blank\" rel=\"noopener\">https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/</a></p>\n</blockquote><hr><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>随机森林是由多颗CART树组成的，具体决策树部分知识<a href=\"/2018/03/decision-tree/\">在这里</a></p><p>前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。\n打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。\n该动物到底是老鼠还是松鼠，要依据<strong>投票情况</strong>来确定，获得票数最多的类别就是森林的分类结果。\n森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。\n少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。\n将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想\n（bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）。</p><a id=\"more\"></a>\n\n\n\n\n\n<p>有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？</p>\n<p>每棵树的按照如下规则生成：</p>\n<p>（1）如果训练集大小为N，对于每棵树而言，<strong>随机且有放回</strong>地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。</p>\n<p>（2）如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树(节点)进行分裂时，从这m个特征中选择最优的；\n<em>具体意思是：每次进行分支的时候，都重新随机选择m个特征，计算这m个特征里，分裂效果最好的那个特征进行分类</em></p>\n<p>（3）每棵树都尽最大程度的生长，并且没有剪枝过程。</p>\n<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性（采样数据、采样特征）。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>\n<p>随机森林分类效果（错误率）与两个因素有关：</p>\n<ul>\n<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>\n<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>\n</ul>\n<hr>\n<h3 id=\"袋外错误率-out-of-bag-error-oob-error\"><a href=\"#袋外错误率-out-of-bag-error-oob-error\" class=\"headerlink\" title=\"袋外错误率(out of bag error, oob error)\"></a>袋外错误率(out of bag error, oob error)</h3><p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error。</p>\n<p>随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。\n它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。</p>\n<p>我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。\n所以对于每棵树而言（假设对于第k棵树），大约有1/3(\\( 1/e \\approx 0.368 \\))的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。</p>\n<p>而这样的采样特点就允许我们进行oob估计，它的计算方式如下：(note：以样本为单位)</p>\n<p>（1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；</p>\n<p>（2）然后以简单多数投票作为该样本的分类结果；</p>\n<p>（3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。</p>\n<p>oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。</p>\n<hr>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><p>描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。</p>\n<p>收入层次 :</p>\n<p>  Band 1 : &lt; 40,000</p>\n<p>  Band 2: 40,000 – 150,000</p>\n<p>  Band 3: &gt; 150,000</p>\n<p>随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。</p>\n<p>CART 1 : Variable Age</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-1.png\" alt=\"age\"></p>\n<p>CART 2 : Variable Gender</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-2.png\" alt=\"gender\"></p>\n<p>CART 3 : Variable Education</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-3.png\" alt=\"edu\"></p>\n<p>CART 4 : Variable Residence</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-4.png\" alt=\"res\"></p>\n<p>CART 5 : Variable Industry</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-5.png\" alt=\"industy\"></p>\n<p>我们要预测的某个人的信息如下：</p>\n<ol>\n<li>Age : 35 years</li>\n<li>Gender : Male</li>\n<li>Highest Educational Qualification : Diploma holder</li>\n<li>Industry : Manufacturing</li>\n<li>Residence : Metro</li>\n</ol>\n<p>根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-6.png\" alt=\"level\"></p>\n<p>最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（&lt; 40,000）。</p>\n<hr>\n<h3 id=\"特点及细节\"><a href=\"#特点及细节\" class=\"headerlink\" title=\"特点及细节\"></a>特点及细节</h3><ul>\n<li>在当前所有算法中，具有极好的准确率</li>\n<li>能够有效地运行在大数据集上</li>\n<li>能够处理具有高维特征的输入样本，而且不需要降维</li>\n<li>能够评估各个特征在分类问题上的重要性</li>\n<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>\n<li>对于缺省值问题也能够获得很好得结果</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">为什么要随机抽样训练集？</span><br><span class=\"line\">如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有<span class=\"keyword\">bagging的必要；</span></span><br><span class=\"line\"><span class=\"keyword\"></span></span><br><span class=\"line\"><span class=\"keyword\">为什么要有放回地抽样？</span></span><br><span class=\"line\"><span class=\"keyword\">如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是\"有偏的\"，都是\"片面的\"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是\"求同\"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是\"盲人摸象\"。</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。</span><br><span class=\"line\">所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RF特征选择</span><br><span class=\"line\">首先特征选择的目标有两个：</span><br><span class=\"line\"><span class=\"number\">1</span>：找到与分类结果高度相关的特征变量。</span><br><span class=\"line\"><span class=\"number\">2</span>：选择出数目较少的特征变量并且能够充分的预测应变量的结果。</span><br><span class=\"line\"></span><br><span class=\"line\">特征选择的步骤：</span><br><span class=\"line\">（<span class=\"number\">1</span>）对于每一棵决策树，计算其oob error</span><br><span class=\"line\">（<span class=\"number\">2</span>）随机的修改OOB中的每个特征xi的值，计算oob error_2，再次计算重要性</span><br><span class=\"line\">（<span class=\"number\">3</span>）按照特征的重要性排序，然后剔除后面不重要的特征</span><br><span class=\"line\">（<span class=\"number\">4</span>）然后重复以上步骤，直到选出m个特征。</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RF特征重要性的度量方法</span><br><span class=\"line\">（<span class=\"number\">1</span>）对于每一棵决策树，计算其oob error_0</span><br><span class=\"line\">（<span class=\"number\">2</span>）选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1</span><br><span class=\"line\">（<span class=\"number\">3</span>）特征的重要性 = ∑(oob error_1-oob error_0) / 随机森林中决策树的个数</span><br><span class=\"line\">（<span class=\"number\">4</span>）对随机森林中的特征变量按照特征重要性降序排序。</span><br><span class=\"line\">（<span class=\"number\">5</span>）然后重复以上步骤，直到选出m个特征。</span><br><span class=\"line\"></span><br><span class=\"line\">解释：</span><br><span class=\"line\">用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51177435\" target=\"_blank\" rel=\"noopener\">随机森林原理篇</a></p>\n<p><a href=\"https://www.cnblogs.com/maybe2030/p/4585705.html\" target=\"_blank\" rel=\"noopener\">随机森林（Random Forest）</a></p>\n<p><a href=\"http://www.cnblogs.com/Bfrican/p/4463292.html\" target=\"_blank\" rel=\"noopener\">Introduction to Random forest (博主：爱67)</a></p>\n<p><a href=\"https://www.jianshu.com/p/57e862d695f2\" target=\"_blank\" rel=\"noopener\">随即森林原理-简书</a></p>\n<p><a href=\"https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\" target=\"_blank\" rel=\"noopener\">Introduction to Random forest – Simplified</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"随即森林-Random-Forest\"><a href=\"#随即森林-Random-Forest\" class=\"headerlink\" title=\" 随即森林 - Random Forest \"></a><center> 随即森林 - Random Forest </center></h2><p>目录</p><ul>\n<li>基本概念</li>\n<li>袋外错误率(out of bag error, oob error)</li>\n<li>例子</li>\n<li>特点及细节</li>\n</ul><blockquote>\n<p>TODO 分布式实现原理 \n<a href=\"https://www.jianshu.com/p/d90189008864\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/p/d90189008864</a> \n<a href=\"https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/\" target=\"_blank\" rel=\"noopener\">https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-random-forest/</a></p>\n</blockquote><hr><h3 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h3><p>随机森林是由多颗CART树组成的，具体决策树部分知识<a href=\"/2018/03/decision-tree/\">在这里</a></p><p>前面提到，随机森林中有许多的分类树。我们要将一个输入样本进行分类，我们需要将输入样本输入到每棵树中进行分类。\n打个形象的比喻：森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。\n该动物到底是老鼠还是松鼠，要依据<strong>投票情况</strong>来确定，获得票数最多的类别就是森林的分类结果。\n森林中的每棵树都是独立的，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。\n少数优秀的树的预测结果将会超脱于芸芸“噪音”，做出一个好的预测。\n将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想\n（bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性）。</p>","more":"\n\n\n\n\n\n<p>有了树我们就可以分类了，但是森林中的每棵树是怎么生成的呢？</p>\n<p>每棵树的按照如下规则生成：</p>\n<p>（1）如果训练集大小为N，对于每棵树而言，<strong>随机且有放回</strong>地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本。</p>\n<p>（2）如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树(节点)进行分裂时，从这m个特征中选择最优的；\n<em>具体意思是：每次进行分支的时候，都重新随机选择m个特征，计算这m个特征里，分裂效果最好的那个特征进行分类</em></p>\n<p>（3）每棵树都尽最大程度的生长，并且没有剪枝过程。</p>\n<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性（采样数据、采样特征）。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>\n<p>随机森林分类效果（错误率）与两个因素有关：</p>\n<ul>\n<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>\n<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>\n</ul>\n<hr>\n<h3 id=\"袋外错误率-out-of-bag-error-oob-error\"><a href=\"#袋外错误率-out-of-bag-error-oob-error\" class=\"headerlink\" title=\"袋外错误率(out of bag error, oob error)\"></a>袋外错误率(out of bag error, oob error)</h3><p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error。</p>\n<p>随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。\n它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。</p>\n<p>我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。\n所以对于每棵树而言（假设对于第k棵树），大约有1/3(\\( 1/e \\approx 0.368 \\))的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。</p>\n<p>而这样的采样特点就允许我们进行oob估计，它的计算方式如下：(note：以样本为单位)</p>\n<p>（1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；</p>\n<p>（2）然后以简单多数投票作为该样本的分类结果；</p>\n<p>（3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。</p>\n<p>oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。</p>\n<hr>\n<h3 id=\"例子\"><a href=\"#例子\" class=\"headerlink\" title=\"例子\"></a>例子</h3><p>描述：根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。</p>\n<p>收入层次 :</p>\n<p>  Band 1 : &lt; 40,000</p>\n<p>  Band 2: 40,000 – 150,000</p>\n<p>  Band 3: &gt; 150,000</p>\n<p>随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。</p>\n<p>CART 1 : Variable Age</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-1.png\" alt=\"age\"></p>\n<p>CART 2 : Variable Gender</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-2.png\" alt=\"gender\"></p>\n<p>CART 3 : Variable Education</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-3.png\" alt=\"edu\"></p>\n<p>CART 4 : Variable Residence</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-4.png\" alt=\"res\"></p>\n<p>CART 5 : Variable Industry</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-5.png\" alt=\"industy\"></p>\n<p>我们要预测的某个人的信息如下：</p>\n<ol>\n<li>Age : 35 years</li>\n<li>Gender : Male</li>\n<li>Highest Educational Qualification : Diploma holder</li>\n<li>Industry : Manufacturing</li>\n<li>Residence : Metro</li>\n</ol>\n<p>根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况：</p>\n<p><img src=\"/posts_res/2018-04-21-randomforest/3-6.png\" alt=\"level\"></p>\n<p>最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（&lt; 40,000）。</p>\n<hr>\n<h3 id=\"特点及细节\"><a href=\"#特点及细节\" class=\"headerlink\" title=\"特点及细节\"></a>特点及细节</h3><ul>\n<li>在当前所有算法中，具有极好的准确率</li>\n<li>能够有效地运行在大数据集上</li>\n<li>能够处理具有高维特征的输入样本，而且不需要降维</li>\n<li>能够评估各个特征在分类问题上的重要性</li>\n<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>\n<li>对于缺省值问题也能够获得很好得结果</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">为什么要随机抽样训练集？</span><br><span class=\"line\">如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有<span class=\"keyword\">bagging的必要；</span></span><br><span class=\"line\"><span class=\"keyword\"></span></span><br><span class=\"line\"><span class=\"keyword\">为什么要有放回地抽样？</span></span><br><span class=\"line\"><span class=\"keyword\">如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是\"有偏的\"，都是\"片面的\"，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是\"求同\"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是\"盲人摸象\"。</span></span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。</span><br><span class=\"line\">所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RF特征选择</span><br><span class=\"line\">首先特征选择的目标有两个：</span><br><span class=\"line\"><span class=\"number\">1</span>：找到与分类结果高度相关的特征变量。</span><br><span class=\"line\"><span class=\"number\">2</span>：选择出数目较少的特征变量并且能够充分的预测应变量的结果。</span><br><span class=\"line\"></span><br><span class=\"line\">特征选择的步骤：</span><br><span class=\"line\">（<span class=\"number\">1</span>）对于每一棵决策树，计算其oob error</span><br><span class=\"line\">（<span class=\"number\">2</span>）随机的修改OOB中的每个特征xi的值，计算oob error_2，再次计算重要性</span><br><span class=\"line\">（<span class=\"number\">3</span>）按照特征的重要性排序，然后剔除后面不重要的特征</span><br><span class=\"line\">（<span class=\"number\">4</span>）然后重复以上步骤，直到选出m个特征。</span><br></pre></td></tr></table></figure>\n<p><br></p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">RF特征重要性的度量方法</span><br><span class=\"line\">（<span class=\"number\">1</span>）对于每一棵决策树，计算其oob error_0</span><br><span class=\"line\">（<span class=\"number\">2</span>）选取一个特征，随机对特征加入噪声干扰，再次计算oob error_1</span><br><span class=\"line\">（<span class=\"number\">3</span>）特征的重要性 = ∑(oob error_1-oob error_0) / 随机森林中决策树的个数</span><br><span class=\"line\">（<span class=\"number\">4</span>）对随机森林中的特征变量按照特征重要性降序排序。</span><br><span class=\"line\">（<span class=\"number\">5</span>）然后重复以上步骤，直到选出m个特征。</span><br><span class=\"line\"></span><br><span class=\"line\">解释：</span><br><span class=\"line\">用这个公式来度量特征重要性，原因是：给某个特征随机的加入噪声后，如果oob error增大，说明这个特征对样本分类的结果影响比较大，说明重要程度比较高。</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><blockquote>\n<p><a href=\"https://blog.csdn.net/a819825294/article/details/51177435\" target=\"_blank\" rel=\"noopener\">随机森林原理篇</a></p>\n<p><a href=\"https://www.cnblogs.com/maybe2030/p/4585705.html\" target=\"_blank\" rel=\"noopener\">随机森林（Random Forest）</a></p>\n<p><a href=\"http://www.cnblogs.com/Bfrican/p/4463292.html\" target=\"_blank\" rel=\"noopener\">Introduction to Random forest (博主：爱67)</a></p>\n<p><a href=\"https://www.jianshu.com/p/57e862d695f2\" target=\"_blank\" rel=\"noopener\">随即森林原理-简书</a></p>\n<p><a href=\"https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\" target=\"_blank\" rel=\"noopener\">Introduction to Random forest – Simplified</a></p>\n</blockquote>\n"},{"layout":"post","title":"Word2Vec Tutorial","date":"2018-05-03T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n\n目录\n\n* Word2Vec - CBOW\n* Word2Vec - Skip-Gram\n* Word2Vec的Tricks\n\n\n---------\n\n### 1. Word2Vec - CBOW\n\nCBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型，\n不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。\n\n先来模型结构图，如下：\n\n![CBOW_Structure](/posts_res/2018-05-03-word2vec/1-1.jpg)\n\n可以看出，CBOW模型是预测\\\\( P(w\\_t \\| w\\_{t-k}, w\\_{t-(k-1)}, ..., w\\_{t-1}, w\\_{t+1}, w\\_{t+2}, ..., w\\_{t+k}) \\\\)。\n\n从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下：\n\n```cpp\n// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)\n\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\nb = next_random % window;\n\n// in -> hidden\ncw = 0;\nfor (a = b; a < window * 2 + 1 - b; a++)\n    if (a != window) \n    {\n        c = sentence_position - window + a;\n        if (c < 0) continue;\n        if (c >= sentence_length) continue;\n        last_word = sen[c];\n        if (last_word == -1) continue;\n        for (c = 0; c < layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];\n        cw++;\n    }\n```\n\n其中``sentence_position``为当前``word``在句子中的下标，以一个具体的句子``A B C D``为例，\n第一次进入到下面代码时，当前``word``为A，``sentence_position``为0，``b``是一个随机生成的0到window-1的词，\n整个窗口的大小为(2 * window + 1 - 2 * b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动，\n其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值``b``的大小决定了当前窗口的大小。\n代码中的``neu1``即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。\n\n<br>\n\n*CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。*\n\n#### Hierarchical Softmax\n\n该算法结合了Huffman编码，每个词``w``都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\\\( n(w,j) \\\\)为这条路径上的第\\\\(j\\\\)个节点，且\\\\( L(w) \\\\)为这条路径的长度，注意\\\\(j\\\\)从1开始编码，即\\\\( n(w,1)=root, n(w,L(w))=w \\\\)。\n对于第\\\\(j\\\\)个节点，Hierarchical Softmax算法定义的``Lable``为 1-code[j]，而输出为\n\\\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\\\]\n\n``Loss``为负的log似然，即：\n\\\\[\nLoss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f)\n\\\\]\n\n那么梯度为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{neu1} \n& = \\frac{\\partial Loss}{\\partial neu1} \\\\\\ \n& = - (1-code[j]) \\cdot (1-f) \\cdot syn1 + code[j] \\cdot f \\cdot syn1 \\\\\\\n& = - (1-code[j] - f) \\cdot syn1 \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{syn1} \n& = \\frac{\\partial Loss}{\\partial syn1} \\\\\\ \n& = - (1-code[j]) \\cdot (1-f) \\cdot neu1 + code[j] \\cdot f \\cdot neu1 \\\\\\\n& = - (1-code[j] - f) \\cdot neu1 \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n需要注意的是，word2vec源码中的``g``实际为负梯度中公共部分与``Learningrate alpha``的乘积。\n\n```cpp\n// Hierarchical Softmax\nif (hs)\n    for (d = 0; d < vocab[word].codelen; d++) \n    {\n        f = 0;\n        l2 = vocab[word].point[d] * layer1_size;\n        // Propagate hidden -> output\n        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2]; // syn1 is weight between hidden and output\n\n        if (f <= -MAX_EXP) continue;\n        else if (f >= MAX_EXP) continue;\n        else\n            // expTable to speed running\n            f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n\n        // 'g' is the common part of gradient multiplied by the learning rate\n        g = (1 - vocab[word].code[d] - f) * alpha;\n\n        // Propagate errors output -> hidden\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n\n        // Learn weights hidden -> output\n        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];\n    }\n```\n\n\n#### Negative Sampling\n\n在源码中就是随机生成``negative``个负例(也有可能少于这个数，当随机撞上原来的word跳过)。\n原来的``word``为正例，``Label``为1，其他随机生成的``Lable``为0，输出``f``仍为：\n\\\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\\\]\n\n``Loss``为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即：\n\\\\[\nLoss = -Likelihood = - label \\cdot log f - (1-label) \\cdot log (1-f)\n\\\\]\n\n那么梯度为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{neu1} \n& = \\frac{\\partial Loss}{\\partial neu1} \\\\\\\n& = -label \\cdot (1-f) \\cdot syn1 + (1-label) \\cdot f \\cdot syn1 \\\\\\\n& = -(label - f) \\cdot syn1\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{syn1} \n& = \\frac{\\partial Loss}{\\partial syn1} \\\\\\\n& = -label \\cdot (1-f) \\cdot neu1 + (1-label) \\cdot f \\cdot neu1 \\\\\\\n& = -(label - f) \\cdot neu1\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n同样注意代码中``g``并非梯度，可以看作是乘了``Learningrate alpha``的error(label与输出f的差)。\n\n```cpp\n// NEGATIVE SAMPLING\nif (negative > 0)\n    for (d = 0; d < negative + 1; d++) \n    {\n        if (d == 0) \n        {\n            target = word;\n            label = 1;\n        } \n        else \n        {\n            next_random = next_random * (unsigned long long) 25214903917 + 11;\n            target = table[(next_random >> 16) % table_size];\n            if (target == 0) target = next_random % (vocab_size - 1) + 1;\n            if (target == word) continue;\n            label = 0;\n        }\n        l2 = target * layer1_size;\n        f = 0;\n        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1neg[c + l2];\n        if (f > MAX_EXP) g = (label - 1) * alpha;\n        else if (f < -MAX_EXP) g = (label - 0) * alpha;\n        else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];\n        for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * neu1[c];\n    }\n```\n\n\n#### 隐含层到输入层的梯度传播\n\n因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。\n\n```cpp\n// hidden -> in\nfor (a = b; a < window * 2 + 1 - b; a++)\n    if (a != window) \n    {\n        c = sentence_position - window + a;\n        if (c < 0) continue;\n        if (c >= sentence_length) continue;\n        last_word = sen[c];\n        if (last_word == -1) continue;\n        for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];\n    }\n```\n\n\n----------\n\n### 2. Word2Vec - Skip-Gram\n\nSkip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\\\\( P(w\\_i \\| w\\_t) \\\\)，其中\\\\( t-c \\leq i \\leq t+c, 且 i \\not = t\\\\)，\n``c``是决定上下文窗口大小的常数，``c``越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。\n\n![skip-gram](/posts_res/2018-05-03-word2vec/2-1.jpg)\n\n假设存在一个\\\\( w\\_1, w\\_2, w\\_3, ..., w\\_T \\\\)的词组序列，Skip-Gram的目标最大化：\n\\\\[\n\\frac{1}{T} \\mathop{\\sum\\_{t=1}^T} \\mathop{\\sum\\_{-c \\leq j \\leq c, j \\not= 0}} log p(w\\_{t+j} \\| w\\_t)\n\\\\]\n\n基本的Skip-Gram模型定义\\\\( P(w\\_o \\| w\\_i) \\\\)为：\n\\\\[\nP(w\\_o \\| w\\_i) = e^{v\\_{w\\_o}^T v\\_{w\\_i}} / \\sum\\_{w=1}^W e^{v\\_w^T v\\_{w\\_i}}\n\\\\]\n\n从公式中不能看出，Skip-Gram是一个对称的模型，如果\\\\(w\\_t\\\\)为中心词时，\\\\( w\\_k \\\\)在其窗口内，\n则\\\\( w\\_t \\\\)也必然在以\\\\( w\\_k \\\\)为中心词的同样大小的窗口内，也就是：\n\\\\[\n\\frac{1}{T} \\sum\\_{t=1}^T \\sum\\_{-c \\leq j \\leq c, j \\not=0} log P(w\\_{t+j} \\| w\\_t) = \\frac{1}{T} \\sum\\_{t=1}^T \\sum\\_{-c \\leq j \\leq c, j \\not=0} log P(w\\_t \\| w\\_{t+j})\n\\\\]\n\n同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指**在一定窗口内的词两两会计算概率**，即使它们之间隔着一些词，\n这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。\n\n<br>\n\n*与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。*\n\nHierarchical Softmax算法也结合了Huffman编码，每个词``w``都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\\\( n(w,j) \\\\)为这条路径上的第``j``个节点，且``L(w)``为这条路径的长度，注意``j``从1开始编码，即\\\\( n(w,1)=root, n(w, L(w))=w\\\\)。\n层级softmax定义的概率\\\\( P(w \\| w\\_I) \\\\)为：\n\\\\[\nP(w \\| w\\_I) = \\prod\\_{j=1}^{L(w)-1} \\sigma \\lbrace \\vartheta [ n(w, j+1) = ch(n(w, j)) ] \\cdot v\\_{n(w,j)}^{'T} v\\_I \\rbrace\n\\\\]\n其中:\n\\\\[\n\\vartheta(x) = \n\\begin{cases}\n1, \\quad if \\quad x \\quad is \\quad true \\\\\\\n-1, \\quad otherwise\n\\end{cases}\n\\\\]\n\\\\( ch(n(w, j)) \\\\)既可以是\\\\( n(w,j)\\\\)的左子节点也可以是\\\\( n(w,j) \\\\)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。\n\nLoss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nLoss\\_pair \n& = -Log Likelihood\\_pair \\\\\\\n& = - log P(w\\| w\\_I) \\\\\\\n& = - \\sum\\_{j=1}^{L(w)-1} log \\lbrace \\sigma [ \\vartheta ( n(w,j+1)=ch(n(w,j)) ) \\cdot v\\_{n(w,j)}^{'T} v\\_I ] \\rbrace \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**如果当前节点是左子节点**，即\\\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\\\)为``true``\n\n那么\\\\( Loss = - log \\lbrace \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\\\)，则梯度为：\n\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}} = - (1-\\sigma (v\\_{n(w,j)}^{'T} v\\_I)) \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} =  \\frac{\\partial Loss}{\\partial v\\_I} = - (1-\\sigma (v\\_{n(w,j)}^{'T} v\\_I)) \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n**如果当前节点是右子节点**，即\\\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\\\)为``false``\n\n那么\\\\( Loss = - log \\lbrace \\sigma( - v\\_{n(w,j)}^{'T} v\\_I) \\rbrace = - log(1- \\sigma(v\\_{n(w,j)}^{'T} v\\_I)) \\\\)，则梯度为：\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}} = \\sigma (v\\_{n(w,j)}^{'T} v\\_I) \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} =  \\frac{\\partial Loss}{\\partial v\\_I} = \\sigma (v\\_{n(w,j)}^{'T} v\\_I) \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n**合并式(15)和式(16)，得：**\n\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}}\n= - \\lbrace 1 - code[j] - \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} = \\frac{\\partial Loss}{\\partial v\\_I}\n= - \\lbrace 1 - code[j] - \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n此处相关的代码如下，其中``g``是梯度中的公共部分\\\\( (1-code[j]-\\sigma( v\\_{n(w,j)}^{'T} v\\_I)) \\\\)与``Learningrate alpha``的乘积。\n\n```cpp\n// HIERARCHICAL SOFTMAX\nif (hs)\n    for (d = 0; d < vocab[word].codelen; d++) \n    {\n        f = 0;\n        l2 = vocab[word].point[d] * layer1_size;\n\n        // Propagate hidden -> output\n        for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];\n\n        if (f <= -MAX_EXP) continue;\n        else if (f >= MAX_EXP) continue;\n        else f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n\n        // 'g' is the gradient multiplied by the learning rate\n        g = (1 - vocab[word].code[d] - f) * alpha;\n\n        // Propagate errors output -> hidden\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n\n        // Learn weights hidden -> output\n        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];\n    }\n```\n\nNegative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下：\n\n```cpp\n// NEGATIVE SAMPLING\nif (negative > 0)\n    for (d = 0; d < negative + 1; d++) \n    {\n        if (d == 0) \n        {\n            target = word;\n            label = 1;\n        } \n        else \n        {\n            next_random = next_random * (unsigned long long) 25214903917 + 11;\n            target = table[(next_random >> 16) % table_size];\n            if (target == 0) target = next_random % (vocab_size - 1) + 1;\n            if (target == word) continue;\n            label = 0;\n        }\n        l2 = target * layer1_size;\n        f = 0;\n        for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];\n        if (f > MAX_EXP) g = (label - 1) * alpha;\n        else if (f < -MAX_EXP) g = (label - 0) * alpha;\n        else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];\n        for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];\n    }\n```\n\n\n--------\n\n### 3. Word2Vec的Tricks\n\n#### 3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？\n\n前面提到到 Skip-Gram 中的条件概率为：\n\\\\[\nP(w\\_o \\| w\\_i) = e^{v\\_{w\\_o}^T v\\_{w\\_i}} / \\sum\\_{w=1}^W e^{v\\_w^T v\\_{w\\_i}}\n\\\\]\n\n这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\\\\( p(w\\_o \\| w\\_i) \\\\)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。\n\n另外一种方法是只更新当前 \\\\( w\\_o、 w\\_i \\\\)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。\n\nHierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\\\\( w\\_i \\\\)，先让模型判断\\\\( w\\_o \\\\)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。\n\nNegative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。\n\n\n#### 3.2 指数运算\n\n由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为：\n\n```text\nexpTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6\nexpTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。\n```\n\n相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。\n\n```cpp\nexpTable = (real *) malloc((EXP_TABLE_SIZE + 1) * sizeof(real));\nfor (i = 0; i < EXP_TABLE_SIZE; i++) \n{\n    expTable[i] = exp((i / (real) EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table\n    expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)\n}\n```\n\n相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的\n下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。\n\n```cpp\nfor (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];\nif (f > MAX_EXP) g = (label - 1) * alpha;\nelse if (f < -MAX_EXP) g = (label - 0) * alpha;\nelse g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n```\n\n\n#### 3.3 按word分布随机抽样\n\nword2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料：\n\n[http://en.wikipedia.org/wiki/Categorical_distribution](http://en.wikipedia.org/wiki/Categorical_distribution)，\n\n[http://en.wikipedia.org/wiki/Categorical_distribution#Sampling](http://en.wikipedia.org/wiki/Categorical_distribution#Sampling)\n\nCategorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。\n维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下：\n\n\n**方法一**：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。\n\n事先准备：\n```text\n1. 计算每个类别或词的未归一化分布值（对于词来即词频）；\n2. 把上面计算的值加和，利用这个和进行概率归一化；\n3. 引入类别或词的一定顺序，比如词的下标；\n4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。\n```\n\n使用时：\n```text\n1. 随机抽取一个 0 到 1 之间的随机数；\n2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；\n3. 返回 CDF 对应的类别或词。\n```\n\n使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。\n\n\n**方法二**：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。\n\n```text\n1. r = 1;\n2. s = 0;\n3. for (i = 1; i <= k; i++) \n4. {\n5.     v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r\n6.     //产生 v 个下标为 i 的样本\n7.     for (j = 1; j <= v; j++)\n8.         z[s++] = i;\n9.     n = n – v;    //下次需要继续生成 n 个样本\n10.    r = r – p[i]; //剩余样本的概率和\n11. }\n12. 随机重新排列(shuffle)z中的左右样本；\n13. 返回 z。\n```\n\n这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。\n\n\n**方法三**：即word2vec实现方法\n\n```text\n(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)\n|_____________a___________|________b________|____c____|\n\n(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。\n|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|\n\nword2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。\n\nvoid InitUnigramTable() \n{\n    int a, i;\n    double train_words_pow = 0;\n    double d1, power = 0.75;\n    table = (int *) malloc(table_size * sizeof(int));\n    for (a = 0; a < vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);\n    i = 0;\n    d1 = pow(vocab[i].cn, power) / train_words_pow;\n    for (a = 0; a < table_size; a++) \n    {\n        table[a] = i;\n        if (a / (double) table_size > d1) \n        {\n            i++;\n            d1 += pow(vocab[i].cn, power) / train_words_pow;\n        }\n        if (i >= vocab_size) i = vocab_size - 1;\n    }\n}\n```\n\n该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。\n\n\n#### 3.4 哈希编码\n\n个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下：\n\n```cpp\n// Returns hash value of a word\nint GetWordHash(char *word) {\n    unsigned long long a, hash = 0;\n    for (a = 0; a < strlen(word); a++) hash = hash * 257 + word[a];\n    hash = hash % vocab_hash_size;\n    return hash;\n}\n\n// Returns position of a word in the vocabulary; if the word is not found, returns -1\nint SearchVocab(char *word) {\n    unsigned int hash = GetWordHash(word);\n    while (1) {\n        if (vocab_hash[hash] == -1) return -1;\n        if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];\n        hash = (hash + 1) % vocab_hash_size;\n    }\n    return -1;\n}\n```\n\n\n#### 3.5 随机数\n\n作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。\n\n```cpp\nreal ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\nif (ran < (next_random & 0xFFFF) / (real) 65536) continue;\n```\n\n\n#### 3.6 高频词亚采样\n\n这里的亚采样是指 Sub-Sampling，每个词\\\\( w\\_i \\\\)被丢弃的概率为：\n\\\\[\nP(w\\_i) = 1 - \\sqrt{\\frac{sample}{freq(w\\_i)}}\n\\\\]\n\nsample是一个可以设置的参数， demo-word.sh 中是 10-3，\\\\( freq(w\\_i) \\\\)则为\\\\( w\\_i \\\\)的词频。具体的实现代码如下:\n\n```cpp\nreal ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\n```\n\n从具体代码可以看出，\\\\(w\\_i\\\\)被丢弃的概率为：\n\\\\[\nP(w\\_i) = 1- ( \\sqrt{\\frac{sample}{freq(w\\_i)}} + \\frac{sample}{freq(w\\_i)} )\n\\\\]\n\n\n--------\n\n### 参考\n\n>\n1. [Getting started with Word2Vec](https://textprocessing.org/getting-started-with-word2vec)\n2. [word2vec源码](https://code.google.com/archive/p/word2vec/)\n3. [word2vec源码 - 下载](/posts_res/2018-05-03-word2vec/source-archive.zip)\n4. [Word2Vec详解.pdf](/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf)\n5. [Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙](http://techblog.youdao.com/?p=915)\n6. [机器学习算法实现解析——word2vec源码解析](https://blog.csdn.net/google19890102/article/details/51887344)\n7. [Word2Vec-知其然知其所以然](https://www.zybuluo.com/Dounm/note/591752)\n8. [word2vec 中的数学原理详解](http://www.cnblogs.com/peghoty/p/3857839.html)\n\n","source":"_posts/2018-05-03-word2vec.md","raw":"---\nlayout: post\ntitle: Word2Vec Tutorial\ndate: 2018-05-03 12:10 +0800\ncategories: 深度学习\ntags:\n- Embedding\n- 神经网络\nmathjax: true\ncopyright: false\n---\n\n\n目录\n\n* Word2Vec - CBOW\n* Word2Vec - Skip-Gram\n* Word2Vec的Tricks\n\n\n---------\n\n### 1. Word2Vec - CBOW\n\nCBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型，\n不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。\n\n先来模型结构图，如下：\n\n![CBOW_Structure](/posts_res/2018-05-03-word2vec/1-1.jpg)\n\n可以看出，CBOW模型是预测\\\\( P(w\\_t \\| w\\_{t-k}, w\\_{t-(k-1)}, ..., w\\_{t-1}, w\\_{t+1}, w\\_{t+2}, ..., w\\_{t+k}) \\\\)。\n\n从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下：\n\n```cpp\n// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)\n\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\nb = next_random % window;\n\n// in -> hidden\ncw = 0;\nfor (a = b; a < window * 2 + 1 - b; a++)\n    if (a != window) \n    {\n        c = sentence_position - window + a;\n        if (c < 0) continue;\n        if (c >= sentence_length) continue;\n        last_word = sen[c];\n        if (last_word == -1) continue;\n        for (c = 0; c < layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];\n        cw++;\n    }\n```\n\n其中``sentence_position``为当前``word``在句子中的下标，以一个具体的句子``A B C D``为例，\n第一次进入到下面代码时，当前``word``为A，``sentence_position``为0，``b``是一个随机生成的0到window-1的词，\n整个窗口的大小为(2 * window + 1 - 2 * b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动，\n其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值``b``的大小决定了当前窗口的大小。\n代码中的``neu1``即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。\n\n<br>\n\n*CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。*\n\n#### Hierarchical Softmax\n\n该算法结合了Huffman编码，每个词``w``都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\\\( n(w,j) \\\\)为这条路径上的第\\\\(j\\\\)个节点，且\\\\( L(w) \\\\)为这条路径的长度，注意\\\\(j\\\\)从1开始编码，即\\\\( n(w,1)=root, n(w,L(w))=w \\\\)。\n对于第\\\\(j\\\\)个节点，Hierarchical Softmax算法定义的``Lable``为 1-code[j]，而输出为\n\\\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\\\]\n\n``Loss``为负的log似然，即：\n\\\\[\nLoss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f)\n\\\\]\n\n那么梯度为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{neu1} \n& = \\frac{\\partial Loss}{\\partial neu1} \\\\\\ \n& = - (1-code[j]) \\cdot (1-f) \\cdot syn1 + code[j] \\cdot f \\cdot syn1 \\\\\\\n& = - (1-code[j] - f) \\cdot syn1 \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{syn1} \n& = \\frac{\\partial Loss}{\\partial syn1} \\\\\\ \n& = - (1-code[j]) \\cdot (1-f) \\cdot neu1 + code[j] \\cdot f \\cdot neu1 \\\\\\\n& = - (1-code[j] - f) \\cdot neu1 \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n需要注意的是，word2vec源码中的``g``实际为负梯度中公共部分与``Learningrate alpha``的乘积。\n\n```cpp\n// Hierarchical Softmax\nif (hs)\n    for (d = 0; d < vocab[word].codelen; d++) \n    {\n        f = 0;\n        l2 = vocab[word].point[d] * layer1_size;\n        // Propagate hidden -> output\n        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2]; // syn1 is weight between hidden and output\n\n        if (f <= -MAX_EXP) continue;\n        else if (f >= MAX_EXP) continue;\n        else\n            // expTable to speed running\n            f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n\n        // 'g' is the common part of gradient multiplied by the learning rate\n        g = (1 - vocab[word].code[d] - f) * alpha;\n\n        // Propagate errors output -> hidden\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n\n        // Learn weights hidden -> output\n        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];\n    }\n```\n\n\n#### Negative Sampling\n\n在源码中就是随机生成``negative``个负例(也有可能少于这个数，当随机撞上原来的word跳过)。\n原来的``word``为正例，``Label``为1，其他随机生成的``Lable``为0，输出``f``仍为：\n\\\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\\\]\n\n``Loss``为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即：\n\\\\[\nLoss = -Likelihood = - label \\cdot log f - (1-label) \\cdot log (1-f)\n\\\\]\n\n那么梯度为：\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{neu1} \n& = \\frac{\\partial Loss}{\\partial neu1} \\\\\\\n& = -label \\cdot (1-f) \\cdot syn1 + (1-label) \\cdot f \\cdot syn1 \\\\\\\n& = -(label - f) \\cdot syn1\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nGradient\\_{syn1} \n& = \\frac{\\partial Loss}{\\partial syn1} \\\\\\\n& = -label \\cdot (1-f) \\cdot neu1 + (1-label) \\cdot f \\cdot neu1 \\\\\\\n& = -(label - f) \\cdot neu1\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n同样注意代码中``g``并非梯度，可以看作是乘了``Learningrate alpha``的error(label与输出f的差)。\n\n```cpp\n// NEGATIVE SAMPLING\nif (negative > 0)\n    for (d = 0; d < negative + 1; d++) \n    {\n        if (d == 0) \n        {\n            target = word;\n            label = 1;\n        } \n        else \n        {\n            next_random = next_random * (unsigned long long) 25214903917 + 11;\n            target = table[(next_random >> 16) % table_size];\n            if (target == 0) target = next_random % (vocab_size - 1) + 1;\n            if (target == word) continue;\n            label = 0;\n        }\n        l2 = target * layer1_size;\n        f = 0;\n        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1neg[c + l2];\n        if (f > MAX_EXP) g = (label - 1) * alpha;\n        else if (f < -MAX_EXP) g = (label - 0) * alpha;\n        else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];\n        for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * neu1[c];\n    }\n```\n\n\n#### 隐含层到输入层的梯度传播\n\n因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。\n\n```cpp\n// hidden -> in\nfor (a = b; a < window * 2 + 1 - b; a++)\n    if (a != window) \n    {\n        c = sentence_position - window + a;\n        if (c < 0) continue;\n        if (c >= sentence_length) continue;\n        last_word = sen[c];\n        if (last_word == -1) continue;\n        for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];\n    }\n```\n\n\n----------\n\n### 2. Word2Vec - Skip-Gram\n\nSkip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\\\\( P(w\\_i \\| w\\_t) \\\\)，其中\\\\( t-c \\leq i \\leq t+c, 且 i \\not = t\\\\)，\n``c``是决定上下文窗口大小的常数，``c``越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。\n\n![skip-gram](/posts_res/2018-05-03-word2vec/2-1.jpg)\n\n假设存在一个\\\\( w\\_1, w\\_2, w\\_3, ..., w\\_T \\\\)的词组序列，Skip-Gram的目标最大化：\n\\\\[\n\\frac{1}{T} \\mathop{\\sum\\_{t=1}^T} \\mathop{\\sum\\_{-c \\leq j \\leq c, j \\not= 0}} log p(w\\_{t+j} \\| w\\_t)\n\\\\]\n\n基本的Skip-Gram模型定义\\\\( P(w\\_o \\| w\\_i) \\\\)为：\n\\\\[\nP(w\\_o \\| w\\_i) = e^{v\\_{w\\_o}^T v\\_{w\\_i}} / \\sum\\_{w=1}^W e^{v\\_w^T v\\_{w\\_i}}\n\\\\]\n\n从公式中不能看出，Skip-Gram是一个对称的模型，如果\\\\(w\\_t\\\\)为中心词时，\\\\( w\\_k \\\\)在其窗口内，\n则\\\\( w\\_t \\\\)也必然在以\\\\( w\\_k \\\\)为中心词的同样大小的窗口内，也就是：\n\\\\[\n\\frac{1}{T} \\sum\\_{t=1}^T \\sum\\_{-c \\leq j \\leq c, j \\not=0} log P(w\\_{t+j} \\| w\\_t) = \\frac{1}{T} \\sum\\_{t=1}^T \\sum\\_{-c \\leq j \\leq c, j \\not=0} log P(w\\_t \\| w\\_{t+j})\n\\\\]\n\n同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指**在一定窗口内的词两两会计算概率**，即使它们之间隔着一些词，\n这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。\n\n<br>\n\n*与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。*\n\nHierarchical Softmax算法也结合了Huffman编码，每个词``w``都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\\\( n(w,j) \\\\)为这条路径上的第``j``个节点，且``L(w)``为这条路径的长度，注意``j``从1开始编码，即\\\\( n(w,1)=root, n(w, L(w))=w\\\\)。\n层级softmax定义的概率\\\\( P(w \\| w\\_I) \\\\)为：\n\\\\[\nP(w \\| w\\_I) = \\prod\\_{j=1}^{L(w)-1} \\sigma \\lbrace \\vartheta [ n(w, j+1) = ch(n(w, j)) ] \\cdot v\\_{n(w,j)}^{'T} v\\_I \\rbrace\n\\\\]\n其中:\n\\\\[\n\\vartheta(x) = \n\\begin{cases}\n1, \\quad if \\quad x \\quad is \\quad true \\\\\\\n-1, \\quad otherwise\n\\end{cases}\n\\\\]\n\\\\( ch(n(w, j)) \\\\)既可以是\\\\( n(w,j)\\\\)的左子节点也可以是\\\\( n(w,j) \\\\)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。\n\nLoss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即\n\\\\[\n\\begin{equation}\n\\begin{aligned}\nLoss\\_pair \n& = -Log Likelihood\\_pair \\\\\\\n& = - log P(w\\| w\\_I) \\\\\\\n& = - \\sum\\_{j=1}^{L(w)-1} log \\lbrace \\sigma [ \\vartheta ( n(w,j+1)=ch(n(w,j)) ) \\cdot v\\_{n(w,j)}^{'T} v\\_I ] \\rbrace \n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**如果当前节点是左子节点**，即\\\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\\\)为``true``\n\n那么\\\\( Loss = - log \\lbrace \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\\\)，则梯度为：\n\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}} = - (1-\\sigma (v\\_{n(w,j)}^{'T} v\\_I)) \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} =  \\frac{\\partial Loss}{\\partial v\\_I} = - (1-\\sigma (v\\_{n(w,j)}^{'T} v\\_I)) \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n**如果当前节点是右子节点**，即\\\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\\\)为``false``\n\n那么\\\\( Loss = - log \\lbrace \\sigma( - v\\_{n(w,j)}^{'T} v\\_I) \\rbrace = - log(1- \\sigma(v\\_{n(w,j)}^{'T} v\\_I)) \\\\)，则梯度为：\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}} = \\sigma (v\\_{n(w,j)}^{'T} v\\_I) \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} =  \\frac{\\partial Loss}{\\partial v\\_I} = \\sigma (v\\_{n(w,j)}^{'T} v\\_I) \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n**合并式(15)和式(16)，得：**\n\n\\\\[\nGradient\\_{v\\_{n(w,j)}^{'}} = \\frac{\\partial Loss}{\\partial v\\_{n(w,j)}^{'}}\n= - \\lbrace 1 - code[j] - \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\cdot v\\_I \\\\\\\nGradient\\_{v\\_I} = \\frac{\\partial Loss}{\\partial v\\_I}\n= - \\lbrace 1 - code[j] - \\sigma( v\\_{n(w,j)}^{'T} v\\_I) \\rbrace \\cdot v\\_{n(w,j)}^{'}\n\\\\]\n\n此处相关的代码如下，其中``g``是梯度中的公共部分\\\\( (1-code[j]-\\sigma( v\\_{n(w,j)}^{'T} v\\_I)) \\\\)与``Learningrate alpha``的乘积。\n\n```cpp\n// HIERARCHICAL SOFTMAX\nif (hs)\n    for (d = 0; d < vocab[word].codelen; d++) \n    {\n        f = 0;\n        l2 = vocab[word].point[d] * layer1_size;\n\n        // Propagate hidden -> output\n        for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];\n\n        if (f <= -MAX_EXP) continue;\n        else if (f >= MAX_EXP) continue;\n        else f = expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];\n\n        // 'g' is the gradient multiplied by the learning rate\n        g = (1 - vocab[word].code[d] - f) * alpha;\n\n        // Propagate errors output -> hidden\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];\n\n        // Learn weights hidden -> output\n        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];\n    }\n```\n\nNegative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下：\n\n```cpp\n// NEGATIVE SAMPLING\nif (negative > 0)\n    for (d = 0; d < negative + 1; d++) \n    {\n        if (d == 0) \n        {\n            target = word;\n            label = 1;\n        } \n        else \n        {\n            next_random = next_random * (unsigned long long) 25214903917 + 11;\n            target = table[(next_random >> 16) % table_size];\n            if (target == 0) target = next_random % (vocab_size - 1) + 1;\n            if (target == word) continue;\n            label = 0;\n        }\n        l2 = target * layer1_size;\n        f = 0;\n        for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];\n        if (f > MAX_EXP) g = (label - 1) * alpha;\n        else if (f < -MAX_EXP) g = (label - 0) * alpha;\n        else g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];\n        for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];\n    }\n```\n\n\n--------\n\n### 3. Word2Vec的Tricks\n\n#### 3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？\n\n前面提到到 Skip-Gram 中的条件概率为：\n\\\\[\nP(w\\_o \\| w\\_i) = e^{v\\_{w\\_o}^T v\\_{w\\_i}} / \\sum\\_{w=1}^W e^{v\\_w^T v\\_{w\\_i}}\n\\\\]\n\n这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\\\\( p(w\\_o \\| w\\_i) \\\\)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。\n\n另外一种方法是只更新当前 \\\\( w\\_o、 w\\_i \\\\)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。\n\nHierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\\\\( w\\_i \\\\)，先让模型判断\\\\( w\\_o \\\\)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。\n\nNegative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。\n\n\n#### 3.2 指数运算\n\n由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为：\n\n```text\nexpTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6\nexpTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。\n```\n\n相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。\n\n```cpp\nexpTable = (real *) malloc((EXP_TABLE_SIZE + 1) * sizeof(real));\nfor (i = 0; i < EXP_TABLE_SIZE; i++) \n{\n    expTable[i] = exp((i / (real) EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table\n    expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)\n}\n```\n\n相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的\n下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。\n\n```cpp\nfor (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];\nif (f > MAX_EXP) g = (label - 1) * alpha;\nelse if (f < -MAX_EXP) g = (label - 0) * alpha;\nelse g = (label - expTable[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;\n```\n\n\n#### 3.3 按word分布随机抽样\n\nword2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料：\n\n[http://en.wikipedia.org/wiki/Categorical_distribution](http://en.wikipedia.org/wiki/Categorical_distribution)，\n\n[http://en.wikipedia.org/wiki/Categorical_distribution#Sampling](http://en.wikipedia.org/wiki/Categorical_distribution#Sampling)\n\nCategorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。\n维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下：\n\n\n**方法一**：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。\n\n事先准备：\n```text\n1. 计算每个类别或词的未归一化分布值（对于词来即词频）；\n2. 把上面计算的值加和，利用这个和进行概率归一化；\n3. 引入类别或词的一定顺序，比如词的下标；\n4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。\n```\n\n使用时：\n```text\n1. 随机抽取一个 0 到 1 之间的随机数；\n2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；\n3. 返回 CDF 对应的类别或词。\n```\n\n使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。\n\n\n**方法二**：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。\n\n```text\n1. r = 1;\n2. s = 0;\n3. for (i = 1; i <= k; i++) \n4. {\n5.     v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r\n6.     //产生 v 个下标为 i 的样本\n7.     for (j = 1; j <= v; j++)\n8.         z[s++] = i;\n9.     n = n – v;    //下次需要继续生成 n 个样本\n10.    r = r – p[i]; //剩余样本的概率和\n11. }\n12. 随机重新排列(shuffle)z中的左右样本；\n13. 返回 z。\n```\n\n这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。\n\n\n**方法三**：即word2vec实现方法\n\n```text\n(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)\n|_____________a___________|________b________|____c____|\n\n(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。\n|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|\n\nword2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。\n\nvoid InitUnigramTable() \n{\n    int a, i;\n    double train_words_pow = 0;\n    double d1, power = 0.75;\n    table = (int *) malloc(table_size * sizeof(int));\n    for (a = 0; a < vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);\n    i = 0;\n    d1 = pow(vocab[i].cn, power) / train_words_pow;\n    for (a = 0; a < table_size; a++) \n    {\n        table[a] = i;\n        if (a / (double) table_size > d1) \n        {\n            i++;\n            d1 += pow(vocab[i].cn, power) / train_words_pow;\n        }\n        if (i >= vocab_size) i = vocab_size - 1;\n    }\n}\n```\n\n该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。\n\n\n#### 3.4 哈希编码\n\n个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下：\n\n```cpp\n// Returns hash value of a word\nint GetWordHash(char *word) {\n    unsigned long long a, hash = 0;\n    for (a = 0; a < strlen(word); a++) hash = hash * 257 + word[a];\n    hash = hash % vocab_hash_size;\n    return hash;\n}\n\n// Returns position of a word in the vocabulary; if the word is not found, returns -1\nint SearchVocab(char *word) {\n    unsigned int hash = GetWordHash(word);\n    while (1) {\n        if (vocab_hash[hash] == -1) return -1;\n        if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];\n        hash = (hash + 1) % vocab_hash_size;\n    }\n    return -1;\n}\n```\n\n\n#### 3.5 随机数\n\n作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。\n\n```cpp\nreal ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\nif (ran < (next_random & 0xFFFF) / (real) 65536) continue;\n```\n\n\n#### 3.6 高频词亚采样\n\n这里的亚采样是指 Sub-Sampling，每个词\\\\( w\\_i \\\\)被丢弃的概率为：\n\\\\[\nP(w\\_i) = 1 - \\sqrt{\\frac{sample}{freq(w\\_i)}}\n\\\\]\n\nsample是一个可以设置的参数， demo-word.sh 中是 10-3，\\\\( freq(w\\_i) \\\\)则为\\\\( w\\_i \\\\)的词频。具体的实现代码如下:\n\n```cpp\nreal ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\nnext_random = next_random * (unsigned long long) 25214903917 + 11;\n```\n\n从具体代码可以看出，\\\\(w\\_i\\\\)被丢弃的概率为：\n\\\\[\nP(w\\_i) = 1- ( \\sqrt{\\frac{sample}{freq(w\\_i)}} + \\frac{sample}{freq(w\\_i)} )\n\\\\]\n\n\n--------\n\n### 参考\n\n>\n1. [Getting started with Word2Vec](https://textprocessing.org/getting-started-with-word2vec)\n2. [word2vec源码](https://code.google.com/archive/p/word2vec/)\n3. [word2vec源码 - 下载](/posts_res/2018-05-03-word2vec/source-archive.zip)\n4. [Word2Vec详解.pdf](/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf)\n5. [Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙](http://techblog.youdao.com/?p=915)\n6. [机器学习算法实现解析——word2vec源码解析](https://blog.csdn.net/google19890102/article/details/51887344)\n7. [Word2Vec-知其然知其所以然](https://www.zybuluo.com/Dounm/note/591752)\n8. [word2vec 中的数学原理详解](http://www.cnblogs.com/peghoty/p/3857839.html)\n\n","slug":"word2vec","published":1,"updated":"2019-08-17T09:27:29.315Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnvy00222qwp16g55few","content":"<p>目录</p><ul>\n<li>Word2Vec - CBOW</li>\n<li>Word2Vec - Skip-Gram</li>\n<li>Word2Vec的Tricks</li>\n</ul><hr><h3 id=\"1-Word2Vec-CBOW\"><a href=\"#1-Word2Vec-CBOW\" class=\"headerlink\" title=\"1. Word2Vec - CBOW\"></a>1. Word2Vec - CBOW</h3><p>CBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型，\n不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。</p><p>先来模型结构图，如下：</p><p><img src=\"/posts_res/2018-05-03-word2vec/1-1.jpg\" alt=\"CBOW_Structure\"></p><p>可以看出，CBOW模型是预测\\( P(w_t | w_{t-k}, w_{t-(k-1)}, …, w_{t-1}, w_{t+1}, w_{t+2}, …, w_{t+k}) \\)。</p><a id=\"more\"></a>\n\n\n\n\n\n\n<p>从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)</span></span><br><span class=\"line\"></span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">b = next_random % window;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// in -&gt; hidden</span></span><br><span class=\"line\">cw = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (a = b; a &lt; window * <span class=\"number\">2</span> + <span class=\"number\">1</span> - b; a++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a != window) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c = sentence_position - window + a;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &lt; <span class=\"number\">0</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &gt;= sentence_length) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        last_word = sen[c];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (last_word == <span class=\"number\">-1</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class=\"line\">        cw++;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>其中<code>sentence_position</code>为当前<code>word</code>在句子中的下标，以一个具体的句子<code>A B C D</code>为例，\n第一次进入到下面代码时，当前<code>word</code>为A，<code>sentence_position</code>为0，<code>b</code>是一个随机生成的0到window-1的词，\n整个窗口的大小为(2 <em> window + 1 - 2 </em> b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动，\n其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值<code>b</code>的大小决定了当前窗口的大小。\n代码中的<code>neu1</code>即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。</p>\n<p><br></p>\n<p><em>CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>\n<h4 id=\"Hierarchical-Softmax\"><a href=\"#Hierarchical-Softmax\" class=\"headerlink\" title=\"Hierarchical Softmax\"></a>Hierarchical Softmax</h4><p>该算法结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\( n(w,j) \\)为这条路径上的第\\(j\\)个节点，且\\( L(w) \\)为这条路径的长度，注意\\(j\\)从1开始编码，即\\( n(w,1)=root, n(w,L(w))=w \\)。\n对于第\\(j\\)个节点，Hierarchical Softmax算法定义的<code>Lable</code>为 1-code[j]，而输出为\n\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\]</p>\n<p><code>Loss</code>为负的log似然，即：\n\\[\nLoss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f)\n\\]</p>\n<p>那么梯度为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{neu1} \n&amp; = \\frac{\\partial Loss}{\\partial neu1} \\\\ \n&amp; = - (1-code[j]) \\cdot (1-f) \\cdot syn1 + code[j] \\cdot f \\cdot syn1 \\\\\n&amp; = - (1-code[j] - f) \\cdot syn1 \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{syn1} \n&amp; = \\frac{\\partial Loss}{\\partial syn1} \\\\ \n&amp; = - (1-code[j]) \\cdot (1-f) \\cdot neu1 + code[j] \\cdot f \\cdot neu1 \\\\\n&amp; = - (1-code[j] - f) \\cdot neu1 \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>需要注意的是，word2vec源码中的<code>g</code>实际为负梯度中公共部分与<code>Learningrate alpha</code>的乘积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Hierarchical Softmax</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hs)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class=\"line\">        <span class=\"comment\">// Propagate hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; <span class=\"comment\">// syn1 is weight between hidden and output</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &lt;= -MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &gt;= MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            <span class=\"comment\">// expTable to speed running</span></span><br><span class=\"line\">            f = expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 'g' is the common part of gradient multiplied by the learning rate</span></span><br><span class=\"line\">        g = (<span class=\"number\">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate errors output -&gt; hidden</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Learn weights hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Negative-Sampling\"><a href=\"#Negative-Sampling\" class=\"headerlink\" title=\"Negative Sampling\"></a>Negative Sampling</h4><p>在源码中就是随机生成<code>negative</code>个负例(也有可能少于这个数，当随机撞上原来的word跳过)。\n原来的<code>word</code>为正例，<code>Label</code>为1，其他随机生成的<code>Lable</code>为0，输出<code>f</code>仍为：\n\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\]</p>\n<p><code>Loss</code>为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即：\n\\[\nLoss = -Likelihood = - label \\cdot log f - (1-label) \\cdot log (1-f)\n\\]</p>\n<p>那么梯度为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{neu1} \n&amp; = \\frac{\\partial Loss}{\\partial neu1} \\\\\n&amp; = -label \\cdot (1-f) \\cdot syn1 + (1-label) \\cdot f \\cdot syn1 \\\\\n&amp; = -(label - f) \\cdot syn1\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{syn1} \n&amp; = \\frac{\\partial Loss}{\\partial syn1} \\\\\n&amp; = -label \\cdot (1-f) \\cdot neu1 + (1-label) \\cdot f \\cdot neu1 \\\\\n&amp; = -(label - f) \\cdot neu1\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>同样注意代码中<code>g</code>并非梯度，可以看作是乘了<code>Learningrate alpha</code>的error(label与输出f的差)。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// NEGATIVE SAMPLING</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (negative &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; negative + <span class=\"number\">1</span>; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (d == <span class=\"number\">0</span>) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            target = word;</span><br><span class=\"line\">            label = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; </span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">            target = table[(next_random &gt;&gt; <span class=\"number\">16</span>) % table_size];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == <span class=\"number\">0</span>) target = next_random % (vocab_size - <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == word) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">            label = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        l2 = target * layer1_size;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * neu1[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"隐含层到输入层的梯度传播\"><a href=\"#隐含层到输入层的梯度传播\" class=\"headerlink\" title=\"隐含层到输入层的梯度传播\"></a>隐含层到输入层的梯度传播</h4><p>因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hidden -&gt; in</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (a = b; a &lt; window * <span class=\"number\">2</span> + <span class=\"number\">1</span> - b; a++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a != window) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c = sentence_position - window + a;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &lt; <span class=\"number\">0</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &gt;= sentence_length) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        last_word = sen[c];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (last_word == <span class=\"number\">-1</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"2-Word2Vec-Skip-Gram\"><a href=\"#2-Word2Vec-Skip-Gram\" class=\"headerlink\" title=\"2. Word2Vec - Skip-Gram\"></a>2. Word2Vec - Skip-Gram</h3><p>Skip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\\( P(w_i | w_t) \\)，其中\\( t-c \\leq i \\leq t+c, 且 i \\not = t\\)，\n<code>c</code>是决定上下文窗口大小的常数，<code>c</code>越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。</p>\n<p><img src=\"/posts_res/2018-05-03-word2vec/2-1.jpg\" alt=\"skip-gram\"></p>\n<p>假设存在一个\\( w_1, w_2, w_3, …, w_T \\)的词组序列，Skip-Gram的目标最大化：\n\\[\n\\frac{1}{T} \\mathop{\\sum_{t=1}^T} \\mathop{\\sum_{-c \\leq j \\leq c, j \\not= 0}} log p(w_{t+j} | w_t)\n\\]</p>\n<p>基本的Skip-Gram模型定义\\( P(w_o | w_i) \\)为：\n\\[\nP(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \\sum_{w=1}^W e^{v_w^T v_{w_i}}\n\\]</p>\n<p>从公式中不能看出，Skip-Gram是一个对称的模型，如果\\(w_t\\)为中心词时，\\( w_k \\)在其窗口内，\n则\\( w_t \\)也必然在以\\( w_k \\)为中心词的同样大小的窗口内，也就是：\n\\[\n\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\not=0} log P(w_{t+j} | w_t) = \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\not=0} log P(w_t | w_{t+j})\n\\]</p>\n<p>同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指<strong>在一定窗口内的词两两会计算概率</strong>，即使它们之间隔着一些词，\n这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。</p>\n<p><br></p>\n<p><em>与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>\n<p>Hierarchical Softmax算法也结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\( n(w,j) \\)为这条路径上的第<code>j</code>个节点，且<code>L(w)</code>为这条路径的长度，注意<code>j</code>从1开始编码，即\\( n(w,1)=root, n(w, L(w))=w\\)。\n层级softmax定义的概率\\( P(w | w_I) \\)为：\n\\[\nP(w | w_I) = \\prod_{j=1}^{L(w)-1} \\sigma \\lbrace \\vartheta [ n(w, j+1) = ch(n(w, j)) ] \\cdot v_{n(w,j)}^{‘T} v_I \\rbrace\n\\]\n其中:\n\\[\n\\vartheta(x) = \n\\begin{cases}\n1, \\quad if \\quad x \\quad is \\quad true \\\\\n-1, \\quad otherwise\n\\end{cases}\n\\]\n\\( ch(n(w, j)) \\)既可以是\\( n(w,j)\\)的左子节点也可以是\\( n(w,j) \\)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。</p>\n<p>Loss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即\n\\[\n\\begin{equation}\n\\begin{aligned}\nLoss_pair \n&amp; = -Log Likelihood_pair \\\\\n&amp; = - log P(w| w_I) \\\\\n&amp; = - \\sum_{j=1}^{L(w)-1} log \\lbrace \\sigma [ \\vartheta ( n(w,j+1)=ch(n(w,j)) ) \\cdot v_{n(w,j)}^{‘T} v_I ] \\rbrace \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>如果当前节点是左子节点</strong>，即\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\)为<code>true</code></p>\n<p>那么\\( Loss = - log \\lbrace \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\)，则梯度为：</p>\n<p>\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}} = - (1-\\sigma (v_{n(w,j)}^{‘T} v_I)) \\cdot v_I \\\\\nGradient_{v_I} =  \\frac{\\partial Loss}{\\partial v_I} = - (1-\\sigma (v_{n(w,j)}^{‘T} v_I)) \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p><strong>如果当前节点是右子节点</strong>，即\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\)为<code>false</code></p>\n<p>那么\\( Loss = - log \\lbrace \\sigma( - v_{n(w,j)}^{‘T} v_I) \\rbrace = - log(1- \\sigma(v_{n(w,j)}^{‘T} v_I)) \\)，则梯度为：\n\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}} = \\sigma (v_{n(w,j)}^{‘T} v_I) \\cdot v_I \\\\\nGradient_{v_I} =  \\frac{\\partial Loss}{\\partial v_I} = \\sigma (v_{n(w,j)}^{‘T} v_I) \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p><strong>合并式(15)和式(16)，得：</strong></p>\n<p>\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}}\n= - \\lbrace 1 - code[j] - \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\cdot v_I \\\\\nGradient_{v_I} = \\frac{\\partial Loss}{\\partial v_I}\n= - \\lbrace 1 - code[j] - \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p>此处相关的代码如下，其中<code>g</code>是梯度中的公共部分\\( (1-code[j]-\\sigma( v_{n(w,j)}^{‘T} v_I)) \\)与<code>Learningrate alpha</code>的乘积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// HIERARCHICAL SOFTMAX</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hs)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &lt;= -MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &gt;= MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> f = expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class=\"line\">        g = (<span class=\"number\">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate errors output -&gt; hidden</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Learn weights hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>Negative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// NEGATIVE SAMPLING</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (negative &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; negative + <span class=\"number\">1</span>; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (d == <span class=\"number\">0</span>) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            target = word;</span><br><span class=\"line\">            label = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; </span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">            target = table[(next_random &gt;&gt; <span class=\"number\">16</span>) % table_size];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == <span class=\"number\">0</span>) target = next_random % (vocab_size - <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == word) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">            label = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        l2 = target * layer1_size;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"3-Word2Vec的Tricks\"><a href=\"#3-Word2Vec的Tricks\" class=\"headerlink\" title=\"3. Word2Vec的Tricks\"></a>3. Word2Vec的Tricks</h3><h4 id=\"3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？\"><a href=\"#3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？\" class=\"headerlink\" title=\"3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？\"></a>3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？</h4><p>前面提到到 Skip-Gram 中的条件概率为：\n\\[\nP(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \\sum_{w=1}^W e^{v_w^T v_{w_i}}\n\\]</p>\n<p>这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\\( p(w_o | w_i) \\)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。</p>\n<p>另外一种方法是只更新当前 \\( w_o、 w_i \\)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。</p>\n<p>Hierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\\( w_i \\)，先让模型判断\\( w_o \\)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。</p>\n<p>Negative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。</p>\n<h4 id=\"3-2-指数运算\"><a href=\"#3-2-指数运算\" class=\"headerlink\" title=\"3.2 指数运算\"></a>3.2 指数运算</h4><p>由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">expTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6</span><br><span class=\"line\">expTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。</span><br></pre></td></tr></table></figure>\n<p>相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">expTable = (real *) <span class=\"built_in\">malloc</span>((EXP_TABLE_SIZE + <span class=\"number\">1</span>) * <span class=\"keyword\">sizeof</span>(real));</span><br><span class=\"line\"><span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; EXP_TABLE_SIZE; i++) </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    expTable[i] = <span class=\"built_in\">exp</span>((i / (real) EXP_TABLE_SIZE * <span class=\"number\">2</span> - <span class=\"number\">1</span>) * MAX_EXP); <span class=\"comment\">// Precompute the exp() table</span></span><br><span class=\"line\">    expTable[i] = expTable[i] / (expTable[i] + <span class=\"number\">1</span>);                   <span class=\"comment\">// Precompute f(x) = x / (x + 1)</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的\n下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class=\"line\"><span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\"><span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\"><span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3-按word分布随机抽样\"><a href=\"#3-3-按word分布随机抽样\" class=\"headerlink\" title=\"3.3 按word分布随机抽样\"></a>3.3 按word分布随机抽样</h4><p>word2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料：</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Categorical_distribution\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/Categorical_distribution</a>，</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Categorical_distribution#Sampling\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/Categorical_distribution#Sampling</a></p>\n<p>Categorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。\n维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下：</p>\n<p><strong>方法一</strong>：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。</p>\n<p>事先准备：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 计算每个类别或词的未归一化分布值（对于词来即词频）；</span><br><span class=\"line\">2. 把上面计算的值加和，利用这个和进行概率归一化；</span><br><span class=\"line\">3. 引入类别或词的一定顺序，比如词的下标；</span><br><span class=\"line\">4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。</span><br></pre></td></tr></table></figure></p>\n<p>使用时：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 随机抽取一个 0 到 1 之间的随机数；</span><br><span class=\"line\">2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；</span><br><span class=\"line\">3. 返回 CDF 对应的类别或词。</span><br></pre></td></tr></table></figure></p>\n<p>使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。</p>\n<p><strong>方法二</strong>：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. r = 1;</span><br><span class=\"line\">2. s = 0;</span><br><span class=\"line\">3. for (i = 1; i &lt;= k; i++) </span><br><span class=\"line\">4. &#123;</span><br><span class=\"line\">5.     v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r</span><br><span class=\"line\">6.     //产生 v 个下标为 i 的样本</span><br><span class=\"line\">7.     for (j = 1; j &lt;= v; j++)</span><br><span class=\"line\">8.         z[s++] = i;</span><br><span class=\"line\">9.     n = n – v;    //下次需要继续生成 n 个样本</span><br><span class=\"line\">10.    r = r – p[i]; //剩余样本的概率和</span><br><span class=\"line\">11. &#125;</span><br><span class=\"line\">12. 随机重新排列(shuffle)z中的左右样本；</span><br><span class=\"line\">13. 返回 z。</span><br></pre></td></tr></table></figure>\n<p>这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。</p>\n<p><strong>方法三</strong>：即word2vec实现方法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)</span><br><span class=\"line\">|_____________a___________|________b________|____c____|</span><br><span class=\"line\"></span><br><span class=\"line\">(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。</span><br><span class=\"line\">|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|</span><br><span class=\"line\"></span><br><span class=\"line\">word2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。</span><br><span class=\"line\"></span><br><span class=\"line\">void InitUnigramTable() </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    int a, i;</span><br><span class=\"line\">    double train_words_pow = 0;</span><br><span class=\"line\">    double d1, power = 0.75;</span><br><span class=\"line\">    table = (int *) malloc(table_size * sizeof(int));</span><br><span class=\"line\">    for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</span><br><span class=\"line\">    i = 0;</span><br><span class=\"line\">    d1 = pow(vocab[i].cn, power) / train_words_pow;</span><br><span class=\"line\">    for (a = 0; a &lt; table_size; a++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        table[a] = i;</span><br><span class=\"line\">        if (a / (double) table_size &gt; d1) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            i++;</span><br><span class=\"line\">            d1 += pow(vocab[i].cn, power) / train_words_pow;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (i &gt;= vocab_size) i = vocab_size - 1;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。</p>\n<h4 id=\"3-4-哈希编码\"><a href=\"#3-4-哈希编码\" class=\"headerlink\" title=\"3.4 哈希编码\"></a>3.4 哈希编码</h4><p>个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Returns hash value of a word</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">GetWordHash</span><span class=\"params\">(<span class=\"keyword\">char</span> *word)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> a, hash = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (a = <span class=\"number\">0</span>; a &lt; <span class=\"built_in\">strlen</span>(word); a++) hash = hash * <span class=\"number\">257</span> + word[a];</span><br><span class=\"line\">    hash = hash % vocab_hash_size;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> hash;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Returns position of a word in the vocabulary; if the word is not found, returns -1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">SearchVocab</span><span class=\"params\">(<span class=\"keyword\">char</span> *word)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span> hash = GetWordHash(word);</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (vocab_hash[hash] == <span class=\"number\">-1</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!<span class=\"built_in\">strcmp</span>(word, vocab[vocab_hash[hash]].word)) <span class=\"keyword\">return</span> vocab_hash[hash];</span><br><span class=\"line\">        hash = (hash + <span class=\"number\">1</span>) % vocab_hash_size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-5-随机数\"><a href=\"#3-5-随机数\" class=\"headerlink\" title=\"3.5 随机数\"></a>3.5 随机数</h4><p>作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">real ran = (<span class=\"built_in\">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class=\"number\">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (ran &lt; (next_random &amp; <span class=\"number\">0xFFFF</span>) / (real) <span class=\"number\">65536</span>) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-6-高频词亚采样\"><a href=\"#3-6-高频词亚采样\" class=\"headerlink\" title=\"3.6 高频词亚采样\"></a>3.6 高频词亚采样</h4><p>这里的亚采样是指 Sub-Sampling，每个词\\( w_i \\)被丢弃的概率为：\n\\[\nP(w_i) = 1 - \\sqrt{\\frac{sample}{freq(w_i)}}\n\\]</p>\n<p>sample是一个可以设置的参数， demo-word.sh 中是 10-3，\\( freq(w_i) \\)则为\\( w_i \\)的词频。具体的实现代码如下:</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">real ran = (<span class=\"built_in\">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class=\"number\">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br></pre></td></tr></table></figure>\n<p>从具体代码可以看出，\\(w_i\\)被丢弃的概率为：\n\\[\nP(w_i) = 1- ( \\sqrt{\\frac{sample}{freq(w_i)}} + \\frac{sample}{freq(w_i)} )\n\\]</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://textprocessing.org/getting-started-with-word2vec\" target=\"_blank\" rel=\"noopener\">Getting started with Word2Vec</a></li>\n<li><a href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\" rel=\"noopener\">word2vec源码</a></li>\n<li><a href=\"/posts_res/2018-05-03-word2vec/source-archive.zip\">word2vec源码 - 下载</a></li>\n<li><a href=\"/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf\">Word2Vec详解.pdf</a></li>\n<li><a href=\"http://techblog.youdao.com/?p=915\" target=\"_blank\" rel=\"noopener\">Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙</a></li>\n<li><a href=\"https://blog.csdn.net/google19890102/article/details/51887344\" target=\"_blank\" rel=\"noopener\">机器学习算法实现解析——word2vec源码解析</a></li>\n<li><a href=\"https://www.zybuluo.com/Dounm/note/591752\" target=\"_blank\" rel=\"noopener\">Word2Vec-知其然知其所以然</a></li>\n<li><a href=\"http://www.cnblogs.com/peghoty/p/3857839.html\" target=\"_blank\" rel=\"noopener\">word2vec 中的数学原理详解</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>Word2Vec - CBOW</li>\n<li>Word2Vec - Skip-Gram</li>\n<li>Word2Vec的Tricks</li>\n</ul><hr><h3 id=\"1-Word2Vec-CBOW\"><a href=\"#1-Word2Vec-CBOW\" class=\"headerlink\" title=\"1. Word2Vec - CBOW\"></a>1. Word2Vec - CBOW</h3><p>CBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型，\n不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。</p><p>先来模型结构图，如下：</p><p><img src=\"/posts_res/2018-05-03-word2vec/1-1.jpg\" alt=\"CBOW_Structure\"></p><p>可以看出，CBOW模型是预测\\( P(w_t | w_{t-k}, w_{t-(k-1)}, …, w_{t-1}, w_{t+1}, w_{t+2}, …, w_{t+k}) \\)。</p>","more":"\n\n\n\n\n\n\n<p>从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)</span></span><br><span class=\"line\"></span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">b = next_random % window;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// in -&gt; hidden</span></span><br><span class=\"line\">cw = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (a = b; a &lt; window * <span class=\"number\">2</span> + <span class=\"number\">1</span> - b; a++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a != window) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c = sentence_position - window + a;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &lt; <span class=\"number\">0</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &gt;= sentence_length) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        last_word = sen[c];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (last_word == <span class=\"number\">-1</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class=\"line\">        cw++;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>其中<code>sentence_position</code>为当前<code>word</code>在句子中的下标，以一个具体的句子<code>A B C D</code>为例，\n第一次进入到下面代码时，当前<code>word</code>为A，<code>sentence_position</code>为0，<code>b</code>是一个随机生成的0到window-1的词，\n整个窗口的大小为(2 <em> window + 1 - 2 </em> b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动，\n其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值<code>b</code>的大小决定了当前窗口的大小。\n代码中的<code>neu1</code>即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。</p>\n<p><br></p>\n<p><em>CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>\n<h4 id=\"Hierarchical-Softmax\"><a href=\"#Hierarchical-Softmax\" class=\"headerlink\" title=\"Hierarchical Softmax\"></a>Hierarchical Softmax</h4><p>该算法结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\( n(w,j) \\)为这条路径上的第\\(j\\)个节点，且\\( L(w) \\)为这条路径的长度，注意\\(j\\)从1开始编码，即\\( n(w,1)=root, n(w,L(w))=w \\)。\n对于第\\(j\\)个节点，Hierarchical Softmax算法定义的<code>Lable</code>为 1-code[j]，而输出为\n\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\]</p>\n<p><code>Loss</code>为负的log似然，即：\n\\[\nLoss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f)\n\\]</p>\n<p>那么梯度为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{neu1} \n&amp; = \\frac{\\partial Loss}{\\partial neu1} \\\\ \n&amp; = - (1-code[j]) \\cdot (1-f) \\cdot syn1 + code[j] \\cdot f \\cdot syn1 \\\\\n&amp; = - (1-code[j] - f) \\cdot syn1 \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{syn1} \n&amp; = \\frac{\\partial Loss}{\\partial syn1} \\\\ \n&amp; = - (1-code[j]) \\cdot (1-f) \\cdot neu1 + code[j] \\cdot f \\cdot neu1 \\\\\n&amp; = - (1-code[j] - f) \\cdot neu1 \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>需要注意的是，word2vec源码中的<code>g</code>实际为负梯度中公共部分与<code>Learningrate alpha</code>的乘积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Hierarchical Softmax</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hs)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class=\"line\">        <span class=\"comment\">// Propagate hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; <span class=\"comment\">// syn1 is weight between hidden and output</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &lt;= -MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &gt;= MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span></span><br><span class=\"line\">            <span class=\"comment\">// expTable to speed running</span></span><br><span class=\"line\">            f = expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 'g' is the common part of gradient multiplied by the learning rate</span></span><br><span class=\"line\">        g = (<span class=\"number\">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate errors output -&gt; hidden</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Learn weights hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"Negative-Sampling\"><a href=\"#Negative-Sampling\" class=\"headerlink\" title=\"Negative Sampling\"></a>Negative Sampling</h4><p>在源码中就是随机生成<code>negative</code>个负例(也有可能少于这个数，当随机撞上原来的word跳过)。\n原来的<code>word</code>为正例，<code>Label</code>为1，其他随机生成的<code>Lable</code>为0，输出<code>f</code>仍为：\n\\[\nf = \\sigma ( neu1^T \\cdot syn1 )\n\\]</p>\n<p><code>Loss</code>为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即：\n\\[\nLoss = -Likelihood = - label \\cdot log f - (1-label) \\cdot log (1-f)\n\\]</p>\n<p>那么梯度为：\n\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{neu1} \n&amp; = \\frac{\\partial Loss}{\\partial neu1} \\\\\n&amp; = -label \\cdot (1-f) \\cdot syn1 + (1-label) \\cdot f \\cdot syn1 \\\\\n&amp; = -(label - f) \\cdot syn1\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\nGradient_{syn1} \n&amp; = \\frac{\\partial Loss}{\\partial syn1} \\\\\n&amp; = -label \\cdot (1-f) \\cdot neu1 + (1-label) \\cdot f \\cdot neu1 \\\\\n&amp; = -(label - f) \\cdot neu1\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>同样注意代码中<code>g</code>并非梯度，可以看作是乘了<code>Learningrate alpha</code>的error(label与输出f的差)。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// NEGATIVE SAMPLING</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (negative &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; negative + <span class=\"number\">1</span>; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (d == <span class=\"number\">0</span>) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            target = word;</span><br><span class=\"line\">            label = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; </span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">            target = table[(next_random &gt;&gt; <span class=\"number\">16</span>) % table_size];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == <span class=\"number\">0</span>) target = next_random % (vocab_size - <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == word) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">            label = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        l2 = target * layer1_size;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * neu1[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"隐含层到输入层的梯度传播\"><a href=\"#隐含层到输入层的梯度传播\" class=\"headerlink\" title=\"隐含层到输入层的梯度传播\"></a>隐含层到输入层的梯度传播</h4><p>因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hidden -&gt; in</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (a = b; a &lt; window * <span class=\"number\">2</span> + <span class=\"number\">1</span> - b; a++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a != window) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c = sentence_position - window + a;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &lt; <span class=\"number\">0</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (c &gt;= sentence_length) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        last_word = sen[c];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (last_word == <span class=\"number\">-1</span>) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"2-Word2Vec-Skip-Gram\"><a href=\"#2-Word2Vec-Skip-Gram\" class=\"headerlink\" title=\"2. Word2Vec - Skip-Gram\"></a>2. Word2Vec - Skip-Gram</h3><p>Skip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\\( P(w_i | w_t) \\)，其中\\( t-c \\leq i \\leq t+c, 且 i \\not = t\\)，\n<code>c</code>是决定上下文窗口大小的常数，<code>c</code>越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。</p>\n<p><img src=\"/posts_res/2018-05-03-word2vec/2-1.jpg\" alt=\"skip-gram\"></p>\n<p>假设存在一个\\( w_1, w_2, w_3, …, w_T \\)的词组序列，Skip-Gram的目标最大化：\n\\[\n\\frac{1}{T} \\mathop{\\sum_{t=1}^T} \\mathop{\\sum_{-c \\leq j \\leq c, j \\not= 0}} log p(w_{t+j} | w_t)\n\\]</p>\n<p>基本的Skip-Gram模型定义\\( P(w_o | w_i) \\)为：\n\\[\nP(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \\sum_{w=1}^W e^{v_w^T v_{w_i}}\n\\]</p>\n<p>从公式中不能看出，Skip-Gram是一个对称的模型，如果\\(w_t\\)为中心词时，\\( w_k \\)在其窗口内，\n则\\( w_t \\)也必然在以\\( w_k \\)为中心词的同样大小的窗口内，也就是：\n\\[\n\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\not=0} log P(w_{t+j} | w_t) = \\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\not=0} log P(w_t | w_{t+j})\n\\]</p>\n<p>同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指<strong>在一定窗口内的词两两会计算概率</strong>，即使它们之间隔着一些词，\n这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。</p>\n<p><br></p>\n<p><em>与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>\n<p>Hierarchical Softmax算法也结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。\n假设\\( n(w,j) \\)为这条路径上的第<code>j</code>个节点，且<code>L(w)</code>为这条路径的长度，注意<code>j</code>从1开始编码，即\\( n(w,1)=root, n(w, L(w))=w\\)。\n层级softmax定义的概率\\( P(w | w_I) \\)为：\n\\[\nP(w | w_I) = \\prod_{j=1}^{L(w)-1} \\sigma \\lbrace \\vartheta [ n(w, j+1) = ch(n(w, j)) ] \\cdot v_{n(w,j)}^{‘T} v_I \\rbrace\n\\]\n其中:\n\\[\n\\vartheta(x) = \n\\begin{cases}\n1, \\quad if \\quad x \\quad is \\quad true \\\\\n-1, \\quad otherwise\n\\end{cases}\n\\]\n\\( ch(n(w, j)) \\)既可以是\\( n(w,j)\\)的左子节点也可以是\\( n(w,j) \\)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。</p>\n<p>Loss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即\n\\[\n\\begin{equation}\n\\begin{aligned}\nLoss_pair \n&amp; = -Log Likelihood_pair \\\\\n&amp; = - log P(w| w_I) \\\\\n&amp; = - \\sum_{j=1}^{L(w)-1} log \\lbrace \\sigma [ \\vartheta ( n(w,j+1)=ch(n(w,j)) ) \\cdot v_{n(w,j)}^{‘T} v_I ] \\rbrace \n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>如果当前节点是左子节点</strong>，即\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\)为<code>true</code></p>\n<p>那么\\( Loss = - log \\lbrace \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\)，则梯度为：</p>\n<p>\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}} = - (1-\\sigma (v_{n(w,j)}^{‘T} v_I)) \\cdot v_I \\\\\nGradient_{v_I} =  \\frac{\\partial Loss}{\\partial v_I} = - (1-\\sigma (v_{n(w,j)}^{‘T} v_I)) \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p><strong>如果当前节点是右子节点</strong>，即\\( \\vartheta( n(w,j+1)=ch(n(w,j)) ) \\)为<code>false</code></p>\n<p>那么\\( Loss = - log \\lbrace \\sigma( - v_{n(w,j)}^{‘T} v_I) \\rbrace = - log(1- \\sigma(v_{n(w,j)}^{‘T} v_I)) \\)，则梯度为：\n\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}} = \\sigma (v_{n(w,j)}^{‘T} v_I) \\cdot v_I \\\\\nGradient_{v_I} =  \\frac{\\partial Loss}{\\partial v_I} = \\sigma (v_{n(w,j)}^{‘T} v_I) \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p><strong>合并式(15)和式(16)，得：</strong></p>\n<p>\\[\nGradient_{v_{n(w,j)}^{‘}} = \\frac{\\partial Loss}{\\partial v_{n(w,j)}^{‘}}\n= - \\lbrace 1 - code[j] - \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\cdot v_I \\\\\nGradient_{v_I} = \\frac{\\partial Loss}{\\partial v_I}\n= - \\lbrace 1 - code[j] - \\sigma( v_{n(w,j)}^{‘T} v_I) \\rbrace \\cdot v_{n(w,j)}^{‘}\n\\]</p>\n<p>此处相关的代码如下，其中<code>g</code>是梯度中的公共部分\\( (1-code[j]-\\sigma( v_{n(w,j)}^{‘T} v_I)) \\)与<code>Learningrate alpha</code>的乘积。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// HIERARCHICAL SOFTMAX</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hs)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &lt;= -MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &gt;= MAX_EXP) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> f = expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class=\"line\">        g = (<span class=\"number\">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Propagate errors output -&gt; hidden</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// Learn weights hidden -&gt; output</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<p>Negative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// NEGATIVE SAMPLING</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (negative &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (d = <span class=\"number\">0</span>; d &lt; negative + <span class=\"number\">1</span>; d++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (d == <span class=\"number\">0</span>) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            target = word;</span><br><span class=\"line\">            label = <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125; </span><br><span class=\"line\">        <span class=\"keyword\">else</span> </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\">            target = table[(next_random &gt;&gt; <span class=\"number\">16</span>) % table_size];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == <span class=\"number\">0</span>) target = next_random % (vocab_size - <span class=\"number\">1</span>) + <span class=\"number\">1</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (target == word) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">            label = <span class=\"number\">0</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        l2 = target * layer1_size;</span><br><span class=\"line\">        f = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<hr>\n<h3 id=\"3-Word2Vec的Tricks\"><a href=\"#3-Word2Vec的Tricks\" class=\"headerlink\" title=\"3. Word2Vec的Tricks\"></a>3. Word2Vec的Tricks</h3><h4 id=\"3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？\"><a href=\"#3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？\" class=\"headerlink\" title=\"3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？\"></a>3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？</h4><p>前面提到到 Skip-Gram 中的条件概率为：\n\\[\nP(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \\sum_{w=1}^W e^{v_w^T v_{w_i}}\n\\]</p>\n<p>这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\\( p(w_o | w_i) \\)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。</p>\n<p>另外一种方法是只更新当前 \\( w_o、 w_i \\)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。</p>\n<p>Hierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\\( w_i \\)，先让模型判断\\( w_o \\)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。</p>\n<p>Negative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。</p>\n<h4 id=\"3-2-指数运算\"><a href=\"#3-2-指数运算\" class=\"headerlink\" title=\"3.2 指数运算\"></a>3.2 指数运算</h4><p>由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">expTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6</span><br><span class=\"line\">expTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。</span><br></pre></td></tr></table></figure>\n<p>相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">expTable = (real *) <span class=\"built_in\">malloc</span>((EXP_TABLE_SIZE + <span class=\"number\">1</span>) * <span class=\"keyword\">sizeof</span>(real));</span><br><span class=\"line\"><span class=\"keyword\">for</span> (i = <span class=\"number\">0</span>; i &lt; EXP_TABLE_SIZE; i++) </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    expTable[i] = <span class=\"built_in\">exp</span>((i / (real) EXP_TABLE_SIZE * <span class=\"number\">2</span> - <span class=\"number\">1</span>) * MAX_EXP); <span class=\"comment\">// Precompute the exp() table</span></span><br><span class=\"line\">    expTable[i] = expTable[i] / (expTable[i] + <span class=\"number\">1</span>);                   <span class=\"comment\">// Precompute f(x) = x / (x + 1)</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的\n下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class=\"line\"><span class=\"keyword\">if</span> (f &gt; MAX_EXP) g = (label - <span class=\"number\">1</span>) * alpha;</span><br><span class=\"line\"><span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (f &lt; -MAX_EXP) g = (label - <span class=\"number\">0</span>) * alpha;</span><br><span class=\"line\"><span class=\"keyword\">else</span> g = (label - expTable[(<span class=\"keyword\">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class=\"number\">2</span>))]) * alpha;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-3-按word分布随机抽样\"><a href=\"#3-3-按word分布随机抽样\" class=\"headerlink\" title=\"3.3 按word分布随机抽样\"></a>3.3 按word分布随机抽样</h4><p>word2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料：</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Categorical_distribution\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/Categorical_distribution</a>，</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Categorical_distribution#Sampling\" target=\"_blank\" rel=\"noopener\">http://en.wikipedia.org/wiki/Categorical_distribution#Sampling</a></p>\n<p>Categorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。\n维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下：</p>\n<p><strong>方法一</strong>：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。</p>\n<p>事先准备：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 计算每个类别或词的未归一化分布值（对于词来即词频）；</span><br><span class=\"line\">2. 把上面计算的值加和，利用这个和进行概率归一化；</span><br><span class=\"line\">3. 引入类别或词的一定顺序，比如词的下标；</span><br><span class=\"line\">4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。</span><br></pre></td></tr></table></figure></p>\n<p>使用时：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 随机抽取一个 0 到 1 之间的随机数；</span><br><span class=\"line\">2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；</span><br><span class=\"line\">3. 返回 CDF 对应的类别或词。</span><br></pre></td></tr></table></figure></p>\n<p>使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。</p>\n<p><strong>方法二</strong>：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. r = 1;</span><br><span class=\"line\">2. s = 0;</span><br><span class=\"line\">3. for (i = 1; i &lt;= k; i++) </span><br><span class=\"line\">4. &#123;</span><br><span class=\"line\">5.     v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r</span><br><span class=\"line\">6.     //产生 v 个下标为 i 的样本</span><br><span class=\"line\">7.     for (j = 1; j &lt;= v; j++)</span><br><span class=\"line\">8.         z[s++] = i;</span><br><span class=\"line\">9.     n = n – v;    //下次需要继续生成 n 个样本</span><br><span class=\"line\">10.    r = r – p[i]; //剩余样本的概率和</span><br><span class=\"line\">11. &#125;</span><br><span class=\"line\">12. 随机重新排列(shuffle)z中的左右样本；</span><br><span class=\"line\">13. 返回 z。</span><br></pre></td></tr></table></figure>\n<p>这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。</p>\n<p><strong>方法三</strong>：即word2vec实现方法</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)</span><br><span class=\"line\">|_____________a___________|________b________|____c____|</span><br><span class=\"line\"></span><br><span class=\"line\">(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。</span><br><span class=\"line\">|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|</span><br><span class=\"line\"></span><br><span class=\"line\">word2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。</span><br><span class=\"line\"></span><br><span class=\"line\">void InitUnigramTable() </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    int a, i;</span><br><span class=\"line\">    double train_words_pow = 0;</span><br><span class=\"line\">    double d1, power = 0.75;</span><br><span class=\"line\">    table = (int *) malloc(table_size * sizeof(int));</span><br><span class=\"line\">    for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</span><br><span class=\"line\">    i = 0;</span><br><span class=\"line\">    d1 = pow(vocab[i].cn, power) / train_words_pow;</span><br><span class=\"line\">    for (a = 0; a &lt; table_size; a++) </span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        table[a] = i;</span><br><span class=\"line\">        if (a / (double) table_size &gt; d1) </span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            i++;</span><br><span class=\"line\">            d1 += pow(vocab[i].cn, power) / train_words_pow;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        if (i &gt;= vocab_size) i = vocab_size - 1;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。</p>\n<h4 id=\"3-4-哈希编码\"><a href=\"#3-4-哈希编码\" class=\"headerlink\" title=\"3.4 哈希编码\"></a>3.4 哈希编码</h4><p>个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// Returns hash value of a word</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">GetWordHash</span><span class=\"params\">(<span class=\"keyword\">char</span> *word)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span> a, hash = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (a = <span class=\"number\">0</span>; a &lt; <span class=\"built_in\">strlen</span>(word); a++) hash = hash * <span class=\"number\">257</span> + word[a];</span><br><span class=\"line\">    hash = hash % vocab_hash_size;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> hash;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// Returns position of a word in the vocabulary; if the word is not found, returns -1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">SearchVocab</span><span class=\"params\">(<span class=\"keyword\">char</span> *word)</span> </span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">unsigned</span> <span class=\"keyword\">int</span> hash = GetWordHash(word);</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (<span class=\"number\">1</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (vocab_hash[hash] == <span class=\"number\">-1</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!<span class=\"built_in\">strcmp</span>(word, vocab[vocab_hash[hash]].word)) <span class=\"keyword\">return</span> vocab_hash[hash];</span><br><span class=\"line\">        hash = (hash + <span class=\"number\">1</span>) % vocab_hash_size;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-5-随机数\"><a href=\"#3-5-随机数\" class=\"headerlink\" title=\"3.5 随机数\"></a>3.5 随机数</h4><p>作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">real ran = (<span class=\"built_in\">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class=\"number\">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (ran &lt; (next_random &amp; <span class=\"number\">0xFFFF</span>) / (real) <span class=\"number\">65536</span>) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-6-高频词亚采样\"><a href=\"#3-6-高频词亚采样\" class=\"headerlink\" title=\"3.6 高频词亚采样\"></a>3.6 高频词亚采样</h4><p>这里的亚采样是指 Sub-Sampling，每个词\\( w_i \\)被丢弃的概率为：\n\\[\nP(w_i) = 1 - \\sqrt{\\frac{sample}{freq(w_i)}}\n\\]</p>\n<p>sample是一个可以设置的参数， demo-word.sh 中是 10-3，\\( freq(w_i) \\)则为\\( w_i \\)的词频。具体的实现代码如下:</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">real ran = (<span class=\"built_in\">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class=\"number\">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class=\"line\">next_random = next_random * (<span class=\"keyword\">unsigned</span> <span class=\"keyword\">long</span> <span class=\"keyword\">long</span>) <span class=\"number\">25214903917</span> + <span class=\"number\">11</span>;</span><br></pre></td></tr></table></figure>\n<p>从具体代码可以看出，\\(w_i\\)被丢弃的概率为：\n\\[\nP(w_i) = 1- ( \\sqrt{\\frac{sample}{freq(w_i)}} + \\frac{sample}{freq(w_i)} )\n\\]</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://textprocessing.org/getting-started-with-word2vec\" target=\"_blank\" rel=\"noopener\">Getting started with Word2Vec</a></li>\n<li><a href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\" rel=\"noopener\">word2vec源码</a></li>\n<li><a href=\"/posts_res/2018-05-03-word2vec/source-archive.zip\">word2vec源码 - 下载</a></li>\n<li><a href=\"/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf\">Word2Vec详解.pdf</a></li>\n<li><a href=\"http://techblog.youdao.com/?p=915\" target=\"_blank\" rel=\"noopener\">Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙</a></li>\n<li><a href=\"https://blog.csdn.net/google19890102/article/details/51887344\" target=\"_blank\" rel=\"noopener\">机器学习算法实现解析——word2vec源码解析</a></li>\n<li><a href=\"https://www.zybuluo.com/Dounm/note/591752\" target=\"_blank\" rel=\"noopener\">Word2Vec-知其然知其所以然</a></li>\n<li><a href=\"http://www.cnblogs.com/peghoty/p/3857839.html\" target=\"_blank\" rel=\"noopener\">word2vec 中的数学原理详解</a></li>\n</ol>\n"},{"layout":"post","title":"最长回文子串","date":"2018-05-10T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n\n### 题目\n\n给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。\n\n示例1：\n\n```text\n输入: \"babad\"\n输出: \"bab\"\n注意: \"aba\"也是一个有效答案。\n```\n\n示例2：\n\n```text\n输入: \"cbbd\"\n输出: \"bb\"\n```\n\n\n------------\n\n### 方法一 - 中心扩展算法\n\n事实上，只需使用恒定的空间，我们就可以在 \\\\( O(n^2) \\\\) 的时间内解决这个问题。\n\n我们观察到回文中心的两侧互为镜像。因此，回文可以从它的中心展开，并且只有 \\\\( 2n - 1 \\\\)个这样的中心\n[其中自身\\\\( n \\\\)个，间隙\\\\( n-1 \\\\)个，故可能的中心有\\\\( 2n-1\\\\)个]。\n\n```cpp\nint expandAroundCenter(string s, int left, int right)\n{\n\twhile (left >= 0 && right < s.length() && s[left] == s[right])\n\t{  left--; right++; }\n\treturn right - left - 1;\n}\n\nstring longestPalindrome(string s) \n{ // O(n^2) + O(1)\n\tint start = 0, length = 0;\n\tfor (int i = 0; i < s.length(); i++)\n\t{\n\t\tint len1 = expandAroundCenter(s, i, i); // i 为中心位置，回文字符奇数个\n\t\tint len2 = expandAroundCenter(s, i, i + 1); // i和i+1之间为中心位置，回文字符偶数个\n\t\tint len = len1 > len2 ? len1 : len2; // 当前最长的回文串\n\t\tif (len > length) // 更新最长的回文串\n\t\t{\n\t\t\tstart = i - (len - 1) / 2;\n\t\t\tlength = len;\n\t\t}\n\t}\n\treturn s.substr(start, length);\n}\n```\n\n时间复杂度：\\\\( O(n^2)\\\\)，由于围绕中心来扩展回文会耗去 \\\\( O(n)\\\\) 的时间，所以总的复杂度为 \\\\( O(n^2)\\\\)。\n\n空间复杂度：\\\\( O(1) \\\\)。 \n\n\n------\n\n### 方法二 - Manacher算法\n\n首先通过在每个字符的两边都插入一个特殊的符号，将所有可能的奇数或偶数长度的回文子串都转换成了奇数长度。\n比如 ``abba`` 变成 ``#a#b#b#a#``， ``aba``变成 ``#a#b#a#``。\n\n此外，为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如``$#a#b#a#``。\n\n以字符串``12212321``为例，插入``#``和这两个特殊符号，变成了``T[]=#1#2#2#1#2#3#2#1#``，然后用一个数组 ``P[i]`` 来记录以字符 ``T[i]`` 为中心的最长回文子串向左或向右扩张的长度（包括``T[i]``，半径长度）。\n\n比如T和P的对应关系：\n\n* T # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 #\n* P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1\n\n可以看出，``P[i]-1``正好是原字符串中最长回文串的总长度，为``5``。\n\n接下来怎么计算 ``P[i]`` 呢？Manacher算法增加两个辅助变量``C``和``R``，其中``C``表示最大回文子串中心的位置，``R``则为``C+P[C]``，也就是最大回文子串的边界。得到一个很重要的结论：\n\n* 如果``R > i``，那么``P[i] >= Min(P[2 * C - i], R - i)``\n\n**下面详细说明这个结论怎么来的**\n\n当 ``R - i > P[j]`` 的时候，以``T[j]``为中心的回文子串包含在以``T[C]``为中心的回文子串中，由于 ``i`` 和 ``j`` 对称，以``T[i]``为中心的回文子串必然包含在以``T[C]``为中心的回文子串中，所以必有 ``P[i] = P[j]``，见下图。\n\n![1](/posts_res/2018-05-10-LongestPalindromicSubstring/1.png)\n\n当 ``R - i <= P[j]`` 的时候，以``T[j]``为中心的回文子串不一定完全包含于以``T[id]``为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以``T[i]``为中心的回文子串，其向右至少会扩张到``R``的位置，也就是说 ``R - i <= P[i]``。至于``R``之后的部分是否对称，就只能老老实实去匹配了。\n\n![2](/posts_res/2018-05-10-LongestPalindromicSubstring/2.png)\n\n对于 ``R <= i`` 的情况，无法对 ``P[i]`` 做更多的假设，只能``P[i] = 1``，然后再去匹配了。\n\n```cpp\nstring preProcess(string s) \n{\n\tint n = s.length();\n\tif (n == 0) \n\t\treturn \"^$\";\n\tstring ret = \"^\";\n\tfor (int i = 0; i < n; i++)\n\t\tret += \"#\" + s.substr(i, 1);\n\tret += \"#$\";\n\treturn ret;\n}\n\nstring longestPalindrome(string s) \n{\n\tstring T = preProcess(s);\n\tint n = T.length();\n\tint* P = new int[n];\n\tint C = 0, R = 0;\n\tfor (int i = 1; i < n - 1; i++) \n\t{\n\t\tint j = 2 * C - i; // 等价于 j = C - (i-C) = 2 * C - i\n\n\t\tP[i] = (R > i) ? min(R - i, P[j]) : 0;\n\n\t\t// 尝试拓展中心为i的回文串\n\t\twhile (T[i + 1 + P[i]] == T[i - 1 - P[i]])\n\t\t\tP[i]++;\n\n\t\t// If palindrome centered at i expand past R, \n\t\t// adjust center based on expanded palindrome.\n\t\tif (i + P[i] > R) \n\t\t{\n\t\t\tC = i;\n\t\t\tR = i + P[i];\n\t\t}\n\t}\n\t// Find the maximum element in P.\n\tint maxLen = 0;\n\tint centerIndex = 0;\n\tfor (int i = 1; i < n - 1; i++) \n\t{\n\t\tif (P[i] > maxLen) \n\t\t{\n\t\t\tmaxLen = P[i];\n\t\t\tcenterIndex = i;\n\t\t}\n\t}\n\tdelete[] P;\n\treturn s.substr((centerIndex - 1 - maxLen) / 2, maxLen);\n}\n```\n\n时间复杂度：\\\\( O(n) \\\\)。\n\n空间复杂度：\\\\( O(n) \\\\)。\n\n\n------\n\n### 参考\n\n>\n1. [Longest Palindromic Substring Part II](https://articles.leetcode.com/longest-palindromic-substring-part-ii/)\n2. [Manacher's ALGORITHM: O(n)时间求字符串的最长回文子串 ](https://www.felix021.com/blog/read.php?2040)\n3. [最长回文子串-解决方案](https://leetcode-cn.com/problems/longest-palindromic-substring/solution/)\n","source":"_posts/2018-05-10-LongestPalindromicSubstring.md","raw":"---\nlayout: post\ntitle: 最长回文子串\ndate: 2018-05-10 12:10 +0800\ncategories: LeetCode\ntags:\n- 回溯\nmathjax: true\ncopyright: false\n---\n\n\n### 题目\n\n给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。\n\n示例1：\n\n```text\n输入: \"babad\"\n输出: \"bab\"\n注意: \"aba\"也是一个有效答案。\n```\n\n示例2：\n\n```text\n输入: \"cbbd\"\n输出: \"bb\"\n```\n\n\n------------\n\n### 方法一 - 中心扩展算法\n\n事实上，只需使用恒定的空间，我们就可以在 \\\\( O(n^2) \\\\) 的时间内解决这个问题。\n\n我们观察到回文中心的两侧互为镜像。因此，回文可以从它的中心展开，并且只有 \\\\( 2n - 1 \\\\)个这样的中心\n[其中自身\\\\( n \\\\)个，间隙\\\\( n-1 \\\\)个，故可能的中心有\\\\( 2n-1\\\\)个]。\n\n```cpp\nint expandAroundCenter(string s, int left, int right)\n{\n\twhile (left >= 0 && right < s.length() && s[left] == s[right])\n\t{  left--; right++; }\n\treturn right - left - 1;\n}\n\nstring longestPalindrome(string s) \n{ // O(n^2) + O(1)\n\tint start = 0, length = 0;\n\tfor (int i = 0; i < s.length(); i++)\n\t{\n\t\tint len1 = expandAroundCenter(s, i, i); // i 为中心位置，回文字符奇数个\n\t\tint len2 = expandAroundCenter(s, i, i + 1); // i和i+1之间为中心位置，回文字符偶数个\n\t\tint len = len1 > len2 ? len1 : len2; // 当前最长的回文串\n\t\tif (len > length) // 更新最长的回文串\n\t\t{\n\t\t\tstart = i - (len - 1) / 2;\n\t\t\tlength = len;\n\t\t}\n\t}\n\treturn s.substr(start, length);\n}\n```\n\n时间复杂度：\\\\( O(n^2)\\\\)，由于围绕中心来扩展回文会耗去 \\\\( O(n)\\\\) 的时间，所以总的复杂度为 \\\\( O(n^2)\\\\)。\n\n空间复杂度：\\\\( O(1) \\\\)。 \n\n\n------\n\n### 方法二 - Manacher算法\n\n首先通过在每个字符的两边都插入一个特殊的符号，将所有可能的奇数或偶数长度的回文子串都转换成了奇数长度。\n比如 ``abba`` 变成 ``#a#b#b#a#``， ``aba``变成 ``#a#b#a#``。\n\n此外，为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如``$#a#b#a#``。\n\n以字符串``12212321``为例，插入``#``和这两个特殊符号，变成了``T[]=#1#2#2#1#2#3#2#1#``，然后用一个数组 ``P[i]`` 来记录以字符 ``T[i]`` 为中心的最长回文子串向左或向右扩张的长度（包括``T[i]``，半径长度）。\n\n比如T和P的对应关系：\n\n* T # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 #\n* P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1\n\n可以看出，``P[i]-1``正好是原字符串中最长回文串的总长度，为``5``。\n\n接下来怎么计算 ``P[i]`` 呢？Manacher算法增加两个辅助变量``C``和``R``，其中``C``表示最大回文子串中心的位置，``R``则为``C+P[C]``，也就是最大回文子串的边界。得到一个很重要的结论：\n\n* 如果``R > i``，那么``P[i] >= Min(P[2 * C - i], R - i)``\n\n**下面详细说明这个结论怎么来的**\n\n当 ``R - i > P[j]`` 的时候，以``T[j]``为中心的回文子串包含在以``T[C]``为中心的回文子串中，由于 ``i`` 和 ``j`` 对称，以``T[i]``为中心的回文子串必然包含在以``T[C]``为中心的回文子串中，所以必有 ``P[i] = P[j]``，见下图。\n\n![1](/posts_res/2018-05-10-LongestPalindromicSubstring/1.png)\n\n当 ``R - i <= P[j]`` 的时候，以``T[j]``为中心的回文子串不一定完全包含于以``T[id]``为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以``T[i]``为中心的回文子串，其向右至少会扩张到``R``的位置，也就是说 ``R - i <= P[i]``。至于``R``之后的部分是否对称，就只能老老实实去匹配了。\n\n![2](/posts_res/2018-05-10-LongestPalindromicSubstring/2.png)\n\n对于 ``R <= i`` 的情况，无法对 ``P[i]`` 做更多的假设，只能``P[i] = 1``，然后再去匹配了。\n\n```cpp\nstring preProcess(string s) \n{\n\tint n = s.length();\n\tif (n == 0) \n\t\treturn \"^$\";\n\tstring ret = \"^\";\n\tfor (int i = 0; i < n; i++)\n\t\tret += \"#\" + s.substr(i, 1);\n\tret += \"#$\";\n\treturn ret;\n}\n\nstring longestPalindrome(string s) \n{\n\tstring T = preProcess(s);\n\tint n = T.length();\n\tint* P = new int[n];\n\tint C = 0, R = 0;\n\tfor (int i = 1; i < n - 1; i++) \n\t{\n\t\tint j = 2 * C - i; // 等价于 j = C - (i-C) = 2 * C - i\n\n\t\tP[i] = (R > i) ? min(R - i, P[j]) : 0;\n\n\t\t// 尝试拓展中心为i的回文串\n\t\twhile (T[i + 1 + P[i]] == T[i - 1 - P[i]])\n\t\t\tP[i]++;\n\n\t\t// If palindrome centered at i expand past R, \n\t\t// adjust center based on expanded palindrome.\n\t\tif (i + P[i] > R) \n\t\t{\n\t\t\tC = i;\n\t\t\tR = i + P[i];\n\t\t}\n\t}\n\t// Find the maximum element in P.\n\tint maxLen = 0;\n\tint centerIndex = 0;\n\tfor (int i = 1; i < n - 1; i++) \n\t{\n\t\tif (P[i] > maxLen) \n\t\t{\n\t\t\tmaxLen = P[i];\n\t\t\tcenterIndex = i;\n\t\t}\n\t}\n\tdelete[] P;\n\treturn s.substr((centerIndex - 1 - maxLen) / 2, maxLen);\n}\n```\n\n时间复杂度：\\\\( O(n) \\\\)。\n\n空间复杂度：\\\\( O(n) \\\\)。\n\n\n------\n\n### 参考\n\n>\n1. [Longest Palindromic Substring Part II](https://articles.leetcode.com/longest-palindromic-substring-part-ii/)\n2. [Manacher's ALGORITHM: O(n)时间求字符串的最长回文子串 ](https://www.felix021.com/blog/read.php?2040)\n3. [最长回文子串-解决方案](https://leetcode-cn.com/problems/longest-palindromic-substring/solution/)\n","slug":"LongestPalindromicSubstring","published":1,"updated":"2019-08-17T09:36:14.655Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw000262qwp5oc3mnnw","content":"<h3 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h3><p>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。</p><p>示例1：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入: &quot;babad&quot;</span><br><span class=\"line\">输出: &quot;bab&quot;</span><br><span class=\"line\">注意: &quot;aba&quot;也是一个有效答案。</span><br></pre></td></tr></table></figure><p>示例2：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入: &quot;cbbd&quot;</span><br><span class=\"line\">输出: &quot;bb&quot;</span><br></pre></td></tr></table></figure><hr><h3 id=\"方法一-中心扩展算法\"><a href=\"#方法一-中心扩展算法\" class=\"headerlink\" title=\"方法一 - 中心扩展算法\"></a>方法一 - 中心扩展算法</h3><p>事实上，只需使用恒定的空间，我们就可以在 \\( O(n^2) \\) 的时间内解决这个问题。</p><p>我们观察到回文中心的两侧互为镜像。因此，回文可以从它的中心展开，并且只有 \\( 2n - 1 \\)个这样的中心\n[其中自身\\( n \\)个，间隙\\( n-1 \\)个，故可能的中心有\\( 2n-1\\)个]。</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">expandAroundCenter</span><span class=\"params\">(<span class=\"built_in\">string</span> s, <span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (left &gt;= <span class=\"number\">0</span> &amp;&amp; right &lt; s.length() &amp;&amp; s[left] == s[right])</span><br><span class=\"line\">\t&#123;  left--; right++; &#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> right - left - <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">longestPalindrome</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123; <span class=\"comment\">// O(n^2) + O(1)</span></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, length = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len1 = expandAroundCenter(s, i, i); <span class=\"comment\">// i 为中心位置，回文字符奇数个</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len2 = expandAroundCenter(s, i, i + <span class=\"number\">1</span>); <span class=\"comment\">// i和i+1之间为中心位置，回文字符偶数个</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len = len1 &gt; len2 ? len1 : len2; <span class=\"comment\">// 当前最长的回文串</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (len &gt; length) <span class=\"comment\">// 更新最长的回文串</span></span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tstart = i - (len - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\t\t\tlength = len;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> s.substr(start, length);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>时间复杂度：\\( O(n^2)\\)，由于围绕中心来扩展回文会耗去 \\( O(n)\\) 的时间，所以总的复杂度为 \\( O(n^2)\\)。</p>\n<p>空间复杂度：\\( O(1) \\)。 </p>\n<hr>\n<h3 id=\"方法二-Manacher算法\"><a href=\"#方法二-Manacher算法\" class=\"headerlink\" title=\"方法二 - Manacher算法\"></a>方法二 - Manacher算法</h3><p>首先通过在每个字符的两边都插入一个特殊的符号，将所有可能的奇数或偶数长度的回文子串都转换成了奇数长度。\n比如 <code>abba</code> 变成 <code>#a#b#b#a#</code>， <code>aba</code>变成 <code>#a#b#a#</code>。</p>\n<p>此外，为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如<code>$#a#b#a#</code>。</p>\n<p>以字符串<code>12212321</code>为例，插入<code>#</code>和这两个特殊符号，变成了<code>T[]=#1#2#2#1#2#3#2#1#</code>，然后用一个数组 <code>P[i]</code> 来记录以字符 <code>T[i]</code> 为中心的最长回文子串向左或向右扩张的长度（包括<code>T[i]</code>，半径长度）。</p>\n<p>比如T和P的对应关系：</p>\n<ul>\n<li>T # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 #</li>\n<li>P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1</li>\n</ul>\n<p>可以看出，<code>P[i]-1</code>正好是原字符串中最长回文串的总长度，为<code>5</code>。</p>\n<p>接下来怎么计算 <code>P[i]</code> 呢？Manacher算法增加两个辅助变量<code>C</code>和<code>R</code>，其中<code>C</code>表示最大回文子串中心的位置，<code>R</code>则为<code>C+P[C]</code>，也就是最大回文子串的边界。得到一个很重要的结论：</p>\n<ul>\n<li>如果<code>R &gt; i</code>，那么<code>P[i] &gt;= Min(P[2 * C - i], R - i)</code></li>\n</ul>\n<p><strong>下面详细说明这个结论怎么来的</strong></p>\n<p>当 <code>R - i &gt; P[j]</code> 的时候，以<code>T[j]</code>为中心的回文子串包含在以<code>T[C]</code>为中心的回文子串中，由于 <code>i</code> 和 <code>j</code> 对称，以<code>T[i]</code>为中心的回文子串必然包含在以<code>T[C]</code>为中心的回文子串中，所以必有 <code>P[i] = P[j]</code>，见下图。</p>\n<p><img src=\"/posts_res/2018-05-10-LongestPalindromicSubstring/1.png\" alt=\"1\"></p>\n<p>当 <code>R - i &lt;= P[j]</code> 的时候，以<code>T[j]</code>为中心的回文子串不一定完全包含于以<code>T[id]</code>为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以<code>T[i]</code>为中心的回文子串，其向右至少会扩张到<code>R</code>的位置，也就是说 <code>R - i &lt;= P[i]</code>。至于<code>R</code>之后的部分是否对称，就只能老老实实去匹配了。</p>\n<p><img src=\"/posts_res/2018-05-10-LongestPalindromicSubstring/2.png\" alt=\"2\"></p>\n<p>对于 <code>R &lt;= i</code> 的情况，无法对 <code>P[i]</code> 做更多的假设，只能<code>P[i] = 1</code>，然后再去匹配了。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">preProcess</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> n = s.length();</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (n == <span class=\"number\">0</span>) </span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"^$\"</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">string</span> ret = <span class=\"string\">\"^\"</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i++)</span><br><span class=\"line\">\t\tret += <span class=\"string\">\"#\"</span> + s.substr(i, <span class=\"number\">1</span>);</span><br><span class=\"line\">\tret += <span class=\"string\">\"#$\"</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">longestPalindrome</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">string</span> T = preProcess(s);</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> n = T.length();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span>* P = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[n];</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> C = <span class=\"number\">0</span>, R = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; i++) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> j = <span class=\"number\">2</span> * C - i; <span class=\"comment\">// 等价于 j = C - (i-C) = 2 * C - i</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tP[i] = (R &gt; i) ? min(R - i, P[j]) : <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 尝试拓展中心为i的回文串</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (T[i + <span class=\"number\">1</span> + P[i]] == T[i - <span class=\"number\">1</span> - P[i]])</span><br><span class=\"line\">\t\t\tP[i]++;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// If palindrome centered at i expand past R, </span></span><br><span class=\"line\">\t\t<span class=\"comment\">// adjust center based on expanded palindrome.</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (i + P[i] &gt; R) </span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tC = i;</span><br><span class=\"line\">\t\t\tR = i + P[i];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">// Find the maximum element in P.</span></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> maxLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> centerIndex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; i++) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (P[i] &gt; maxLen) </span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmaxLen = P[i];</span><br><span class=\"line\">\t\t\tcenterIndex = i;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">delete</span>[] P;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> s.substr((centerIndex - <span class=\"number\">1</span> - maxLen) / <span class=\"number\">2</span>, maxLen);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>时间复杂度：\\( O(n) \\)。</p>\n<p>空间复杂度：\\( O(n) \\)。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://articles.leetcode.com/longest-palindromic-substring-part-ii/\" target=\"_blank\" rel=\"noopener\">Longest Palindromic Substring Part II</a></li>\n<li><a href=\"https://www.felix021.com/blog/read.php?2040\" target=\"_blank\" rel=\"noopener\">Manacher’s ALGORITHM: O(n)时间求字符串的最长回文子串 </a></li>\n<li><a href=\"https://leetcode-cn.com/problems/longest-palindromic-substring/solution/\" target=\"_blank\" rel=\"noopener\">最长回文子串-解决方案</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h3><p>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。</p><p>示例1：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入: &quot;babad&quot;</span><br><span class=\"line\">输出: &quot;bab&quot;</span><br><span class=\"line\">注意: &quot;aba&quot;也是一个有效答案。</span><br></pre></td></tr></table></figure><p>示例2：</p><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输入: &quot;cbbd&quot;</span><br><span class=\"line\">输出: &quot;bb&quot;</span><br></pre></td></tr></table></figure><hr><h3 id=\"方法一-中心扩展算法\"><a href=\"#方法一-中心扩展算法\" class=\"headerlink\" title=\"方法一 - 中心扩展算法\"></a>方法一 - 中心扩展算法</h3><p>事实上，只需使用恒定的空间，我们就可以在 \\( O(n^2) \\) 的时间内解决这个问题。</p><p>我们观察到回文中心的两侧互为镜像。因此，回文可以从它的中心展开，并且只有 \\( 2n - 1 \\)个这样的中心\n[其中自身\\( n \\)个，间隙\\( n-1 \\)个，故可能的中心有\\( 2n-1\\)个]。</p>","more":"\n\n\n\n\n\n\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">expandAroundCenter</span><span class=\"params\">(<span class=\"built_in\">string</span> s, <span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span> (left &gt;= <span class=\"number\">0</span> &amp;&amp; right &lt; s.length() &amp;&amp; s[left] == s[right])</span><br><span class=\"line\">\t&#123;  left--; right++; &#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> right - left - <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">longestPalindrome</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123; <span class=\"comment\">// O(n^2) + O(1)</span></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> start = <span class=\"number\">0</span>, length = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; s.length(); i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len1 = expandAroundCenter(s, i, i); <span class=\"comment\">// i 为中心位置，回文字符奇数个</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len2 = expandAroundCenter(s, i, i + <span class=\"number\">1</span>); <span class=\"comment\">// i和i+1之间为中心位置，回文字符偶数个</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> len = len1 &gt; len2 ? len1 : len2; <span class=\"comment\">// 当前最长的回文串</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (len &gt; length) <span class=\"comment\">// 更新最长的回文串</span></span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tstart = i - (len - <span class=\"number\">1</span>) / <span class=\"number\">2</span>;</span><br><span class=\"line\">\t\t\tlength = len;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> s.substr(start, length);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>时间复杂度：\\( O(n^2)\\)，由于围绕中心来扩展回文会耗去 \\( O(n)\\) 的时间，所以总的复杂度为 \\( O(n^2)\\)。</p>\n<p>空间复杂度：\\( O(1) \\)。 </p>\n<hr>\n<h3 id=\"方法二-Manacher算法\"><a href=\"#方法二-Manacher算法\" class=\"headerlink\" title=\"方法二 - Manacher算法\"></a>方法二 - Manacher算法</h3><p>首先通过在每个字符的两边都插入一个特殊的符号，将所有可能的奇数或偶数长度的回文子串都转换成了奇数长度。\n比如 <code>abba</code> 变成 <code>#a#b#b#a#</code>， <code>aba</code>变成 <code>#a#b#a#</code>。</p>\n<p>此外，为了进一步减少编码的复杂度，可以在字符串的开始加入另一个特殊字符，这样就不用特殊处理越界问题，比如<code>$#a#b#a#</code>。</p>\n<p>以字符串<code>12212321</code>为例，插入<code>#</code>和这两个特殊符号，变成了<code>T[]=#1#2#2#1#2#3#2#1#</code>，然后用一个数组 <code>P[i]</code> 来记录以字符 <code>T[i]</code> 为中心的最长回文子串向左或向右扩张的长度（包括<code>T[i]</code>，半径长度）。</p>\n<p>比如T和P的对应关系：</p>\n<ul>\n<li>T # 1 # 2 # 2 # 1 # 2 # 3 # 2 # 1 #</li>\n<li>P 1 2 1 2 5 2 1 4 1 2 1 6 1 2 1 2 1</li>\n</ul>\n<p>可以看出，<code>P[i]-1</code>正好是原字符串中最长回文串的总长度，为<code>5</code>。</p>\n<p>接下来怎么计算 <code>P[i]</code> 呢？Manacher算法增加两个辅助变量<code>C</code>和<code>R</code>，其中<code>C</code>表示最大回文子串中心的位置，<code>R</code>则为<code>C+P[C]</code>，也就是最大回文子串的边界。得到一个很重要的结论：</p>\n<ul>\n<li>如果<code>R &gt; i</code>，那么<code>P[i] &gt;= Min(P[2 * C - i], R - i)</code></li>\n</ul>\n<p><strong>下面详细说明这个结论怎么来的</strong></p>\n<p>当 <code>R - i &gt; P[j]</code> 的时候，以<code>T[j]</code>为中心的回文子串包含在以<code>T[C]</code>为中心的回文子串中，由于 <code>i</code> 和 <code>j</code> 对称，以<code>T[i]</code>为中心的回文子串必然包含在以<code>T[C]</code>为中心的回文子串中，所以必有 <code>P[i] = P[j]</code>，见下图。</p>\n<p><img src=\"/posts_res/2018-05-10-LongestPalindromicSubstring/1.png\" alt=\"1\"></p>\n<p>当 <code>R - i &lt;= P[j]</code> 的时候，以<code>T[j]</code>为中心的回文子串不一定完全包含于以<code>T[id]</code>为中心的回文子串中，但是基于对称性可知，下图中两个绿框所包围的部分是相同的，也就是说以<code>T[i]</code>为中心的回文子串，其向右至少会扩张到<code>R</code>的位置，也就是说 <code>R - i &lt;= P[i]</code>。至于<code>R</code>之后的部分是否对称，就只能老老实实去匹配了。</p>\n<p><img src=\"/posts_res/2018-05-10-LongestPalindromicSubstring/2.png\" alt=\"2\"></p>\n<p>对于 <code>R &lt;= i</code> 的情况，无法对 <code>P[i]</code> 做更多的假设，只能<code>P[i] = 1</code>，然后再去匹配了。</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">preProcess</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> n = s.length();</span><br><span class=\"line\">\t<span class=\"keyword\">if</span> (n == <span class=\"number\">0</span>) </span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">\"^$\"</span>;</span><br><span class=\"line\">\t<span class=\"built_in\">string</span> ret = <span class=\"string\">\"^\"</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i++)</span><br><span class=\"line\">\t\tret += <span class=\"string\">\"#\"</span> + s.substr(i, <span class=\"number\">1</span>);</span><br><span class=\"line\">\tret += <span class=\"string\">\"#$\"</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"built_in\">string</span> <span class=\"title\">longestPalindrome</span><span class=\"params\">(<span class=\"built_in\">string</span> s)</span> </span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"built_in\">string</span> T = preProcess(s);</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> n = T.length();</span><br><span class=\"line\">\t<span class=\"keyword\">int</span>* P = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[n];</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> C = <span class=\"number\">0</span>, R = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; i++) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">int</span> j = <span class=\"number\">2</span> * C - i; <span class=\"comment\">// 等价于 j = C - (i-C) = 2 * C - i</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t\tP[i] = (R &gt; i) ? min(R - i, P[j]) : <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// 尝试拓展中心为i的回文串</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">while</span> (T[i + <span class=\"number\">1</span> + P[i]] == T[i - <span class=\"number\">1</span> - P[i]])</span><br><span class=\"line\">\t\t\tP[i]++;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t<span class=\"comment\">// If palindrome centered at i expand past R, </span></span><br><span class=\"line\">\t\t<span class=\"comment\">// adjust center based on expanded palindrome.</span></span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (i + P[i] &gt; R) </span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tC = i;</span><br><span class=\"line\">\t\t\tR = i + P[i];</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">// Find the maximum element in P.</span></span><br><span class=\"line\">\t<span class=\"keyword\">int</span> maxLen = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">int</span> centerIndex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">1</span>; i &lt; n - <span class=\"number\">1</span>; i++) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (P[i] &gt; maxLen) </span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tmaxLen = P[i];</span><br><span class=\"line\">\t\t\tcenterIndex = i;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">delete</span>[] P;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> s.substr((centerIndex - <span class=\"number\">1</span> - maxLen) / <span class=\"number\">2</span>, maxLen);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>时间复杂度：\\( O(n) \\)。</p>\n<p>空间复杂度：\\( O(n) \\)。</p>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"https://articles.leetcode.com/longest-palindromic-substring-part-ii/\" target=\"_blank\" rel=\"noopener\">Longest Palindromic Substring Part II</a></li>\n<li><a href=\"https://www.felix021.com/blog/read.php?2040\" target=\"_blank\" rel=\"noopener\">Manacher’s ALGORITHM: O(n)时间求字符串的最长回文子串 </a></li>\n<li><a href=\"https://leetcode-cn.com/problems/longest-palindromic-substring/solution/\" target=\"_blank\" rel=\"noopener\">最长回文子串-解决方案</a></li>\n</ol>\n"},{"layout":"post","title":"梯度下降","date":"2018-05-20T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n目录\n\n* 1 批量梯度下降BGD\n* 2 随机梯度下降SGD\n* 3 小批量梯度下降MBGD\n\n------\n\n在应用机器学习算法时，通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。\n\n下面以线性回归算法来对三种梯度下降法进行比较。\n\n一般线性回归函数的假设函数为：\n\\\\[\nh\\_{\\theta} = \\sum\\_{j=1}^n \\theta\\_j x\\_j\n\\\\]\n\n对应的损失函数形式为：\n\\\\[\nJ\\_{train}(\\theta) = \\frac{1}{2m} \\sum\\_{i=1}^m (h\\_{\\theta} (x^{(i)}) - y^{(i)})^2\n\\\\]\n\n其中的\\\\( \\frac{1}{2} \\\\)是为了计算方便，上标表示第 i 个样本，下标 j 表示第 j 维度。\n\n下图为一个二维参数\\\\( \\theta\\_0 \\\\) 和 \\\\( \\theta\\_1 \\\\) 组对应损失函数的可视化图：\n\n![theta01](/posts_res/2018-05-20-gradientdescent/0-1.png)\n\n\n------\n\n### 1. 批量梯度下降BGD\n\n批量梯度下降法(Batch Gradient Descent，BGD)是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。\n具体的操作流程如下：\n\n* 1 随机初始化\\\\( \\theta \\\\)；\n* 2 更新 \\\\( \\theta \\\\)使得损失函数减小，直到满足要求时停止；\n\\\\[\n\\theta\\_j = \\theta\\_j - \\alpha \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)\n\\\\]\n这里\\\\( \\alpha \\\\)表示学习率。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)\n& = \\frac{\\partial}{\\partial \\theta\\_j} \\frac{1}{2} (h\\_{\\theta}(x) - y )^2 \\\\\\\n& = 2 \\cdot \\frac{1}{2} ( h\\_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta\\_j} ( h\\_{\\theta}(x) - y ) \\\\\\\n& = (h\\_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta\\_j} \\left( \\sum\\_{i=0}^n \\theta\\_i x\\_i - y  \\right) \\\\\\\n& = (h\\_{\\theta}(x) - y) x\\_j\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n则对所有数据点，上述损失函数的偏导(累加和)为：\n\\\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta\\_j} = - \\frac{1}{m} \\sum\\_{i=1}^m (y^{(i)} - h\\_{\\theta}(x^{(i)})) x\\_j^{(i)}\n\\\\]\n\n在最小化损失函数的过程中，需要不断反复的更新\\\\( \\theta \\\\)使得误差函数减小，更新方式如下：\n\\\\[\n\\theta\\_j^{'} = \\theta\\_j + \\frac{1}{m} \\sum\\_{i=1}^m ( y^{(i)} - h\\_{\\theta} (x^{(i)}) x\\_j^{(i)}\n\\\\]\n\nBGD得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目 m 很大，那么这种方法的迭代速度很慢！\n所以，这就引入了另外一种方法，随机梯度下降。\n\n* **优点：全局最优解；易于并行实现**\n* **缺点：样本数目很多时，训练过程很慢**\n\n从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：\n\n![bgd](/posts_res/2018-05-20-gradientdescent/1-1.png)\n\n\n--------\n\n### 2. 随机梯度下降SGD\n\n由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。\n随机梯度下降法(Stochastic Gradient Descent，SGD)正是为了解决批量梯度下降法这一弊端而提出的。\n\n将上面的损失函数写为如下形式：\n\\\\[\n\\theta\\_j^{'} = \\theta\\_j + ( y^{(i)} - h\\_{\\theta} (x^{(i)}) ) x\\_j^{(i)}\n\\\\]\n\n利用每个样本的损失函数对 \\\\( \\theta \\\\) 求偏导得到对应的梯度，来更新 \\\\( \\theta \\\\)：\n\n随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将\\\\( \\theta\\\\)迭代到最优解了，\n对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。\n\n* **优点：训练速度快**\n* **缺点：准确度下降，不是全局最优；不易于并行实现**\n\n从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：\n\n![sgd](/posts_res/2018-05-20-gradientdescent/2-1.png)\n\n\n-----\n\n### 3. 小批量梯度下降MBGD\n\n有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法(Mini-batch Gradient Descent，MBGD)的初衷。\n\nMBGD在每次更新参数时使用 b 个样本( b一般为10 )，其具体的伪代码形式为：\n\n![mbgd](/posts_res/2018-05-20-gradientdescent/3-1.png)\n\n\n-----\n\n### 4. 总结\n\n1. BGD：每次迭代使用所有的样本；\n2. SGD：每次迭代使用一个样本；\n3. MBGD：每次迭代使用 b 个样本；\n\n\n------\n\n### 参考\n\n>\n1. [[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD](http://www.cnblogs.com/maybe2030/p/5089753.html)\n2. [详解梯度下降法的三种形式BGD、SGD以及MBGD](https://zhuanlan.zhihu.com/p/25765735)\n\n","source":"_posts/2018-05-20-gradientdescent.md","raw":"---\nlayout: post\ntitle: 梯度下降\ndate: 2018-05-20 12:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n目录\n\n* 1 批量梯度下降BGD\n* 2 随机梯度下降SGD\n* 3 小批量梯度下降MBGD\n\n------\n\n在应用机器学习算法时，通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。\n\n下面以线性回归算法来对三种梯度下降法进行比较。\n\n一般线性回归函数的假设函数为：\n\\\\[\nh\\_{\\theta} = \\sum\\_{j=1}^n \\theta\\_j x\\_j\n\\\\]\n\n对应的损失函数形式为：\n\\\\[\nJ\\_{train}(\\theta) = \\frac{1}{2m} \\sum\\_{i=1}^m (h\\_{\\theta} (x^{(i)}) - y^{(i)})^2\n\\\\]\n\n其中的\\\\( \\frac{1}{2} \\\\)是为了计算方便，上标表示第 i 个样本，下标 j 表示第 j 维度。\n\n下图为一个二维参数\\\\( \\theta\\_0 \\\\) 和 \\\\( \\theta\\_1 \\\\) 组对应损失函数的可视化图：\n\n![theta01](/posts_res/2018-05-20-gradientdescent/0-1.png)\n\n\n------\n\n### 1. 批量梯度下降BGD\n\n批量梯度下降法(Batch Gradient Descent，BGD)是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。\n具体的操作流程如下：\n\n* 1 随机初始化\\\\( \\theta \\\\)；\n* 2 更新 \\\\( \\theta \\\\)使得损失函数减小，直到满足要求时停止；\n\\\\[\n\\theta\\_j = \\theta\\_j - \\alpha \\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)\n\\\\]\n这里\\\\( \\alpha \\\\)表示学习率。\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta\\_j} J(\\theta)\n& = \\frac{\\partial}{\\partial \\theta\\_j} \\frac{1}{2} (h\\_{\\theta}(x) - y )^2 \\\\\\\n& = 2 \\cdot \\frac{1}{2} ( h\\_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta\\_j} ( h\\_{\\theta}(x) - y ) \\\\\\\n& = (h\\_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta\\_j} \\left( \\sum\\_{i=0}^n \\theta\\_i x\\_i - y  \\right) \\\\\\\n& = (h\\_{\\theta}(x) - y) x\\_j\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n则对所有数据点，上述损失函数的偏导(累加和)为：\n\\\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta\\_j} = - \\frac{1}{m} \\sum\\_{i=1}^m (y^{(i)} - h\\_{\\theta}(x^{(i)})) x\\_j^{(i)}\n\\\\]\n\n在最小化损失函数的过程中，需要不断反复的更新\\\\( \\theta \\\\)使得误差函数减小，更新方式如下：\n\\\\[\n\\theta\\_j^{'} = \\theta\\_j + \\frac{1}{m} \\sum\\_{i=1}^m ( y^{(i)} - h\\_{\\theta} (x^{(i)}) x\\_j^{(i)}\n\\\\]\n\nBGD得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目 m 很大，那么这种方法的迭代速度很慢！\n所以，这就引入了另外一种方法，随机梯度下降。\n\n* **优点：全局最优解；易于并行实现**\n* **缺点：样本数目很多时，训练过程很慢**\n\n从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：\n\n![bgd](/posts_res/2018-05-20-gradientdescent/1-1.png)\n\n\n--------\n\n### 2. 随机梯度下降SGD\n\n由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。\n随机梯度下降法(Stochastic Gradient Descent，SGD)正是为了解决批量梯度下降法这一弊端而提出的。\n\n将上面的损失函数写为如下形式：\n\\\\[\n\\theta\\_j^{'} = \\theta\\_j + ( y^{(i)} - h\\_{\\theta} (x^{(i)}) ) x\\_j^{(i)}\n\\\\]\n\n利用每个样本的损失函数对 \\\\( \\theta \\\\) 求偏导得到对应的梯度，来更新 \\\\( \\theta \\\\)：\n\n随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将\\\\( \\theta\\\\)迭代到最优解了，\n对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。\n\n* **优点：训练速度快**\n* **缺点：准确度下降，不是全局最优；不易于并行实现**\n\n从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：\n\n![sgd](/posts_res/2018-05-20-gradientdescent/2-1.png)\n\n\n-----\n\n### 3. 小批量梯度下降MBGD\n\n有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法(Mini-batch Gradient Descent，MBGD)的初衷。\n\nMBGD在每次更新参数时使用 b 个样本( b一般为10 )，其具体的伪代码形式为：\n\n![mbgd](/posts_res/2018-05-20-gradientdescent/3-1.png)\n\n\n-----\n\n### 4. 总结\n\n1. BGD：每次迭代使用所有的样本；\n2. SGD：每次迭代使用一个样本；\n3. MBGD：每次迭代使用 b 个样本；\n\n\n------\n\n### 参考\n\n>\n1. [[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD](http://www.cnblogs.com/maybe2030/p/5089753.html)\n2. [详解梯度下降法的三种形式BGD、SGD以及MBGD](https://zhuanlan.zhihu.com/p/25765735)\n\n","slug":"gradientdescent","published":1,"updated":"2019-08-17T09:36:51.493Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw100292qwplbnung7q","content":"<p>目录</p><ul>\n<li>1 批量梯度下降BGD</li>\n<li>2 随机梯度下降SGD</li>\n<li>3 小批量梯度下降MBGD</li>\n</ul><hr><p>在应用机器学习算法时，通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。</p><p>下面以线性回归算法来对三种梯度下降法进行比较。</p><p>一般线性回归函数的假设函数为：\n\\[\nh_{\\theta} = \\sum_{j=1}^n \\theta_j x_j\n\\]</p><p>对应的损失函数形式为：\n\\[\nJ_{train}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta} (x^{(i)}) - y^{(i)})^2\n\\]</p><p>其中的\\( \\frac{1}{2} \\)是为了计算方便，上标表示第 i 个样本，下标 j 表示第 j 维度。</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<p>下图为一个二维参数\\( \\theta_0 \\) 和 \\( \\theta_1 \\) 组对应损失函数的可视化图：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/0-1.png\" alt=\"theta01\"></p>\n<hr>\n<h3 id=\"1-批量梯度下降BGD\"><a href=\"#1-批量梯度下降BGD\" class=\"headerlink\" title=\"1. 批量梯度下降BGD\"></a>1. 批量梯度下降BGD</h3><p>批量梯度下降法(Batch Gradient Descent，BGD)是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。\n具体的操作流程如下：</p>\n<ul>\n<li>1 随机初始化\\( \\theta \\)；</li>\n<li>2 更新 \\( \\theta \\)使得损失函数减小，直到满足要求时停止；\n\\[\n\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n\\]\n这里\\( \\alpha \\)表示学习率。</li>\n</ul>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n&amp; = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_{\\theta}(x) - y )^2 \\\\\n&amp; = 2 \\cdot \\frac{1}{2} ( h_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta_j} ( h_{\\theta}(x) - y ) \\\\\n&amp; = (h_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( \\sum_{i=0}^n \\theta_i x_i - y  \\right) \\\\\n&amp; = (h_{\\theta}(x) - y) x_j\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>则对所有数据点，上述损失函数的偏导(累加和)为：\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = - \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_{\\theta}(x^{(i)})) x_j^{(i)}\n\\]</p>\n<p>在最小化损失函数的过程中，需要不断反复的更新\\( \\theta \\)使得误差函数减小，更新方式如下：\n\\[\n\\theta_j^{‘} = \\theta_j + \\frac{1}{m} \\sum_{i=1}^m ( y^{(i)} - h_{\\theta} (x^{(i)}) x_j^{(i)}\n\\]</p>\n<p>BGD得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目 m 很大，那么这种方法的迭代速度很慢！\n所以，这就引入了另外一种方法，随机梯度下降。</p>\n<ul>\n<li><strong>优点：全局最优解；易于并行实现</strong></li>\n<li><strong>缺点：样本数目很多时，训练过程很慢</strong></li>\n</ul>\n<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/1-1.png\" alt=\"bgd\"></p>\n<hr>\n<h3 id=\"2-随机梯度下降SGD\"><a href=\"#2-随机梯度下降SGD\" class=\"headerlink\" title=\"2. 随机梯度下降SGD\"></a>2. 随机梯度下降SGD</h3><p>由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。\n随机梯度下降法(Stochastic Gradient Descent，SGD)正是为了解决批量梯度下降法这一弊端而提出的。</p>\n<p>将上面的损失函数写为如下形式：\n\\[\n\\theta_j^{‘} = \\theta_j + ( y^{(i)} - h_{\\theta} (x^{(i)}) ) x_j^{(i)}\n\\]</p>\n<p>利用每个样本的损失函数对 \\( \\theta \\) 求偏导得到对应的梯度，来更新 \\( \\theta \\)：</p>\n<p>随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将\\( \\theta\\)迭代到最优解了，\n对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>\n<ul>\n<li><strong>优点：训练速度快</strong></li>\n<li><strong>缺点：准确度下降，不是全局最优；不易于并行实现</strong></li>\n</ul>\n<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/2-1.png\" alt=\"sgd\"></p>\n<hr>\n<h3 id=\"3-小批量梯度下降MBGD\"><a href=\"#3-小批量梯度下降MBGD\" class=\"headerlink\" title=\"3. 小批量梯度下降MBGD\"></a>3. 小批量梯度下降MBGD</h3><p>有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法(Mini-batch Gradient Descent，MBGD)的初衷。</p>\n<p>MBGD在每次更新参数时使用 b 个样本( b一般为10 )，其具体的伪代码形式为：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/3-1.png\" alt=\"mbgd\"></p>\n<hr>\n<h3 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4. 总结\"></a>4. 总结</h3><ol>\n<li>BGD：每次迭代使用所有的样本；</li>\n<li>SGD：每次迭代使用一个样本；</li>\n<li>MBGD：每次迭代使用 b 个样本；</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/maybe2030/p/5089753.html\" target=\"_blank\" rel=\"noopener\">[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/25765735\" target=\"_blank\" rel=\"noopener\">详解梯度下降法的三种形式BGD、SGD以及MBGD</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>1 批量梯度下降BGD</li>\n<li>2 随机梯度下降SGD</li>\n<li>3 小批量梯度下降MBGD</li>\n</ul><hr><p>在应用机器学习算法时，通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。</p><p>下面以线性回归算法来对三种梯度下降法进行比较。</p><p>一般线性回归函数的假设函数为：\n\\[\nh_{\\theta} = \\sum_{j=1}^n \\theta_j x_j\n\\]</p><p>对应的损失函数形式为：\n\\[\nJ_{train}(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta} (x^{(i)}) - y^{(i)})^2\n\\]</p><p>其中的\\( \\frac{1}{2} \\)是为了计算方便，上标表示第 i 个样本，下标 j 表示第 j 维度。</p>","more":"\n\n\n\n\n\n\n\n<p>下图为一个二维参数\\( \\theta_0 \\) 和 \\( \\theta_1 \\) 组对应损失函数的可视化图：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/0-1.png\" alt=\"theta01\"></p>\n<hr>\n<h3 id=\"1-批量梯度下降BGD\"><a href=\"#1-批量梯度下降BGD\" class=\"headerlink\" title=\"1. 批量梯度下降BGD\"></a>1. 批量梯度下降BGD</h3><p>批量梯度下降法(Batch Gradient Descent，BGD)是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。\n具体的操作流程如下：</p>\n<ul>\n<li>1 随机初始化\\( \\theta \\)；</li>\n<li>2 更新 \\( \\theta \\)使得损失函数减小，直到满足要求时停止；\n\\[\n\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n\\]\n这里\\( \\alpha \\)表示学习率。</li>\n</ul>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n&amp; = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_{\\theta}(x) - y )^2 \\\\\n&amp; = 2 \\cdot \\frac{1}{2} ( h_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta_j} ( h_{\\theta}(x) - y ) \\\\\n&amp; = (h_{\\theta}(x) - y ) \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left( \\sum_{i=0}^n \\theta_i x_i - y  \\right) \\\\\n&amp; = (h_{\\theta}(x) - y) x_j\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>则对所有数据点，上述损失函数的偏导(累加和)为：\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = - \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - h_{\\theta}(x^{(i)})) x_j^{(i)}\n\\]</p>\n<p>在最小化损失函数的过程中，需要不断反复的更新\\( \\theta \\)使得误差函数减小，更新方式如下：\n\\[\n\\theta_j^{‘} = \\theta_j + \\frac{1}{m} \\sum_{i=1}^m ( y^{(i)} - h_{\\theta} (x^{(i)}) x_j^{(i)}\n\\]</p>\n<p>BGD得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目 m 很大，那么这种方法的迭代速度很慢！\n所以，这就引入了另外一种方法，随机梯度下降。</p>\n<ul>\n<li><strong>优点：全局最优解；易于并行实现</strong></li>\n<li><strong>缺点：样本数目很多时，训练过程很慢</strong></li>\n</ul>\n<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/1-1.png\" alt=\"bgd\"></p>\n<hr>\n<h3 id=\"2-随机梯度下降SGD\"><a href=\"#2-随机梯度下降SGD\" class=\"headerlink\" title=\"2. 随机梯度下降SGD\"></a>2. 随机梯度下降SGD</h3><p>由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。\n随机梯度下降法(Stochastic Gradient Descent，SGD)正是为了解决批量梯度下降法这一弊端而提出的。</p>\n<p>将上面的损失函数写为如下形式：\n\\[\n\\theta_j^{‘} = \\theta_j + ( y^{(i)} - h_{\\theta} (x^{(i)}) ) x_j^{(i)}\n\\]</p>\n<p>利用每个样本的损失函数对 \\( \\theta \\) 求偏导得到对应的梯度，来更新 \\( \\theta \\)：</p>\n<p>随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将\\( \\theta\\)迭代到最优解了，\n对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。\n但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>\n<ul>\n<li><strong>优点：训练速度快</strong></li>\n<li><strong>缺点：准确度下降，不是全局最优；不易于并行实现</strong></li>\n</ul>\n<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/2-1.png\" alt=\"sgd\"></p>\n<hr>\n<h3 id=\"3-小批量梯度下降MBGD\"><a href=\"#3-小批量梯度下降MBGD\" class=\"headerlink\" title=\"3. 小批量梯度下降MBGD\"></a>3. 小批量梯度下降MBGD</h3><p>有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法(Mini-batch Gradient Descent，MBGD)的初衷。</p>\n<p>MBGD在每次更新参数时使用 b 个样本( b一般为10 )，其具体的伪代码形式为：</p>\n<p><img src=\"/posts_res/2018-05-20-gradientdescent/3-1.png\" alt=\"mbgd\"></p>\n<hr>\n<h3 id=\"4-总结\"><a href=\"#4-总结\" class=\"headerlink\" title=\"4. 总结\"></a>4. 总结</h3><ol>\n<li>BGD：每次迭代使用所有的样本；</li>\n<li>SGD：每次迭代使用一个样本；</li>\n<li>MBGD：每次迭代使用 b 个样本；</li>\n</ol>\n<hr>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><p>&gt;</p>\n<ol>\n<li><a href=\"http://www.cnblogs.com/maybe2030/p/5089753.html\" target=\"_blank\" rel=\"noopener\">[Machine Learning] 梯度下降法的三种形式BGD、SGD以及MBGD</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/25765735\" target=\"_blank\" rel=\"noopener\">详解梯度下降法的三种形式BGD、SGD以及MBGD</a></li>\n</ol>\n"},{"layout":"post","title":"反向传播 BP & BPTT","date":"2018-05-23T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n目录\n\n* 反向传播BP\n* 随时间反向传播BPTT\n\n------\n\n### 1. 反向传播BP\n\n[[Calculus on Computational Graphs: Backpropagation，英文原版]](http://colah.github.io/posts/2015-08-Backprop/)、\n[[详解反向传播算法，中文翻译理解]](https://zhuanlan.zhihu.com/p/25081671)\n\n解释了为什么从上而下计算梯度。一言以蔽之：从下而上会有重复计算，当参数数量太多，时间耗费太严重；从上而下每个节点只计算一次。\n\n[如何直观地解释 back propagation 算法？](https://www.zhihu.com/question/27239198/answer/89853077)中的例子比较清晰的刻画了反向传播的优势。\n\n[A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)每行代码都有解释。\n\n[CHAPTER 2How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)讲的特别详细，\n中文版在[这里 - 神经⽹络与深度学习](/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf)。\n\n------\n\n<br>\n\n------\n\n**以下部分参考Neural Networks and Deep Learning(神经⽹络与深度学习P37 - P42)**\n\n反向传播的四个方程式：\n\n\\\\[\n\\delta^L = \\nabla \\_a C \\odot \\sigma' (z^L) \\tag{BP1}\n\\\\]\n\n\\\\[\n\\delta^l = ( (w^{l+1})^T \\delta^{l+1} ) \\odot \\sigma' (z^l) \\tag{BP2}\n\\\\]\n\n\\\\[\n\\frac{\\partial C}{\\partial b\\_j^l} = \\delta\\_j^l  \\tag{BP3}\n\\\\]\n\n\\\\[\n\\frac{\\partial C}{\\partial w\\_{jk}^l} = a\\_k^{l-1} \\delta\\_j^l  \\tag{BP4}\n\\\\]\n\n证明上面的四个方程式：\n\n**证明BP1**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta\\_j^L \n& = \\frac{\\partial C}{\\partial z\\_j^L} \\\\\\\n& = \\frac{\\partial C}{\\partial a\\_j^L} \\cdot \\frac{\\partial a\\_j^L}{\\partial z\\_j^L} \\\\\\\n& = \\frac{\\partial C}{\\partial a\\_j^L} \\cdot \\sigma' (z\\_j^L) \\quad (\\because a\\_j^L = \\sigma (z\\_j^L))\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP2**\n\n\\\\[\n\\begin{aligned}\n\\because z_k^{l+1} &= \\sum\\_j w\\_{kj}^{l+1} a\\_j^l + b\\_k^{l+1} = \\sum\\_j w\\_{kj}^{l+1} \\cdot \\sigma(z\\_j^l) + b\\_k^{l+1} \\\\\\\n\\therefore \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_j^l} & = w\\_{kj}^{l+1} \\cdot \\sigma' (z\\_j^l)\n\\end{aligned}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta\\_j^l\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\\\\\\n& = \\sum\\_k \\frac{\\partial C}{\\partial z\\_k^{l+1}} \\cdot \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_j^l} \\\\\\\n& = \\sum\\_k \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_k^l} \\cdot \\delta\\_k^{l+1} \\\\\\\n& = \\sum\\_k w\\_{kj}^{l+1} \\cdot \\sigma' (z\\_j^l) \\cdot \\delta\\_k^{l+1} \\\\\\\n& = \\sum\\_k w\\_{kj}^{l+1} \\cdot \\delta\\_k^{l+1} \\cdot \\sigma' (z\\_j^l)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP3**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial b\\_j^l}\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\cdot \\frac{\\partial z\\_j^l}{\\partial b\\_j^l} \\\\\\\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\quad (\\because z\\_j^l = \\sum\\_k w\\_{jk}^l \\cdot a\\_k^{l-1} + b\\_j^l \\therefore \\frac{\\partial z\\_j^l}{\\partial b\\_j^l} = 1) \\\\\\\n& = \\delta\\_j^l\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP4**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial w\\_{jk}^l}\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\cdot \\frac{\\partial z\\_j^l}{\\partial w\\_{jk}^l} \\\\\\\n& = \\delta\\_j^l \\cdot a\\_k^{l-1} \\quad (\\because z\\_j^l = \\sum\\_k w\\_{jk}^l \\cdot a\\_k^{l-1} + b\\_j^l)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\n---------\n\n<br>\n\n----------\n\n**以下摘抄自：**[反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n\n反向传播算法的思路如下：给定一个样例 \\\\( (x,y) \\\\)，首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \\\\( h\\_{W,b}(x) \\\\) 的输出值。\n之后，针对第 \\\\( l \\\\) 层的每一个节点 \\\\( i \\\\)，计算出其 “残差” \\\\( \\delta^{(l)}\\_i \\\\)，该残差表明了该节点对最终输出值的残差产生了多少影响。\n对于最终的输出节点，可以直接算出网络产生的激活值与实际值之间的差距，将这个差距定义为 \\\\( \\delta^{(n_l)}\\_i \\\\)（第 \\\\( n_l \\\\) 层表示输出层）。\n对于隐藏单元如何处理呢？基于节点（第 \\\\( l+1 \\\\) 层节点）残差的加权平均值计算 \\\\( \\delta^{(l)}\\_i \\\\)，这些节点以 \\\\( a^{(l)}\\_i \\\\) 作为输入。\n下面将给出反向传导算法的细节:\n\n* (1) 进行前馈传导计算，利用前向传导公式，得到 \\\\( L\\_2, L\\_3, \\ldots \\\\) 直到输出层 \\\\( L\\_{n\\_l} \\\\) 的激活值。\n* (2) 对于第 \\\\( n\\_l \\\\) 层（输出层）的每个输出单元 \\\\( i \\\\) ，根据以下公式计算残差：\n\\\\[ \n\\delta^{(n\\_l)}\\_i \n= \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\|\\| y - h\\_{W,b}(x) \\|\\| ^2\n= - (y\\_i - a^{(n\\_l)}\\_i) \\cdot f'(z^{(n\\_l)}\\_i)\n\\\\]\n\n**推导过程**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n\\_l)}\\_i\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} J(W,b; x,y) \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\|\\|y - h\\_{W,b}(x) \\|\\|^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}} (y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}} (y\\_j - f(z\\_j^{(n\\_l)}))^2 \\\\\\\n& = - (y\\_i - f(z\\_i^{(n\\_l)})) \\cdot f'(z^{(n\\_l)}\\_i) \\\\\\\n& = - (y\\_i - a^{(n\\_l)}\\_i) \\cdot f'(z^{(n\\_l)}\\_i)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n* (3) 对 \\\\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\\\) 的各个层，第 \\\\( l \\\\) 层的第 \\\\( i \\\\) 个节点的残差计算方法如下：\n\\\\[\n\\delta^{(l)}\\_i = \\left( \\sum\\_{j=1}^{s\\_{l+1}} W^{(l)}\\_{ji} \\delta^{(l+1)}\\_j \\right) f'(z^{(l)}\\_i)\n\\\\]\n\n**推导过程**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n\\_l-1)}\\_i \n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} J(W,b; x,y) \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} \\frac{1}{2} \\|\\|y - h\\_{W,b}(x) \\|\\|^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}}(y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{1}{2} \\sum\\_{j=1}^{S\\_{(n\\_l)}}\\frac{\\partial}{\\partial z^{n\\_l-1}\\_i}(y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{1}{2} \\sum\\_{j=1}^{S\\_{(n\\_l)}}\\frac{\\partial}{\\partial z^{n\\_l-1}\\_i}(y\\_j - f(z\\_j^{(n\\_l)}))^2 \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} - (y\\_j - f(z\\_j^{(n\\_l)})) \\cdot \\frac{\\partial}{\\partial z\\_i^{(n\\_l-1)}}f(z\\_j^{(n\\_l)}) \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} - (y\\_j - f(z\\_j^{(n\\_l)})) \\cdot f'(z\\_j^{(n\\_l)}) \\cdot \\frac{\\partial z\\_j^{(n\\_l)}}{\\partial z\\_i^{(n\\_l-1)}} \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} \\delta\\_j^{(n\\_l)} \\cdot \\frac{\\partial z\\_j^{(n\\_l)}}{\\partial z\\_i^{(n\\_l-1)}} \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n_l}} \\left(\\delta\\_j^{(n\\_l)} \\cdot \\frac{\\partial}{\\partial z\\_i^{(n\\_l-1)}}\\sum\\_{k=1}^{S\\_{n\\_l-1}}f(z\\_k^{(n\\_l-1)}) \\cdot W\\_{jk}^{n\\_l-1} \\right) \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} \\delta\\_j^{(n\\_l)} \\cdot  W\\_{ji}^{n\\_l-1} \\cdot f'(z\\_i^{(n\\_l-1)}) \\\\\\\n& = \\left( \\sum\\_{j=1}^{S\\_{n\\_l}} W\\_{ji}^{n\\_l-1} \\delta\\_j^{(n\\_l)} \\right) f'(z\\_i^{(n\\_l-1)}) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n将上式中的 \\\\( n_l-1 \\\\) 与 \\\\( n_l \\\\) 的关系替换为 \\\\( l \\\\) 与 \\\\( l+1 \\\\) 的关系，就可以得到：\n\\\\[\n\\delta^{(l)}\\_i = \\left( \\sum\\_{j=1}^{s_{l+1}} W^{(l)}\\_{ji} \\delta^{(l+1)}_j \\right) f'(z^{(l)}_i)\n\\\\]\n\n以上逐次从后向前求导的过程即为“反向传导”的本意所在。\n\n* (4) 计算需要的偏导数，计算方法如下：\n\\\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial W\\_{ij}^{(l)}} J(W,b; x, y) &= a^{(l)}\\_j \\delta\\_i^{(l+1)} \\\\\\\n\\frac{\\partial}{\\partial b\\_{i}^{(l)}} J(W,b; x, y) &= \\delta\\_i^{(l+1)}.\n\\end{aligned}\n\\\\]\n\n<br>\n\n最后，用矩阵-向量表示法重写以上算法。我们使用 “ \\\\( \\bullet \\\\)” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。\n若 \\\\( a = b \\bullet c \\\\)，则 \\\\( a_i = b_i c_i \\\\)。扩展 \\\\( f(\\cdot) \\\\) 的定义，使其包含向量运算，于是有 \\\\( f'([z_1, z_2, z_3]) = [f'(z_1), f'(z_2), f'(z_3)] \\\\)。\n\n那么，反向传播算法可表示为以下几个步骤：\n\n1. 进行前馈传导计算，利用前向传导公式，得到 \\\\( L_2, L_3, \\ldots \\\\) 直到输出层 \\\\( L_{n_l} \\\\) 的激活值。\n2. 对输出层（第 \\\\( n_l \\\\) 层），计算:\\\\[\\begin{align} \\delta^{(n_l)} = - (y - a^{(n_l)}) \\bullet f'(z^{(n_l)}) \\end{align}\\\\]\n3. 对于 \\\\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\\\) 的各层，计算：\\\\[\\begin{align} \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f'(z^{(l)}) \\end{align} \\\\]\n4. 计算最终需要的偏导数值：\\\\[ \\begin{align} \\nabla_{W^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)} (a^{(l)})^T, \\\\\\ \\nabla_{b^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)}.\\end{align} \\\\]\n\n<br>\n\n**实现中应注意：**\n\n在以上的第2步和第3步中，我们需要为每一个 \\\\( i \\\\) 值计算其 \\\\( f'(z^{(l)}_i) \\\\) 。\n假设 \\\\( f(z) \\\\) 是sigmoid函数，并且我们已经在前向传导运算中得到了 \\\\( a^{(l)}_i \\\\) 。\n那么，使用我们早先推导出的 \\\\( f'(z) \\\\) 表达式，就可以计算得到 \\\\( f'(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i) \\\\)。\n\n最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，\\\\( \\Delta W^{(l)} \\\\) 是一个与矩阵 \\\\( W^{(l)} \\\\) 维度相同的矩阵，\n\\\\( \\Delta b^{(l)} \\\\) 是一个与 \\\\( b^{(l)} \\\\) 维度相同的向量。\n注意这里 “\\\\( \\Delta W^{(l)} \\\\) ”是一个矩阵，而不是“\\\\( \\Delta \\\\) 与 \\\\( W^{(l)} \\\\) 相乘”。\n\n下面，我们实现批量梯度下降法中的一次迭代：\n\n* (1) 对于所有 \\\\( l \\\\)，令 \\\\( \\Delta W^{(l)} := 0, \\Delta b^{(l)} := 0 \\\\) (设置为全零矩阵或全零向量)\n* (2) 对于 \\\\( i = 1 \\\\) 到 \\\\( m \\\\)，\n* (2.1) 使用反向传播算法计算 \\\\( \\nabla_{W^{(l)}} J(W,b;x,y) \\\\) 和 \\\\( \\nabla_{b^{(l)}} J(W,b;x,y) \\\\);\n* (2.2) 计算 \\\\( \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y) \\\\);\n* (2.3) 计算 \\\\( \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y) \\\\);\n* (3) 更新权重参数：\n\\\\[\n\\begin{align}\nW^{(l)} &= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\\\\nb^{(l)} &= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\n\\end{align}\n\\\\]\n\n现在，可以重复梯度下降法的迭代步骤来减小代价函数 \\\\( J(W,b) \\\\) 的值，进而求解我们的神经网络。\n\n\n-------\n\n### 2. [转]随时间反向传播BPTT\n\n转自：[hschen0712/machine-learning-notes](https://github.com/hschen0712/machine-learning-notes/blob/master/Deep-Learning/back-propagation-through-time.ipynb)\n\nRNN（递归神经网络，Recurrent Neural Network）是一种具有长时记忆能力的神经网络模型，被广泛应用于序列标注（Sequence Labeling）问题。\n在序列标注问题中，模型的输入是一段时间序列，记为$ x = \\lbrace x_1, x_2, ..., x_T \\rbrace $，\n我们的目标是为输入序列的每个元素打上标签集合中的对应标签，记为$ y = \\lbrace y_1, y_2, ..., y_T \\rbrace $。\n\nNLP中的大部分任务（比如分词、实体识别、词性标注）都可以最终归结为序列标注问题。\n这类问题中，输入是语料库中一段由 $T$ 个词（或字）构成的文本 $ x = \\lbrace x_1, x_2, ..., x_T \\rbrace $（其中$x_t$表示文本中的第$t$个词）；\n输出是每个词对应的标签，根据任务的不同标签的形式也各不相同，但本质上都是针对每个词根据它的上下文进行标签的分类。\n\n一个典型的RNN的结构如下图所示：\n\n![rnn](/posts_res/2018-05-23-backpropagation/2-1.jpg)\n\n从图中可以看到，一个RNN通常由三层组成，分别是输入层、隐藏层和输出层。\n与一般的神经网络不同的地方是，RNN的隐藏层存在一条有向反馈边，正是这种反馈机制赋予了RNN记忆能力。\n要理解左边的图可能有点难度，我们可以将其展开为右边这种更直观的形式，其中RNN的每个神经元接受当前时刻的输入$x_t$ 以及上一时刻隐单元的输出$h_{t-1}$，\n计算出当前神经元的输入 $s_t$，经过激活函数变换得到输出 $h_t$，并传递给下一时刻的隐单元。\n此外，我们还需要注意到RNN中每个时刻上的神经元的参数都是相同的（类似CNN的权值共享），这么做一方面是减小参数空间，保证泛化能力；\n另一方面是为了赋予RNN记忆能力，将有用信息存储在$W_{in},W_{rec},W_{out}$三个矩阵中。\n\n由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化，这要求我们提出新的优化算法。\nRNN中最常用的优化算法是随时间反向传播（BackPropagation Through Time，BPTT），下文将叙述BPTT算法的数学推导。\n\n\n#### 2.1 符号注解\n\n<table>\n <tr>\n  <th>符号</th>\n  <th>注解</th>\n </tr>\n <tr>\n  <td>$K$</td>\n  <td>词汇表的大小</td>\n </tr>\n<tr>\n  <td>$T$</td>\n  <td>句子的长度</td>\n </tr>\n<tr>\n  <td>$H$</td>\n  <td>隐藏层单元数</td>\n </tr>\n<tr>\n  <td>$x=\\lbrace x_1, x_2, ..., x_T \\rbrace $</td>\n  <td>句子的单词序列</td>\n </tr>\n<tr>\n  <td>$x_t \\in R^{K \\times 1}$</td>\n  <td>第$t$时刻RNN的输入，one-hot vector</td>\n </tr>\n<tr>\n  <td>$\\hat{y}_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻softmax层的输出，估计每个词出现的概率</td>\n </tr>\n<tr>\n  <td>$y_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻的label，为每个词出现的概率，one-hot vector</td>\n </tr>\n<tr>\n  <td>$E_t$</td>\n  <td>第$t$时刻（第$t$个word）的损失函数，定义为交叉熵误差$E_t = - y_t^T log (\\hat{y}_t) $</td>\n </tr>\n<tr>\n  <td>$E$</td>\n  <td>一个句子的损失函数，由各个时刻（即每个word）的损失函数组成,$E=\\sum_t^T E_t$（注:由于要推导的是SGD算法，更新梯度是相对于一个训练样例而言的，因此一次只考虑一个句子的误差，而不是整个训练集的误差（对应BGD算法））</td>\n </tr>\n<tr>\n  <td>$s_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输入</td>\n </tr>\n<tr>\n  <td>$h_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输出</td>\n </tr>\n<tr>\n  <td>$z_t \\in R^{K \\times 1}$</td>\n  <td>输出层的汇集输入</td>\n </tr>\n<tr>\n  <td>$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$\t</td>\n  <td>第$t$个时刻损失函数$E_t$对第$k$时刻带权输入$s_k$的导数</td>\n </tr>\n<tr>\n  <td>$r_t=\\hat{y}_t-y_t$\t</td>\n  <td>残差向量</td>\n </tr>\n<tr>\n  <td>$W_{in}\\in\\mathbb{R}^{H\\times K}$\t</td>\n  <td>从输入层到隐藏层的权值</td>\n </tr>\n<tr>\n  <td>$W_{rec}\\in\\mathbb{R}^{H\\times H}$\t</td>\n  <td>隐藏层上一个时刻到当前时刻的权值</td>\n </tr>\n<tr>\n  <td>$W_{out}\\in\\mathbb{R}^{K\\times H}$\t</td>\n  <td>隐藏层到输出层的权值</td>\n </tr>\n</table>\n\n上述符号之间的关系:\n\n$$\n\\begin{cases}\ns_t &= W_{rec} h_{t-1} + W_{in} x_t \\\\\nh_t &= tanh(s_t) \\\\\nz_t &= W_{out} h_t \\\\\n\\hat{y}_t &= softmax( z_t ) \\\\\nE_t &= - y_t^T log( \\hat{y}_t) \\\\\nE &= \\sum_t^T E_t\n\\end{cases}\n$$\n\n这里有必要对上面的一些符号进行进一步解释。\n\n1. 本文只讨论输入为one-hot vector的情况，这种向量的特点是茫茫0海中的一个1，即只用一个1表示某个单词的出现；其余的0均表示单词不出现。\n2. RNN要预测的输出是一个one-hot vector，表示下一个时刻各个单词出现的概率。\n3. 由于$y_t$是one-hot vector，不妨假设$y_{t,j} = 1( y_{t,i} =0 ,i \\neq j)$，那么当前时刻的交叉熵为 $$ E_t = - y_t^T log(\\hat{y}_t) = -log(\\hat{y}_{t,j}) $$。也就是说如果 $t$ 出现的是第 $j$ 个词，那么计算交叉熵时候只要看 $\\hat{y}_t$ 的第$j$个分量即可。\n4. 由于$x_t$是one-hot向量，假设第$j$个词出现，则$W_{in}x_t$相当于把$W_{in}$的第$j$列选出来，因此这一步是不用进行任何矩阵运算的，直接做下标操作即可。\n\nBPTT与BP类似，是在时间上反传的梯度下降算法。\nRNN中，我们的目的是求得 $\\frac{\\partial E}{\\partial W_{in}}, \\frac{\\partial E}{\\partial W_{rec}}, \\frac{\\partial E}{\\partial W_{out}}$，\n根据这三个变化率来优化三个参数 $W_{in},W_{rec},W_{out}$。注意到 $\\frac{\\partial E}{\\partial W_{in}} = \\sum_t \\frac{\\partial E_t}{\\partial W_{in}}$，\n因此我们只要对每个时刻的损失函数求偏导数再加起来即可。矩阵求导有两种布局方式：分母布局（Denominator Layout）和分子布局（Numerator Layout），关于分子布局和分母布局的区别，请参考文献3。\n如果这里采用分子布局，那么更新梯度时还需要将梯度矩阵进行一次转置，因此出于数学上的方便，后续的矩阵求导都将采用分母布局。\n\n<br>\n\n#### 2.2 计算 $\\frac{\\partial E_t}{\\partial W_{out}}$\n\n注意到$E_t$是$W_{out}$的复合函数，参考文献3中Scalar-by-matrix identities一节中关于复合矩阵函数求导法则（右边的是分母布局）： \n\n![matrix](/posts_res/2018-05-23-backpropagation/2-2.jpg)\n\n我们有：\n\n$$\n\\begin{aligned}\n\\frac{\\partial E_t}{\\partial W_{out}(i,j)}\n&= tr \\left( \\left( \\frac{\\partial E_t}{\\partial z_t} \\right)^T \\cdot \\frac{\\partial z_t}{\\partial W_{out}(i,j)} \\right) \\\\\n&= tr \\left( (\\hat{y}_t - y_t)^T \\cdot \\begin{bmatrix} 0\\\\ \\vdots \\\\ \\frac{\\partial z_{t}^{(i)}}{\\partial W_{out}(i,j)} \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right) \\\\\n&= r_t^{(i)} h_t^{(j)}\n\\end{aligned}\n$$\n\n其中$r_t^{(i)}=(\\hat{y}_t-y_t)^{(i)}$表示残差向量第$i$个分量，$h_t^{(j)}$表示$h_t$的第j个分量。\n\n上述结果可以改写为：\n\n$$\\frac{\\partial E_t}{\\partial W_{out}} = r_t \\otimes h_t$$\n\n$$ \\frac{\\partial E}{\\partial W_{out}} = \\sum_{t=0}^T r_t\\otimes h_t $$\n\n其中$\\otimes$表示向量外积。\n\n<br>\n\n#### 2.3 计算$\\frac{\\partial E_t}{\\partial W_{rec}}$\n\n由于$W_{rec}$是各个时刻共享的，所以$t$时刻之前的每个时刻$W_{rec}$的变化都对$E_t$有贡献，反过来求偏导时，也要考虑之前每个时刻$W_{rec}$对$E$的影响。\n我们以$s_k$为中间变量，应用链式法则：\n\n$$\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\frac{\\partial s_k}{\\partial W_{rec}} \\frac{\\partial E_t}{\\partial s_k}$$\n\n但由于$\\frac{\\partial s_k}{\\partial W_{rec}}$（分子向量，分母矩阵）以目前的数学发展水平是没办法求的，\n因此我们要求这个偏导，可以拆解为$E_t$对$W_{rec}(i,j)$的偏导数：\n\n$$\n\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} \n= \\sum_{k=0}^t tr[(\\frac{\\partial E_t}{\\partial s_k})^T \\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n= \\sum_{k=0}^t  tr[(\\delta_k^{(t)})^T\\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n$$\n\n其中，$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$，遵循\n\n$$s_k\\to h_k\\to s_{k+1}\\to ...\\to E_t$$\n\n的传递关系。\n\n应用链式法则有： \n\n$$\n\\delta^{(t)}_k = \\frac{\\partial h_k}{\\partial s_k}\\frac{\\partial s_{k+1}}{\\partial h_k} \\frac{\\partial E_t}{\\partial s_{k+1}}\n= diag(1-h_k\\odot h_k)W_{rec}^T\\delta^{(t)}_{k+1}=(W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n$$\n\n其中，$\\odot$表示向量点乘（element-wise product）。注意$E_t$求导时链式法则的顺序，$E_t$是关于$s_k$的符合函数，且求导链上的每个变量都是向量，\n根据参考文献3，这种情况下应用分母布局的链式法则，方向应该是相反的。\n\n接下来计算$\\delta^{(t)}_t$：\n\n$$\n\\delta^{(t)}_t = \\frac{\\partial E_t}{\\partial s_t} = \\frac{\\partial h_t}{\\partial s_t}\\frac{\\partial z_t}{\\partial h_t}\\frac{\\partial E_t}{\\partial z_t}\n= diag(1-h_t\\odot h_t)\\cdot W_{out}^T\\cdot(\\hat{y}_t-y_t)=(W_{out}^T(\\hat{y}_t-y_t))\\odot (1-h_t\\odot h_t)\n$$\n\n于是，我们得到了关于$\\delta$ 的递推关系式:\n\n$$\n\\begin{cases}\n\\delta^{(t)}_t &= (W_{out}^T r_t)\\odot (1-h_t\\odot h_t) \\\\ \n\\delta^{(t)}_k &= (W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n\\end{cases}\n$$\n\n由 $ \\delta^{(t)}_t $ 出发，\n我们可以往前推出每一个 $ \\delta $ ，\n将 $$ \\delta^{(t)}_0,...,\\delta^{(t)}_t $$ \n代入 $$\\frac{\\partial E_t}{\\partial W_{rec}(i,j)}$$\n有：\n\n$$\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} = \\sum_{k=0}^t \\delta_k^{(t)} h_{k-1}^{(j)} $$\n\n将上式写成矩阵形式：\n\n$$\n\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1} \\\\\n\\frac{\\partial E}{\\partial W_{rec}} =\\sum_{t=0}^T \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1}\n$$\n\n不失严谨性，定义$h_{-1}$为全0的向量。\n\n<br>\n\n#### 2.4 计算$\\frac{\\partial E_t}{\\partial W_{in}}$\n\n按照上述思路，我们可以得到 \n\n$$ \\frac{\\partial E_t}{\\partial W_{in}} = \\sum_{k=0}^t \\delta_k \\otimes x_{k} $$\n\n由于$x_k$是个one-hot vector，假设$x_k(m)=1$，那么我们在更新$W$时只需要更新$W$的第$m$列即可。\n\n\n<br>\n\n#### 2.5 参数更新\n\n我们有了$E_t$关于各个参数的偏导数，就可以用梯度下降来更新各参数了:\n\n$$\n\\begin{cases}\nW_{in} &= W_{in} - \\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes x_{k} \\\\\nW_{rec}&=W_{rec}-\\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes h_{k-1} \\\\\nW_{out}&=W_{out}-\\lambda \\sum_{t=0}^T r_t \\otimes h_t\n\\end{cases}\n$$\n\n其中 $$r_t= \\hat{y}_t - y_t$$，\n$$ \\delta_t = \\frac{\\partial E_t}{\\partial s_t} = (W_{out}^T r_t) \\odot (1 - h_t\\odot h_t), \\lambda > 0 $$ 表示学习率。\n\n<br>\n\n#### 2.6 部分思考\n\n- 为什么RNN中要对隐藏层的输出进行一次运算$z_t=W_{out}h_t$，然后再对$z_t$进行一次softmax，而不是直接对$h_t$进行softmax求得概率？为什么要有$W_{out}$这个参数？\n    - 答：$x_t$是一个$K\\times 1$的向量，我们要将它映射到一个$H\\times 1$的$h_t$（其中$H$是隐神经元的个数），从$x_t$到$h_t$相当于对词向量做了一次编码；最终我们要得到的是一个$K\\times 1$的向量（这里$K$是词汇表大小），表示每个词接下来出现的概率，所以我们需要一个矩阵$K\\times H$的$W_{out}$来将$h_t$映射回原来的空间去，这个过程相当于解码。因此，RNN可以理解为一种编解码网络。\n- $W_{in},W_{rec},W_{out}$三个参数分别有什么意义？\n    - 答： $W_{in}$将$K\\times 1$的one-hot词向量映射到$H\\times 1$隐藏层空间，将输入转化为计算机内部可理解可处理的形式，这个过程可以理解为一次编码过程；$W_{rec}$则是隐含层到自身的一个映射，它定义了模型如何结合上文信息，在编码中融入了之前的“记忆”；$W_{in},W_{rec}$结合了当前输入单词和之前的记忆，形成了当前时刻的知识状态。$W_{out}$是隐含层到输出的映射，$z=W_{out}h$是映射后的分数，这个过程相当于一次解码。这个解码后的分数再经过一层softmax转化为概率输出来，我们挑选概率最高的那个作为我们的预测。作为总结， RNN的记忆由两部分构成，一部分是当前的输入，另一部分是之前的记忆。\n- BPTT和BP的区别在哪？为什么不能用BP算法训练RNN？\n    - 答：BP算法只考虑了误差的导数在上下层级之间梯度的反向传播；而BPTT则同时包含了梯度在纵向层级间的反向传播和在时间维度上的反向传播，同时在两个方向上进行参数优化。\n- 文中词$x_t$的特征是一个one-hot vector，这里能不能替换为word2vec训练出的词向量？效果和性能如何？\n    - 答：RNNLM本身自带了训练词向量的过程。由于$x_t$是one-hot向量，假设出现的词的索引为$j$，那么$W_{in}x_t$就是把$W_{in}$的第$j$列$W[:,j]$取出，这个列向量就相当于该词的词向量。实际上用语言模型训练词向量的思想最早可以追溯到03年Bengio的一篇论文《A neural probabilistic language model 》，这篇论文中作者使用一个神经网络模型来训练n-gram模型，顺便学到了词向量。本文出于数学推导以及代码实现上的方便采用了one-hot向量作为输入。实际工程中，词汇表通常都是几百万，内存没办法装下几百万维的稠密矩阵，所以工程上基本上没有用one-hot的，基本都是用词向量。\n\n\n-------\n\n>\n1. [使用RNN解决NLP中序列标注问题的通用优化思路](http://blog.csdn.net/malefactor/article/details/50725480)\n2. [wildml的rnn tutorial part3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)\n3. [Matrix Calculus Wiki](https://en.wikipedia.org/wiki/Matrix_calculus)\n4. [《神经网络与深度学习讲义》 邱锡鹏]()\n","source":"_posts/2018-05-23-backpropagation.md","raw":"---\nlayout: post\ntitle: 反向传播 BP & BPTT\ndate: 2018-05-23 12:10 +0800\ncategories: 深度学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n目录\n\n* 反向传播BP\n* 随时间反向传播BPTT\n\n------\n\n### 1. 反向传播BP\n\n[[Calculus on Computational Graphs: Backpropagation，英文原版]](http://colah.github.io/posts/2015-08-Backprop/)、\n[[详解反向传播算法，中文翻译理解]](https://zhuanlan.zhihu.com/p/25081671)\n\n解释了为什么从上而下计算梯度。一言以蔽之：从下而上会有重复计算，当参数数量太多，时间耗费太严重；从上而下每个节点只计算一次。\n\n[如何直观地解释 back propagation 算法？](https://www.zhihu.com/question/27239198/answer/89853077)中的例子比较清晰的刻画了反向传播的优势。\n\n[A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)每行代码都有解释。\n\n[CHAPTER 2How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)讲的特别详细，\n中文版在[这里 - 神经⽹络与深度学习](/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf)。\n\n------\n\n<br>\n\n------\n\n**以下部分参考Neural Networks and Deep Learning(神经⽹络与深度学习P37 - P42)**\n\n反向传播的四个方程式：\n\n\\\\[\n\\delta^L = \\nabla \\_a C \\odot \\sigma' (z^L) \\tag{BP1}\n\\\\]\n\n\\\\[\n\\delta^l = ( (w^{l+1})^T \\delta^{l+1} ) \\odot \\sigma' (z^l) \\tag{BP2}\n\\\\]\n\n\\\\[\n\\frac{\\partial C}{\\partial b\\_j^l} = \\delta\\_j^l  \\tag{BP3}\n\\\\]\n\n\\\\[\n\\frac{\\partial C}{\\partial w\\_{jk}^l} = a\\_k^{l-1} \\delta\\_j^l  \\tag{BP4}\n\\\\]\n\n证明上面的四个方程式：\n\n**证明BP1**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta\\_j^L \n& = \\frac{\\partial C}{\\partial z\\_j^L} \\\\\\\n& = \\frac{\\partial C}{\\partial a\\_j^L} \\cdot \\frac{\\partial a\\_j^L}{\\partial z\\_j^L} \\\\\\\n& = \\frac{\\partial C}{\\partial a\\_j^L} \\cdot \\sigma' (z\\_j^L) \\quad (\\because a\\_j^L = \\sigma (z\\_j^L))\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP2**\n\n\\\\[\n\\begin{aligned}\n\\because z_k^{l+1} &= \\sum\\_j w\\_{kj}^{l+1} a\\_j^l + b\\_k^{l+1} = \\sum\\_j w\\_{kj}^{l+1} \\cdot \\sigma(z\\_j^l) + b\\_k^{l+1} \\\\\\\n\\therefore \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_j^l} & = w\\_{kj}^{l+1} \\cdot \\sigma' (z\\_j^l)\n\\end{aligned}\n\\\\]\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta\\_j^l\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\\\\\\n& = \\sum\\_k \\frac{\\partial C}{\\partial z\\_k^{l+1}} \\cdot \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_j^l} \\\\\\\n& = \\sum\\_k \\frac{\\partial z\\_k^{l+1}}{\\partial z\\_k^l} \\cdot \\delta\\_k^{l+1} \\\\\\\n& = \\sum\\_k w\\_{kj}^{l+1} \\cdot \\sigma' (z\\_j^l) \\cdot \\delta\\_k^{l+1} \\\\\\\n& = \\sum\\_k w\\_{kj}^{l+1} \\cdot \\delta\\_k^{l+1} \\cdot \\sigma' (z\\_j^l)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP3**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial b\\_j^l}\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\cdot \\frac{\\partial z\\_j^l}{\\partial b\\_j^l} \\\\\\\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\quad (\\because z\\_j^l = \\sum\\_k w\\_{jk}^l \\cdot a\\_k^{l-1} + b\\_j^l \\therefore \\frac{\\partial z\\_j^l}{\\partial b\\_j^l} = 1) \\\\\\\n& = \\delta\\_j^l\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n**证明BP4**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial w\\_{jk}^l}\n& = \\frac{\\partial C}{\\partial z\\_j^l} \\cdot \\frac{\\partial z\\_j^l}{\\partial w\\_{jk}^l} \\\\\\\n& = \\delta\\_j^l \\cdot a\\_k^{l-1} \\quad (\\because z\\_j^l = \\sum\\_k w\\_{jk}^l \\cdot a\\_k^{l-1} + b\\_j^l)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n\n---------\n\n<br>\n\n----------\n\n**以下摘抄自：**[反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n\n反向传播算法的思路如下：给定一个样例 \\\\( (x,y) \\\\)，首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \\\\( h\\_{W,b}(x) \\\\) 的输出值。\n之后，针对第 \\\\( l \\\\) 层的每一个节点 \\\\( i \\\\)，计算出其 “残差” \\\\( \\delta^{(l)}\\_i \\\\)，该残差表明了该节点对最终输出值的残差产生了多少影响。\n对于最终的输出节点，可以直接算出网络产生的激活值与实际值之间的差距，将这个差距定义为 \\\\( \\delta^{(n_l)}\\_i \\\\)（第 \\\\( n_l \\\\) 层表示输出层）。\n对于隐藏单元如何处理呢？基于节点（第 \\\\( l+1 \\\\) 层节点）残差的加权平均值计算 \\\\( \\delta^{(l)}\\_i \\\\)，这些节点以 \\\\( a^{(l)}\\_i \\\\) 作为输入。\n下面将给出反向传导算法的细节:\n\n* (1) 进行前馈传导计算，利用前向传导公式，得到 \\\\( L\\_2, L\\_3, \\ldots \\\\) 直到输出层 \\\\( L\\_{n\\_l} \\\\) 的激活值。\n* (2) 对于第 \\\\( n\\_l \\\\) 层（输出层）的每个输出单元 \\\\( i \\\\) ，根据以下公式计算残差：\n\\\\[ \n\\delta^{(n\\_l)}\\_i \n= \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\|\\| y - h\\_{W,b}(x) \\|\\| ^2\n= - (y\\_i - a^{(n\\_l)}\\_i) \\cdot f'(z^{(n\\_l)}\\_i)\n\\\\]\n\n**推导过程**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n\\_l)}\\_i\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} J(W,b; x,y) \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\|\\|y - h\\_{W,b}(x) \\|\\|^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}} (y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}} (y\\_j - f(z\\_j^{(n\\_l)}))^2 \\\\\\\n& = - (y\\_i - f(z\\_i^{(n\\_l)})) \\cdot f'(z^{(n\\_l)}\\_i) \\\\\\\n& = - (y\\_i - a^{(n\\_l)}\\_i) \\cdot f'(z^{(n\\_l)}\\_i)\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n* (3) 对 \\\\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\\\) 的各个层，第 \\\\( l \\\\) 层的第 \\\\( i \\\\) 个节点的残差计算方法如下：\n\\\\[\n\\delta^{(l)}\\_i = \\left( \\sum\\_{j=1}^{s\\_{l+1}} W^{(l)}\\_{ji} \\delta^{(l+1)}\\_j \\right) f'(z^{(l)}\\_i)\n\\\\]\n\n**推导过程**\n\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n\\_l-1)}\\_i \n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} J(W,b; x,y) \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} \\frac{1}{2} \\|\\|y - h\\_{W,b}(x) \\|\\|^2 \\\\\\\n& = \\frac{\\partial}{\\partial z^{(n\\_l-1)}\\_i} \\frac{1}{2} \\sum\\_{j=1}^{S\\_{n\\_l}}(y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{1}{2} \\sum\\_{j=1}^{S\\_{(n\\_l)}}\\frac{\\partial}{\\partial z^{n\\_l-1}\\_i}(y\\_j - a\\_j^{(n\\_l)})^2 \\\\\\\n& = \\frac{1}{2} \\sum\\_{j=1}^{S\\_{(n\\_l)}}\\frac{\\partial}{\\partial z^{n\\_l-1}\\_i}(y\\_j - f(z\\_j^{(n\\_l)}))^2 \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} - (y\\_j - f(z\\_j^{(n\\_l)})) \\cdot \\frac{\\partial}{\\partial z\\_i^{(n\\_l-1)}}f(z\\_j^{(n\\_l)}) \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} - (y\\_j - f(z\\_j^{(n\\_l)})) \\cdot f'(z\\_j^{(n\\_l)}) \\cdot \\frac{\\partial z\\_j^{(n\\_l)}}{\\partial z\\_i^{(n\\_l-1)}} \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} \\delta\\_j^{(n\\_l)} \\cdot \\frac{\\partial z\\_j^{(n\\_l)}}{\\partial z\\_i^{(n\\_l-1)}} \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n_l}} \\left(\\delta\\_j^{(n\\_l)} \\cdot \\frac{\\partial}{\\partial z\\_i^{(n\\_l-1)}}\\sum\\_{k=1}^{S\\_{n\\_l-1}}f(z\\_k^{(n\\_l-1)}) \\cdot W\\_{jk}^{n\\_l-1} \\right) \\\\\\\n& = \\sum\\_{j=1}^{S\\_{n\\_l}} \\delta\\_j^{(n\\_l)} \\cdot  W\\_{ji}^{n\\_l-1} \\cdot f'(z\\_i^{(n\\_l-1)}) \\\\\\\n& = \\left( \\sum\\_{j=1}^{S\\_{n\\_l}} W\\_{ji}^{n\\_l-1} \\delta\\_j^{(n\\_l)} \\right) f'(z\\_i^{(n\\_l-1)}) \\\\\\\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n将上式中的 \\\\( n_l-1 \\\\) 与 \\\\( n_l \\\\) 的关系替换为 \\\\( l \\\\) 与 \\\\( l+1 \\\\) 的关系，就可以得到：\n\\\\[\n\\delta^{(l)}\\_i = \\left( \\sum\\_{j=1}^{s_{l+1}} W^{(l)}\\_{ji} \\delta^{(l+1)}_j \\right) f'(z^{(l)}_i)\n\\\\]\n\n以上逐次从后向前求导的过程即为“反向传导”的本意所在。\n\n* (4) 计算需要的偏导数，计算方法如下：\n\\\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial W\\_{ij}^{(l)}} J(W,b; x, y) &= a^{(l)}\\_j \\delta\\_i^{(l+1)} \\\\\\\n\\frac{\\partial}{\\partial b\\_{i}^{(l)}} J(W,b; x, y) &= \\delta\\_i^{(l+1)}.\n\\end{aligned}\n\\\\]\n\n<br>\n\n最后，用矩阵-向量表示法重写以上算法。我们使用 “ \\\\( \\bullet \\\\)” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。\n若 \\\\( a = b \\bullet c \\\\)，则 \\\\( a_i = b_i c_i \\\\)。扩展 \\\\( f(\\cdot) \\\\) 的定义，使其包含向量运算，于是有 \\\\( f'([z_1, z_2, z_3]) = [f'(z_1), f'(z_2), f'(z_3)] \\\\)。\n\n那么，反向传播算法可表示为以下几个步骤：\n\n1. 进行前馈传导计算，利用前向传导公式，得到 \\\\( L_2, L_3, \\ldots \\\\) 直到输出层 \\\\( L_{n_l} \\\\) 的激活值。\n2. 对输出层（第 \\\\( n_l \\\\) 层），计算:\\\\[\\begin{align} \\delta^{(n_l)} = - (y - a^{(n_l)}) \\bullet f'(z^{(n_l)}) \\end{align}\\\\]\n3. 对于 \\\\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\\\) 的各层，计算：\\\\[\\begin{align} \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f'(z^{(l)}) \\end{align} \\\\]\n4. 计算最终需要的偏导数值：\\\\[ \\begin{align} \\nabla_{W^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)} (a^{(l)})^T, \\\\\\ \\nabla_{b^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)}.\\end{align} \\\\]\n\n<br>\n\n**实现中应注意：**\n\n在以上的第2步和第3步中，我们需要为每一个 \\\\( i \\\\) 值计算其 \\\\( f'(z^{(l)}_i) \\\\) 。\n假设 \\\\( f(z) \\\\) 是sigmoid函数，并且我们已经在前向传导运算中得到了 \\\\( a^{(l)}_i \\\\) 。\n那么，使用我们早先推导出的 \\\\( f'(z) \\\\) 表达式，就可以计算得到 \\\\( f'(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i) \\\\)。\n\n最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，\\\\( \\Delta W^{(l)} \\\\) 是一个与矩阵 \\\\( W^{(l)} \\\\) 维度相同的矩阵，\n\\\\( \\Delta b^{(l)} \\\\) 是一个与 \\\\( b^{(l)} \\\\) 维度相同的向量。\n注意这里 “\\\\( \\Delta W^{(l)} \\\\) ”是一个矩阵，而不是“\\\\( \\Delta \\\\) 与 \\\\( W^{(l)} \\\\) 相乘”。\n\n下面，我们实现批量梯度下降法中的一次迭代：\n\n* (1) 对于所有 \\\\( l \\\\)，令 \\\\( \\Delta W^{(l)} := 0, \\Delta b^{(l)} := 0 \\\\) (设置为全零矩阵或全零向量)\n* (2) 对于 \\\\( i = 1 \\\\) 到 \\\\( m \\\\)，\n* (2.1) 使用反向传播算法计算 \\\\( \\nabla_{W^{(l)}} J(W,b;x,y) \\\\) 和 \\\\( \\nabla_{b^{(l)}} J(W,b;x,y) \\\\);\n* (2.2) 计算 \\\\( \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y) \\\\);\n* (2.3) 计算 \\\\( \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y) \\\\);\n* (3) 更新权重参数：\n\\\\[\n\\begin{align}\nW^{(l)} &= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\\\\nb^{(l)} &= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\n\\end{align}\n\\\\]\n\n现在，可以重复梯度下降法的迭代步骤来减小代价函数 \\\\( J(W,b) \\\\) 的值，进而求解我们的神经网络。\n\n\n-------\n\n### 2. [转]随时间反向传播BPTT\n\n转自：[hschen0712/machine-learning-notes](https://github.com/hschen0712/machine-learning-notes/blob/master/Deep-Learning/back-propagation-through-time.ipynb)\n\nRNN（递归神经网络，Recurrent Neural Network）是一种具有长时记忆能力的神经网络模型，被广泛应用于序列标注（Sequence Labeling）问题。\n在序列标注问题中，模型的输入是一段时间序列，记为$ x = \\lbrace x_1, x_2, ..., x_T \\rbrace $，\n我们的目标是为输入序列的每个元素打上标签集合中的对应标签，记为$ y = \\lbrace y_1, y_2, ..., y_T \\rbrace $。\n\nNLP中的大部分任务（比如分词、实体识别、词性标注）都可以最终归结为序列标注问题。\n这类问题中，输入是语料库中一段由 $T$ 个词（或字）构成的文本 $ x = \\lbrace x_1, x_2, ..., x_T \\rbrace $（其中$x_t$表示文本中的第$t$个词）；\n输出是每个词对应的标签，根据任务的不同标签的形式也各不相同，但本质上都是针对每个词根据它的上下文进行标签的分类。\n\n一个典型的RNN的结构如下图所示：\n\n![rnn](/posts_res/2018-05-23-backpropagation/2-1.jpg)\n\n从图中可以看到，一个RNN通常由三层组成，分别是输入层、隐藏层和输出层。\n与一般的神经网络不同的地方是，RNN的隐藏层存在一条有向反馈边，正是这种反馈机制赋予了RNN记忆能力。\n要理解左边的图可能有点难度，我们可以将其展开为右边这种更直观的形式，其中RNN的每个神经元接受当前时刻的输入$x_t$ 以及上一时刻隐单元的输出$h_{t-1}$，\n计算出当前神经元的输入 $s_t$，经过激活函数变换得到输出 $h_t$，并传递给下一时刻的隐单元。\n此外，我们还需要注意到RNN中每个时刻上的神经元的参数都是相同的（类似CNN的权值共享），这么做一方面是减小参数空间，保证泛化能力；\n另一方面是为了赋予RNN记忆能力，将有用信息存储在$W_{in},W_{rec},W_{out}$三个矩阵中。\n\n由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化，这要求我们提出新的优化算法。\nRNN中最常用的优化算法是随时间反向传播（BackPropagation Through Time，BPTT），下文将叙述BPTT算法的数学推导。\n\n\n#### 2.1 符号注解\n\n<table>\n <tr>\n  <th>符号</th>\n  <th>注解</th>\n </tr>\n <tr>\n  <td>$K$</td>\n  <td>词汇表的大小</td>\n </tr>\n<tr>\n  <td>$T$</td>\n  <td>句子的长度</td>\n </tr>\n<tr>\n  <td>$H$</td>\n  <td>隐藏层单元数</td>\n </tr>\n<tr>\n  <td>$x=\\lbrace x_1, x_2, ..., x_T \\rbrace $</td>\n  <td>句子的单词序列</td>\n </tr>\n<tr>\n  <td>$x_t \\in R^{K \\times 1}$</td>\n  <td>第$t$时刻RNN的输入，one-hot vector</td>\n </tr>\n<tr>\n  <td>$\\hat{y}_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻softmax层的输出，估计每个词出现的概率</td>\n </tr>\n<tr>\n  <td>$y_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻的label，为每个词出现的概率，one-hot vector</td>\n </tr>\n<tr>\n  <td>$E_t$</td>\n  <td>第$t$时刻（第$t$个word）的损失函数，定义为交叉熵误差$E_t = - y_t^T log (\\hat{y}_t) $</td>\n </tr>\n<tr>\n  <td>$E$</td>\n  <td>一个句子的损失函数，由各个时刻（即每个word）的损失函数组成,$E=\\sum_t^T E_t$（注:由于要推导的是SGD算法，更新梯度是相对于一个训练样例而言的，因此一次只考虑一个句子的误差，而不是整个训练集的误差（对应BGD算法））</td>\n </tr>\n<tr>\n  <td>$s_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输入</td>\n </tr>\n<tr>\n  <td>$h_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输出</td>\n </tr>\n<tr>\n  <td>$z_t \\in R^{K \\times 1}$</td>\n  <td>输出层的汇集输入</td>\n </tr>\n<tr>\n  <td>$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$\t</td>\n  <td>第$t$个时刻损失函数$E_t$对第$k$时刻带权输入$s_k$的导数</td>\n </tr>\n<tr>\n  <td>$r_t=\\hat{y}_t-y_t$\t</td>\n  <td>残差向量</td>\n </tr>\n<tr>\n  <td>$W_{in}\\in\\mathbb{R}^{H\\times K}$\t</td>\n  <td>从输入层到隐藏层的权值</td>\n </tr>\n<tr>\n  <td>$W_{rec}\\in\\mathbb{R}^{H\\times H}$\t</td>\n  <td>隐藏层上一个时刻到当前时刻的权值</td>\n </tr>\n<tr>\n  <td>$W_{out}\\in\\mathbb{R}^{K\\times H}$\t</td>\n  <td>隐藏层到输出层的权值</td>\n </tr>\n</table>\n\n上述符号之间的关系:\n\n$$\n\\begin{cases}\ns_t &= W_{rec} h_{t-1} + W_{in} x_t \\\\\nh_t &= tanh(s_t) \\\\\nz_t &= W_{out} h_t \\\\\n\\hat{y}_t &= softmax( z_t ) \\\\\nE_t &= - y_t^T log( \\hat{y}_t) \\\\\nE &= \\sum_t^T E_t\n\\end{cases}\n$$\n\n这里有必要对上面的一些符号进行进一步解释。\n\n1. 本文只讨论输入为one-hot vector的情况，这种向量的特点是茫茫0海中的一个1，即只用一个1表示某个单词的出现；其余的0均表示单词不出现。\n2. RNN要预测的输出是一个one-hot vector，表示下一个时刻各个单词出现的概率。\n3. 由于$y_t$是one-hot vector，不妨假设$y_{t,j} = 1( y_{t,i} =0 ,i \\neq j)$，那么当前时刻的交叉熵为 $$ E_t = - y_t^T log(\\hat{y}_t) = -log(\\hat{y}_{t,j}) $$。也就是说如果 $t$ 出现的是第 $j$ 个词，那么计算交叉熵时候只要看 $\\hat{y}_t$ 的第$j$个分量即可。\n4. 由于$x_t$是one-hot向量，假设第$j$个词出现，则$W_{in}x_t$相当于把$W_{in}$的第$j$列选出来，因此这一步是不用进行任何矩阵运算的，直接做下标操作即可。\n\nBPTT与BP类似，是在时间上反传的梯度下降算法。\nRNN中，我们的目的是求得 $\\frac{\\partial E}{\\partial W_{in}}, \\frac{\\partial E}{\\partial W_{rec}}, \\frac{\\partial E}{\\partial W_{out}}$，\n根据这三个变化率来优化三个参数 $W_{in},W_{rec},W_{out}$。注意到 $\\frac{\\partial E}{\\partial W_{in}} = \\sum_t \\frac{\\partial E_t}{\\partial W_{in}}$，\n因此我们只要对每个时刻的损失函数求偏导数再加起来即可。矩阵求导有两种布局方式：分母布局（Denominator Layout）和分子布局（Numerator Layout），关于分子布局和分母布局的区别，请参考文献3。\n如果这里采用分子布局，那么更新梯度时还需要将梯度矩阵进行一次转置，因此出于数学上的方便，后续的矩阵求导都将采用分母布局。\n\n<br>\n\n#### 2.2 计算 $\\frac{\\partial E_t}{\\partial W_{out}}$\n\n注意到$E_t$是$W_{out}$的复合函数，参考文献3中Scalar-by-matrix identities一节中关于复合矩阵函数求导法则（右边的是分母布局）： \n\n![matrix](/posts_res/2018-05-23-backpropagation/2-2.jpg)\n\n我们有：\n\n$$\n\\begin{aligned}\n\\frac{\\partial E_t}{\\partial W_{out}(i,j)}\n&= tr \\left( \\left( \\frac{\\partial E_t}{\\partial z_t} \\right)^T \\cdot \\frac{\\partial z_t}{\\partial W_{out}(i,j)} \\right) \\\\\n&= tr \\left( (\\hat{y}_t - y_t)^T \\cdot \\begin{bmatrix} 0\\\\ \\vdots \\\\ \\frac{\\partial z_{t}^{(i)}}{\\partial W_{out}(i,j)} \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right) \\\\\n&= r_t^{(i)} h_t^{(j)}\n\\end{aligned}\n$$\n\n其中$r_t^{(i)}=(\\hat{y}_t-y_t)^{(i)}$表示残差向量第$i$个分量，$h_t^{(j)}$表示$h_t$的第j个分量。\n\n上述结果可以改写为：\n\n$$\\frac{\\partial E_t}{\\partial W_{out}} = r_t \\otimes h_t$$\n\n$$ \\frac{\\partial E}{\\partial W_{out}} = \\sum_{t=0}^T r_t\\otimes h_t $$\n\n其中$\\otimes$表示向量外积。\n\n<br>\n\n#### 2.3 计算$\\frac{\\partial E_t}{\\partial W_{rec}}$\n\n由于$W_{rec}$是各个时刻共享的，所以$t$时刻之前的每个时刻$W_{rec}$的变化都对$E_t$有贡献，反过来求偏导时，也要考虑之前每个时刻$W_{rec}$对$E$的影响。\n我们以$s_k$为中间变量，应用链式法则：\n\n$$\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\frac{\\partial s_k}{\\partial W_{rec}} \\frac{\\partial E_t}{\\partial s_k}$$\n\n但由于$\\frac{\\partial s_k}{\\partial W_{rec}}$（分子向量，分母矩阵）以目前的数学发展水平是没办法求的，\n因此我们要求这个偏导，可以拆解为$E_t$对$W_{rec}(i,j)$的偏导数：\n\n$$\n\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} \n= \\sum_{k=0}^t tr[(\\frac{\\partial E_t}{\\partial s_k})^T \\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n= \\sum_{k=0}^t  tr[(\\delta_k^{(t)})^T\\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n$$\n\n其中，$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$，遵循\n\n$$s_k\\to h_k\\to s_{k+1}\\to ...\\to E_t$$\n\n的传递关系。\n\n应用链式法则有： \n\n$$\n\\delta^{(t)}_k = \\frac{\\partial h_k}{\\partial s_k}\\frac{\\partial s_{k+1}}{\\partial h_k} \\frac{\\partial E_t}{\\partial s_{k+1}}\n= diag(1-h_k\\odot h_k)W_{rec}^T\\delta^{(t)}_{k+1}=(W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n$$\n\n其中，$\\odot$表示向量点乘（element-wise product）。注意$E_t$求导时链式法则的顺序，$E_t$是关于$s_k$的符合函数，且求导链上的每个变量都是向量，\n根据参考文献3，这种情况下应用分母布局的链式法则，方向应该是相反的。\n\n接下来计算$\\delta^{(t)}_t$：\n\n$$\n\\delta^{(t)}_t = \\frac{\\partial E_t}{\\partial s_t} = \\frac{\\partial h_t}{\\partial s_t}\\frac{\\partial z_t}{\\partial h_t}\\frac{\\partial E_t}{\\partial z_t}\n= diag(1-h_t\\odot h_t)\\cdot W_{out}^T\\cdot(\\hat{y}_t-y_t)=(W_{out}^T(\\hat{y}_t-y_t))\\odot (1-h_t\\odot h_t)\n$$\n\n于是，我们得到了关于$\\delta$ 的递推关系式:\n\n$$\n\\begin{cases}\n\\delta^{(t)}_t &= (W_{out}^T r_t)\\odot (1-h_t\\odot h_t) \\\\ \n\\delta^{(t)}_k &= (W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n\\end{cases}\n$$\n\n由 $ \\delta^{(t)}_t $ 出发，\n我们可以往前推出每一个 $ \\delta $ ，\n将 $$ \\delta^{(t)}_0,...,\\delta^{(t)}_t $$ \n代入 $$\\frac{\\partial E_t}{\\partial W_{rec}(i,j)}$$\n有：\n\n$$\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} = \\sum_{k=0}^t \\delta_k^{(t)} h_{k-1}^{(j)} $$\n\n将上式写成矩阵形式：\n\n$$\n\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1} \\\\\n\\frac{\\partial E}{\\partial W_{rec}} =\\sum_{t=0}^T \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1}\n$$\n\n不失严谨性，定义$h_{-1}$为全0的向量。\n\n<br>\n\n#### 2.4 计算$\\frac{\\partial E_t}{\\partial W_{in}}$\n\n按照上述思路，我们可以得到 \n\n$$ \\frac{\\partial E_t}{\\partial W_{in}} = \\sum_{k=0}^t \\delta_k \\otimes x_{k} $$\n\n由于$x_k$是个one-hot vector，假设$x_k(m)=1$，那么我们在更新$W$时只需要更新$W$的第$m$列即可。\n\n\n<br>\n\n#### 2.5 参数更新\n\n我们有了$E_t$关于各个参数的偏导数，就可以用梯度下降来更新各参数了:\n\n$$\n\\begin{cases}\nW_{in} &= W_{in} - \\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes x_{k} \\\\\nW_{rec}&=W_{rec}-\\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes h_{k-1} \\\\\nW_{out}&=W_{out}-\\lambda \\sum_{t=0}^T r_t \\otimes h_t\n\\end{cases}\n$$\n\n其中 $$r_t= \\hat{y}_t - y_t$$，\n$$ \\delta_t = \\frac{\\partial E_t}{\\partial s_t} = (W_{out}^T r_t) \\odot (1 - h_t\\odot h_t), \\lambda > 0 $$ 表示学习率。\n\n<br>\n\n#### 2.6 部分思考\n\n- 为什么RNN中要对隐藏层的输出进行一次运算$z_t=W_{out}h_t$，然后再对$z_t$进行一次softmax，而不是直接对$h_t$进行softmax求得概率？为什么要有$W_{out}$这个参数？\n    - 答：$x_t$是一个$K\\times 1$的向量，我们要将它映射到一个$H\\times 1$的$h_t$（其中$H$是隐神经元的个数），从$x_t$到$h_t$相当于对词向量做了一次编码；最终我们要得到的是一个$K\\times 1$的向量（这里$K$是词汇表大小），表示每个词接下来出现的概率，所以我们需要一个矩阵$K\\times H$的$W_{out}$来将$h_t$映射回原来的空间去，这个过程相当于解码。因此，RNN可以理解为一种编解码网络。\n- $W_{in},W_{rec},W_{out}$三个参数分别有什么意义？\n    - 答： $W_{in}$将$K\\times 1$的one-hot词向量映射到$H\\times 1$隐藏层空间，将输入转化为计算机内部可理解可处理的形式，这个过程可以理解为一次编码过程；$W_{rec}$则是隐含层到自身的一个映射，它定义了模型如何结合上文信息，在编码中融入了之前的“记忆”；$W_{in},W_{rec}$结合了当前输入单词和之前的记忆，形成了当前时刻的知识状态。$W_{out}$是隐含层到输出的映射，$z=W_{out}h$是映射后的分数，这个过程相当于一次解码。这个解码后的分数再经过一层softmax转化为概率输出来，我们挑选概率最高的那个作为我们的预测。作为总结， RNN的记忆由两部分构成，一部分是当前的输入，另一部分是之前的记忆。\n- BPTT和BP的区别在哪？为什么不能用BP算法训练RNN？\n    - 答：BP算法只考虑了误差的导数在上下层级之间梯度的反向传播；而BPTT则同时包含了梯度在纵向层级间的反向传播和在时间维度上的反向传播，同时在两个方向上进行参数优化。\n- 文中词$x_t$的特征是一个one-hot vector，这里能不能替换为word2vec训练出的词向量？效果和性能如何？\n    - 答：RNNLM本身自带了训练词向量的过程。由于$x_t$是one-hot向量，假设出现的词的索引为$j$，那么$W_{in}x_t$就是把$W_{in}$的第$j$列$W[:,j]$取出，这个列向量就相当于该词的词向量。实际上用语言模型训练词向量的思想最早可以追溯到03年Bengio的一篇论文《A neural probabilistic language model 》，这篇论文中作者使用一个神经网络模型来训练n-gram模型，顺便学到了词向量。本文出于数学推导以及代码实现上的方便采用了one-hot向量作为输入。实际工程中，词汇表通常都是几百万，内存没办法装下几百万维的稠密矩阵，所以工程上基本上没有用one-hot的，基本都是用词向量。\n\n\n-------\n\n>\n1. [使用RNN解决NLP中序列标注问题的通用优化思路](http://blog.csdn.net/malefactor/article/details/50725480)\n2. [wildml的rnn tutorial part3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)\n3. [Matrix Calculus Wiki](https://en.wikipedia.org/wiki/Matrix_calculus)\n4. [《神经网络与深度学习讲义》 邱锡鹏]()\n","slug":"backpropagation","published":1,"updated":"2019-08-17T09:37:07.759Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw3002d2qwpru76wciq","content":"<p>目录</p><ul>\n<li>反向传播BP</li>\n<li>随时间反向传播BPTT</li>\n</ul><hr><h3 id=\"1-反向传播BP\"><a href=\"#1-反向传播BP\" class=\"headerlink\" title=\"1. 反向传播BP\"></a>1. 反向传播BP</h3><p><a href=\"http://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\" rel=\"noopener\">[Calculus on Computational Graphs: Backpropagation，英文原版]</a>、\n<a href=\"https://zhuanlan.zhihu.com/p/25081671\" target=\"_blank\" rel=\"noopener\">[详解反向传播算法，中文翻译理解]</a></p><p>解释了为什么从上而下计算梯度。一言以蔽之：从下而上会有重复计算，当参数数量太多，时间耗费太严重；从上而下每个节点只计算一次。</p><p><a href=\"https://www.zhihu.com/question/27239198/answer/89853077\" target=\"_blank\" rel=\"noopener\">如何直观地解释 back propagation 算法？</a>中的例子比较清晰的刻画了反向传播的优势。</p><p><a href=\"https://iamtrask.github.io/2015/07/12/basic-python-network/\" target=\"_blank\" rel=\"noopener\">A Neural Network in 11 lines of Python</a>每行代码都有解释。</p><p><a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" target=\"_blank\" rel=\"noopener\">CHAPTER 2How the backpropagation algorithm works</a>讲的特别详细，\n中文版在<a href=\"/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf\">这里 - 神经⽹络与深度学习</a>。</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<hr>\n<p><br></p>\n<hr>\n<p><strong>以下部分参考Neural Networks and Deep Learning(神经⽹络与深度学习P37 - P42)</strong></p>\n<p>反向传播的四个方程式：</p>\n<p>\\[\n\\delta^L = \\nabla _a C \\odot \\sigma’ (z^L) \\tag{BP1}\n\\]</p>\n<p>\\[\n\\delta^l = ( (w^{l+1})^T \\delta^{l+1} ) \\odot \\sigma’ (z^l) \\tag{BP2}\n\\]</p>\n<p>\\[\n\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l  \\tag{BP3}\n\\]</p>\n<p>\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l  \\tag{BP4}\n\\]</p>\n<p>证明上面的四个方程式：</p>\n<p><strong>证明BP1</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta_j^L \n&amp; = \\frac{\\partial C}{\\partial z_j^L} \\\\\n&amp; = \\frac{\\partial C}{\\partial a_j^L} \\cdot \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\\n&amp; = \\frac{\\partial C}{\\partial a_j^L} \\cdot \\sigma’ (z_j^L) \\quad (\\because a_j^L = \\sigma (z_j^L))\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP2</strong></p>\n<p>\\[\n\\begin{aligned}\n\\because z<em>k^{l+1} &amp;= \\sum_j w\\</em>{kj}^{l+1} a_j^l + b_k^{l+1} = \\sum_j w_{kj}^{l+1} \\cdot \\sigma(z_j^l) + b_k^{l+1} \\\\\n\\therefore \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} &amp; = w_{kj}^{l+1} \\cdot \\sigma’ (z_j^l)\n\\end{aligned}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta_j^l\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\\\\n&amp; = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}} \\cdot \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n&amp; = \\sum_k \\frac{\\partial z_k^{l+1}}{\\partial z_k^l} \\cdot \\delta_k^{l+1} \\\\\n&amp; = \\sum_k w_{kj}^{l+1} \\cdot \\sigma’ (z_j^l) \\cdot \\delta_k^{l+1} \\\\\n&amp; = \\sum_k w_{kj}^{l+1} \\cdot \\delta_k^{l+1} \\cdot \\sigma’ (z_j^l)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP3</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial b_j^l}\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial b_j^l} \\\\\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\quad (\\because z_j^l = \\sum_k w_{jk}^l \\cdot a_k^{l-1} + b_j^l \\therefore \\frac{\\partial z_j^l}{\\partial b_j^l} = 1) \\\\\n&amp; = \\delta_j^l\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP4</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial w_{jk}^l}\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l} \\\\\n&amp; = \\delta_j^l \\cdot a_k^{l-1} \\quad (\\because z_j^l = \\sum_k w_{jk}^l \\cdot a_k^{l-1} + b_j^l)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<hr>\n<p><br></p>\n<hr>\n<p><strong>以下摘抄自：</strong><a href=\"http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95\" target=\"_blank\" rel=\"noopener\">反向传导算法</a></p>\n<p>反向传播算法的思路如下：给定一个样例 \\( (x,y) \\)，首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \\( h_{W,b}(x) \\) 的输出值。\n之后，针对第 \\( l \\) 层的每一个节点 \\( i \\)，计算出其 “残差” \\( \\delta^{(l)}_i \\)，该残差表明了该节点对最终输出值的残差产生了多少影响。\n对于最终的输出节点，可以直接算出网络产生的激活值与实际值之间的差距，将这个差距定义为 \\( \\delta^{(n_l)}_i \\)（第 \\( n_l \\) 层表示输出层）。\n对于隐藏单元如何处理呢？基于节点（第 \\( l+1 \\) 层节点）残差的加权平均值计算 \\( \\delta^{(l)}_i \\)，这些节点以 \\( a^{(l)}_i \\) 作为输入。\n下面将给出反向传导算法的细节:</p>\n<ul>\n<li>(1) 进行前馈传导计算，利用前向传导公式，得到 \\( L_2, L_3, \\ldots \\) 直到输出层 \\( L_{n_l} \\) 的激活值。</li>\n<li>(2) 对于第 \\( n_l \\) 层（输出层）的每个输出单元 \\( i \\) ，根据以下公式计算残差：\n\\[ \n\\delta^{(n_l)}_i \n= \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} || y - h_{W,b}(x) || ^2\n= - (y_i - a^{(n_l)}_i) \\cdot f’(z^{(n_l)}_i)\n\\]</li>\n</ul>\n<p><strong>推导过程</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n_l)}_i\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} J(W,b; x,y) \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} ||y - h_{W,b}(x) ||^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}} (y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}} (y_j - f(z_j^{(n_l)}))^2 \\\\\n&amp; = - (y_i - f(z_i^{(n_l)})) \\cdot f’(z^{(n_l)}_i) \\\\\n&amp; = - (y_i - a^{(n_l)}_i) \\cdot f’(z^{(n_l)}_i)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<ul>\n<li>(3) 对 \\( l = n<em>l-1, n_l-2, n_l-3, \\ldots, 2 \\) 的各个层，第 \\( l \\) 层的第 \\( i \\) 个节点的残差计算方法如下：\n\\[\n\\delta^{(l)}_i = \\left( \\sum\\</em>{j=1}^{s_{l+1}} W^{(l)}_{ji} \\delta^{(l+1)}_j \\right) f’(z^{(l)}_i)\n\\]</li>\n</ul>\n<p><strong>推导过程</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n_l-1)}_i \n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} J(W,b; x,y) \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} \\frac{1}{2} ||y - h_{W,b}(x) ||^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}}(y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{1}{2} \\sum_{j=1}^{S_{(n_l)}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{1}{2} \\sum_{j=1}^{S_{(n_l)}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j - f(z_j^{(n_l)}))^2 \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \\cdot \\frac{\\partial}{\\partial z_i^{(n_l-1)}}f(z_j^{(n_l)}) \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \\cdot f’(z_j^{(n_l)}) \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{(n_l-1)}} \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{(n_l-1)}} \\\\\n&amp; = \\sum_{j=1}^{S_{n<em>l}} \\left(\\delta_j^{(n_l)} \\cdot \\frac{\\partial}{\\partial z_i^{(n_l-1)}}\\sum\\</em>{k=1}^{S_{n_l-1}}f(z_k^{(n_l-1)}) \\cdot W_{jk}^{n_l-1} \\right) \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot  W_{ji}^{n_l-1} \\cdot f’(z_i^{(n_l-1)}) \\\\\n&amp; = \\left( \\sum_{j=1}^{S_{n_l}} W_{ji}^{n_l-1} \\delta_j^{(n_l)} \\right) f’(z_i^{(n_l-1)}) \\\\\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>将上式中的 \\( n<em>l-1 \\) 与 \\( n_l \\) 的关系替换为 \\( l \\) 与 \\( l+1 \\) 的关系，就可以得到：\n\\[\n\\delta^{(l)}_i = \\left( \\sum\\</em>{j=1}^{s<em>{l+1}} W^{(l)}\\</em>{ji} \\delta^{(l+1)}_j \\right) f’(z^{(l)}_i)\n\\]</p>\n<p>以上逐次从后向前求导的过程即为“反向传导”的本意所在。</p>\n<ul>\n<li>(4) 计算需要的偏导数，计算方法如下：\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) &amp;= a^{(l)}_j \\delta_i^{(l+1)} \\\\\n\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) &amp;= \\delta_i^{(l+1)}.\n\\end{aligned}\n\\]</li>\n</ul>\n<p><br></p>\n<p>最后，用矩阵-向量表示法重写以上算法。我们使用 “ \\( \\bullet \\)” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。\n若 \\( a = b \\bullet c \\)，则 \\( a_i = b_i c_i \\)。扩展 \\( f(\\cdot) \\) 的定义，使其包含向量运算，于是有 \\( f’([z_1, z_2, z_3]) = [f’(z_1), f’(z_2), f’(z_3)] \\)。</p>\n<p>那么，反向传播算法可表示为以下几个步骤：</p>\n<ol>\n<li>进行前馈传导计算，利用前向传导公式，得到 \\( L<em>2, L_3, \\ldots \\) 直到输出层 \\( L</em>{n_l} \\) 的激活值。</li>\n<li>对输出层（第 \\( n_l \\) 层），计算:\\[\\begin{align} \\delta^{(n_l)} = - (y - a^{(n_l)}) \\bullet f’(z^{(n_l)}) \\end{align}\\]</li>\n<li>对于 \\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\) 的各层，计算：\\[\\begin{align} \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f’(z^{(l)}) \\end{align} \\]</li>\n<li>计算最终需要的偏导数值：\\[ \\begin{align} \\nabla<em>{W^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)} (a^{(l)})^T, \\\\ \\nabla</em>{b^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)}.\\end{align} \\]</li>\n</ol>\n<p><br></p>\n<p><strong>实现中应注意：</strong></p>\n<p>在以上的第2步和第3步中，我们需要为每一个 \\( i \\) 值计算其 \\( f’(z^{(l)}_i) \\) 。\n假设 \\( f(z) \\) 是sigmoid函数，并且我们已经在前向传导运算中得到了 \\( a^{(l)}_i \\) 。\n那么，使用我们早先推导出的 \\( f’(z) \\) 表达式，就可以计算得到 \\( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i) \\)。</p>\n<p>最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，\\( \\Delta W^{(l)} \\) 是一个与矩阵 \\( W^{(l)} \\) 维度相同的矩阵，\n\\( \\Delta b^{(l)} \\) 是一个与 \\( b^{(l)} \\) 维度相同的向量。\n注意这里 “\\( \\Delta W^{(l)} \\) ”是一个矩阵，而不是“\\( \\Delta \\) 与 \\( W^{(l)} \\) 相乘”。</p>\n<p>下面，我们实现批量梯度下降法中的一次迭代：</p>\n<ul>\n<li>(1) 对于所有 \\( l \\)，令 \\( \\Delta W^{(l)} := 0, \\Delta b^{(l)} := 0 \\) (设置为全零矩阵或全零向量)</li>\n<li>(2) 对于 \\( i = 1 \\) 到 \\( m \\)，</li>\n<li>(2.1) 使用反向传播算法计算 \\( \\nabla<em>{W^{(l)}} J(W,b;x,y) \\) 和 \\( \\nabla</em>{b^{(l)}} J(W,b;x,y) \\);</li>\n<li>(2.2) 计算 \\( \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y) \\);</li>\n<li>(2.3) 计算 \\( \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y) \\);</li>\n<li>(3) 更新权重参数：\n\\[\n\\begin{align}\nW^{(l)} &amp;= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\\nb^{(l)} &amp;= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\n\\end{align}\n\\]</li>\n</ul>\n<p>现在，可以重复梯度下降法的迭代步骤来减小代价函数 \\( J(W,b) \\) 的值，进而求解我们的神经网络。</p>\n<hr>\n<h3 id=\"2-转-随时间反向传播BPTT\"><a href=\"#2-转-随时间反向传播BPTT\" class=\"headerlink\" title=\"2. [转]随时间反向传播BPTT\"></a>2. [转]随时间反向传播BPTT</h3><p>转自：<a href=\"https://github.com/hschen0712/machine-learning-notes/blob/master/Deep-Learning/back-propagation-through-time.ipynb\" target=\"_blank\" rel=\"noopener\">hschen0712/machine-learning-notes</a></p>\n<p>RNN（递归神经网络，Recurrent Neural Network）是一种具有长时记忆能力的神经网络模型，被广泛应用于序列标注（Sequence Labeling）问题。\n在序列标注问题中，模型的输入是一段时间序列，记为$ x = \\lbrace x_1, x_2, …, x_T \\rbrace $，\n我们的目标是为输入序列的每个元素打上标签集合中的对应标签，记为$ y = \\lbrace y_1, y_2, …, y_T \\rbrace $。</p>\n<p>NLP中的大部分任务（比如分词、实体识别、词性标注）都可以最终归结为序列标注问题。\n这类问题中，输入是语料库中一段由 $T$ 个词（或字）构成的文本 $ x = \\lbrace x_1, x_2, …, x_T \\rbrace $（其中$x_t$表示文本中的第$t$个词）；\n输出是每个词对应的标签，根据任务的不同标签的形式也各不相同，但本质上都是针对每个词根据它的上下文进行标签的分类。</p>\n<p>一个典型的RNN的结构如下图所示：</p>\n<p><img src=\"/posts_res/2018-05-23-backpropagation/2-1.jpg\" alt=\"rnn\"></p>\n<p>从图中可以看到，一个RNN通常由三层组成，分别是输入层、隐藏层和输出层。\n与一般的神经网络不同的地方是，RNN的隐藏层存在一条有向反馈边，正是这种反馈机制赋予了RNN记忆能力。\n要理解左边的图可能有点难度，我们可以将其展开为右边这种更直观的形式，其中RNN的每个神经元接受当前时刻的输入$x<em>t$ 以及上一时刻隐单元的输出$h</em>{t-1}$，\n计算出当前神经元的输入 $s<em>t$，经过激活函数变换得到输出 $h_t$，并传递给下一时刻的隐单元。\n此外，我们还需要注意到RNN中每个时刻上的神经元的参数都是相同的（类似CNN的权值共享），这么做一方面是减小参数空间，保证泛化能力；\n另一方面是为了赋予RNN记忆能力，将有用信息存储在$W</em>{in},W<em>{rec},W</em>{out}$三个矩阵中。</p>\n<p>由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化，这要求我们提出新的优化算法。\nRNN中最常用的优化算法是随时间反向传播（BackPropagation Through Time，BPTT），下文将叙述BPTT算法的数学推导。</p>\n<h4 id=\"2-1-符号注解\"><a href=\"#2-1-符号注解\" class=\"headerlink\" title=\"2.1 符号注解\"></a>2.1 符号注解</h4><table>\n <tr>\n  <th>符号</th>\n  <th>注解</th>\n </tr>\n <tr>\n  <td>$K$</td>\n  <td>词汇表的大小</td>\n </tr>\n<tr>\n  <td>$T$</td>\n  <td>句子的长度</td>\n </tr>\n<tr>\n  <td>$H$</td>\n  <td>隐藏层单元数</td>\n </tr>\n<tr>\n  <td>$x=\\lbrace x_1, x_2, ..., x_T \\rbrace $</td>\n  <td>句子的单词序列</td>\n </tr>\n<tr>\n  <td>$x_t \\in R^{K \\times 1}$</td>\n  <td>第$t$时刻RNN的输入，one-hot vector</td>\n </tr>\n<tr>\n  <td>$\\hat{y}_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻softmax层的输出，估计每个词出现的概率</td>\n </tr>\n<tr>\n  <td>$y_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻的label，为每个词出现的概率，one-hot vector</td>\n </tr>\n<tr>\n  <td>$E_t$</td>\n  <td>第$t$时刻（第$t$个word）的损失函数，定义为交叉熵误差$E_t = - y_t^T log (\\hat{y}_t) $</td>\n </tr>\n<tr>\n  <td>$E$</td>\n  <td>一个句子的损失函数，由各个时刻（即每个word）的损失函数组成,$E=\\sum_t^T E_t$（注:由于要推导的是SGD算法，更新梯度是相对于一个训练样例而言的，因此一次只考虑一个句子的误差，而不是整个训练集的误差（对应BGD算法））</td>\n </tr>\n<tr>\n  <td>$s_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输入</td>\n </tr>\n<tr>\n  <td>$h_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输出</td>\n </tr>\n<tr>\n  <td>$z_t \\in R^{K \\times 1}$</td>\n  <td>输出层的汇集输入</td>\n </tr>\n<tr>\n  <td>$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$    </td>\n  <td>第$t$个时刻损失函数$E_t$对第$k$时刻带权输入$s_k$的导数</td>\n </tr>\n<tr>\n  <td>$r_t=\\hat{y}_t-y_t$    </td>\n  <td>残差向量</td>\n </tr>\n<tr>\n  <td>$W_{in}\\in\\mathbb{R}^{H\\times K}$    </td>\n  <td>从输入层到隐藏层的权值</td>\n </tr>\n<tr>\n  <td>$W_{rec}\\in\\mathbb{R}^{H\\times H}$    </td>\n  <td>隐藏层上一个时刻到当前时刻的权值</td>\n </tr>\n<tr>\n  <td>$W_{out}\\in\\mathbb{R}^{K\\times H}$    </td>\n  <td>隐藏层到输出层的权值</td>\n </tr>\n</table>\n\n<p>上述符号之间的关系:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\ns_t &= W_{rec} h_{t-1} + W_{in} x_t \\\\\nh_t &= tanh(s_t) \\\\\nz_t &= W_{out} h_t \\\\\n\\hat{y}_t &= softmax( z_t ) \\\\\nE_t &= - y_t^T log( \\hat{y}_t) \\\\\nE &= \\sum_t^T E_t\n\\end{cases}</script><p>这里有必要对上面的一些符号进行进一步解释。</p>\n<ol>\n<li>本文只讨论输入为one-hot vector的情况，这种向量的特点是茫茫0海中的一个1，即只用一个1表示某个单词的出现；其余的0均表示单词不出现。</li>\n<li>RNN要预测的输出是一个one-hot vector，表示下一个时刻各个单词出现的概率。</li>\n<li>由于$y<em>t$是one-hot vector，不妨假设$y</em>{t,j} = 1( y<em>{t,i} =0 ,i \\neq j)$，那么当前时刻的交叉熵为 $$ E_t = - y_t^T log(\\hat{y}_t) = -log(\\hat{y}</em>{t,j}) $$。也就是说如果 $t$ 出现的是第 $j$ 个词，那么计算交叉熵时候只要看 $\\hat{y}_t$ 的第$j$个分量即可。</li>\n<li>由于$x<em>t$是one-hot向量，假设第$j$个词出现，则$W</em>{in}x<em>t$相当于把$W</em>{in}$的第$j$列选出来，因此这一步是不用进行任何矩阵运算的，直接做下标操作即可。</li>\n</ol>\n<p>BPTT与BP类似，是在时间上反传的梯度下降算法。\nRNN中，我们的目的是求得 $\\frac{\\partial E}{\\partial W<em>{in}}, \\frac{\\partial E}{\\partial W</em>{rec}}, \\frac{\\partial E}{\\partial W<em>{out}}$，\n根据这三个变化率来优化三个参数 $W</em>{in},W<em>{rec},W</em>{out}$。注意到 $\\frac{\\partial E}{\\partial W<em>{in}} = \\sum_t \\frac{\\partial E_t}{\\partial W</em>{in}}$，\n因此我们只要对每个时刻的损失函数求偏导数再加起来即可。矩阵求导有两种布局方式：分母布局（Denominator Layout）和分子布局（Numerator Layout），关于分子布局和分母布局的区别，请参考文献3。\n如果这里采用分子布局，那么更新梯度时还需要将梯度矩阵进行一次转置，因此出于数学上的方便，后续的矩阵求导都将采用分母布局。</p>\n<p><br></p>\n<h4 id=\"2-2-计算-frac-partial-Et-partial-W-out\"><a href=\"#2-2-计算-frac-partial-Et-partial-W-out\" class=\"headerlink\" title=\"2.2 计算 $\\frac{\\partial Et}{\\partial W{out}}$\"></a>2.2 计算 $\\frac{\\partial E<em>t}{\\partial W</em>{out}}$</h4><p>注意到$E<em>t$是$W</em>{out}$的复合函数，参考文献3中Scalar-by-matrix identities一节中关于复合矩阵函数求导法则（右边的是分母布局）： </p>\n<p><img src=\"/posts_res/2018-05-23-backpropagation/2-2.jpg\" alt=\"matrix\"></p>\n<p>我们有：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial E_t}{\\partial W_{out}(i,j)}\n&= tr \\left( \\left( \\frac{\\partial E_t}{\\partial z_t} \\right)^T \\cdot \\frac{\\partial z_t}{\\partial W_{out}(i,j)} \\right) \\\\\n&= tr \\left( (\\hat{y}_t - y_t)^T \\cdot \\begin{bmatrix} 0\\\\ \\vdots \\\\ \\frac{\\partial z_{t}^{(i)}}{\\partial W_{out}(i,j)} \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right) \\\\\n&= r_t^{(i)} h_t^{(j)}\n\\end{aligned}</script><p>其中$r_t^{(i)}=(\\hat{y}_t-y_t)^{(i)}$表示残差向量第$i$个分量，$h_t^{(j)}$表示$h_t$的第j个分量。</p>\n<p>上述结果可以改写为：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{out}} = r_t \\otimes h_t</script><script type=\"math/tex; mode=display\">\\frac{\\partial E}{\\partial W_{out}} = \\sum_{t=0}^T r_t\\otimes h_t</script><p>其中$\\otimes$表示向量外积。</p>\n<p><br></p>\n<h4 id=\"2-3-计算-frac-partial-Et-partial-W-rec\"><a href=\"#2-3-计算-frac-partial-Et-partial-W-rec\" class=\"headerlink\" title=\"2.3 计算$\\frac{\\partial Et}{\\partial W{rec}}$\"></a>2.3 计算$\\frac{\\partial E<em>t}{\\partial W</em>{rec}}$</h4><p>由于$W<em>{rec}$是各个时刻共享的，所以$t$时刻之前的每个时刻$W</em>{rec}$的变化都对$E<em>t$有贡献，反过来求偏导时，也要考虑之前每个时刻$W</em>{rec}$对$E$的影响。\n我们以$s_k$为中间变量，应用链式法则：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\frac{\\partial s_k}{\\partial W_{rec}} \\frac{\\partial E_t}{\\partial s_k}</script><p>但由于$\\frac{\\partial s<em>k}{\\partial W</em>{rec}}$（分子向量，分母矩阵）以目前的数学发展水平是没办法求的，\n因此我们要求这个偏导，可以拆解为$E<em>t$对$W</em>{rec}(i,j)$的偏导数：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} \n= \\sum_{k=0}^t tr[(\\frac{\\partial E_t}{\\partial s_k})^T \\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n= \\sum_{k=0}^t  tr[(\\delta_k^{(t)})^T\\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]</script><p>其中，$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$，遵循</p>\n<script type=\"math/tex; mode=display\">s_k\\to h_k\\to s_{k+1}\\to ...\\to E_t</script><p>的传递关系。</p>\n<p>应用链式法则有： </p>\n<script type=\"math/tex; mode=display\">\n\\delta^{(t)}_k = \\frac{\\partial h_k}{\\partial s_k}\\frac{\\partial s_{k+1}}{\\partial h_k} \\frac{\\partial E_t}{\\partial s_{k+1}}\n= diag(1-h_k\\odot h_k)W_{rec}^T\\delta^{(t)}_{k+1}=(W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)</script><p>其中，$\\odot$表示向量点乘（element-wise product）。注意$E_t$求导时链式法则的顺序，$E_t$是关于$s_k$的符合函数，且求导链上的每个变量都是向量，\n根据参考文献3，这种情况下应用分母布局的链式法则，方向应该是相反的。</p>\n<p>接下来计算$\\delta^{(t)}_t$：</p>\n<script type=\"math/tex; mode=display\">\n\\delta^{(t)}_t = \\frac{\\partial E_t}{\\partial s_t} = \\frac{\\partial h_t}{\\partial s_t}\\frac{\\partial z_t}{\\partial h_t}\\frac{\\partial E_t}{\\partial z_t}\n= diag(1-h_t\\odot h_t)\\cdot W_{out}^T\\cdot(\\hat{y}_t-y_t)=(W_{out}^T(\\hat{y}_t-y_t))\\odot (1-h_t\\odot h_t)</script><p>于是，我们得到了关于$\\delta$ 的递推关系式:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\delta^{(t)}_t &= (W_{out}^T r_t)\\odot (1-h_t\\odot h_t) \\\\ \n\\delta^{(t)}_k &= (W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n\\end{cases}</script><p>由 $ \\delta^{(t)}<em>t $ 出发，\n我们可以往前推出每一个 $ \\delta $ ，\n将 <script type=\"math/tex\">\\delta^{(t)}_0,...,\\delta^{(t)}_t</script> \n代入 $$\\frac{\\partial E_t}{\\partial W</em>{rec}(i,j)}$$\n有：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} = \\sum_{k=0}^t \\delta_k^{(t)} h_{k-1}^{(j)}</script><p>将上式写成矩阵形式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1} \\\\\n\\frac{\\partial E}{\\partial W_{rec}} =\\sum_{t=0}^T \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1}</script><p>不失严谨性，定义$h_{-1}$为全0的向量。</p>\n<p><br></p>\n<h4 id=\"2-4-计算-frac-partial-Et-partial-W-in\"><a href=\"#2-4-计算-frac-partial-Et-partial-W-in\" class=\"headerlink\" title=\"2.4 计算$\\frac{\\partial Et}{\\partial W{in}}$\"></a>2.4 计算$\\frac{\\partial E<em>t}{\\partial W</em>{in}}$</h4><p>按照上述思路，我们可以得到 </p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{in}} = \\sum_{k=0}^t \\delta_k \\otimes x_{k}</script><p>由于$x_k$是个one-hot vector，假设$x_k(m)=1$，那么我们在更新$W$时只需要更新$W$的第$m$列即可。</p>\n<p><br></p>\n<h4 id=\"2-5-参数更新\"><a href=\"#2-5-参数更新\" class=\"headerlink\" title=\"2.5 参数更新\"></a>2.5 参数更新</h4><p>我们有了$E_t$关于各个参数的偏导数，就可以用梯度下降来更新各参数了:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nW_{in} &= W_{in} - \\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes x_{k} \\\\\nW_{rec}&=W_{rec}-\\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes h_{k-1} \\\\\nW_{out}&=W_{out}-\\lambda \\sum_{t=0}^T r_t \\otimes h_t\n\\end{cases}</script><p>其中 <script type=\"math/tex\">r_t= \\hat{y}_t - y_t</script>，</p>\n<p><script type=\"math/tex\">\\delta_t = \\frac{\\partial E_t}{\\partial s_t} = (W_{out}^T r_t) \\odot (1 - h_t\\odot h_t), \\lambda > 0</script> 表示学习率。</p>\n<p><br></p>\n<h4 id=\"2-6-部分思考\"><a href=\"#2-6-部分思考\" class=\"headerlink\" title=\"2.6 部分思考\"></a>2.6 部分思考</h4><ul>\n<li>为什么RNN中要对隐藏层的输出进行一次运算$z<em>t=W</em>{out}h<em>t$，然后再对$z_t$进行一次softmax，而不是直接对$h_t$进行softmax求得概率？为什么要有$W</em>{out}$这个参数？<ul>\n<li>答：$x<em>t$是一个$K\\times 1$的向量，我们要将它映射到一个$H\\times 1$的$h_t$（其中$H$是隐神经元的个数），从$x_t$到$h_t$相当于对词向量做了一次编码；最终我们要得到的是一个$K\\times 1$的向量（这里$K$是词汇表大小），表示每个词接下来出现的概率，所以我们需要一个矩阵$K\\times H$的$W</em>{out}$来将$h_t$映射回原来的空间去，这个过程相当于解码。因此，RNN可以理解为一种编解码网络。</li>\n</ul>\n</li>\n<li>$W<em>{in},W</em>{rec},W_{out}$三个参数分别有什么意义？<ul>\n<li>答： $W<em>{in}$将$K\\times 1$的one-hot词向量映射到$H\\times 1$隐藏层空间，将输入转化为计算机内部可理解可处理的形式，这个过程可以理解为一次编码过程；$W</em>{rec}$则是隐含层到自身的一个映射，它定义了模型如何结合上文信息，在编码中融入了之前的“记忆”；$W<em>{in},W</em>{rec}$结合了当前输入单词和之前的记忆，形成了当前时刻的知识状态。$W<em>{out}$是隐含层到输出的映射，$z=W</em>{out}h$是映射后的分数，这个过程相当于一次解码。这个解码后的分数再经过一层softmax转化为概率输出来，我们挑选概率最高的那个作为我们的预测。作为总结， RNN的记忆由两部分构成，一部分是当前的输入，另一部分是之前的记忆。</li>\n</ul>\n</li>\n<li>BPTT和BP的区别在哪？为什么不能用BP算法训练RNN？<ul>\n<li>答：BP算法只考虑了误差的导数在上下层级之间梯度的反向传播；而BPTT则同时包含了梯度在纵向层级间的反向传播和在时间维度上的反向传播，同时在两个方向上进行参数优化。</li>\n</ul>\n</li>\n<li>文中词$x_t$的特征是一个one-hot vector，这里能不能替换为word2vec训练出的词向量？效果和性能如何？<ul>\n<li>答：RNNLM本身自带了训练词向量的过程。由于$x<em>t$是one-hot向量，假设出现的词的索引为$j$，那么$W</em>{in}x<em>t$就是把$W</em>{in}$的第$j$列$W[:,j]$取出，这个列向量就相当于该词的词向量。实际上用语言模型训练词向量的思想最早可以追溯到03年Bengio的一篇论文《A neural probabilistic language model 》，这篇论文中作者使用一个神经网络模型来训练n-gram模型，顺便学到了词向量。本文出于数学推导以及代码实现上的方便采用了one-hot向量作为输入。实际工程中，词汇表通常都是几百万，内存没办法装下几百万维的稠密矩阵，所以工程上基本上没有用one-hot的，基本都是用词向量。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"http://blog.csdn.net/malefactor/article/details/50725480\" target=\"_blank\" rel=\"noopener\">使用RNN解决NLP中序列标注问题的通用优化思路</a></li>\n<li><a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" target=\"_blank\" rel=\"noopener\">wildml的rnn tutorial part3</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\" target=\"_blank\" rel=\"noopener\">Matrix Calculus Wiki</a></li>\n<li><a href>《神经网络与深度学习讲义》 邱锡鹏</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>反向传播BP</li>\n<li>随时间反向传播BPTT</li>\n</ul><hr><h3 id=\"1-反向传播BP\"><a href=\"#1-反向传播BP\" class=\"headerlink\" title=\"1. 反向传播BP\"></a>1. 反向传播BP</h3><p><a href=\"http://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\" rel=\"noopener\">[Calculus on Computational Graphs: Backpropagation，英文原版]</a>、\n<a href=\"https://zhuanlan.zhihu.com/p/25081671\" target=\"_blank\" rel=\"noopener\">[详解反向传播算法，中文翻译理解]</a></p><p>解释了为什么从上而下计算梯度。一言以蔽之：从下而上会有重复计算，当参数数量太多，时间耗费太严重；从上而下每个节点只计算一次。</p><p><a href=\"https://www.zhihu.com/question/27239198/answer/89853077\" target=\"_blank\" rel=\"noopener\">如何直观地解释 back propagation 算法？</a>中的例子比较清晰的刻画了反向传播的优势。</p><p><a href=\"https://iamtrask.github.io/2015/07/12/basic-python-network/\" target=\"_blank\" rel=\"noopener\">A Neural Network in 11 lines of Python</a>每行代码都有解释。</p><p><a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\" target=\"_blank\" rel=\"noopener\">CHAPTER 2How the backpropagation algorithm works</a>讲的特别详细，\n中文版在<a href=\"/posts_res/2018-05-23-backpropagation/neural network and deep learning.pdf\">这里 - 神经⽹络与深度学习</a>。</p>","more":"\n\n\n\n\n\n\n\n<hr>\n<p><br></p>\n<hr>\n<p><strong>以下部分参考Neural Networks and Deep Learning(神经⽹络与深度学习P37 - P42)</strong></p>\n<p>反向传播的四个方程式：</p>\n<p>\\[\n\\delta^L = \\nabla _a C \\odot \\sigma’ (z^L) \\tag{BP1}\n\\]</p>\n<p>\\[\n\\delta^l = ( (w^{l+1})^T \\delta^{l+1} ) \\odot \\sigma’ (z^l) \\tag{BP2}\n\\]</p>\n<p>\\[\n\\frac{\\partial C}{\\partial b_j^l} = \\delta_j^l  \\tag{BP3}\n\\]</p>\n<p>\\[\n\\frac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l  \\tag{BP4}\n\\]</p>\n<p>证明上面的四个方程式：</p>\n<p><strong>证明BP1</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta_j^L \n&amp; = \\frac{\\partial C}{\\partial z_j^L} \\\\\n&amp; = \\frac{\\partial C}{\\partial a_j^L} \\cdot \\frac{\\partial a_j^L}{\\partial z_j^L} \\\\\n&amp; = \\frac{\\partial C}{\\partial a_j^L} \\cdot \\sigma’ (z_j^L) \\quad (\\because a_j^L = \\sigma (z_j^L))\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP2</strong></p>\n<p>\\[\n\\begin{aligned}\n\\because z<em>k^{l+1} &amp;= \\sum_j w\\</em>{kj}^{l+1} a_j^l + b_k^{l+1} = \\sum_j w_{kj}^{l+1} \\cdot \\sigma(z_j^l) + b_k^{l+1} \\\\\n\\therefore \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} &amp; = w_{kj}^{l+1} \\cdot \\sigma’ (z_j^l)\n\\end{aligned}\n\\]</p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta_j^l\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\\\\n&amp; = \\sum_k \\frac{\\partial C}{\\partial z_k^{l+1}} \\cdot \\frac{\\partial z_k^{l+1}}{\\partial z_j^l} \\\\\n&amp; = \\sum_k \\frac{\\partial z_k^{l+1}}{\\partial z_k^l} \\cdot \\delta_k^{l+1} \\\\\n&amp; = \\sum_k w_{kj}^{l+1} \\cdot \\sigma’ (z_j^l) \\cdot \\delta_k^{l+1} \\\\\n&amp; = \\sum_k w_{kj}^{l+1} \\cdot \\delta_k^{l+1} \\cdot \\sigma’ (z_j^l)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP3</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial b_j^l}\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial b_j^l} \\\\\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\quad (\\because z_j^l = \\sum_k w_{jk}^l \\cdot a_k^{l-1} + b_j^l \\therefore \\frac{\\partial z_j^l}{\\partial b_j^l} = 1) \\\\\n&amp; = \\delta_j^l\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p><strong>证明BP4</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial C}{\\partial w_{jk}^l}\n&amp; = \\frac{\\partial C}{\\partial z_j^l} \\cdot \\frac{\\partial z_j^l}{\\partial w_{jk}^l} \\\\\n&amp; = \\delta_j^l \\cdot a_k^{l-1} \\quad (\\because z_j^l = \\sum_k w_{jk}^l \\cdot a_k^{l-1} + b_j^l)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<hr>\n<p><br></p>\n<hr>\n<p><strong>以下摘抄自：</strong><a href=\"http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95\" target=\"_blank\" rel=\"noopener\">反向传导算法</a></p>\n<p>反向传播算法的思路如下：给定一个样例 \\( (x,y) \\)，首先进行“前向传导”运算，计算出网络中所有的激活值，包括 \\( h_{W,b}(x) \\) 的输出值。\n之后，针对第 \\( l \\) 层的每一个节点 \\( i \\)，计算出其 “残差” \\( \\delta^{(l)}_i \\)，该残差表明了该节点对最终输出值的残差产生了多少影响。\n对于最终的输出节点，可以直接算出网络产生的激活值与实际值之间的差距，将这个差距定义为 \\( \\delta^{(n_l)}_i \\)（第 \\( n_l \\) 层表示输出层）。\n对于隐藏单元如何处理呢？基于节点（第 \\( l+1 \\) 层节点）残差的加权平均值计算 \\( \\delta^{(l)}_i \\)，这些节点以 \\( a^{(l)}_i \\) 作为输入。\n下面将给出反向传导算法的细节:</p>\n<ul>\n<li>(1) 进行前馈传导计算，利用前向传导公式，得到 \\( L_2, L_3, \\ldots \\) 直到输出层 \\( L_{n_l} \\) 的激活值。</li>\n<li>(2) 对于第 \\( n_l \\) 层（输出层）的每个输出单元 \\( i \\) ，根据以下公式计算残差：\n\\[ \n\\delta^{(n_l)}_i \n= \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} || y - h_{W,b}(x) || ^2\n= - (y_i - a^{(n_l)}_i) \\cdot f’(z^{(n_l)}_i)\n\\]</li>\n</ul>\n<p><strong>推导过程</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n_l)}_i\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} J(W,b; x,y) \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} ||y - h_{W,b}(x) ||^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}} (y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}} (y_j - f(z_j^{(n_l)}))^2 \\\\\n&amp; = - (y_i - f(z_i^{(n_l)})) \\cdot f’(z^{(n_l)}_i) \\\\\n&amp; = - (y_i - a^{(n_l)}_i) \\cdot f’(z^{(n_l)}_i)\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<ul>\n<li>(3) 对 \\( l = n<em>l-1, n_l-2, n_l-3, \\ldots, 2 \\) 的各个层，第 \\( l \\) 层的第 \\( i \\) 个节点的残差计算方法如下：\n\\[\n\\delta^{(l)}_i = \\left( \\sum\\</em>{j=1}^{s_{l+1}} W^{(l)}_{ji} \\delta^{(l+1)}_j \\right) f’(z^{(l)}_i)\n\\]</li>\n</ul>\n<p><strong>推导过程</strong></p>\n<p>\\[\n\\begin{equation}\n\\begin{aligned}\n\\delta^{(n_l-1)}_i \n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} J(W,b; x,y) \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} \\frac{1}{2} ||y - h_{W,b}(x) ||^2 \\\\\n&amp; = \\frac{\\partial}{\\partial z^{(n_l-1)}_i} \\frac{1}{2} \\sum_{j=1}^{S_{n_l}}(y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{1}{2} \\sum_{j=1}^{S_{(n_l)}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j - a_j^{(n_l)})^2 \\\\\n&amp; = \\frac{1}{2} \\sum_{j=1}^{S_{(n_l)}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j - f(z_j^{(n_l)}))^2 \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \\cdot \\frac{\\partial}{\\partial z_i^{(n_l-1)}}f(z_j^{(n_l)}) \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} - (y_j - f(z_j^{(n_l)})) \\cdot f’(z_j^{(n_l)}) \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{(n_l-1)}} \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{(n_l-1)}} \\\\\n&amp; = \\sum_{j=1}^{S_{n<em>l}} \\left(\\delta_j^{(n_l)} \\cdot \\frac{\\partial}{\\partial z_i^{(n_l-1)}}\\sum\\</em>{k=1}^{S_{n_l-1}}f(z_k^{(n_l-1)}) \\cdot W_{jk}^{n_l-1} \\right) \\\\\n&amp; = \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot  W_{ji}^{n_l-1} \\cdot f’(z_i^{(n_l-1)}) \\\\\n&amp; = \\left( \\sum_{j=1}^{S_{n_l}} W_{ji}^{n_l-1} \\delta_j^{(n_l)} \\right) f’(z_i^{(n_l-1)}) \\\\\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<p>将上式中的 \\( n<em>l-1 \\) 与 \\( n_l \\) 的关系替换为 \\( l \\) 与 \\( l+1 \\) 的关系，就可以得到：\n\\[\n\\delta^{(l)}_i = \\left( \\sum\\</em>{j=1}^{s<em>{l+1}} W^{(l)}\\</em>{ji} \\delta^{(l+1)}_j \\right) f’(z^{(l)}_i)\n\\]</p>\n<p>以上逐次从后向前求导的过程即为“反向传导”的本意所在。</p>\n<ul>\n<li>(4) 计算需要的偏导数，计算方法如下：\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) &amp;= a^{(l)}_j \\delta_i^{(l+1)} \\\\\n\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) &amp;= \\delta_i^{(l+1)}.\n\\end{aligned}\n\\]</li>\n</ul>\n<p><br></p>\n<p>最后，用矩阵-向量表示法重写以上算法。我们使用 “ \\( \\bullet \\)” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。\n若 \\( a = b \\bullet c \\)，则 \\( a_i = b_i c_i \\)。扩展 \\( f(\\cdot) \\) 的定义，使其包含向量运算，于是有 \\( f’([z_1, z_2, z_3]) = [f’(z_1), f’(z_2), f’(z_3)] \\)。</p>\n<p>那么，反向传播算法可表示为以下几个步骤：</p>\n<ol>\n<li>进行前馈传导计算，利用前向传导公式，得到 \\( L<em>2, L_3, \\ldots \\) 直到输出层 \\( L</em>{n_l} \\) 的激活值。</li>\n<li>对输出层（第 \\( n_l \\) 层），计算:\\[\\begin{align} \\delta^{(n_l)} = - (y - a^{(n_l)}) \\bullet f’(z^{(n_l)}) \\end{align}\\]</li>\n<li>对于 \\( l = n_l-1, n_l-2, n_l-3, \\ldots, 2 \\) 的各层，计算：\\[\\begin{align} \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f’(z^{(l)}) \\end{align} \\]</li>\n<li>计算最终需要的偏导数值：\\[ \\begin{align} \\nabla<em>{W^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)} (a^{(l)})^T, \\\\ \\nabla</em>{b^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)}.\\end{align} \\]</li>\n</ol>\n<p><br></p>\n<p><strong>实现中应注意：</strong></p>\n<p>在以上的第2步和第3步中，我们需要为每一个 \\( i \\) 值计算其 \\( f’(z^{(l)}_i) \\) 。\n假设 \\( f(z) \\) 是sigmoid函数，并且我们已经在前向传导运算中得到了 \\( a^{(l)}_i \\) 。\n那么，使用我们早先推导出的 \\( f’(z) \\) 表达式，就可以计算得到 \\( f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i) \\)。</p>\n<p>最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，\\( \\Delta W^{(l)} \\) 是一个与矩阵 \\( W^{(l)} \\) 维度相同的矩阵，\n\\( \\Delta b^{(l)} \\) 是一个与 \\( b^{(l)} \\) 维度相同的向量。\n注意这里 “\\( \\Delta W^{(l)} \\) ”是一个矩阵，而不是“\\( \\Delta \\) 与 \\( W^{(l)} \\) 相乘”。</p>\n<p>下面，我们实现批量梯度下降法中的一次迭代：</p>\n<ul>\n<li>(1) 对于所有 \\( l \\)，令 \\( \\Delta W^{(l)} := 0, \\Delta b^{(l)} := 0 \\) (设置为全零矩阵或全零向量)</li>\n<li>(2) 对于 \\( i = 1 \\) 到 \\( m \\)，</li>\n<li>(2.1) 使用反向传播算法计算 \\( \\nabla<em>{W^{(l)}} J(W,b;x,y) \\) 和 \\( \\nabla</em>{b^{(l)}} J(W,b;x,y) \\);</li>\n<li>(2.2) 计算 \\( \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y) \\);</li>\n<li>(2.3) 计算 \\( \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y) \\);</li>\n<li>(3) 更新权重参数：\n\\[\n\\begin{align}\nW^{(l)} &amp;= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\\nb^{(l)} &amp;= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\n\\end{align}\n\\]</li>\n</ul>\n<p>现在，可以重复梯度下降法的迭代步骤来减小代价函数 \\( J(W,b) \\) 的值，进而求解我们的神经网络。</p>\n<hr>\n<h3 id=\"2-转-随时间反向传播BPTT\"><a href=\"#2-转-随时间反向传播BPTT\" class=\"headerlink\" title=\"2. [转]随时间反向传播BPTT\"></a>2. [转]随时间反向传播BPTT</h3><p>转自：<a href=\"https://github.com/hschen0712/machine-learning-notes/blob/master/Deep-Learning/back-propagation-through-time.ipynb\" target=\"_blank\" rel=\"noopener\">hschen0712/machine-learning-notes</a></p>\n<p>RNN（递归神经网络，Recurrent Neural Network）是一种具有长时记忆能力的神经网络模型，被广泛应用于序列标注（Sequence Labeling）问题。\n在序列标注问题中，模型的输入是一段时间序列，记为$ x = \\lbrace x_1, x_2, …, x_T \\rbrace $，\n我们的目标是为输入序列的每个元素打上标签集合中的对应标签，记为$ y = \\lbrace y_1, y_2, …, y_T \\rbrace $。</p>\n<p>NLP中的大部分任务（比如分词、实体识别、词性标注）都可以最终归结为序列标注问题。\n这类问题中，输入是语料库中一段由 $T$ 个词（或字）构成的文本 $ x = \\lbrace x_1, x_2, …, x_T \\rbrace $（其中$x_t$表示文本中的第$t$个词）；\n输出是每个词对应的标签，根据任务的不同标签的形式也各不相同，但本质上都是针对每个词根据它的上下文进行标签的分类。</p>\n<p>一个典型的RNN的结构如下图所示：</p>\n<p><img src=\"/posts_res/2018-05-23-backpropagation/2-1.jpg\" alt=\"rnn\"></p>\n<p>从图中可以看到，一个RNN通常由三层组成，分别是输入层、隐藏层和输出层。\n与一般的神经网络不同的地方是，RNN的隐藏层存在一条有向反馈边，正是这种反馈机制赋予了RNN记忆能力。\n要理解左边的图可能有点难度，我们可以将其展开为右边这种更直观的形式，其中RNN的每个神经元接受当前时刻的输入$x<em>t$ 以及上一时刻隐单元的输出$h</em>{t-1}$，\n计算出当前神经元的输入 $s<em>t$，经过激活函数变换得到输出 $h_t$，并传递给下一时刻的隐单元。\n此外，我们还需要注意到RNN中每个时刻上的神经元的参数都是相同的（类似CNN的权值共享），这么做一方面是减小参数空间，保证泛化能力；\n另一方面是为了赋予RNN记忆能力，将有用信息存储在$W</em>{in},W<em>{rec},W</em>{out}$三个矩阵中。</p>\n<p>由于RNN是一种基于时序数据的神经网络模型，因此传统的BP算法并不适用于该模型的优化，这要求我们提出新的优化算法。\nRNN中最常用的优化算法是随时间反向传播（BackPropagation Through Time，BPTT），下文将叙述BPTT算法的数学推导。</p>\n<h4 id=\"2-1-符号注解\"><a href=\"#2-1-符号注解\" class=\"headerlink\" title=\"2.1 符号注解\"></a>2.1 符号注解</h4><table>\n <tr>\n  <th>符号</th>\n  <th>注解</th>\n </tr>\n <tr>\n  <td>$K$</td>\n  <td>词汇表的大小</td>\n </tr>\n<tr>\n  <td>$T$</td>\n  <td>句子的长度</td>\n </tr>\n<tr>\n  <td>$H$</td>\n  <td>隐藏层单元数</td>\n </tr>\n<tr>\n  <td>$x=\\lbrace x_1, x_2, ..., x_T \\rbrace $</td>\n  <td>句子的单词序列</td>\n </tr>\n<tr>\n  <td>$x_t \\in R^{K \\times 1}$</td>\n  <td>第$t$时刻RNN的输入，one-hot vector</td>\n </tr>\n<tr>\n  <td>$\\hat{y}_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻softmax层的输出，估计每个词出现的概率</td>\n </tr>\n<tr>\n  <td>$y_t \\in R^{K \\times 1} $</td>\n  <td>第$t$时刻的label，为每个词出现的概率，one-hot vector</td>\n </tr>\n<tr>\n  <td>$E_t$</td>\n  <td>第$t$时刻（第$t$个word）的损失函数，定义为交叉熵误差$E_t = - y_t^T log (\\hat{y}_t) $</td>\n </tr>\n<tr>\n  <td>$E$</td>\n  <td>一个句子的损失函数，由各个时刻（即每个word）的损失函数组成,$E=\\sum_t^T E_t$（注:由于要推导的是SGD算法，更新梯度是相对于一个训练样例而言的，因此一次只考虑一个句子的误差，而不是整个训练集的误差（对应BGD算法））</td>\n </tr>\n<tr>\n  <td>$s_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输入</td>\n </tr>\n<tr>\n  <td>$h_t \\in R^{H \\times 1}$</td>\n  <td>第$t$个时刻RNN隐藏层的输出</td>\n </tr>\n<tr>\n  <td>$z_t \\in R^{K \\times 1}$</td>\n  <td>输出层的汇集输入</td>\n </tr>\n<tr>\n  <td>$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$    </td>\n  <td>第$t$个时刻损失函数$E_t$对第$k$时刻带权输入$s_k$的导数</td>\n </tr>\n<tr>\n  <td>$r_t=\\hat{y}_t-y_t$    </td>\n  <td>残差向量</td>\n </tr>\n<tr>\n  <td>$W_{in}\\in\\mathbb{R}^{H\\times K}$    </td>\n  <td>从输入层到隐藏层的权值</td>\n </tr>\n<tr>\n  <td>$W_{rec}\\in\\mathbb{R}^{H\\times H}$    </td>\n  <td>隐藏层上一个时刻到当前时刻的权值</td>\n </tr>\n<tr>\n  <td>$W_{out}\\in\\mathbb{R}^{K\\times H}$    </td>\n  <td>隐藏层到输出层的权值</td>\n </tr>\n</table>\n\n<p>上述符号之间的关系:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\ns_t &= W_{rec} h_{t-1} + W_{in} x_t \\\\\nh_t &= tanh(s_t) \\\\\nz_t &= W_{out} h_t \\\\\n\\hat{y}_t &= softmax( z_t ) \\\\\nE_t &= - y_t^T log( \\hat{y}_t) \\\\\nE &= \\sum_t^T E_t\n\\end{cases}</script><p>这里有必要对上面的一些符号进行进一步解释。</p>\n<ol>\n<li>本文只讨论输入为one-hot vector的情况，这种向量的特点是茫茫0海中的一个1，即只用一个1表示某个单词的出现；其余的0均表示单词不出现。</li>\n<li>RNN要预测的输出是一个one-hot vector，表示下一个时刻各个单词出现的概率。</li>\n<li>由于$y<em>t$是one-hot vector，不妨假设$y</em>{t,j} = 1( y<em>{t,i} =0 ,i \\neq j)$，那么当前时刻的交叉熵为 $$ E_t = - y_t^T log(\\hat{y}_t) = -log(\\hat{y}</em>{t,j}) $$。也就是说如果 $t$ 出现的是第 $j$ 个词，那么计算交叉熵时候只要看 $\\hat{y}_t$ 的第$j$个分量即可。</li>\n<li>由于$x<em>t$是one-hot向量，假设第$j$个词出现，则$W</em>{in}x<em>t$相当于把$W</em>{in}$的第$j$列选出来，因此这一步是不用进行任何矩阵运算的，直接做下标操作即可。</li>\n</ol>\n<p>BPTT与BP类似，是在时间上反传的梯度下降算法。\nRNN中，我们的目的是求得 $\\frac{\\partial E}{\\partial W<em>{in}}, \\frac{\\partial E}{\\partial W</em>{rec}}, \\frac{\\partial E}{\\partial W<em>{out}}$，\n根据这三个变化率来优化三个参数 $W</em>{in},W<em>{rec},W</em>{out}$。注意到 $\\frac{\\partial E}{\\partial W<em>{in}} = \\sum_t \\frac{\\partial E_t}{\\partial W</em>{in}}$，\n因此我们只要对每个时刻的损失函数求偏导数再加起来即可。矩阵求导有两种布局方式：分母布局（Denominator Layout）和分子布局（Numerator Layout），关于分子布局和分母布局的区别，请参考文献3。\n如果这里采用分子布局，那么更新梯度时还需要将梯度矩阵进行一次转置，因此出于数学上的方便，后续的矩阵求导都将采用分母布局。</p>\n<p><br></p>\n<h4 id=\"2-2-计算-frac-partial-Et-partial-W-out\"><a href=\"#2-2-计算-frac-partial-Et-partial-W-out\" class=\"headerlink\" title=\"2.2 计算 $\\frac{\\partial Et}{\\partial W{out}}$\"></a>2.2 计算 $\\frac{\\partial E<em>t}{\\partial W</em>{out}}$</h4><p>注意到$E<em>t$是$W</em>{out}$的复合函数，参考文献3中Scalar-by-matrix identities一节中关于复合矩阵函数求导法则（右边的是分母布局）： </p>\n<p><img src=\"/posts_res/2018-05-23-backpropagation/2-2.jpg\" alt=\"matrix\"></p>\n<p>我们有：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\frac{\\partial E_t}{\\partial W_{out}(i,j)}\n&= tr \\left( \\left( \\frac{\\partial E_t}{\\partial z_t} \\right)^T \\cdot \\frac{\\partial z_t}{\\partial W_{out}(i,j)} \\right) \\\\\n&= tr \\left( (\\hat{y}_t - y_t)^T \\cdot \\begin{bmatrix} 0\\\\ \\vdots \\\\ \\frac{\\partial z_{t}^{(i)}}{\\partial W_{out}(i,j)} \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\right) \\\\\n&= r_t^{(i)} h_t^{(j)}\n\\end{aligned}</script><p>其中$r_t^{(i)}=(\\hat{y}_t-y_t)^{(i)}$表示残差向量第$i$个分量，$h_t^{(j)}$表示$h_t$的第j个分量。</p>\n<p>上述结果可以改写为：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{out}} = r_t \\otimes h_t</script><script type=\"math/tex; mode=display\">\\frac{\\partial E}{\\partial W_{out}} = \\sum_{t=0}^T r_t\\otimes h_t</script><p>其中$\\otimes$表示向量外积。</p>\n<p><br></p>\n<h4 id=\"2-3-计算-frac-partial-Et-partial-W-rec\"><a href=\"#2-3-计算-frac-partial-Et-partial-W-rec\" class=\"headerlink\" title=\"2.3 计算$\\frac{\\partial Et}{\\partial W{rec}}$\"></a>2.3 计算$\\frac{\\partial E<em>t}{\\partial W</em>{rec}}$</h4><p>由于$W<em>{rec}$是各个时刻共享的，所以$t$时刻之前的每个时刻$W</em>{rec}$的变化都对$E<em>t$有贡献，反过来求偏导时，也要考虑之前每个时刻$W</em>{rec}$对$E$的影响。\n我们以$s_k$为中间变量，应用链式法则：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\frac{\\partial s_k}{\\partial W_{rec}} \\frac{\\partial E_t}{\\partial s_k}</script><p>但由于$\\frac{\\partial s<em>k}{\\partial W</em>{rec}}$（分子向量，分母矩阵）以目前的数学发展水平是没办法求的，\n因此我们要求这个偏导，可以拆解为$E<em>t$对$W</em>{rec}(i,j)$的偏导数：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} \n= \\sum_{k=0}^t tr[(\\frac{\\partial E_t}{\\partial s_k})^T \\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]\n= \\sum_{k=0}^t  tr[(\\delta_k^{(t)})^T\\frac{\\partial s_k}{\\partial W_{rec}(i,j)}]</script><p>其中，$\\delta^{(t)}_k=\\frac{\\partial E_t}{\\partial s_k}$，遵循</p>\n<script type=\"math/tex; mode=display\">s_k\\to h_k\\to s_{k+1}\\to ...\\to E_t</script><p>的传递关系。</p>\n<p>应用链式法则有： </p>\n<script type=\"math/tex; mode=display\">\n\\delta^{(t)}_k = \\frac{\\partial h_k}{\\partial s_k}\\frac{\\partial s_{k+1}}{\\partial h_k} \\frac{\\partial E_t}{\\partial s_{k+1}}\n= diag(1-h_k\\odot h_k)W_{rec}^T\\delta^{(t)}_{k+1}=(W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)</script><p>其中，$\\odot$表示向量点乘（element-wise product）。注意$E_t$求导时链式法则的顺序，$E_t$是关于$s_k$的符合函数，且求导链上的每个变量都是向量，\n根据参考文献3，这种情况下应用分母布局的链式法则，方向应该是相反的。</p>\n<p>接下来计算$\\delta^{(t)}_t$：</p>\n<script type=\"math/tex; mode=display\">\n\\delta^{(t)}_t = \\frac{\\partial E_t}{\\partial s_t} = \\frac{\\partial h_t}{\\partial s_t}\\frac{\\partial z_t}{\\partial h_t}\\frac{\\partial E_t}{\\partial z_t}\n= diag(1-h_t\\odot h_t)\\cdot W_{out}^T\\cdot(\\hat{y}_t-y_t)=(W_{out}^T(\\hat{y}_t-y_t))\\odot (1-h_t\\odot h_t)</script><p>于是，我们得到了关于$\\delta$ 的递推关系式:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\n\\delta^{(t)}_t &= (W_{out}^T r_t)\\odot (1-h_t\\odot h_t) \\\\ \n\\delta^{(t)}_k &= (W_{rec}^T\\delta^{(t)}_{k+1})\\odot (1-h_k\\odot h_k)\n\\end{cases}</script><p>由 $ \\delta^{(t)}<em>t $ 出发，\n我们可以往前推出每一个 $ \\delta $ ，\n将 <script type=\"math/tex\">\\delta^{(t)}_0,...,\\delta^{(t)}_t</script> \n代入 $$\\frac{\\partial E_t}{\\partial W</em>{rec}(i,j)}$$\n有：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{rec}(i,j)} = \\sum_{k=0}^t \\delta_k^{(t)} h_{k-1}^{(j)}</script><p>将上式写成矩阵形式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial E_t}{\\partial W_{rec}} = \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1} \\\\\n\\frac{\\partial E}{\\partial W_{rec}} =\\sum_{t=0}^T \\sum_{k=0}^t \\delta^{(t)}_k \\otimes h_{k-1}</script><p>不失严谨性，定义$h_{-1}$为全0的向量。</p>\n<p><br></p>\n<h4 id=\"2-4-计算-frac-partial-Et-partial-W-in\"><a href=\"#2-4-计算-frac-partial-Et-partial-W-in\" class=\"headerlink\" title=\"2.4 计算$\\frac{\\partial Et}{\\partial W{in}}$\"></a>2.4 计算$\\frac{\\partial E<em>t}{\\partial W</em>{in}}$</h4><p>按照上述思路，我们可以得到 </p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial E_t}{\\partial W_{in}} = \\sum_{k=0}^t \\delta_k \\otimes x_{k}</script><p>由于$x_k$是个one-hot vector，假设$x_k(m)=1$，那么我们在更新$W$时只需要更新$W$的第$m$列即可。</p>\n<p><br></p>\n<h4 id=\"2-5-参数更新\"><a href=\"#2-5-参数更新\" class=\"headerlink\" title=\"2.5 参数更新\"></a>2.5 参数更新</h4><p>我们有了$E_t$关于各个参数的偏导数，就可以用梯度下降来更新各参数了:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nW_{in} &= W_{in} - \\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes x_{k} \\\\\nW_{rec}&=W_{rec}-\\lambda \\sum_{t=0}^T \\sum_{k=0}^t \\delta_k \\otimes h_{k-1} \\\\\nW_{out}&=W_{out}-\\lambda \\sum_{t=0}^T r_t \\otimes h_t\n\\end{cases}</script><p>其中 <script type=\"math/tex\">r_t= \\hat{y}_t - y_t</script>，</p>\n<p><script type=\"math/tex\">\\delta_t = \\frac{\\partial E_t}{\\partial s_t} = (W_{out}^T r_t) \\odot (1 - h_t\\odot h_t), \\lambda > 0</script> 表示学习率。</p>\n<p><br></p>\n<h4 id=\"2-6-部分思考\"><a href=\"#2-6-部分思考\" class=\"headerlink\" title=\"2.6 部分思考\"></a>2.6 部分思考</h4><ul>\n<li>为什么RNN中要对隐藏层的输出进行一次运算$z<em>t=W</em>{out}h<em>t$，然后再对$z_t$进行一次softmax，而不是直接对$h_t$进行softmax求得概率？为什么要有$W</em>{out}$这个参数？<ul>\n<li>答：$x<em>t$是一个$K\\times 1$的向量，我们要将它映射到一个$H\\times 1$的$h_t$（其中$H$是隐神经元的个数），从$x_t$到$h_t$相当于对词向量做了一次编码；最终我们要得到的是一个$K\\times 1$的向量（这里$K$是词汇表大小），表示每个词接下来出现的概率，所以我们需要一个矩阵$K\\times H$的$W</em>{out}$来将$h_t$映射回原来的空间去，这个过程相当于解码。因此，RNN可以理解为一种编解码网络。</li>\n</ul>\n</li>\n<li>$W<em>{in},W</em>{rec},W_{out}$三个参数分别有什么意义？<ul>\n<li>答： $W<em>{in}$将$K\\times 1$的one-hot词向量映射到$H\\times 1$隐藏层空间，将输入转化为计算机内部可理解可处理的形式，这个过程可以理解为一次编码过程；$W</em>{rec}$则是隐含层到自身的一个映射，它定义了模型如何结合上文信息，在编码中融入了之前的“记忆”；$W<em>{in},W</em>{rec}$结合了当前输入单词和之前的记忆，形成了当前时刻的知识状态。$W<em>{out}$是隐含层到输出的映射，$z=W</em>{out}h$是映射后的分数，这个过程相当于一次解码。这个解码后的分数再经过一层softmax转化为概率输出来，我们挑选概率最高的那个作为我们的预测。作为总结， RNN的记忆由两部分构成，一部分是当前的输入，另一部分是之前的记忆。</li>\n</ul>\n</li>\n<li>BPTT和BP的区别在哪？为什么不能用BP算法训练RNN？<ul>\n<li>答：BP算法只考虑了误差的导数在上下层级之间梯度的反向传播；而BPTT则同时包含了梯度在纵向层级间的反向传播和在时间维度上的反向传播，同时在两个方向上进行参数优化。</li>\n</ul>\n</li>\n<li>文中词$x_t$的特征是一个one-hot vector，这里能不能替换为word2vec训练出的词向量？效果和性能如何？<ul>\n<li>答：RNNLM本身自带了训练词向量的过程。由于$x<em>t$是one-hot向量，假设出现的词的索引为$j$，那么$W</em>{in}x<em>t$就是把$W</em>{in}$的第$j$列$W[:,j]$取出，这个列向量就相当于该词的词向量。实际上用语言模型训练词向量的思想最早可以追溯到03年Bengio的一篇论文《A neural probabilistic language model 》，这篇论文中作者使用一个神经网络模型来训练n-gram模型，顺便学到了词向量。本文出于数学推导以及代码实现上的方便采用了one-hot向量作为输入。实际工程中，词汇表通常都是几百万，内存没办法装下几百万维的稠密矩阵，所以工程上基本上没有用one-hot的，基本都是用词向量。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"http://blog.csdn.net/malefactor/article/details/50725480\" target=\"_blank\" rel=\"noopener\">使用RNN解决NLP中序列标注问题的通用优化思路</a></li>\n<li><a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" target=\"_blank\" rel=\"noopener\">wildml的rnn tutorial part3</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\" target=\"_blank\" rel=\"noopener\">Matrix Calculus Wiki</a></li>\n<li><a href>《神经网络与深度学习讲义》 邱锡鹏</a></li>\n</ol>\n"},{"layout":"post","title":"牛顿法&拟牛顿法","date":"2018-06-03T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n目录\n\n* 1.原始牛顿法\n* 2.阻尼牛顿法\n* 3.拟牛顿条件\n* 4.DFP算法\n* 5.BFGS算法\n* 6.L-BFGS算法\n\n\n------\n\n考虑如下无约束的极小化问题\n\n$$ min_x f(x) $$\n\n其中 $ x=(x_1, x_2, ..., x_n)^T \\in R^n $，由于本文不对收敛性进行讨论，因此对目标函数 $ f: R^n \\rightarrow R $做一个较苛刻的假设。\n这里假定$ f $为凸函数，且二阶连续可微，此外记极小化问题的解为 $ x^{\\ast} $ .\n\n\n---------\n\n### 1.原始牛顿法\n\n牛顿法的基本思想是：**在现有极小点估计值的附近对 $ f(x) $ 做二阶泰勒展开，进而找到极小点的下一个估计值。**\n\n为简单起见，首先考虑$ n=1 $的情况，设$ x\\_k $ 为当前的极小点估计值，则\n\n$$ \\phi (x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k) (x-x_k)^2 $$\n\n表示$ f(x) $ 在 $ x\\_k $附近的二阶泰勒展开式（略去了关于$ x-x\\_k $ 的高阶项）。由于求的是最值，由极值必要条件可知，$ \\phi(x) $应该满足\n\n$$ \\phi ' (x) = 0 \\quad \\Rightarrow \\quad f'(x_k) + f^{''}(x_k) (x - x_k) = 0 $$\n\n从而求得：\n\n\\\\[\nx = x\\_k - \\frac{f'(x\\_k)}{f^{''}(x\\_k)}\n\\\\]\n\n于是，若给定初始值$ x\\_0 $，则可以构造如下的迭代公式：\n\n$$ x_{k+1} = x_k - \\frac{f'(x_k)}{f^{''}(x_k)}, \\quad k=0,1,... $$\n\n产生序列$ \\lbrace x\\_k \\rbrace $来逼近 $ f(x) $的极小点，在一定条件下，$ \\lbrace x\\_k \\rbrace $可以收敛到$ f(x) $的极小点。\n\n<br>\n\n对于 $ n>1 $的情形，二阶泰勒展开式可以做推广，此时：\n\n$$ \\phi(X) = f(X_k) + \\nabla f(X_k) \\cdot (X - X_k) + \\frac{1}{2} \\cdot (X - X_k)^T \\nabla^2 f(X_k) \\cdot (X - X_k) $$\n\n其中$ \\nabla f $ 为 $ f $的梯度向量，$ \\nabla^2 f $为 $ f $的海森矩阵（Hessian matrix），其定义分别为：\n\n$$ \\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x_1} \\\\\\ \\frac{\\partial f}{\\partial x_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial f}{\\partial x_n}  \\end{matrix} \\right] $$\n\n$$\n\\nabla^2 f = \n\\left[\n\\begin{matrix} \n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x^2_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\\n &  &  \\ddots &  \\\\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\\\\\\n\\end{matrix}\n\\right]\n$$\n\n注意，$ \\nabla f $ 和 $ \\nabla^2 f $ 中的元素均为关于$ x $ 的函数，以下分别将其简记为$g$和$H$，特别地，若$f$的混合偏导数可交换次序，则海森矩阵$H$为对称矩阵。\n而$ \\nabla f(X\\_k) $ 和 $ \\nabla^2 f(X\\_k) $则表示将$X$取为$X\\_k$后得到的实值向量和矩阵，以下分别将其简记为$ g\\_k $和$H\\_k$（这里字母$g$表示$gradient$，$H$表示$Hessian$）。\n\n同样地，由于是求极小点，极值必要条件要求它为$ \\phi (X)$的驻点，即\n\n$$ \\nabla \\phi (X) = 0 \\quad \\Rightarrow \\quad g_k + H_k \\cdot (x - x_k) = 0 $$\n\n进一步，若矩阵$ H\\_k $非奇异，则可解得\n\n$$ X = X_k - H_k^{-1} \\cdot g_k $$\n\n于是，给定初始值$ X\\_0 $，则同样可以构造出迭代格式\n\n$$ X_{k+1} = X_k - H_k^{-1} \\cdot g_k, \\quad k=0,1,2... $$\n\n这就是原始的牛顿迭代法，其迭代格式中的搜索方向$ d\\_k = - H\\_k^{-1} \\cdot  g\\_k $称为**牛顿方向**，下面给出牛顿法的完整算法描述。\n\n>\n1. 给定初值$ x\\_0 $和精度阈值$ \\epsilon $，并令$ k=0$；\n2. 计算$ g\\_k $和$ H\\_k $；\n3. 若$ \\|\\| gk \\|\\| < \\epsilon $，则停止迭代；否则确定搜索方向$ d\\_k = -H\\_k^{-1} \\cdot g\\_k $；\n4. 计算新的迭代点 $ x\\_{k+1} = x\\_k + d\\_k $；\n5. 令$ k = k+1 $，转至第2步。 \n\n原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$ f(x\\_{k+1} > f(x\\_k) $的情况，\n这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$ \\lbrace x\\_k \\rbrace $的发散而导致计算失败。\n\n\n-------------\n\n### 2.阻尼牛顿法\n\n为消除原始牛顿法中的弊端，提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍采用 $ d\\_k $，但每次迭代需沿此方向做一维搜索(line search)，寻求最优的步长因子$ \\lambda\\_k $，即\n\n$$ \\lambda_k = \\arg\\min_{\\lambda \\in R} f(x_k + \\lambda d_k) \\tag{*} $$\n\n下面给出一个阻尼牛顿法的完整算法描述：\n\n>\n1. 给定初值$ x\\_0 $和精度阈值$ \\epsilon $，并令$ k=0$；\n2. 计算$ g\\_k $和$ H\\_k $；\n3. 若$ \\|\\| gk \\|\\| < \\epsilon $，则停止迭代；否则确定搜索方向$ d\\_k = -H\\_k^{-1} \\cdot g\\_k $；\n4. 利用$(*)$式得到步长$ \\lambda\\_k $，并令 $ x\\_{k+1} = x\\_k + \\lambda\\_k d\\_k $；\n5. 令$ k = k+1 $，转至第2步。 \n\n\n**小结**\n\n牛顿法是梯度(下降)发的进一步发展，梯度法利用目标函数的一阶偏导数信息，以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；\n而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛。\n\n但是牛顿法主要存在以下两个缺点：\n1. 对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定；\n2. 计算相当复杂，除需计算梯度以外，还需计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$ n $的平方比增加，当$n$很大时这个问题尤为突出。\n\n\n-----------\n\n### 3.拟牛顿条件\n\n上面说到了牛顿法的两个缺点，为了客服这两个问题，人们提出了拟牛顿法，这个方法的基本思想是：**不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数。**\n\n不同的构造方法就产生了不同的拟牛顿法，都是对牛顿法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。\n\n下面先推到拟牛顿条件，或者叫拟牛顿方程、割线条件，它是用来提供理论指导，指出了用来近似的矩阵应该满足的条件。\n\n为了明确起见，下文中用$ B $ 表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$ H^{-1} $的近似，即 $ B \\approx H, D \\approx H^{-1} $。\n\n<br>\n\n假设经过 $ k+1 $ 次迭代后得到 $ x_{k+1} $，此时将目标函数 $ f(x) $ 在 $ x_{k+1} $附近作泰勒展开，取二阶近似，得到：\n\n$$ f(x) \\approx f(x_{k+1} + \\nabla f(x_{k+1}) \\cdot (x - x_{k+1}) + \\frac{1}{2} \\cdot (x - x_{k+1}^T \\cdot \\nabla^2 f(x_{k+1}) \\cdot (x - x_{k+1}) \\tag{**} $$\n\n在$ (**) $式两边同时作用一个梯度算子 $ \\nabla $， 可得\n\n$$ \\nabla f(x) \\approx \\nabla f(x_{k+1}) + H_{k+1} \\cdot (x - x_{k+1}) \\tag{***} $$\n\n在$ (***) $式中取$ x = x_k$，并整理可得：\n\n$$ g_{k+1} - g_k = H_{k+1} \\cdot ( x_{k+1} - x_k ) $$\n\n引入新的记号：\n\n$$ s_k = x_{k+1} - x_k, \\quad y_k = g_{k+1} - g_k $$\n\n则上式可以重新表示为：\n\n$$ y_k \\approx H_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k \\approx H_{k+1}^{-1} \\cdot y_k $$\n\n这就是所谓的**拟牛顿条件**，它对迭代过程中的海森矩阵$ H_{k+1}$作约束，因此，对$ H_{k+1} $做近似的$B_{k+1}$，以及对$ H_{k+1}^{-1} $做近似的$  D_{k+1}$ 可以将\n\n$$ y_k = B_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k = D_{k+1} \\cdot y_k $$\n\n作为指导。\n\n\n-----------------\n\n### 4.DFP算法\n\n该算法的核心是：**通过迭代的方法，对$ H_{k+1}^{-1}$做近似，迭代格式为：**\n\n$$ D_{k+1} = D_k + \\Delta D_k, \\quad k=0,1,2...$$\n\n其中$ D_0$通常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$ \\Delta D_k$如何构造。\n\n这里采用“待定法”，即首先将$\\Delta D_k$待定成某种形式，然后结合拟牛顿条件来推到。这里我们将$ \\Delta D_k $ 待定为：\n\n$$ \\Delta D_k = \\alpha u u^T + \\beta v v^T $$\n\n其中$ \\alpha, \\beta$为待定系数，$ u, v \\in R^n $为待定向量，从形式上来看，这种待定公式至少保证了矩阵$ \\Delta D_k $的对称性（因为$uu^T$和$vv^T$都是对称矩阵）\n\n\n$$\n\\begin{cases}\nD_{k+1} = D_k + \\Delta D_k \\\\\n\\Delta D_k = \\alpha u u^T + \\beta v v^T \\\\\ns_k = D_{k+1} \\cdot y_k \\\\\n\\end{cases}\n\\Longleftrightarrow\ns_k = D_k y_k + \\alpha u u^T y_k + \\beta v v^T y_k\n$$\n\n调整上面的结论：\n\n$$\n\\begin{equation}\n\\begin{aligned}\ns_k\n& = D_k y_k + u (\\alpha u^T y_k) + v(\\beta v^T y_k) \\\\\n& = D_k y_k + (\\alpha u^T y_k) u + (\\beta v^T y_k) v\n\\end{aligned}\n\\end{equation}\n\\tag{+}\n$$\n\n括号中的$\\alpha u^T y_k$和$\\beta v^T y_k$是两个数，既然是数，我们不妨作如下简单赋值：\n\n$$ \\alpha u^T y_k = 1, \\qquad \\beta v^T y_k = -1  \\tag{++}$$\n\n即\n\n$$ \\alpha = \\frac{1}{u^T y_k}, \\qquad \\beta = - \\frac{1}{v^T y_k} $$\n\n将$ (++) $式代入 $ (+) $，得到：\n\n$$ u - v = s_k - D_k y_k $$\n\n要使上式成立，不妨就直接取\n\n$$ u = s_k, \\qquad v = D_k y_k $$\n\n再将$(21)$代入$(19)$，便得\n\n$$ \\alpha = \\frac{1}{s_k^T y_k}, \\qquad \\beta = - \\frac{1}{(D_k y_k)^T y_k} = - \\frac{1}{y_k^T D_k y_k}$$\n\n其中第二个等式用到了$D_k$的对称性。\n\n至此，我们已经可以将校正矩阵$ \\Delta D_k $构造出来了，\n\n$$ \\Delta D_k = \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $$\n\n综上，我们给出DFP算法的一个完整算法描述：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - D_k \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ D_{k+1} = D_k + \\Delta D_k = D_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n\n-------------\n\n### 5.BFGS算法\n\n与DFP算法相比，BFGS算法性能更佳，目前它已成为求解无约束非线性优化问题最常用的方法之一。\nBFGS算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果。\n\nBFGS算法中核心公式的推到和DFP算法完全类似，只是互换了其中$s_k$和$y_k$的位置。\n需要注意的是，BFGS算法是直接逼近海森矩阵，即$ B_k \\approx H_k$，仍采用迭代方法，设迭代格式为：\n\n$$ B_{k+1} = B_k + \\Delta B_k, \\qquad k=0,1,2... $$\n\n其中的 $ B_0 $也常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$\\Delta B_k$如何构造。同样，将其待定为：\n\n$$ \\Delta B_k = \\alpha u u^T + \\beta v v^T $$\n\n类比DFP算法，可得\n\n$$ y_k = B_k s_k + (\\alpha u^T s_k)u + (\\beta v^T s_k) v $$\n\n通过令 $ \\alpha u^T s_k = 1,\\qquad \\beta v^T s_k = -1 $，以及 $ u = y_k, \\qquad v = B_k s_k $，可以算得：\n\n$$ \\alpha = \\frac{1}{y_k^T s_k}, \\qquad \\beta = - \\frac{1}{s_k^T B_k s_k} $$\n\n综上，便得到了如下校正矩阵$ \\Delta B_k $的公式：\n\n$$ \\Delta B_k = \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $$\n\n先讲把矩阵$ \\Delta B_k $ 和 $ \\Delta D_k $拿出来对比一下，是不是除了将$D$换成$B$外，其他只是将$s_k$和$y_k$互调了一下位置呢？\n\n最后，我们给出BFGS算法的一个完整算法描述：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ B_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - B_k^{-1} \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ B_{k+1} = B_k + \\Delta B_k = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n\n上面算法中的第2步通常是通过求解线性代数方程组$ B_k d_k = -g_k $来进行，然而更一般的做法是通过对第6步中的递推关系，\n应用$Sherman-Morrison$公式，直接给出$B_{k+1}^{-1}$和$B_k^{-1}$之间的关系：\n\n$$ B_{k+1}^{-1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) B_k^{-1} \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $$\n\n利用上式，我们很容易将上述BFGS算法进行改进，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$（这样做仅仅只是符号上看起来舒服起见）。\n这样，整个算法中不再需要求解线性代数方程组，由矩阵-向量运算就可以完成了。\n\n改进后的BFGS算法如下：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - D_k \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n至此，关于DFP算法和BFGS算法的介绍就完成了。\n\n最后再补充一下一维搜索(line search)的问题，在之前几个算法描述中，为简单起见，均采用了$(*)$时来计算步长$ \\lambda_k $，其实这是一种精确搜索；\n实际应用中，还有像Wolfe型搜索、Armijo搜索以及满足Goldstein条件的非精确搜索。这里我们以Wolfe搜索为例，简单介绍。\n\n设$ \\tilde{\\beta} \\in (0, \\frac{1}{2}), \\beta \\in (\\tilde{\\beta}, 1)$，所谓的Wolfe搜索是指$ \\lambda_k $满足如下**Wolfe条件**\n\n$$\n\\begin{cases}\nf(x_k + \\lambda_k d_k) \\qquad \\leq f(x_k) + \\tilde{\\beta} \\lambda_k d_k^T g_k; \\\\\nd_k^T g(x_k + \\lambda_k d_k) \\geq \\beta d_k^T g_k\n\\end{cases}\n$$\n\n带非精确搜索的拟牛顿法的研究是从1976年Powell的工作开始的，他证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。\n\n\n------------------\n\n### 6.L-BFGS算法\n\n在BFGS算法中，需要用到一个$NxN$的矩阵$D_k$，当 N 很大时，存储这个矩阵将变得很耗计算机资源，考虑 N 为 10万 的情形，且 用double型（8字节）来存储$D_k$，需要多大的内存呢？来计算一下：\n\n$$ \\frac{N阶矩阵的字节数}{1GB的字节数} \\qquad = \\qquad \\frac{10^5 \\times 10^5 \\times 8}{2^{10} \\times 2^{10} \\times 2^{10}} \\qquad = \\qquad 74.5(GB) $$\n\n即使考虑到矩阵$D_k$的对称性，内存还可以降一半，但是这对于一般的服务器仍然是难以承受的。况且我们还只是假定了 N=10W 的情况，在实际机器学习问题中，这只能算是中小规模。\n\nL-BFGS(Limited-memory BFGS)算法就是通过对BFGS算法进行改造，从而减少其迭代过程中所需内存开销的算法。\n它对BFGS算法进行了近似，其基本思想是：**不再存储完整的矩阵$D_k$，而是存储计算过程中的向量序列$\\{s_i\\}，\\{y_i\\}$，需要矩阵$D_k$时，利用向量序列$\\{s_i\\}，\\{y_i\\}$的计算来代替。**\n而且，向量序列$\\{s_i\\}，\\{y_i\\}$也不是所有的都存，而是固定存最新的$m$个（参数$m$可由用户根据机器的内存自行指定），每次计算$D_k$时，只利用最新的$m$个$\\{s_i\\}$和$m$个$\\{y_i\\}$，\n显然，这样我们将存储由原来的$O(n^2)$降到了$O(mN)$。\n\n<br>\n\n接下来，讨论L-BFGS算法的具体实现过程，我们的出发点是改进的BFGS算法中的第6步中的迭代式：\n\n$$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $$\n\n若记$ \\rho_k = \\frac{1}{y_k^T s_k}，V_k = I - \\rho_k y_k s_k^T $，则上式可写成\n\n$$ D_{k+1} = V_k^T D_k V_k + \\rho_k s_k s_k^T \\tag{6.*} $$\n\n如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如$D_0 = I$），则可利用$(6.*)$式，依次可得：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_1 &= V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T; \\\\\nD_2 &= V_1^T D_1 V_1 + \\rho_1 s_1 s_1^T \\\\\n&= v_1^T (V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T) V_1 + \\rho_1 s_1 s_1^T \\\\\n&= V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T \\\\\nD_3 &= V_2^T D_2 V_2 + \\rho_2 s_2 s_2^T \\\\\n&= V_2^T (V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T) V_2 + \\rho_2 s_2 s_2^T \\\\\n&= v_2^T V_1^T V_0^T D_0 V_0 V_1 V_2 + V_2^T V_1^T \\rho_0 s_0 s_0^T V_1 V_2 + V_2^T \\rho_1 s_1 s_1^T V_2 + \\rho_2 s_2 s_2^T \\\\\n\\cdots\n\\end{aligned}\n\\end{equation}\n$$\n\n一般地，我们有：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_1^T V_0^T) D_0 (V_0 V_1 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_2^T V_1^T) (\\rho_0 s_0 s_0^T) (V_1 V_2 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_3^T V_2^T) (\\rho_1 s_1 s_1^T) (V_2 V_3 \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.**}\n$$\n\n由上式可以看到，计算$D_{k+1}$需要用到$ \\lbrace s_i, y_i \\rbrace_{i=0}^k $，因此，若从$ s_0, y_0 $开始连续存储 $ m $ 组的话，只能存储到$ s_{m-1}, y_{m-1} $，\n亦即只能依次计算 $ D_1, D_2, \\cdots $，直到$ D_m $，那$ D_{m+1}, D_{m+2} $该如何计算呢？\n\n自然地，如果一定要丢弃一些向量，那么肯定优先考虑那些最早生成的向量，具体来说，计算$D_{m+1}$时，我们保存$ \\lbrace s_i, y_i \\rbrace_{i=1}^m $，丢掉$ \\lbrace s_0, y_0 \\rbrace;$ \n计算$ D_{m+2} $时，我们保存$ \\lbrace s_i, y_i\\rbrace_{i=2}^{m+1} $，丢掉了$ \\lbrace s_i, y_i \\rbrace_{i=0}^1; \\cdots $\n\n但是舍弃掉一些向量后，就只能近似计算了，当$k+1 > m$时，仿照$(6.**)$式，可以构造近似计算公式：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-m+2}^T V_{k-m+1}^T) D_0 (V_{k-m+1} V_{k-m+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+3}^T V_{k-m+2}^T) (\\rho_0 s_0 s_0^T) (V_{k-m+2} V_{k-m+3} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+4}^T V_{k-m+3}^T) (\\rho_1 s_1 s_1^T) (V_{k-m+3} V_{k-m+4} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.***}\n$$\n\n$ (6.\\*\\*) $ 和 $ (6.\\*\\*\\*) $ 被称为$ special BFGS matrices $，若引入 $ \\hat{m} = min\\lbrace k, m-1 \\rbrace $，则还可以将两式合并成\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+1}^T V_{k-\\hat{m}}^T) D_0 (V_{k-\\hat{m}} V_{k-\\hat{m}+1} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+2}^T V_{k-\\hat{m+1}}^T) (\\rho_0 s_0 s_0^T) (V_{k-\\hat{m}+1} V_{k-\\hat{m}+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+3}^T V_{k-\\hat{m+2}}^T) (\\rho_1 s_1 s_1^T) (V_{k-\\hat{m}+2} V_{k-\\hat{m}+3} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_{k} s_{k} s_{k}^T\n\\end{aligned}\n\\end{equation}\n$$\n\n事实上，由BFGS算法流程易知，$D_k$的作用仅用来计算$ D_k g_k $ 获取搜索方向，因此，若能利用上式设计出一种计算$ D_k g_k $的快速算法，则大功告成。\n\n> **($D_k \\cdot g_k$的快速算法)**\n> \nStep 1 初始化\n$$\n\\delta = \n\\begin{cases}\n0, \\qquad k \\leq m \\\\\nk-m, \\qquad k > m\n\\end{cases};\n\\qquad\nL = \n\\begin{cases}\nk, \\qquad k \\leq m \\\\\nm, \\qquad k > m\n\\end{cases};\n\\qquad\nq_L = g_k\n$$\n>\nStep 2 后向循环\n$$\nFOR \\quad i=L-1, L-2, \\cdots, 1, 0 \\quad DO \\\\\n\\{ \\\\\n\\qquad j=i+\\delta; \\\\\n\\qquad \\alpha_i = \\rho_j s_j^T q_{i+1}; // \\alpha_i 需要保存下来，前向循环要用！！ \\\\\n\\qquad q_i = q_{i+1} - \\alpha_i y_j \\\\\n\\}\n$$\n>\nStep 3 前向循环\n$$\nr_0 = D_0 \\cdot q_0; \\\\\nFOR \\quad i=0,1,\\cdots,L-2,L-1 \\quad DO \\\\\n\\{ \\\\\n\\qquad j = i+\\delta; \\\\\n\\qquad \\beta_j = \\rho_j y_j^T r_i; \\\\\n\\qquad r_{i+1} = r_i + (\\alpha_i - \\beta_i) s_j \\\\\n\\}\n$$\n\n\n最后算出的$r_L$即为$H_k \\cdot g_k $的值。\n\n\n---------\n\n>\n[牛顿法与拟牛顿法学习笔记](https://blog.csdn.net/itplus/article/details/21896453)\n\n","source":"_posts/2018-06-03-newstonmethod.md","raw":"---\nlayout: post\ntitle: 牛顿法&拟牛顿法\ndate: 2018-06-03 12:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n目录\n\n* 1.原始牛顿法\n* 2.阻尼牛顿法\n* 3.拟牛顿条件\n* 4.DFP算法\n* 5.BFGS算法\n* 6.L-BFGS算法\n\n\n------\n\n考虑如下无约束的极小化问题\n\n$$ min_x f(x) $$\n\n其中 $ x=(x_1, x_2, ..., x_n)^T \\in R^n $，由于本文不对收敛性进行讨论，因此对目标函数 $ f: R^n \\rightarrow R $做一个较苛刻的假设。\n这里假定$ f $为凸函数，且二阶连续可微，此外记极小化问题的解为 $ x^{\\ast} $ .\n\n\n---------\n\n### 1.原始牛顿法\n\n牛顿法的基本思想是：**在现有极小点估计值的附近对 $ f(x) $ 做二阶泰勒展开，进而找到极小点的下一个估计值。**\n\n为简单起见，首先考虑$ n=1 $的情况，设$ x\\_k $ 为当前的极小点估计值，则\n\n$$ \\phi (x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k) (x-x_k)^2 $$\n\n表示$ f(x) $ 在 $ x\\_k $附近的二阶泰勒展开式（略去了关于$ x-x\\_k $ 的高阶项）。由于求的是最值，由极值必要条件可知，$ \\phi(x) $应该满足\n\n$$ \\phi ' (x) = 0 \\quad \\Rightarrow \\quad f'(x_k) + f^{''}(x_k) (x - x_k) = 0 $$\n\n从而求得：\n\n\\\\[\nx = x\\_k - \\frac{f'(x\\_k)}{f^{''}(x\\_k)}\n\\\\]\n\n于是，若给定初始值$ x\\_0 $，则可以构造如下的迭代公式：\n\n$$ x_{k+1} = x_k - \\frac{f'(x_k)}{f^{''}(x_k)}, \\quad k=0,1,... $$\n\n产生序列$ \\lbrace x\\_k \\rbrace $来逼近 $ f(x) $的极小点，在一定条件下，$ \\lbrace x\\_k \\rbrace $可以收敛到$ f(x) $的极小点。\n\n<br>\n\n对于 $ n>1 $的情形，二阶泰勒展开式可以做推广，此时：\n\n$$ \\phi(X) = f(X_k) + \\nabla f(X_k) \\cdot (X - X_k) + \\frac{1}{2} \\cdot (X - X_k)^T \\nabla^2 f(X_k) \\cdot (X - X_k) $$\n\n其中$ \\nabla f $ 为 $ f $的梯度向量，$ \\nabla^2 f $为 $ f $的海森矩阵（Hessian matrix），其定义分别为：\n\n$$ \\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x_1} \\\\\\ \\frac{\\partial f}{\\partial x_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial f}{\\partial x_n}  \\end{matrix} \\right] $$\n\n$$\n\\nabla^2 f = \n\\left[\n\\begin{matrix} \n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x^2_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\\n &  &  \\ddots &  \\\\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\\\\\\n\\end{matrix}\n\\right]\n$$\n\n注意，$ \\nabla f $ 和 $ \\nabla^2 f $ 中的元素均为关于$ x $ 的函数，以下分别将其简记为$g$和$H$，特别地，若$f$的混合偏导数可交换次序，则海森矩阵$H$为对称矩阵。\n而$ \\nabla f(X\\_k) $ 和 $ \\nabla^2 f(X\\_k) $则表示将$X$取为$X\\_k$后得到的实值向量和矩阵，以下分别将其简记为$ g\\_k $和$H\\_k$（这里字母$g$表示$gradient$，$H$表示$Hessian$）。\n\n同样地，由于是求极小点，极值必要条件要求它为$ \\phi (X)$的驻点，即\n\n$$ \\nabla \\phi (X) = 0 \\quad \\Rightarrow \\quad g_k + H_k \\cdot (x - x_k) = 0 $$\n\n进一步，若矩阵$ H\\_k $非奇异，则可解得\n\n$$ X = X_k - H_k^{-1} \\cdot g_k $$\n\n于是，给定初始值$ X\\_0 $，则同样可以构造出迭代格式\n\n$$ X_{k+1} = X_k - H_k^{-1} \\cdot g_k, \\quad k=0,1,2... $$\n\n这就是原始的牛顿迭代法，其迭代格式中的搜索方向$ d\\_k = - H\\_k^{-1} \\cdot  g\\_k $称为**牛顿方向**，下面给出牛顿法的完整算法描述。\n\n>\n1. 给定初值$ x\\_0 $和精度阈值$ \\epsilon $，并令$ k=0$；\n2. 计算$ g\\_k $和$ H\\_k $；\n3. 若$ \\|\\| gk \\|\\| < \\epsilon $，则停止迭代；否则确定搜索方向$ d\\_k = -H\\_k^{-1} \\cdot g\\_k $；\n4. 计算新的迭代点 $ x\\_{k+1} = x\\_k + d\\_k $；\n5. 令$ k = k+1 $，转至第2步。 \n\n原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$ f(x\\_{k+1} > f(x\\_k) $的情况，\n这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$ \\lbrace x\\_k \\rbrace $的发散而导致计算失败。\n\n\n-------------\n\n### 2.阻尼牛顿法\n\n为消除原始牛顿法中的弊端，提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍采用 $ d\\_k $，但每次迭代需沿此方向做一维搜索(line search)，寻求最优的步长因子$ \\lambda\\_k $，即\n\n$$ \\lambda_k = \\arg\\min_{\\lambda \\in R} f(x_k + \\lambda d_k) \\tag{*} $$\n\n下面给出一个阻尼牛顿法的完整算法描述：\n\n>\n1. 给定初值$ x\\_0 $和精度阈值$ \\epsilon $，并令$ k=0$；\n2. 计算$ g\\_k $和$ H\\_k $；\n3. 若$ \\|\\| gk \\|\\| < \\epsilon $，则停止迭代；否则确定搜索方向$ d\\_k = -H\\_k^{-1} \\cdot g\\_k $；\n4. 利用$(*)$式得到步长$ \\lambda\\_k $，并令 $ x\\_{k+1} = x\\_k + \\lambda\\_k d\\_k $；\n5. 令$ k = k+1 $，转至第2步。 \n\n\n**小结**\n\n牛顿法是梯度(下降)发的进一步发展，梯度法利用目标函数的一阶偏导数信息，以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；\n而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛。\n\n但是牛顿法主要存在以下两个缺点：\n1. 对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定；\n2. 计算相当复杂，除需计算梯度以外，还需计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$ n $的平方比增加，当$n$很大时这个问题尤为突出。\n\n\n-----------\n\n### 3.拟牛顿条件\n\n上面说到了牛顿法的两个缺点，为了客服这两个问题，人们提出了拟牛顿法，这个方法的基本思想是：**不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数。**\n\n不同的构造方法就产生了不同的拟牛顿法，都是对牛顿法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。\n\n下面先推到拟牛顿条件，或者叫拟牛顿方程、割线条件，它是用来提供理论指导，指出了用来近似的矩阵应该满足的条件。\n\n为了明确起见，下文中用$ B $ 表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$ H^{-1} $的近似，即 $ B \\approx H, D \\approx H^{-1} $。\n\n<br>\n\n假设经过 $ k+1 $ 次迭代后得到 $ x_{k+1} $，此时将目标函数 $ f(x) $ 在 $ x_{k+1} $附近作泰勒展开，取二阶近似，得到：\n\n$$ f(x) \\approx f(x_{k+1} + \\nabla f(x_{k+1}) \\cdot (x - x_{k+1}) + \\frac{1}{2} \\cdot (x - x_{k+1}^T \\cdot \\nabla^2 f(x_{k+1}) \\cdot (x - x_{k+1}) \\tag{**} $$\n\n在$ (**) $式两边同时作用一个梯度算子 $ \\nabla $， 可得\n\n$$ \\nabla f(x) \\approx \\nabla f(x_{k+1}) + H_{k+1} \\cdot (x - x_{k+1}) \\tag{***} $$\n\n在$ (***) $式中取$ x = x_k$，并整理可得：\n\n$$ g_{k+1} - g_k = H_{k+1} \\cdot ( x_{k+1} - x_k ) $$\n\n引入新的记号：\n\n$$ s_k = x_{k+1} - x_k, \\quad y_k = g_{k+1} - g_k $$\n\n则上式可以重新表示为：\n\n$$ y_k \\approx H_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k \\approx H_{k+1}^{-1} \\cdot y_k $$\n\n这就是所谓的**拟牛顿条件**，它对迭代过程中的海森矩阵$ H_{k+1}$作约束，因此，对$ H_{k+1} $做近似的$B_{k+1}$，以及对$ H_{k+1}^{-1} $做近似的$  D_{k+1}$ 可以将\n\n$$ y_k = B_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k = D_{k+1} \\cdot y_k $$\n\n作为指导。\n\n\n-----------------\n\n### 4.DFP算法\n\n该算法的核心是：**通过迭代的方法，对$ H_{k+1}^{-1}$做近似，迭代格式为：**\n\n$$ D_{k+1} = D_k + \\Delta D_k, \\quad k=0,1,2...$$\n\n其中$ D_0$通常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$ \\Delta D_k$如何构造。\n\n这里采用“待定法”，即首先将$\\Delta D_k$待定成某种形式，然后结合拟牛顿条件来推到。这里我们将$ \\Delta D_k $ 待定为：\n\n$$ \\Delta D_k = \\alpha u u^T + \\beta v v^T $$\n\n其中$ \\alpha, \\beta$为待定系数，$ u, v \\in R^n $为待定向量，从形式上来看，这种待定公式至少保证了矩阵$ \\Delta D_k $的对称性（因为$uu^T$和$vv^T$都是对称矩阵）\n\n\n$$\n\\begin{cases}\nD_{k+1} = D_k + \\Delta D_k \\\\\n\\Delta D_k = \\alpha u u^T + \\beta v v^T \\\\\ns_k = D_{k+1} \\cdot y_k \\\\\n\\end{cases}\n\\Longleftrightarrow\ns_k = D_k y_k + \\alpha u u^T y_k + \\beta v v^T y_k\n$$\n\n调整上面的结论：\n\n$$\n\\begin{equation}\n\\begin{aligned}\ns_k\n& = D_k y_k + u (\\alpha u^T y_k) + v(\\beta v^T y_k) \\\\\n& = D_k y_k + (\\alpha u^T y_k) u + (\\beta v^T y_k) v\n\\end{aligned}\n\\end{equation}\n\\tag{+}\n$$\n\n括号中的$\\alpha u^T y_k$和$\\beta v^T y_k$是两个数，既然是数，我们不妨作如下简单赋值：\n\n$$ \\alpha u^T y_k = 1, \\qquad \\beta v^T y_k = -1  \\tag{++}$$\n\n即\n\n$$ \\alpha = \\frac{1}{u^T y_k}, \\qquad \\beta = - \\frac{1}{v^T y_k} $$\n\n将$ (++) $式代入 $ (+) $，得到：\n\n$$ u - v = s_k - D_k y_k $$\n\n要使上式成立，不妨就直接取\n\n$$ u = s_k, \\qquad v = D_k y_k $$\n\n再将$(21)$代入$(19)$，便得\n\n$$ \\alpha = \\frac{1}{s_k^T y_k}, \\qquad \\beta = - \\frac{1}{(D_k y_k)^T y_k} = - \\frac{1}{y_k^T D_k y_k}$$\n\n其中第二个等式用到了$D_k$的对称性。\n\n至此，我们已经可以将校正矩阵$ \\Delta D_k $构造出来了，\n\n$$ \\Delta D_k = \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $$\n\n综上，我们给出DFP算法的一个完整算法描述：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - D_k \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ D_{k+1} = D_k + \\Delta D_k = D_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n\n-------------\n\n### 5.BFGS算法\n\n与DFP算法相比，BFGS算法性能更佳，目前它已成为求解无约束非线性优化问题最常用的方法之一。\nBFGS算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果。\n\nBFGS算法中核心公式的推到和DFP算法完全类似，只是互换了其中$s_k$和$y_k$的位置。\n需要注意的是，BFGS算法是直接逼近海森矩阵，即$ B_k \\approx H_k$，仍采用迭代方法，设迭代格式为：\n\n$$ B_{k+1} = B_k + \\Delta B_k, \\qquad k=0,1,2... $$\n\n其中的 $ B_0 $也常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$\\Delta B_k$如何构造。同样，将其待定为：\n\n$$ \\Delta B_k = \\alpha u u^T + \\beta v v^T $$\n\n类比DFP算法，可得\n\n$$ y_k = B_k s_k + (\\alpha u^T s_k)u + (\\beta v^T s_k) v $$\n\n通过令 $ \\alpha u^T s_k = 1,\\qquad \\beta v^T s_k = -1 $，以及 $ u = y_k, \\qquad v = B_k s_k $，可以算得：\n\n$$ \\alpha = \\frac{1}{y_k^T s_k}, \\qquad \\beta = - \\frac{1}{s_k^T B_k s_k} $$\n\n综上，便得到了如下校正矩阵$ \\Delta B_k $的公式：\n\n$$ \\Delta B_k = \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $$\n\n先讲把矩阵$ \\Delta B_k $ 和 $ \\Delta D_k $拿出来对比一下，是不是除了将$D$换成$B$外，其他只是将$s_k$和$y_k$互调了一下位置呢？\n\n最后，我们给出BFGS算法的一个完整算法描述：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ B_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - B_k^{-1} \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ B_{k+1} = B_k + \\Delta B_k = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n\n上面算法中的第2步通常是通过求解线性代数方程组$ B_k d_k = -g_k $来进行，然而更一般的做法是通过对第6步中的递推关系，\n应用$Sherman-Morrison$公式，直接给出$B_{k+1}^{-1}$和$B_k^{-1}$之间的关系：\n\n$$ B_{k+1}^{-1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) B_k^{-1} \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $$\n\n利用上式，我们很容易将上述BFGS算法进行改进，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$（这样做仅仅只是符号上看起来舒服起见）。\n这样，整个算法中不再需要求解线性代数方程组，由矩阵-向量运算就可以完成了。\n\n改进后的BFGS算法如下：\n\n>\n1. 给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，\n2. 确定搜索方向$ d_k = - D_k \\cdot g_k $，\n3. 利用$(*)$式得到步长$\\lambda_k$，令$ s_k = \\lambda_k d_k, \\quad x_{k+1}=x_k + s_k $，\n4. 若$ \\|\\| g_{k+1} \\|\\| < \\epsilon$，则算法结束，\n5. 计算 $ y_k = g_{k+1} - g_k $，\n6. 计算$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $，\n7. 令$ k = k+1$，转至第2步。 \n\n至此，关于DFP算法和BFGS算法的介绍就完成了。\n\n最后再补充一下一维搜索(line search)的问题，在之前几个算法描述中，为简单起见，均采用了$(*)$时来计算步长$ \\lambda_k $，其实这是一种精确搜索；\n实际应用中，还有像Wolfe型搜索、Armijo搜索以及满足Goldstein条件的非精确搜索。这里我们以Wolfe搜索为例，简单介绍。\n\n设$ \\tilde{\\beta} \\in (0, \\frac{1}{2}), \\beta \\in (\\tilde{\\beta}, 1)$，所谓的Wolfe搜索是指$ \\lambda_k $满足如下**Wolfe条件**\n\n$$\n\\begin{cases}\nf(x_k + \\lambda_k d_k) \\qquad \\leq f(x_k) + \\tilde{\\beta} \\lambda_k d_k^T g_k; \\\\\nd_k^T g(x_k + \\lambda_k d_k) \\geq \\beta d_k^T g_k\n\\end{cases}\n$$\n\n带非精确搜索的拟牛顿法的研究是从1976年Powell的工作开始的，他证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。\n\n\n------------------\n\n### 6.L-BFGS算法\n\n在BFGS算法中，需要用到一个$NxN$的矩阵$D_k$，当 N 很大时，存储这个矩阵将变得很耗计算机资源，考虑 N 为 10万 的情形，且 用double型（8字节）来存储$D_k$，需要多大的内存呢？来计算一下：\n\n$$ \\frac{N阶矩阵的字节数}{1GB的字节数} \\qquad = \\qquad \\frac{10^5 \\times 10^5 \\times 8}{2^{10} \\times 2^{10} \\times 2^{10}} \\qquad = \\qquad 74.5(GB) $$\n\n即使考虑到矩阵$D_k$的对称性，内存还可以降一半，但是这对于一般的服务器仍然是难以承受的。况且我们还只是假定了 N=10W 的情况，在实际机器学习问题中，这只能算是中小规模。\n\nL-BFGS(Limited-memory BFGS)算法就是通过对BFGS算法进行改造，从而减少其迭代过程中所需内存开销的算法。\n它对BFGS算法进行了近似，其基本思想是：**不再存储完整的矩阵$D_k$，而是存储计算过程中的向量序列$\\{s_i\\}，\\{y_i\\}$，需要矩阵$D_k$时，利用向量序列$\\{s_i\\}，\\{y_i\\}$的计算来代替。**\n而且，向量序列$\\{s_i\\}，\\{y_i\\}$也不是所有的都存，而是固定存最新的$m$个（参数$m$可由用户根据机器的内存自行指定），每次计算$D_k$时，只利用最新的$m$个$\\{s_i\\}$和$m$个$\\{y_i\\}$，\n显然，这样我们将存储由原来的$O(n^2)$降到了$O(mN)$。\n\n<br>\n\n接下来，讨论L-BFGS算法的具体实现过程，我们的出发点是改进的BFGS算法中的第6步中的迭代式：\n\n$$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $$\n\n若记$ \\rho_k = \\frac{1}{y_k^T s_k}，V_k = I - \\rho_k y_k s_k^T $，则上式可写成\n\n$$ D_{k+1} = V_k^T D_k V_k + \\rho_k s_k s_k^T \\tag{6.*} $$\n\n如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如$D_0 = I$），则可利用$(6.*)$式，依次可得：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_1 &= V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T; \\\\\nD_2 &= V_1^T D_1 V_1 + \\rho_1 s_1 s_1^T \\\\\n&= v_1^T (V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T) V_1 + \\rho_1 s_1 s_1^T \\\\\n&= V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T \\\\\nD_3 &= V_2^T D_2 V_2 + \\rho_2 s_2 s_2^T \\\\\n&= V_2^T (V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T) V_2 + \\rho_2 s_2 s_2^T \\\\\n&= v_2^T V_1^T V_0^T D_0 V_0 V_1 V_2 + V_2^T V_1^T \\rho_0 s_0 s_0^T V_1 V_2 + V_2^T \\rho_1 s_1 s_1^T V_2 + \\rho_2 s_2 s_2^T \\\\\n\\cdots\n\\end{aligned}\n\\end{equation}\n$$\n\n一般地，我们有：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_1^T V_0^T) D_0 (V_0 V_1 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_2^T V_1^T) (\\rho_0 s_0 s_0^T) (V_1 V_2 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_3^T V_2^T) (\\rho_1 s_1 s_1^T) (V_2 V_3 \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.**}\n$$\n\n由上式可以看到，计算$D_{k+1}$需要用到$ \\lbrace s_i, y_i \\rbrace_{i=0}^k $，因此，若从$ s_0, y_0 $开始连续存储 $ m $ 组的话，只能存储到$ s_{m-1}, y_{m-1} $，\n亦即只能依次计算 $ D_1, D_2, \\cdots $，直到$ D_m $，那$ D_{m+1}, D_{m+2} $该如何计算呢？\n\n自然地，如果一定要丢弃一些向量，那么肯定优先考虑那些最早生成的向量，具体来说，计算$D_{m+1}$时，我们保存$ \\lbrace s_i, y_i \\rbrace_{i=1}^m $，丢掉$ \\lbrace s_0, y_0 \\rbrace;$ \n计算$ D_{m+2} $时，我们保存$ \\lbrace s_i, y_i\\rbrace_{i=2}^{m+1} $，丢掉了$ \\lbrace s_i, y_i \\rbrace_{i=0}^1; \\cdots $\n\n但是舍弃掉一些向量后，就只能近似计算了，当$k+1 > m$时，仿照$(6.**)$式，可以构造近似计算公式：\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-m+2}^T V_{k-m+1}^T) D_0 (V_{k-m+1} V_{k-m+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+3}^T V_{k-m+2}^T) (\\rho_0 s_0 s_0^T) (V_{k-m+2} V_{k-m+3} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+4}^T V_{k-m+3}^T) (\\rho_1 s_1 s_1^T) (V_{k-m+3} V_{k-m+4} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.***}\n$$\n\n$ (6.\\*\\*) $ 和 $ (6.\\*\\*\\*) $ 被称为$ special BFGS matrices $，若引入 $ \\hat{m} = min\\lbrace k, m-1 \\rbrace $，则还可以将两式合并成\n\n$$\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+1}^T V_{k-\\hat{m}}^T) D_0 (V_{k-\\hat{m}} V_{k-\\hat{m}+1} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+2}^T V_{k-\\hat{m+1}}^T) (\\rho_0 s_0 s_0^T) (V_{k-\\hat{m}+1} V_{k-\\hat{m}+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+3}^T V_{k-\\hat{m+2}}^T) (\\rho_1 s_1 s_1^T) (V_{k-\\hat{m}+2} V_{k-\\hat{m}+3} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_{k} s_{k} s_{k}^T\n\\end{aligned}\n\\end{equation}\n$$\n\n事实上，由BFGS算法流程易知，$D_k$的作用仅用来计算$ D_k g_k $ 获取搜索方向，因此，若能利用上式设计出一种计算$ D_k g_k $的快速算法，则大功告成。\n\n> **($D_k \\cdot g_k$的快速算法)**\n> \nStep 1 初始化\n$$\n\\delta = \n\\begin{cases}\n0, \\qquad k \\leq m \\\\\nk-m, \\qquad k > m\n\\end{cases};\n\\qquad\nL = \n\\begin{cases}\nk, \\qquad k \\leq m \\\\\nm, \\qquad k > m\n\\end{cases};\n\\qquad\nq_L = g_k\n$$\n>\nStep 2 后向循环\n$$\nFOR \\quad i=L-1, L-2, \\cdots, 1, 0 \\quad DO \\\\\n\\{ \\\\\n\\qquad j=i+\\delta; \\\\\n\\qquad \\alpha_i = \\rho_j s_j^T q_{i+1}; // \\alpha_i 需要保存下来，前向循环要用！！ \\\\\n\\qquad q_i = q_{i+1} - \\alpha_i y_j \\\\\n\\}\n$$\n>\nStep 3 前向循环\n$$\nr_0 = D_0 \\cdot q_0; \\\\\nFOR \\quad i=0,1,\\cdots,L-2,L-1 \\quad DO \\\\\n\\{ \\\\\n\\qquad j = i+\\delta; \\\\\n\\qquad \\beta_j = \\rho_j y_j^T r_i; \\\\\n\\qquad r_{i+1} = r_i + (\\alpha_i - \\beta_i) s_j \\\\\n\\}\n$$\n\n\n最后算出的$r_L$即为$H_k \\cdot g_k $的值。\n\n\n---------\n\n>\n[牛顿法与拟牛顿法学习笔记](https://blog.csdn.net/itplus/article/details/21896453)\n\n","slug":"newstonmethod","published":1,"updated":"2019-08-17T09:37:19.157Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw4002g2qwp1fam1zpe","content":"<p>目录</p><ul>\n<li>1.原始牛顿法</li>\n<li>2.阻尼牛顿法</li>\n<li>3.拟牛顿条件</li>\n<li>4.DFP算法</li>\n<li>5.BFGS算法</li>\n<li>6.L-BFGS算法</li>\n</ul><hr><p>考虑如下无约束的极小化问题</p><script type=\"math/tex; mode=display\">min_x f(x)</script><p>其中 $ x=(x_1, x_2, …, x_n)^T \\in R^n $，由于本文不对收敛性进行讨论，因此对目标函数 $ f: R^n \\rightarrow R $做一个较苛刻的假设。\n这里假定$ f $为凸函数，且二阶连续可微，此外记极小化问题的解为 $ x^{\\ast} $ .</p><hr><h3 id=\"1-原始牛顿法\"><a href=\"#1-原始牛顿法\" class=\"headerlink\" title=\"1.原始牛顿法\"></a>1.原始牛顿法</h3><p>牛顿法的基本思想是：<strong>在现有极小点估计值的附近对 $ f(x) $ 做二阶泰勒展开，进而找到极小点的下一个估计值。</strong></p><p>为简单起见，首先考虑$ n=1 $的情况，设$ x_k $ 为当前的极小点估计值，则</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<script type=\"math/tex; mode=display\">\\phi (x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k) (x-x_k)^2</script><p>表示$ f(x) $ 在 $ x_k $附近的二阶泰勒展开式（略去了关于$ x-x_k $ 的高阶项）。由于求的是最值，由极值必要条件可知，$ \\phi(x) $应该满足</p>\n<script type=\"math/tex; mode=display\">\\phi ' (x) = 0 \\quad \\Rightarrow \\quad f'(x_k) + f^{''}(x_k) (x - x_k) = 0</script><p>从而求得：</p>\n<p>\\[\nx = x_k - \\frac{f’(x_k)}{f^{‘’}(x_k)}\n\\]</p>\n<p>于是，若给定初始值$ x_0 $，则可以构造如下的迭代公式：</p>\n<script type=\"math/tex; mode=display\">x_{k+1} = x_k - \\frac{f'(x_k)}{f^{''}(x_k)}, \\quad k=0,1,...</script><p>产生序列$ \\lbrace x_k \\rbrace $来逼近 $ f(x) $的极小点，在一定条件下，$ \\lbrace x_k \\rbrace $可以收敛到$ f(x) $的极小点。</p>\n<p><br></p>\n<p>对于 $ n&gt;1 $的情形，二阶泰勒展开式可以做推广，此时：</p>\n<script type=\"math/tex; mode=display\">\\phi(X) = f(X_k) + \\nabla f(X_k) \\cdot (X - X_k) + \\frac{1}{2} \\cdot (X - X_k)^T \\nabla^2 f(X_k) \\cdot (X - X_k)</script><p>其中$ \\nabla f $ 为 $ f $的梯度向量，$ \\nabla^2 f $为 $ f $的海森矩阵（Hessian matrix），其定义分别为：</p>\n<script type=\"math/tex; mode=display\">\\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x_1} \\\\\\ \\frac{\\partial f}{\\partial x_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial f}{\\partial x_n}  \\end{matrix} \\right]</script><script type=\"math/tex; mode=display\">\n\\nabla^2 f = \n\\left[\n\\begin{matrix} \n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x^2_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\\n &  &  \\ddots &  \\\\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\\\\\\n\\end{matrix}\n\\right]</script><p>注意，$ \\nabla f $ 和 $ \\nabla^2 f $ 中的元素均为关于$ x $ 的函数，以下分别将其简记为$g$和$H$，特别地，若$f$的混合偏导数可交换次序，则海森矩阵$H$为对称矩阵。\n而$ \\nabla f(X_k) $ 和 $ \\nabla^2 f(X_k) $则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$ g_k $和$H_k$（这里字母$g$表示$gradient$，$H$表示$Hessian$）。</p>\n<p>同样地，由于是求极小点，极值必要条件要求它为$ \\phi (X)$的驻点，即</p>\n<script type=\"math/tex; mode=display\">\\nabla \\phi (X) = 0 \\quad \\Rightarrow \\quad g_k + H_k \\cdot (x - x_k) = 0</script><p>进一步，若矩阵$ H_k $非奇异，则可解得</p>\n<script type=\"math/tex; mode=display\">X = X_k - H_k^{-1} \\cdot g_k</script><p>于是，给定初始值$ X_0 $，则同样可以构造出迭代格式</p>\n<script type=\"math/tex; mode=display\">X_{k+1} = X_k - H_k^{-1} \\cdot g_k, \\quad k=0,1,2...</script><p>这就是原始的牛顿迭代法，其迭代格式中的搜索方向$ d_k = - H_k^{-1} \\cdot  g_k $称为<strong>牛顿方向</strong>，下面给出牛顿法的完整算法描述。</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$ x_0 $和精度阈值$ \\epsilon $，并令$ k=0$；</li>\n<li>计算$ g_k $和$ H_k $；</li>\n<li>若$ || gk || &lt; \\epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \\cdot g_k $；</li>\n<li>计算新的迭代点 $ x_{k+1} = x_k + d_k $；</li>\n<li>令$ k = k+1 $，转至第2步。 </li>\n</ol>\n<p>原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$ f(x_{k+1} &gt; f(x_k) $的情况，\n这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$ \\lbrace x_k \\rbrace $的发散而导致计算失败。</p>\n<hr>\n<h3 id=\"2-阻尼牛顿法\"><a href=\"#2-阻尼牛顿法\" class=\"headerlink\" title=\"2.阻尼牛顿法\"></a>2.阻尼牛顿法</h3><p>为消除原始牛顿法中的弊端，提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍采用 $ d_k $，但每次迭代需沿此方向做一维搜索(line search)，寻求最优的步长因子$ \\lambda_k $，即</p>\n<script type=\"math/tex; mode=display\">\\lambda_k = \\arg\\min_{\\lambda \\in R} f(x_k + \\lambda d_k) \\tag{*}</script><p>下面给出一个阻尼牛顿法的完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$ x_0 $和精度阈值$ \\epsilon $，并令$ k=0$；</li>\n<li>计算$ g_k $和$ H_k $；</li>\n<li>若$ || gk || &lt; \\epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \\cdot g_k $；</li>\n<li>利用$(*)$式得到步长$ \\lambda_k $，并令 $ x_{k+1} = x_k + \\lambda_k d_k $；</li>\n<li>令$ k = k+1 $，转至第2步。 </li>\n</ol>\n<p><strong>小结</strong></p>\n<p>牛顿法是梯度(下降)发的进一步发展，梯度法利用目标函数的一阶偏导数信息，以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；\n而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛。</p>\n<p>但是牛顿法主要存在以下两个缺点：</p>\n<ol>\n<li>对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定；</li>\n<li>计算相当复杂，除需计算梯度以外，还需计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$ n $的平方比增加，当$n$很大时这个问题尤为突出。</li>\n</ol>\n<hr>\n<h3 id=\"3-拟牛顿条件\"><a href=\"#3-拟牛顿条件\" class=\"headerlink\" title=\"3.拟牛顿条件\"></a>3.拟牛顿条件</h3><p>上面说到了牛顿法的两个缺点，为了客服这两个问题，人们提出了拟牛顿法，这个方法的基本思想是：<strong>不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数。</strong></p>\n<p>不同的构造方法就产生了不同的拟牛顿法，都是对牛顿法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。</p>\n<p>下面先推到拟牛顿条件，或者叫拟牛顿方程、割线条件，它是用来提供理论指导，指出了用来近似的矩阵应该满足的条件。</p>\n<p>为了明确起见，下文中用$ B $ 表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$ H^{-1} $的近似，即 $ B \\approx H, D \\approx H^{-1} $。</p>\n<p><br></p>\n<p>假设经过 $ k+1 $ 次迭代后得到 $ x<em>{k+1} $，此时将目标函数 $ f(x) $ 在 $ x</em>{k+1} $附近作泰勒展开，取二阶近似，得到：</p>\n<script type=\"math/tex; mode=display\">f(x) \\approx f(x_{k+1} + \\nabla f(x_{k+1}) \\cdot (x - x_{k+1}) + \\frac{1}{2} \\cdot (x - x_{k+1}^T \\cdot \\nabla^2 f(x_{k+1}) \\cdot (x - x_{k+1}) \\tag{**}</script><p>在$ (**) $式两边同时作用一个梯度算子 $ \\nabla $， 可得</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) \\approx \\nabla f(x_{k+1}) + H_{k+1} \\cdot (x - x_{k+1}) \\tag{***}</script><p>在$ (<em>*</em>) $式中取$ x = x_k$，并整理可得：</p>\n<script type=\"math/tex; mode=display\">g_{k+1} - g_k = H_{k+1} \\cdot ( x_{k+1} - x_k )</script><p>引入新的记号：</p>\n<script type=\"math/tex; mode=display\">s_k = x_{k+1} - x_k, \\quad y_k = g_{k+1} - g_k</script><p>则上式可以重新表示为：</p>\n<script type=\"math/tex; mode=display\">y_k \\approx H_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k \\approx H_{k+1}^{-1} \\cdot y_k</script><p>这就是所谓的<strong>拟牛顿条件</strong>，它对迭代过程中的海森矩阵$ H<em>{k+1}$作约束，因此，对$ H</em>{k+1} $做近似的$B<em>{k+1}$，以及对$ H</em>{k+1}^{-1} $做近似的$  D_{k+1}$ 可以将</p>\n<script type=\"math/tex; mode=display\">y_k = B_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k = D_{k+1} \\cdot y_k</script><p>作为指导。</p>\n<hr>\n<h3 id=\"4-DFP算法\"><a href=\"#4-DFP算法\" class=\"headerlink\" title=\"4.DFP算法\"></a>4.DFP算法</h3><p>该算法的核心是：<strong>通过迭代的方法，对$ H_{k+1}^{-1}$做近似，迭代格式为：</strong></p>\n<script type=\"math/tex; mode=display\">D_{k+1} = D_k + \\Delta D_k, \\quad k=0,1,2...</script><p>其中$ D_0$通常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$ \\Delta D_k$如何构造。</p>\n<p>这里采用“待定法”，即首先将$\\Delta D_k$待定成某种形式，然后结合拟牛顿条件来推到。这里我们将$ \\Delta D_k $ 待定为：</p>\n<script type=\"math/tex; mode=display\">\\Delta D_k = \\alpha u u^T + \\beta v v^T</script><p>其中$ \\alpha, \\beta$为待定系数，$ u, v \\in R^n $为待定向量，从形式上来看，这种待定公式至少保证了矩阵$ \\Delta D_k $的对称性（因为$uu^T$和$vv^T$都是对称矩阵）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nD_{k+1} = D_k + \\Delta D_k \\\\\n\\Delta D_k = \\alpha u u^T + \\beta v v^T \\\\\ns_k = D_{k+1} \\cdot y_k \\\\\n\\end{cases}\n\\Longleftrightarrow\ns_k = D_k y_k + \\alpha u u^T y_k + \\beta v v^T y_k</script><p>调整上面的结论：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\ns_k\n& = D_k y_k + u (\\alpha u^T y_k) + v(\\beta v^T y_k) \\\\\n& = D_k y_k + (\\alpha u^T y_k) u + (\\beta v^T y_k) v\n\\end{aligned}\n\\end{equation}\n\\tag{+}</script><p>括号中的$\\alpha u^T y_k$和$\\beta v^T y_k$是两个数，既然是数，我们不妨作如下简单赋值：</p>\n<script type=\"math/tex; mode=display\">\\alpha u^T y_k = 1, \\qquad \\beta v^T y_k = -1  \\tag{++}</script><p>即</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{u^T y_k}, \\qquad \\beta = - \\frac{1}{v^T y_k}</script><p>将$ (++) $式代入 $ (+) $，得到：</p>\n<script type=\"math/tex; mode=display\">u - v = s_k - D_k y_k</script><p>要使上式成立，不妨就直接取</p>\n<script type=\"math/tex; mode=display\">u = s_k, \\qquad v = D_k y_k</script><p>再将$(21)$代入$(19)$，便得</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{s_k^T y_k}, \\qquad \\beta = - \\frac{1}{(D_k y_k)^T y_k} = - \\frac{1}{y_k^T D_k y_k}</script><p>其中第二个等式用到了$D_k$的对称性。</p>\n<p>至此，我们已经可以将校正矩阵$ \\Delta D_k $构造出来了，</p>\n<script type=\"math/tex; mode=display\">\\Delta D_k = \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k}</script><p>综上，我们给出DFP算法的一个完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - D_k \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ D_{k+1} = D_k + \\Delta D_k = D_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<hr>\n<h3 id=\"5-BFGS算法\"><a href=\"#5-BFGS算法\" class=\"headerlink\" title=\"5.BFGS算法\"></a>5.BFGS算法</h3><p>与DFP算法相比，BFGS算法性能更佳，目前它已成为求解无约束非线性优化问题最常用的方法之一。\nBFGS算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果。</p>\n<p>BFGS算法中核心公式的推到和DFP算法完全类似，只是互换了其中$s_k$和$y_k$的位置。\n需要注意的是，BFGS算法是直接逼近海森矩阵，即$ B_k \\approx H_k$，仍采用迭代方法，设迭代格式为：</p>\n<script type=\"math/tex; mode=display\">B_{k+1} = B_k + \\Delta B_k, \\qquad k=0,1,2...</script><p>其中的 $ B_0 $也常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$\\Delta B_k$如何构造。同样，将其待定为：</p>\n<script type=\"math/tex; mode=display\">\\Delta B_k = \\alpha u u^T + \\beta v v^T</script><p>类比DFP算法，可得</p>\n<script type=\"math/tex; mode=display\">y_k = B_k s_k + (\\alpha u^T s_k)u + (\\beta v^T s_k) v</script><p>通过令 $ \\alpha u^T s_k = 1,\\qquad \\beta v^T s_k = -1 $，以及 $ u = y_k, \\qquad v = B_k s_k $，可以算得：</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{y_k^T s_k}, \\qquad \\beta = - \\frac{1}{s_k^T B_k s_k}</script><p>综上，便得到了如下校正矩阵$ \\Delta B_k $的公式：</p>\n<script type=\"math/tex; mode=display\">\\Delta B_k = \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}</script><p>先讲把矩阵$ \\Delta B_k $ 和 $ \\Delta D_k $拿出来对比一下，是不是除了将$D$换成$B$外，其他只是将$s_k$和$y_k$互调了一下位置呢？</p>\n<p>最后，我们给出BFGS算法的一个完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ B_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - B_k^{-1} \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ B_{k+1} = B_k + \\Delta B_k = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<p>上面算法中的第2步通常是通过求解线性代数方程组$ B<em>k d_k = -g_k $来进行，然而更一般的做法是通过对第6步中的递推关系，\n应用$Sherman-Morrison$公式，直接给出$B</em>{k+1}^{-1}$和$B_k^{-1}$之间的关系：</p>\n<script type=\"math/tex; mode=display\">B_{k+1}^{-1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) B_k^{-1} \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k}</script><p>利用上式，我们很容易将上述BFGS算法进行改进，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$（这样做仅仅只是符号上看起来舒服起见）。\n这样，整个算法中不再需要求解线性代数方程组，由矩阵-向量运算就可以完成了。</p>\n<p>改进后的BFGS算法如下：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - D_k \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<p>至此，关于DFP算法和BFGS算法的介绍就完成了。</p>\n<p>最后再补充一下一维搜索(line search)的问题，在之前几个算法描述中，为简单起见，均采用了$(*)$时来计算步长$ \\lambda_k $，其实这是一种精确搜索；\n实际应用中，还有像Wolfe型搜索、Armijo搜索以及满足Goldstein条件的非精确搜索。这里我们以Wolfe搜索为例，简单介绍。</p>\n<p>设$ \\tilde{\\beta} \\in (0, \\frac{1}{2}), \\beta \\in (\\tilde{\\beta}, 1)$，所谓的Wolfe搜索是指$ \\lambda_k $满足如下<strong>Wolfe条件</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nf(x_k + \\lambda_k d_k) \\qquad \\leq f(x_k) + \\tilde{\\beta} \\lambda_k d_k^T g_k; \\\\\nd_k^T g(x_k + \\lambda_k d_k) \\geq \\beta d_k^T g_k\n\\end{cases}</script><p>带非精确搜索的拟牛顿法的研究是从1976年Powell的工作开始的，他证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。</p>\n<hr>\n<h3 id=\"6-L-BFGS算法\"><a href=\"#6-L-BFGS算法\" class=\"headerlink\" title=\"6.L-BFGS算法\"></a>6.L-BFGS算法</h3><p>在BFGS算法中，需要用到一个$NxN$的矩阵$D_k$，当 N 很大时，存储这个矩阵将变得很耗计算机资源，考虑 N 为 10万 的情形，且 用double型（8字节）来存储$D_k$，需要多大的内存呢？来计算一下：</p>\n<script type=\"math/tex; mode=display\">\\frac{N阶矩阵的字节数}{1GB的字节数} \\qquad = \\qquad \\frac{10^5 \\times 10^5 \\times 8}{2^{10} \\times 2^{10} \\times 2^{10}} \\qquad = \\qquad 74.5(GB)</script><p>即使考虑到矩阵$D_k$的对称性，内存还可以降一半，但是这对于一般的服务器仍然是难以承受的。况且我们还只是假定了 N=10W 的情况，在实际机器学习问题中，这只能算是中小规模。</p>\n<p>L-BFGS(Limited-memory BFGS)算法就是通过对BFGS算法进行改造，从而减少其迭代过程中所需内存开销的算法。\n它对BFGS算法进行了近似，其基本思想是：<strong>不再存储完整的矩阵$D_k$，而是存储计算过程中的向量序列${s_i}，{y_i}$，需要矩阵$D_k$时，利用向量序列${s_i}，{y_i}$的计算来代替。</strong>\n而且，向量序列${s_i}，{y_i}$也不是所有的都存，而是固定存最新的$m$个（参数$m$可由用户根据机器的内存自行指定），每次计算$D_k$时，只利用最新的$m$个${s_i}$和$m$个${y_i}$，\n显然，这样我们将存储由原来的$O(n^2)$降到了$O(mN)$。</p>\n<p><br></p>\n<p>接下来，讨论L-BFGS算法的具体实现过程，我们的出发点是改进的BFGS算法中的第6步中的迭代式：</p>\n<script type=\"math/tex; mode=display\">D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k}</script><p>若记$ \\rho_k = \\frac{1}{y_k^T s_k}，V_k = I - \\rho_k y_k s_k^T $，则上式可写成</p>\n<script type=\"math/tex; mode=display\">D_{k+1} = V_k^T D_k V_k + \\rho_k s_k s_k^T \\tag{6.*}</script><p>如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如$D_0 = I$），则可利用$(6.*)$式，依次可得：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_1 &= V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T; \\\\\nD_2 &= V_1^T D_1 V_1 + \\rho_1 s_1 s_1^T \\\\\n&= v_1^T (V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T) V_1 + \\rho_1 s_1 s_1^T \\\\\n&= V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T \\\\\nD_3 &= V_2^T D_2 V_2 + \\rho_2 s_2 s_2^T \\\\\n&= V_2^T (V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T) V_2 + \\rho_2 s_2 s_2^T \\\\\n&= v_2^T V_1^T V_0^T D_0 V_0 V_1 V_2 + V_2^T V_1^T \\rho_0 s_0 s_0^T V_1 V_2 + V_2^T \\rho_1 s_1 s_1^T V_2 + \\rho_2 s_2 s_2^T \\\\\n\\cdots\n\\end{aligned}\n\\end{equation}</script><p>一般地，我们有：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_1^T V_0^T) D_0 (V_0 V_1 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_2^T V_1^T) (\\rho_0 s_0 s_0^T) (V_1 V_2 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_3^T V_2^T) (\\rho_1 s_1 s_1^T) (V_2 V_3 \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.**}</script><p>由上式可以看到，计算$D<em>{k+1}$需要用到$ \\lbrace s_i, y_i \\rbrace</em>{i=0}^k $，因此，若从$ s<em>0, y_0 $开始连续存储 $ m $ 组的话，只能存储到$ s</em>{m-1}, y<em>{m-1} $，\n亦即只能依次计算 $ D_1, D_2, \\cdots $，直到$ D_m $，那$ D</em>{m+1}, D_{m+2} $该如何计算呢？</p>\n<p>自然地，如果一定要丢弃一些向量，那么肯定优先考虑那些最早生成的向量，具体来说，计算$D<em>{m+1}$时，我们保存$ \\lbrace s_i, y_i \\rbrace</em>{i=1}^m $，丢掉$ \\lbrace s<em>0, y_0 \\rbrace;$ \n计算$ D</em>{m+2} $时，我们保存$ \\lbrace s<em>i, y_i\\rbrace</em>{i=2}^{m+1} $，丢掉了$ \\lbrace s<em>i, y_i \\rbrace</em>{i=0}^1; \\cdots $</p>\n<p>但是舍弃掉一些向量后，就只能近似计算了，当$k+1 &gt; m$时，仿照$(6.**)$式，可以构造近似计算公式：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-m+2}^T V_{k-m+1}^T) D_0 (V_{k-m+1} V_{k-m+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+3}^T V_{k-m+2}^T) (\\rho_0 s_0 s_0^T) (V_{k-m+2} V_{k-m+3} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+4}^T V_{k-m+3}^T) (\\rho_1 s_1 s_1^T) (V_{k-m+3} V_{k-m+4} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.***}</script><p>$ (6.**) $ 和 $ (6.***) $ 被称为$ special BFGS matrices $，若引入 $ \\hat{m} = min\\lbrace k, m-1 \\rbrace $，则还可以将两式合并成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+1}^T V_{k-\\hat{m}}^T) D_0 (V_{k-\\hat{m}} V_{k-\\hat{m}+1} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+2}^T V_{k-\\hat{m+1}}^T) (\\rho_0 s_0 s_0^T) (V_{k-\\hat{m}+1} V_{k-\\hat{m}+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+3}^T V_{k-\\hat{m+2}}^T) (\\rho_1 s_1 s_1^T) (V_{k-\\hat{m}+2} V_{k-\\hat{m}+3} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_{k} s_{k} s_{k}^T\n\\end{aligned}\n\\end{equation}</script><p>事实上，由BFGS算法流程易知，$D_k$的作用仅用来计算$ D_k g_k $ 获取搜索方向，因此，若能利用上式设计出一种计算$ D_k g_k $的快速算法，则大功告成。</p>\n<blockquote>\n<p><strong>($D_k \\cdot g_k$的快速算法)</strong></p>\n<p>Step 1 初始化</p>\n<script type=\"math/tex; mode=display\">\n\\delta = \n\\begin{cases}\n0, \\qquad k \\leq m \\\\\nk-m, \\qquad k > m\n\\end{cases};\n\\qquad\nL = \n\\begin{cases}\nk, \\qquad k \\leq m \\\\\nm, \\qquad k > m\n\\end{cases};\n\\qquad\nq_L = g_k</script><p>Step 2 后向循环</p>\n<script type=\"math/tex; mode=display\">\nFOR \\quad i=L-1, L-2, \\cdots, 1, 0 \\quad DO \\\\\n\\{ \\\\\n\\qquad j=i+\\delta; \\\\\n\\qquad \\alpha_i = \\rho_j s_j^T q_{i+1}; // \\alpha_i 需要保存下来，前向循环要用！！ \\\\\n\\qquad q_i = q_{i+1} - \\alpha_i y_j \\\\\n\\}</script><p>Step 3 前向循环</p>\n<script type=\"math/tex; mode=display\">\nr_0 = D_0 \\cdot q_0; \\\\\nFOR \\quad i=0,1,\\cdots,L-2,L-1 \\quad DO \\\\\n\\{ \\\\\n\\qquad j = i+\\delta; \\\\\n\\qquad \\beta_j = \\rho_j y_j^T r_i; \\\\\n\\qquad r_{i+1} = r_i + (\\alpha_i - \\beta_i) s_j \\\\\n\\}</script></blockquote>\n<p>最后算出的$r_L$即为$H_k \\cdot g_k $的值。</p>\n<hr>\n<p>&gt;\n<a href=\"https://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">牛顿法与拟牛顿法学习笔记</a></p>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>1.原始牛顿法</li>\n<li>2.阻尼牛顿法</li>\n<li>3.拟牛顿条件</li>\n<li>4.DFP算法</li>\n<li>5.BFGS算法</li>\n<li>6.L-BFGS算法</li>\n</ul><hr><p>考虑如下无约束的极小化问题</p><script type=\"math/tex; mode=display\">min_x f(x)</script><p>其中 $ x=(x_1, x_2, …, x_n)^T \\in R^n $，由于本文不对收敛性进行讨论，因此对目标函数 $ f: R^n \\rightarrow R $做一个较苛刻的假设。\n这里假定$ f $为凸函数，且二阶连续可微，此外记极小化问题的解为 $ x^{\\ast} $ .</p><hr><h3 id=\"1-原始牛顿法\"><a href=\"#1-原始牛顿法\" class=\"headerlink\" title=\"1.原始牛顿法\"></a>1.原始牛顿法</h3><p>牛顿法的基本思想是：<strong>在现有极小点估计值的附近对 $ f(x) $ 做二阶泰勒展开，进而找到极小点的下一个估计值。</strong></p><p>为简单起见，首先考虑$ n=1 $的情况，设$ x_k $ 为当前的极小点估计值，则</p>","more":"\n\n\n\n\n\n\n\n<script type=\"math/tex; mode=display\">\\phi (x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k) (x-x_k)^2</script><p>表示$ f(x) $ 在 $ x_k $附近的二阶泰勒展开式（略去了关于$ x-x_k $ 的高阶项）。由于求的是最值，由极值必要条件可知，$ \\phi(x) $应该满足</p>\n<script type=\"math/tex; mode=display\">\\phi ' (x) = 0 \\quad \\Rightarrow \\quad f'(x_k) + f^{''}(x_k) (x - x_k) = 0</script><p>从而求得：</p>\n<p>\\[\nx = x_k - \\frac{f’(x_k)}{f^{‘’}(x_k)}\n\\]</p>\n<p>于是，若给定初始值$ x_0 $，则可以构造如下的迭代公式：</p>\n<script type=\"math/tex; mode=display\">x_{k+1} = x_k - \\frac{f'(x_k)}{f^{''}(x_k)}, \\quad k=0,1,...</script><p>产生序列$ \\lbrace x_k \\rbrace $来逼近 $ f(x) $的极小点，在一定条件下，$ \\lbrace x_k \\rbrace $可以收敛到$ f(x) $的极小点。</p>\n<p><br></p>\n<p>对于 $ n&gt;1 $的情形，二阶泰勒展开式可以做推广，此时：</p>\n<script type=\"math/tex; mode=display\">\\phi(X) = f(X_k) + \\nabla f(X_k) \\cdot (X - X_k) + \\frac{1}{2} \\cdot (X - X_k)^T \\nabla^2 f(X_k) \\cdot (X - X_k)</script><p>其中$ \\nabla f $ 为 $ f $的梯度向量，$ \\nabla^2 f $为 $ f $的海森矩阵（Hessian matrix），其定义分别为：</p>\n<script type=\"math/tex; mode=display\">\\nabla f = \\left[ \\begin{matrix} \\frac{\\partial f}{\\partial x_1} \\\\\\ \\frac{\\partial f}{\\partial x_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial f}{\\partial x_n}  \\end{matrix} \\right]</script><script type=\"math/tex; mode=display\">\n\\nabla^2 f = \n\\left[\n\\begin{matrix} \n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x^2_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\\\n &  &  \\ddots &  \\\\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &  \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\\\\\\n\\end{matrix}\n\\right]</script><p>注意，$ \\nabla f $ 和 $ \\nabla^2 f $ 中的元素均为关于$ x $ 的函数，以下分别将其简记为$g$和$H$，特别地，若$f$的混合偏导数可交换次序，则海森矩阵$H$为对称矩阵。\n而$ \\nabla f(X_k) $ 和 $ \\nabla^2 f(X_k) $则表示将$X$取为$X_k$后得到的实值向量和矩阵，以下分别将其简记为$ g_k $和$H_k$（这里字母$g$表示$gradient$，$H$表示$Hessian$）。</p>\n<p>同样地，由于是求极小点，极值必要条件要求它为$ \\phi (X)$的驻点，即</p>\n<script type=\"math/tex; mode=display\">\\nabla \\phi (X) = 0 \\quad \\Rightarrow \\quad g_k + H_k \\cdot (x - x_k) = 0</script><p>进一步，若矩阵$ H_k $非奇异，则可解得</p>\n<script type=\"math/tex; mode=display\">X = X_k - H_k^{-1} \\cdot g_k</script><p>于是，给定初始值$ X_0 $，则同样可以构造出迭代格式</p>\n<script type=\"math/tex; mode=display\">X_{k+1} = X_k - H_k^{-1} \\cdot g_k, \\quad k=0,1,2...</script><p>这就是原始的牛顿迭代法，其迭代格式中的搜索方向$ d_k = - H_k^{-1} \\cdot  g_k $称为<strong>牛顿方向</strong>，下面给出牛顿法的完整算法描述。</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$ x_0 $和精度阈值$ \\epsilon $，并令$ k=0$；</li>\n<li>计算$ g_k $和$ H_k $；</li>\n<li>若$ || gk || &lt; \\epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \\cdot g_k $；</li>\n<li>计算新的迭代点 $ x_{k+1} = x_k + d_k $；</li>\n<li>令$ k = k+1 $，转至第2步。 </li>\n</ol>\n<p>原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$ f(x_{k+1} &gt; f(x_k) $的情况，\n这表明原始牛顿法不能保证函数值稳定地下降，在严重的情况下甚至可能造成迭代点列$ \\lbrace x_k \\rbrace $的发散而导致计算失败。</p>\n<hr>\n<h3 id=\"2-阻尼牛顿法\"><a href=\"#2-阻尼牛顿法\" class=\"headerlink\" title=\"2.阻尼牛顿法\"></a>2.阻尼牛顿法</h3><p>为消除原始牛顿法中的弊端，提出了“阻尼牛顿法”，阻尼牛顿法每次迭代的方向仍采用 $ d_k $，但每次迭代需沿此方向做一维搜索(line search)，寻求最优的步长因子$ \\lambda_k $，即</p>\n<script type=\"math/tex; mode=display\">\\lambda_k = \\arg\\min_{\\lambda \\in R} f(x_k + \\lambda d_k) \\tag{*}</script><p>下面给出一个阻尼牛顿法的完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$ x_0 $和精度阈值$ \\epsilon $，并令$ k=0$；</li>\n<li>计算$ g_k $和$ H_k $；</li>\n<li>若$ || gk || &lt; \\epsilon $，则停止迭代；否则确定搜索方向$ d_k = -H_k^{-1} \\cdot g_k $；</li>\n<li>利用$(*)$式得到步长$ \\lambda_k $，并令 $ x_{k+1} = x_k + \\lambda_k d_k $；</li>\n<li>令$ k = k+1 $，转至第2步。 </li>\n</ol>\n<p><strong>小结</strong></p>\n<p>牛顿法是梯度(下降)发的进一步发展，梯度法利用目标函数的一阶偏导数信息，以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；\n而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛。</p>\n<p>但是牛顿法主要存在以下两个缺点：</p>\n<ol>\n<li>对目标函数有较严格的要求，函数必须具有连续的一、二阶偏导数，海森矩阵必须正定；</li>\n<li>计算相当复杂，除需计算梯度以外，还需计算二阶偏导数矩阵和它的逆矩阵。计算量、存储量均很大，且均以维数$ n $的平方比增加，当$n$很大时这个问题尤为突出。</li>\n</ol>\n<hr>\n<h3 id=\"3-拟牛顿条件\"><a href=\"#3-拟牛顿条件\" class=\"headerlink\" title=\"3.拟牛顿条件\"></a>3.拟牛顿条件</h3><p>上面说到了牛顿法的两个缺点，为了客服这两个问题，人们提出了拟牛顿法，这个方法的基本思想是：<strong>不用二阶偏导数而构造出可以近似海森矩阵（或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数。</strong></p>\n<p>不同的构造方法就产生了不同的拟牛顿法，都是对牛顿法中用来计算搜索方向的海森矩阵（或海森矩阵的逆）作了近似计算罢了。</p>\n<p>下面先推到拟牛顿条件，或者叫拟牛顿方程、割线条件，它是用来提供理论指导，指出了用来近似的矩阵应该满足的条件。</p>\n<p>为了明确起见，下文中用$ B $ 表示对海森矩阵$H$本身的近似，而用$D$表示对海森矩阵的逆$ H^{-1} $的近似，即 $ B \\approx H, D \\approx H^{-1} $。</p>\n<p><br></p>\n<p>假设经过 $ k+1 $ 次迭代后得到 $ x<em>{k+1} $，此时将目标函数 $ f(x) $ 在 $ x</em>{k+1} $附近作泰勒展开，取二阶近似，得到：</p>\n<script type=\"math/tex; mode=display\">f(x) \\approx f(x_{k+1} + \\nabla f(x_{k+1}) \\cdot (x - x_{k+1}) + \\frac{1}{2} \\cdot (x - x_{k+1}^T \\cdot \\nabla^2 f(x_{k+1}) \\cdot (x - x_{k+1}) \\tag{**}</script><p>在$ (**) $式两边同时作用一个梯度算子 $ \\nabla $， 可得</p>\n<script type=\"math/tex; mode=display\">\\nabla f(x) \\approx \\nabla f(x_{k+1}) + H_{k+1} \\cdot (x - x_{k+1}) \\tag{***}</script><p>在$ (<em>*</em>) $式中取$ x = x_k$，并整理可得：</p>\n<script type=\"math/tex; mode=display\">g_{k+1} - g_k = H_{k+1} \\cdot ( x_{k+1} - x_k )</script><p>引入新的记号：</p>\n<script type=\"math/tex; mode=display\">s_k = x_{k+1} - x_k, \\quad y_k = g_{k+1} - g_k</script><p>则上式可以重新表示为：</p>\n<script type=\"math/tex; mode=display\">y_k \\approx H_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k \\approx H_{k+1}^{-1} \\cdot y_k</script><p>这就是所谓的<strong>拟牛顿条件</strong>，它对迭代过程中的海森矩阵$ H<em>{k+1}$作约束，因此，对$ H</em>{k+1} $做近似的$B<em>{k+1}$，以及对$ H</em>{k+1}^{-1} $做近似的$  D_{k+1}$ 可以将</p>\n<script type=\"math/tex; mode=display\">y_k = B_{k+1} \\cdot s_k \\qquad \\Longleftrightarrow \\qquad s_k = D_{k+1} \\cdot y_k</script><p>作为指导。</p>\n<hr>\n<h3 id=\"4-DFP算法\"><a href=\"#4-DFP算法\" class=\"headerlink\" title=\"4.DFP算法\"></a>4.DFP算法</h3><p>该算法的核心是：<strong>通过迭代的方法，对$ H_{k+1}^{-1}$做近似，迭代格式为：</strong></p>\n<script type=\"math/tex; mode=display\">D_{k+1} = D_k + \\Delta D_k, \\quad k=0,1,2...</script><p>其中$ D_0$通常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$ \\Delta D_k$如何构造。</p>\n<p>这里采用“待定法”，即首先将$\\Delta D_k$待定成某种形式，然后结合拟牛顿条件来推到。这里我们将$ \\Delta D_k $ 待定为：</p>\n<script type=\"math/tex; mode=display\">\\Delta D_k = \\alpha u u^T + \\beta v v^T</script><p>其中$ \\alpha, \\beta$为待定系数，$ u, v \\in R^n $为待定向量，从形式上来看，这种待定公式至少保证了矩阵$ \\Delta D_k $的对称性（因为$uu^T$和$vv^T$都是对称矩阵）</p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nD_{k+1} = D_k + \\Delta D_k \\\\\n\\Delta D_k = \\alpha u u^T + \\beta v v^T \\\\\ns_k = D_{k+1} \\cdot y_k \\\\\n\\end{cases}\n\\Longleftrightarrow\ns_k = D_k y_k + \\alpha u u^T y_k + \\beta v v^T y_k</script><p>调整上面的结论：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\ns_k\n& = D_k y_k + u (\\alpha u^T y_k) + v(\\beta v^T y_k) \\\\\n& = D_k y_k + (\\alpha u^T y_k) u + (\\beta v^T y_k) v\n\\end{aligned}\n\\end{equation}\n\\tag{+}</script><p>括号中的$\\alpha u^T y_k$和$\\beta v^T y_k$是两个数，既然是数，我们不妨作如下简单赋值：</p>\n<script type=\"math/tex; mode=display\">\\alpha u^T y_k = 1, \\qquad \\beta v^T y_k = -1  \\tag{++}</script><p>即</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{u^T y_k}, \\qquad \\beta = - \\frac{1}{v^T y_k}</script><p>将$ (++) $式代入 $ (+) $，得到：</p>\n<script type=\"math/tex; mode=display\">u - v = s_k - D_k y_k</script><p>要使上式成立，不妨就直接取</p>\n<script type=\"math/tex; mode=display\">u = s_k, \\qquad v = D_k y_k</script><p>再将$(21)$代入$(19)$，便得</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{s_k^T y_k}, \\qquad \\beta = - \\frac{1}{(D_k y_k)^T y_k} = - \\frac{1}{y_k^T D_k y_k}</script><p>其中第二个等式用到了$D_k$的对称性。</p>\n<p>至此，我们已经可以将校正矩阵$ \\Delta D_k $构造出来了，</p>\n<script type=\"math/tex; mode=display\">\\Delta D_k = \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k}</script><p>综上，我们给出DFP算法的一个完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - D_k \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ D_{k+1} = D_k + \\Delta D_k = D_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{D_k y_k y_k^T D_k}{y_k^T D_k y_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<hr>\n<h3 id=\"5-BFGS算法\"><a href=\"#5-BFGS算法\" class=\"headerlink\" title=\"5.BFGS算法\"></a>5.BFGS算法</h3><p>与DFP算法相比，BFGS算法性能更佳，目前它已成为求解无约束非线性优化问题最常用的方法之一。\nBFGS算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果。</p>\n<p>BFGS算法中核心公式的推到和DFP算法完全类似，只是互换了其中$s_k$和$y_k$的位置。\n需要注意的是，BFGS算法是直接逼近海森矩阵，即$ B_k \\approx H_k$，仍采用迭代方法，设迭代格式为：</p>\n<script type=\"math/tex; mode=display\">B_{k+1} = B_k + \\Delta B_k, \\qquad k=0,1,2...</script><p>其中的 $ B_0 $也常取为单位矩阵$I$，因此，关键是每一步的校正矩阵$\\Delta B_k$如何构造。同样，将其待定为：</p>\n<script type=\"math/tex; mode=display\">\\Delta B_k = \\alpha u u^T + \\beta v v^T</script><p>类比DFP算法，可得</p>\n<script type=\"math/tex; mode=display\">y_k = B_k s_k + (\\alpha u^T s_k)u + (\\beta v^T s_k) v</script><p>通过令 $ \\alpha u^T s_k = 1,\\qquad \\beta v^T s_k = -1 $，以及 $ u = y_k, \\qquad v = B_k s_k $，可以算得：</p>\n<script type=\"math/tex; mode=display\">\\alpha = \\frac{1}{y_k^T s_k}, \\qquad \\beta = - \\frac{1}{s_k^T B_k s_k}</script><p>综上，便得到了如下校正矩阵$ \\Delta B_k $的公式：</p>\n<script type=\"math/tex; mode=display\">\\Delta B_k = \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}</script><p>先讲把矩阵$ \\Delta B_k $ 和 $ \\Delta D_k $拿出来对比一下，是不是除了将$D$换成$B$外，其他只是将$s_k$和$y_k$互调了一下位置呢？</p>\n<p>最后，我们给出BFGS算法的一个完整算法描述：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ B_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - B_k^{-1} \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ B_{k+1} = B_k + \\Delta B_k = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<p>上面算法中的第2步通常是通过求解线性代数方程组$ B<em>k d_k = -g_k $来进行，然而更一般的做法是通过对第6步中的递推关系，\n应用$Sherman-Morrison$公式，直接给出$B</em>{k+1}^{-1}$和$B_k^{-1}$之间的关系：</p>\n<script type=\"math/tex; mode=display\">B_{k+1}^{-1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) B_k^{-1} \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k}</script><p>利用上式，我们很容易将上述BFGS算法进行改进，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$（这样做仅仅只是符号上看起来舒服起见）。\n这样，整个算法中不再需要求解线性代数方程组，由矩阵-向量运算就可以完成了。</p>\n<p>改进后的BFGS算法如下：</p>\n<p>&gt;</p>\n<ol>\n<li>给定初值$x_0$和精度阈值$\\epsilon$，并令$ D_0=I, \\quad k=0$，</li>\n<li>确定搜索方向$ d_k = - D_k \\cdot g_k $，</li>\n<li>利用$(*)$式得到步长$\\lambda<em>k$，令$ s_k = \\lambda_k d_k, \\quad x</em>{k+1}=x_k + s_k $，</li>\n<li>若$ || g_{k+1} || &lt; \\epsilon$，则算法结束，</li>\n<li>计算 $ y<em>k = g</em>{k+1} - g_k $，</li>\n<li>计算$ D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k} $，</li>\n<li>令$ k = k+1$，转至第2步。 </li>\n</ol>\n<p>至此，关于DFP算法和BFGS算法的介绍就完成了。</p>\n<p>最后再补充一下一维搜索(line search)的问题，在之前几个算法描述中，为简单起见，均采用了$(*)$时来计算步长$ \\lambda_k $，其实这是一种精确搜索；\n实际应用中，还有像Wolfe型搜索、Armijo搜索以及满足Goldstein条件的非精确搜索。这里我们以Wolfe搜索为例，简单介绍。</p>\n<p>设$ \\tilde{\\beta} \\in (0, \\frac{1}{2}), \\beta \\in (\\tilde{\\beta}, 1)$，所谓的Wolfe搜索是指$ \\lambda_k $满足如下<strong>Wolfe条件</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{cases}\nf(x_k + \\lambda_k d_k) \\qquad \\leq f(x_k) + \\tilde{\\beta} \\lambda_k d_k^T g_k; \\\\\nd_k^T g(x_k + \\lambda_k d_k) \\geq \\beta d_k^T g_k\n\\end{cases}</script><p>带非精确搜索的拟牛顿法的研究是从1976年Powell的工作开始的，他证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。</p>\n<hr>\n<h3 id=\"6-L-BFGS算法\"><a href=\"#6-L-BFGS算法\" class=\"headerlink\" title=\"6.L-BFGS算法\"></a>6.L-BFGS算法</h3><p>在BFGS算法中，需要用到一个$NxN$的矩阵$D_k$，当 N 很大时，存储这个矩阵将变得很耗计算机资源，考虑 N 为 10万 的情形，且 用double型（8字节）来存储$D_k$，需要多大的内存呢？来计算一下：</p>\n<script type=\"math/tex; mode=display\">\\frac{N阶矩阵的字节数}{1GB的字节数} \\qquad = \\qquad \\frac{10^5 \\times 10^5 \\times 8}{2^{10} \\times 2^{10} \\times 2^{10}} \\qquad = \\qquad 74.5(GB)</script><p>即使考虑到矩阵$D_k$的对称性，内存还可以降一半，但是这对于一般的服务器仍然是难以承受的。况且我们还只是假定了 N=10W 的情况，在实际机器学习问题中，这只能算是中小规模。</p>\n<p>L-BFGS(Limited-memory BFGS)算法就是通过对BFGS算法进行改造，从而减少其迭代过程中所需内存开销的算法。\n它对BFGS算法进行了近似，其基本思想是：<strong>不再存储完整的矩阵$D_k$，而是存储计算过程中的向量序列${s_i}，{y_i}$，需要矩阵$D_k$时，利用向量序列${s_i}，{y_i}$的计算来代替。</strong>\n而且，向量序列${s_i}，{y_i}$也不是所有的都存，而是固定存最新的$m$个（参数$m$可由用户根据机器的内存自行指定），每次计算$D_k$时，只利用最新的$m$个${s_i}$和$m$个${y_i}$，\n显然，这样我们将存储由原来的$O(n^2)$降到了$O(mN)$。</p>\n<p><br></p>\n<p>接下来，讨论L-BFGS算法的具体实现过程，我们的出发点是改进的BFGS算法中的第6步中的迭代式：</p>\n<script type=\"math/tex; mode=display\">D_{k+1} = \\left( I - \\frac{s_k y_k^T}{y_k^T s_k} \\right) D_k \\left( I - \\frac{y_k s_k^T}{y_k^T s_k} \\right) + \\frac{s_k s_k^T}{y_k^T s_k}</script><p>若记$ \\rho_k = \\frac{1}{y_k^T s_k}，V_k = I - \\rho_k y_k s_k^T $，则上式可写成</p>\n<script type=\"math/tex; mode=display\">D_{k+1} = V_k^T D_k V_k + \\rho_k s_k s_k^T \\tag{6.*}</script><p>如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如$D_0 = I$），则可利用$(6.*)$式，依次可得：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_1 &= V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T; \\\\\nD_2 &= V_1^T D_1 V_1 + \\rho_1 s_1 s_1^T \\\\\n&= v_1^T (V_0^T D_0 V_0 + \\rho_0 s_0 s_0^T) V_1 + \\rho_1 s_1 s_1^T \\\\\n&= V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T \\\\\nD_3 &= V_2^T D_2 V_2 + \\rho_2 s_2 s_2^T \\\\\n&= V_2^T (V_1^T V_0^T D_0 V_0 V_1 + V_1^T \\rho_0 s_0 s_0^T V_1 + \\rho_1 s_1 s_1^T) V_2 + \\rho_2 s_2 s_2^T \\\\\n&= v_2^T V_1^T V_0^T D_0 V_0 V_1 V_2 + V_2^T V_1^T \\rho_0 s_0 s_0^T V_1 V_2 + V_2^T \\rho_1 s_1 s_1^T V_2 + \\rho_2 s_2 s_2^T \\\\\n\\cdots\n\\end{aligned}\n\\end{equation}</script><p>一般地，我们有：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_1^T V_0^T) D_0 (V_0 V_1 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_2^T V_1^T) (\\rho_0 s_0 s_0^T) (V_1 V_2 \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_3^T V_2^T) (\\rho_1 s_1 s_1^T) (V_2 V_3 \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.**}</script><p>由上式可以看到，计算$D<em>{k+1}$需要用到$ \\lbrace s_i, y_i \\rbrace</em>{i=0}^k $，因此，若从$ s<em>0, y_0 $开始连续存储 $ m $ 组的话，只能存储到$ s</em>{m-1}, y<em>{m-1} $，\n亦即只能依次计算 $ D_1, D_2, \\cdots $，直到$ D_m $，那$ D</em>{m+1}, D_{m+2} $该如何计算呢？</p>\n<p>自然地，如果一定要丢弃一些向量，那么肯定优先考虑那些最早生成的向量，具体来说，计算$D<em>{m+1}$时，我们保存$ \\lbrace s_i, y_i \\rbrace</em>{i=1}^m $，丢掉$ \\lbrace s<em>0, y_0 \\rbrace;$ \n计算$ D</em>{m+2} $时，我们保存$ \\lbrace s<em>i, y_i\\rbrace</em>{i=2}^{m+1} $，丢掉了$ \\lbrace s<em>i, y_i \\rbrace</em>{i=0}^1; \\cdots $</p>\n<p>但是舍弃掉一些向量后，就只能近似计算了，当$k+1 &gt; m$时，仿照$(6.**)$式，可以构造近似计算公式：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-m+2}^T V_{k-m+1}^T) D_0 (V_{k-m+1} V_{k-m+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+3}^T V_{k-m+2}^T) (\\rho_0 s_0 s_0^T) (V_{k-m+2} V_{k-m+3} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-m+4}^T V_{k-m+3}^T) (\\rho_1 s_1 s_1^T) (V_{k-m+3} V_{k-m+4} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_k s_k s_k^T\n\\end{aligned}\n\\end{equation} \\tag{6.***}</script><p>$ (6.**) $ 和 $ (6.***) $ 被称为$ special BFGS matrices $，若引入 $ \\hat{m} = min\\lbrace k, m-1 \\rbrace $，则还可以将两式合并成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\nD_{k+1} = \n&\\ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+1}^T V_{k-\\hat{m}}^T) D_0 (V_{k-\\hat{m}} V_{k-\\hat{m}+1} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+2}^T V_{k-\\hat{m+1}}^T) (\\rho_0 s_0 s_0^T) (V_{k-\\hat{m}+1} V_{k-\\hat{m}+2} \\cdots V_{k-1} V_k) \\\\\n&+ (V_k^T V_{k-1}^T \\cdots V_{k-\\hat{m}+3}^T V_{k-\\hat{m+2}}^T) (\\rho_1 s_1 s_1^T) (V_{k-\\hat{m}+2} V_{k-\\hat{m}+3} \\cdots V_{k-1} V_k) \\\\\n&+ \\cdots \\\\\n&+ (V_k^T V_{k-1}^T) (\\rho_{k-2} s_{k-2} s_{k-2}^T) (V_{k-1} V_k) \\\\\n&+ V_k^T (\\rho_{k-1} s_{k-1} s_{k-1}^T) V_k \\\\\n&+ \\rho_{k} s_{k} s_{k}^T\n\\end{aligned}\n\\end{equation}</script><p>事实上，由BFGS算法流程易知，$D_k$的作用仅用来计算$ D_k g_k $ 获取搜索方向，因此，若能利用上式设计出一种计算$ D_k g_k $的快速算法，则大功告成。</p>\n<blockquote>\n<p><strong>($D_k \\cdot g_k$的快速算法)</strong></p>\n<p>Step 1 初始化</p>\n<script type=\"math/tex; mode=display\">\n\\delta = \n\\begin{cases}\n0, \\qquad k \\leq m \\\\\nk-m, \\qquad k > m\n\\end{cases};\n\\qquad\nL = \n\\begin{cases}\nk, \\qquad k \\leq m \\\\\nm, \\qquad k > m\n\\end{cases};\n\\qquad\nq_L = g_k</script><p>Step 2 后向循环</p>\n<script type=\"math/tex; mode=display\">\nFOR \\quad i=L-1, L-2, \\cdots, 1, 0 \\quad DO \\\\\n\\{ \\\\\n\\qquad j=i+\\delta; \\\\\n\\qquad \\alpha_i = \\rho_j s_j^T q_{i+1}; // \\alpha_i 需要保存下来，前向循环要用！！ \\\\\n\\qquad q_i = q_{i+1} - \\alpha_i y_j \\\\\n\\}</script><p>Step 3 前向循环</p>\n<script type=\"math/tex; mode=display\">\nr_0 = D_0 \\cdot q_0; \\\\\nFOR \\quad i=0,1,\\cdots,L-2,L-1 \\quad DO \\\\\n\\{ \\\\\n\\qquad j = i+\\delta; \\\\\n\\qquad \\beta_j = \\rho_j y_j^T r_i; \\\\\n\\qquad r_{i+1} = r_i + (\\alpha_i - \\beta_i) s_j \\\\\n\\}</script></blockquote>\n<p>最后算出的$r_L$即为$H_k \\cdot g_k $的值。</p>\n<hr>\n<p>&gt;\n<a href=\"https://blog.csdn.net/itplus/article/details/21896453\" target=\"_blank\" rel=\"noopener\">牛顿法与拟牛顿法学习笔记</a></p>\n"},{"layout":"post","title":"SMOTE过采样技术","date":"2018-06-18T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n\n目录\n\n- 类别不平衡问题\n- 类别不平衡引发的问题\n- 解决不平衡问题的方法\n- SMOTE算法\n\n\n论文：[SMOTE Synthetic Minority Over-sampling Technique](/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf)\n\n\n### 1. 类别不平衡问题\n\n类不平衡 (class-imbalance) 是指在训练分类器中所使用的训练集的类别分布不均。\n比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类别不平衡。\n\n在后文中，把样本数量过少的类别称为“少数类”。\n但实际上，数据集上的类不平衡到底有没有达到需要特殊处理的程度，还要看不处理时训练出来的模型在验证集上的效果，有些时候是没必要处理的。\n\n\n-----------\n\n### 2. 类别不平衡引发的问题\n\n#### 2.1 模型训练过程角度\n\n从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。\n\n使用经验风险 (模型在训练集上的平均损失) 最小化作为模型的学习准则。设损失函数为 0-1 loss(这是一种典型的均等代价的损失函数)，那么优化目标就等价于错误率最小化(也就是accuracy最大化)。\n\n考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。\n训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%。\n于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，训练结束！于是这个模型……\n\n模型没有学习到如何去判别出少数类，这时候模型的召回率会非常低。\n\n\n#### 2.2 模型预测过程角度\n\n考虑二项Logistic回归模型。输入一个样本 $x$ ，模型输出的是其属于正类的概率 $\\hat{y}$ 。当 $\\hat{y} > 0.5$ 时，模型判定该样本属于正类，否则就是属于负类。\n\n为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？\n\n从几率 (odds) 的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为：$ \\frac{\\hat{y}}{1-\\hat{y}} $。\n\n模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 $\\theta$ 等于正类样本数除以全部样本数，那么样本的真实几率为： $ \\frac{\\theta}{1-\\theta} $。\n当观测几率大于真实几率时，也就是 $ \\hat{y} > \\theta $ 时，那么就判定这个样本属于正类。\n\n虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。\n正是因为这个假设，所以认为训练集的观测几率 $ \\frac{\\hat{\\theta}}{1-\\hat{\\theta}} $ 就代表了真实几率 $ \\frac{\\theta}{1-\\theta} $ 。\n\n所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。\n\n\n----------\n\n### 3. 解决不平衡问题的方法\n\n#### 3.1 调整$\\theta$值\n\n根据训练集的正负样本比例，调整 $ \\theta $ 值。\n\n这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。\n\n\n#### 3.2 欠采样\n\n对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。\n\n\n#### 3.3 过采样\n\n对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。下面将介绍一种经典的过采样算法：SMOTE。\n\n\n------------\n\n### 4. SMOTE算法\n\nSMOTE，合成少数类过采样技术。它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，\n即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。\n\n#### 4.1 SMOTE算法流程\n\n设训练集的一个少数类的样本数为 $T$ ，那么SMOTE算法将为这个少数类合成 $NT$ 个新样本。这里要求 $N$ 必须是正整数，如果给定的 $N<1$， 那么算法将“认为”少数类的样本数 $T=NT$，并将强制 $N=1$。\n\n- 考虑该少数类的一个样本 $i$ ，其特征向量为 $x_i, i \\in \\lbrace 1,...,T \\rbrace$：\n    - (1) 首先从该少数类的全部 $T$ 个样本中找到样本 $x_i$ 的 $k$ 近邻（例如用欧氏距离），记为 $ x_{i,near}, near \\in \\lbrace 1,...,k \\rbrace $；\n    - (2) 然后从这 $k$ 近邻中随机选择一个样本 $x_{i,nn}$，再生成一个 $0$ 到 $1$ 之间的随机数 $\\eta_1$，从而合成一个新样本 $x_{i,1} $： \n        - $$ x_{i,1} = x_i + \\eta_1 \\cdot (x_{i,nn} − x_i ) $$\n    - (3) 将步骤(2)重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$x_{i,new}, new \\in \\lbrace 1,...,N \\rbrace $。\n\n那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。\n\n<br>\n\n![smote-2](/posts_res/2018-06-18-smote/smote.jpg)\n\n如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。\nSMOTE算法所合成出的一个新样本 $x_{i,1}$ 相当于是表示样本 $x_i$ 的点和表示样本 $x_{i,nn}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。\n\n**进一步阅读**\n\n1. [解决真实世界问题：如何在不平衡类上使用机器学习？](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650718717&idx=1&sn=85038d7c906c135120a8e1a2f7e565ad&scene=0#wechat_redirect)\n\n\n","source":"_posts/2018-06-18-smote.md","raw":"---\nlayout: post\ntitle: SMOTE过采样技术\ndate: 2018-06-18 12:10 +0800\ncategories: 机器学习\ntags:\n- 特征选择\nmathjax: true\ncopyright: false\n---\n\n\n目录\n\n- 类别不平衡问题\n- 类别不平衡引发的问题\n- 解决不平衡问题的方法\n- SMOTE算法\n\n\n论文：[SMOTE Synthetic Minority Over-sampling Technique](/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf)\n\n\n### 1. 类别不平衡问题\n\n类不平衡 (class-imbalance) 是指在训练分类器中所使用的训练集的类别分布不均。\n比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类别不平衡。\n\n在后文中，把样本数量过少的类别称为“少数类”。\n但实际上，数据集上的类不平衡到底有没有达到需要特殊处理的程度，还要看不处理时训练出来的模型在验证集上的效果，有些时候是没必要处理的。\n\n\n-----------\n\n### 2. 类别不平衡引发的问题\n\n#### 2.1 模型训练过程角度\n\n从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。\n\n使用经验风险 (模型在训练集上的平均损失) 最小化作为模型的学习准则。设损失函数为 0-1 loss(这是一种典型的均等代价的损失函数)，那么优化目标就等价于错误率最小化(也就是accuracy最大化)。\n\n考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。\n训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%。\n于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，训练结束！于是这个模型……\n\n模型没有学习到如何去判别出少数类，这时候模型的召回率会非常低。\n\n\n#### 2.2 模型预测过程角度\n\n考虑二项Logistic回归模型。输入一个样本 $x$ ，模型输出的是其属于正类的概率 $\\hat{y}$ 。当 $\\hat{y} > 0.5$ 时，模型判定该样本属于正类，否则就是属于负类。\n\n为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？\n\n从几率 (odds) 的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为：$ \\frac{\\hat{y}}{1-\\hat{y}} $。\n\n模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 $\\theta$ 等于正类样本数除以全部样本数，那么样本的真实几率为： $ \\frac{\\theta}{1-\\theta} $。\n当观测几率大于真实几率时，也就是 $ \\hat{y} > \\theta $ 时，那么就判定这个样本属于正类。\n\n虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。\n正是因为这个假设，所以认为训练集的观测几率 $ \\frac{\\hat{\\theta}}{1-\\hat{\\theta}} $ 就代表了真实几率 $ \\frac{\\theta}{1-\\theta} $ 。\n\n所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。\n\n\n----------\n\n### 3. 解决不平衡问题的方法\n\n#### 3.1 调整$\\theta$值\n\n根据训练集的正负样本比例，调整 $ \\theta $ 值。\n\n这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。\n\n\n#### 3.2 欠采样\n\n对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。\n\n\n#### 3.3 过采样\n\n对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。下面将介绍一种经典的过采样算法：SMOTE。\n\n\n------------\n\n### 4. SMOTE算法\n\nSMOTE，合成少数类过采样技术。它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，\n即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。\n\n#### 4.1 SMOTE算法流程\n\n设训练集的一个少数类的样本数为 $T$ ，那么SMOTE算法将为这个少数类合成 $NT$ 个新样本。这里要求 $N$ 必须是正整数，如果给定的 $N<1$， 那么算法将“认为”少数类的样本数 $T=NT$，并将强制 $N=1$。\n\n- 考虑该少数类的一个样本 $i$ ，其特征向量为 $x_i, i \\in \\lbrace 1,...,T \\rbrace$：\n    - (1) 首先从该少数类的全部 $T$ 个样本中找到样本 $x_i$ 的 $k$ 近邻（例如用欧氏距离），记为 $ x_{i,near}, near \\in \\lbrace 1,...,k \\rbrace $；\n    - (2) 然后从这 $k$ 近邻中随机选择一个样本 $x_{i,nn}$，再生成一个 $0$ 到 $1$ 之间的随机数 $\\eta_1$，从而合成一个新样本 $x_{i,1} $： \n        - $$ x_{i,1} = x_i + \\eta_1 \\cdot (x_{i,nn} − x_i ) $$\n    - (3) 将步骤(2)重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$x_{i,new}, new \\in \\lbrace 1,...,N \\rbrace $。\n\n那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。\n\n<br>\n\n![smote-2](/posts_res/2018-06-18-smote/smote.jpg)\n\n如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。\nSMOTE算法所合成出的一个新样本 $x_{i,1}$ 相当于是表示样本 $x_i$ 的点和表示样本 $x_{i,nn}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。\n\n**进一步阅读**\n\n1. [解决真实世界问题：如何在不平衡类上使用机器学习？](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650718717&idx=1&sn=85038d7c906c135120a8e1a2f7e565ad&scene=0#wechat_redirect)\n\n\n","slug":"smote","published":1,"updated":"2019-08-17T09:37:34.560Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw5002j2qwpt544m2mg","content":"<p>目录</p><ul>\n<li>类别不平衡问题</li>\n<li>类别不平衡引发的问题</li>\n<li>解决不平衡问题的方法</li>\n<li>SMOTE算法</li>\n</ul><p>论文：<a href=\"/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf\">SMOTE Synthetic Minority Over-sampling Technique</a></p><h3 id=\"1-类别不平衡问题\"><a href=\"#1-类别不平衡问题\" class=\"headerlink\" title=\"1. 类别不平衡问题\"></a>1. 类别不平衡问题</h3><p>类不平衡 (class-imbalance) 是指在训练分类器中所使用的训练集的类别分布不均。\n比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类别不平衡。</p><p>在后文中，把样本数量过少的类别称为“少数类”。\n但实际上，数据集上的类不平衡到底有没有达到需要特殊处理的程度，还要看不处理时训练出来的模型在验证集上的效果，有些时候是没必要处理的。</p><a id=\"more\"></a>\n\n\n\n\n<hr>\n<h3 id=\"2-类别不平衡引发的问题\"><a href=\"#2-类别不平衡引发的问题\" class=\"headerlink\" title=\"2. 类别不平衡引发的问题\"></a>2. 类别不平衡引发的问题</h3><h4 id=\"2-1-模型训练过程角度\"><a href=\"#2-1-模型训练过程角度\" class=\"headerlink\" title=\"2.1 模型训练过程角度\"></a>2.1 模型训练过程角度</h4><p>从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。</p>\n<p>使用经验风险 (模型在训练集上的平均损失) 最小化作为模型的学习准则。设损失函数为 0-1 loss(这是一种典型的均等代价的损失函数)，那么优化目标就等价于错误率最小化(也就是accuracy最大化)。</p>\n<p>考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。\n训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%。\n于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，训练结束！于是这个模型……</p>\n<p>模型没有学习到如何去判别出少数类，这时候模型的召回率会非常低。</p>\n<h4 id=\"2-2-模型预测过程角度\"><a href=\"#2-2-模型预测过程角度\" class=\"headerlink\" title=\"2.2 模型预测过程角度\"></a>2.2 模型预测过程角度</h4><p>考虑二项Logistic回归模型。输入一个样本 $x$ ，模型输出的是其属于正类的概率 $\\hat{y}$ 。当 $\\hat{y} &gt; 0.5$ 时，模型判定该样本属于正类，否则就是属于负类。</p>\n<p>为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？</p>\n<p>从几率 (odds) 的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为：$ \\frac{\\hat{y}}{1-\\hat{y}} $。</p>\n<p>模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 $\\theta$ 等于正类样本数除以全部样本数，那么样本的真实几率为： $ \\frac{\\theta}{1-\\theta} $。\n当观测几率大于真实几率时，也就是 $ \\hat{y} &gt; \\theta $ 时，那么就判定这个样本属于正类。</p>\n<p>虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。\n正是因为这个假设，所以认为训练集的观测几率 $ \\frac{\\hat{\\theta}}{1-\\hat{\\theta}} $ 就代表了真实几率 $ \\frac{\\theta}{1-\\theta} $ 。</p>\n<p>所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。</p>\n<hr>\n<h3 id=\"3-解决不平衡问题的方法\"><a href=\"#3-解决不平衡问题的方法\" class=\"headerlink\" title=\"3. 解决不平衡问题的方法\"></a>3. 解决不平衡问题的方法</h3><h4 id=\"3-1-调整-theta-值\"><a href=\"#3-1-调整-theta-值\" class=\"headerlink\" title=\"3.1 调整$\\theta$值\"></a>3.1 调整$\\theta$值</h4><p>根据训练集的正负样本比例，调整 $ \\theta $ 值。</p>\n<p>这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。</p>\n<h4 id=\"3-2-欠采样\"><a href=\"#3-2-欠采样\" class=\"headerlink\" title=\"3.2 欠采样\"></a>3.2 欠采样</h4><p>对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。</p>\n<h4 id=\"3-3-过采样\"><a href=\"#3-3-过采样\" class=\"headerlink\" title=\"3.3 过采样\"></a>3.3 过采样</h4><p>对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。下面将介绍一种经典的过采样算法：SMOTE。</p>\n<hr>\n<h3 id=\"4-SMOTE算法\"><a href=\"#4-SMOTE算法\" class=\"headerlink\" title=\"4. SMOTE算法\"></a>4. SMOTE算法</h3><p>SMOTE，合成少数类过采样技术。它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，\n即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。</p>\n<h4 id=\"4-1-SMOTE算法流程\"><a href=\"#4-1-SMOTE算法流程\" class=\"headerlink\" title=\"4.1 SMOTE算法流程\"></a>4.1 SMOTE算法流程</h4><p>设训练集的一个少数类的样本数为 $T$ ，那么SMOTE算法将为这个少数类合成 $NT$ 个新样本。这里要求 $N$ 必须是正整数，如果给定的 $N&lt;1$， 那么算法将“认为”少数类的样本数 $T=NT$，并将强制 $N=1$。</p>\n<ul>\n<li>考虑该少数类的一个样本 $i$ ，其特征向量为 $x_i, i \\in \\lbrace 1,…,T \\rbrace$：<ul>\n<li>(1) 首先从该少数类的全部 $T$ 个样本中找到样本 $x<em>i$ 的 $k$ 近邻（例如用欧氏距离），记为 $ x</em>{i,near}, near \\in \\lbrace 1,…,k \\rbrace $；</li>\n<li>(2) 然后从这 $k$ 近邻中随机选择一个样本 $x<em>{i,nn}$，再生成一个 $0$ 到 $1$ 之间的随机数 $\\eta_1$，从而合成一个新样本 $x</em>{i,1} $： <ul>\n<li><script type=\"math/tex; mode=display\">x_{i,1} = x_i + \\eta_1 \\cdot (x_{i,nn} − x_i )</script></li>\n</ul>\n</li>\n<li>(3) 将步骤(2)重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$x_{i,new}, new \\in \\lbrace 1,…,N \\rbrace $。</li>\n</ul>\n</li>\n</ul>\n<p>那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。</p>\n<p><br></p>\n<p><img src=\"/posts_res/2018-06-18-smote/smote.jpg\" alt=\"smote-2\"></p>\n<p>如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。\nSMOTE算法所合成出的一个新样本 $x<em>{i,1}$ 相当于是表示样本 $x_i$ 的点和表示样本 $x</em>{i,nn}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。</p>\n<p><strong>进一步阅读</strong></p>\n<ol>\n<li><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650718717&amp;idx=1&amp;sn=85038d7c906c135120a8e1a2f7e565ad&amp;scene=0#wechat_redirect\" target=\"_blank\" rel=\"noopener\">解决真实世界问题：如何在不平衡类上使用机器学习？</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>类别不平衡问题</li>\n<li>类别不平衡引发的问题</li>\n<li>解决不平衡问题的方法</li>\n<li>SMOTE算法</li>\n</ul><p>论文：<a href=\"/posts_res/2018-06-18-smote/SMOTE Synthetic Minority Over-sampling Technique.pdf\">SMOTE Synthetic Minority Over-sampling Technique</a></p><h3 id=\"1-类别不平衡问题\"><a href=\"#1-类别不平衡问题\" class=\"headerlink\" title=\"1. 类别不平衡问题\"></a>1. 类别不平衡问题</h3><p>类不平衡 (class-imbalance) 是指在训练分类器中所使用的训练集的类别分布不均。\n比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类别不平衡。</p><p>在后文中，把样本数量过少的类别称为“少数类”。\n但实际上，数据集上的类不平衡到底有没有达到需要特殊处理的程度，还要看不处理时训练出来的模型在验证集上的效果，有些时候是没必要处理的。</p>","more":"\n\n\n\n\n<hr>\n<h3 id=\"2-类别不平衡引发的问题\"><a href=\"#2-类别不平衡引发的问题\" class=\"headerlink\" title=\"2. 类别不平衡引发的问题\"></a>2. 类别不平衡引发的问题</h3><h4 id=\"2-1-模型训练过程角度\"><a href=\"#2-1-模型训练过程角度\" class=\"headerlink\" title=\"2.1 模型训练过程角度\"></a>2.1 模型训练过程角度</h4><p>从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。</p>\n<p>使用经验风险 (模型在训练集上的平均损失) 最小化作为模型的学习准则。设损失函数为 0-1 loss(这是一种典型的均等代价的损失函数)，那么优化目标就等价于错误率最小化(也就是accuracy最大化)。</p>\n<p>考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。\n训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%。\n于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，训练结束！于是这个模型……</p>\n<p>模型没有学习到如何去判别出少数类，这时候模型的召回率会非常低。</p>\n<h4 id=\"2-2-模型预测过程角度\"><a href=\"#2-2-模型预测过程角度\" class=\"headerlink\" title=\"2.2 模型预测过程角度\"></a>2.2 模型预测过程角度</h4><p>考虑二项Logistic回归模型。输入一个样本 $x$ ，模型输出的是其属于正类的概率 $\\hat{y}$ 。当 $\\hat{y} &gt; 0.5$ 时，模型判定该样本属于正类，否则就是属于负类。</p>\n<p>为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？</p>\n<p>从几率 (odds) 的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为：$ \\frac{\\hat{y}}{1-\\hat{y}} $。</p>\n<p>模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 $\\theta$ 等于正类样本数除以全部样本数，那么样本的真实几率为： $ \\frac{\\theta}{1-\\theta} $。\n当观测几率大于真实几率时，也就是 $ \\hat{y} &gt; \\theta $ 时，那么就判定这个样本属于正类。</p>\n<p>虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。\n正是因为这个假设，所以认为训练集的观测几率 $ \\frac{\\hat{\\theta}}{1-\\hat{\\theta}} $ 就代表了真实几率 $ \\frac{\\theta}{1-\\theta} $ 。</p>\n<p>所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。</p>\n<hr>\n<h3 id=\"3-解决不平衡问题的方法\"><a href=\"#3-解决不平衡问题的方法\" class=\"headerlink\" title=\"3. 解决不平衡问题的方法\"></a>3. 解决不平衡问题的方法</h3><h4 id=\"3-1-调整-theta-值\"><a href=\"#3-1-调整-theta-值\" class=\"headerlink\" title=\"3.1 调整$\\theta$值\"></a>3.1 调整$\\theta$值</h4><p>根据训练集的正负样本比例，调整 $ \\theta $ 值。</p>\n<p>这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。</p>\n<h4 id=\"3-2-欠采样\"><a href=\"#3-2-欠采样\" class=\"headerlink\" title=\"3.2 欠采样\"></a>3.2 欠采样</h4><p>对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。</p>\n<h4 id=\"3-3-过采样\"><a href=\"#3-3-过采样\" class=\"headerlink\" title=\"3.3 过采样\"></a>3.3 过采样</h4><p>对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。下面将介绍一种经典的过采样算法：SMOTE。</p>\n<hr>\n<h3 id=\"4-SMOTE算法\"><a href=\"#4-SMOTE算法\" class=\"headerlink\" title=\"4. SMOTE算法\"></a>4. SMOTE算法</h3><p>SMOTE，合成少数类过采样技术。它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，\n即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。</p>\n<h4 id=\"4-1-SMOTE算法流程\"><a href=\"#4-1-SMOTE算法流程\" class=\"headerlink\" title=\"4.1 SMOTE算法流程\"></a>4.1 SMOTE算法流程</h4><p>设训练集的一个少数类的样本数为 $T$ ，那么SMOTE算法将为这个少数类合成 $NT$ 个新样本。这里要求 $N$ 必须是正整数，如果给定的 $N&lt;1$， 那么算法将“认为”少数类的样本数 $T=NT$，并将强制 $N=1$。</p>\n<ul>\n<li>考虑该少数类的一个样本 $i$ ，其特征向量为 $x_i, i \\in \\lbrace 1,…,T \\rbrace$：<ul>\n<li>(1) 首先从该少数类的全部 $T$ 个样本中找到样本 $x<em>i$ 的 $k$ 近邻（例如用欧氏距离），记为 $ x</em>{i,near}, near \\in \\lbrace 1,…,k \\rbrace $；</li>\n<li>(2) 然后从这 $k$ 近邻中随机选择一个样本 $x<em>{i,nn}$，再生成一个 $0$ 到 $1$ 之间的随机数 $\\eta_1$，从而合成一个新样本 $x</em>{i,1} $： <ul>\n<li><script type=\"math/tex; mode=display\">x_{i,1} = x_i + \\eta_1 \\cdot (x_{i,nn} − x_i )</script></li>\n</ul>\n</li>\n<li>(3) 将步骤(2)重复进行 $N$ 次，从而可以合成 $N$ 个新样本：$x_{i,new}, new \\in \\lbrace 1,…,N \\rbrace $。</li>\n</ul>\n</li>\n</ul>\n<p>那么，对全部的 $T$ 个少数类样本进行上述操作，便可为该少数类合成 $NT$ 个新样本。</p>\n<p><br></p>\n<p><img src=\"/posts_res/2018-06-18-smote/smote.jpg\" alt=\"smote-2\"></p>\n<p>如果样本的特征维数是 2 维，那么每个样本都可以用二维平面上的一个点来表示。\nSMOTE算法所合成出的一个新样本 $x<em>{i,1}$ 相当于是表示样本 $x_i$ 的点和表示样本 $x</em>{i,nn}$ 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。</p>\n<p><strong>进一步阅读</strong></p>\n<ol>\n<li><a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650718717&amp;idx=1&amp;sn=85038d7c906c135120a8e1a2f7e565ad&amp;scene=0#wechat_redirect\" target=\"_blank\" rel=\"noopener\">解决真实世界问题：如何在不平衡类上使用机器学习？</a></li>\n</ol>\n"},{"layout":"post","title":"递归神经网络RNN初识 - LSTM","date":"2018-06-20T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n目录\n\n* 1.循环神经网络\n* 2.长期依赖的问题\n* 3.LSTM 网络\n* 4.LSTM的变种\n* 5.结论\n\n\n译自：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n----------\n\n### 1.循环神经网络\n\n人类不会每秒钟都从头开始思考。在阅读本文时，你可以根据以前单词的理解来理解每个单词。你不要把所有东西都扔掉，再从头开始思考。 你的想法有持久性。 \n在这方面，传统神经网络不能做到这一点，这是一个很大的缺点。 例如，想象一下，当你想要分析电影中剧情的发展。 传统神经网络是不能够利用电影中以前的事件来推理以后的事情的。 \n而循环神经网络解决了这个问题。 它们是具有循环的网络，允许信息影响持续存在。\n\n![1-1](/posts_res/2018-06-20-rnn1/1-1.jpg)\n\n在上图中，一组神经网络$A$输入$X_t$并输出一个值$h_t$。 循环允许信息从网络的一个步骤传递到下一个。\n这些循环使得循环神经网络显得很神秘。 然而，如果你仔细想想，它们并不与一般的神经网络完全不同。 \n一个循环神经网络可以被认为是同一个网络的多个副本，每一个都传递一个信息给后继者。 \n考虑如果我们展开循环会发生什么：\n\n![1-2](/posts_res/2018-06-20-rnn1/1-2.jpg)\n\n这种链状特征揭示了循环神经网络与序列和列表密切相关，它们是用于此类数据的神经网络的自然结构。\n而且他们确实得到应用！ 在过去几年里，将RNN应用于语音识别，语言建模，翻译，图像字幕等各种问题已经取得了令人难以置信的成功。\n我将讨论可以通过RNN实现的令人惊叹的专长，使Andrej Karpathy的优秀博客\n“[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)”成为可能。 \n但他们真的很神奇。这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，对于许多任务来说，它们比标准版本好得多。\n几乎所有令人兴奋的结果基于循环神经网络是通过他们实现的。这篇文章将探讨这些LSTM。\n\n\n---------\n\n### 2.长期依赖的问题\n\nRNNs吸引人的一点在于它能够将过去的信息连接到当前任务，例如使用先前的视频帧可以为当前帧的理解提供信息。如果RNNs可以做到这一点，他们将非常有用。\n但他们可以吗？这取决于不同的情况。有时候，我们只需要最近的信息来处理当前的任务。\n举个例子，输入法能够通过你输入的上一个词来预测你想要输入的下一个词。比如，我们想要预测“云彩在天上\"，我们不需要更多的信息就能够预测最后两个字。\n在这种情况下，如果相关信息与所需预测的信息间隔很小，则RNN可以学习使用过去的信息。\n\n![2-1](/posts_res/2018-06-20-rnn1/2-1.jpg)\n\n但有些情况下我们就需要更多的文本。比如预测“我来自法国...我法语很流利...”，最近的信息预测这是一个语言名词，但是当我们想确定具体是哪一种语言的时候，\n我们需要找到包含法国的文本，但这个文本很可能间隔非常远。很不幸，当间隔增加，RNNs就会更难以连接相关的信息。\n\n![2-2](/posts_res/2018-06-20-rnn1/2-2.jpg)\n\n理论上，RNNs完全有能力有能力处理这样的“长期依赖”。一个人可以仔细挑选参数来解决这种形式的问题。但实践却表明RNNs似乎不能学习。\n[Hochreiter(1991) German](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)深层次的揭露了这个问题，\n而[Bengio,et al.(1994)](https://link.zhihu.com/?target=http%3A//www-dsi.ing.unifi.it/%7Epaolo/ps/tnn-94-gradient.pdf)解释了一些基本的原因。\n\n但是，LSTMs没有这个问题！\n\n\n--------\n\n### 3.LSTM 网络\n\n长短记忆网络（Long Short Term Memory networks）通常称为“LSTMs\"-是一种特殊的RNN，有能力学习长期的依赖。\nLSTMs最初由[Hochreiter & Schmidhuber (1997)](https://link.zhihu.com/?target=http%3A//deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)提出，\n并在之后的工作中由许多人优化推广。LSTM在各种各样的问题上工作得很好，现在被广泛使用。\n\nLSTM专为解决长期依赖问题而设计。记忆长周期的信息是默认的行为而不是努力需要学习的东西。\n现有的循环神经网络都会有一个链来重复神经网络的模块。在标准的RNN中一般是一个非常简单的结构，如一个$tanh$层。\n\n![3-1](/posts_res/2018-06-20-rnn1/3-1.jpg)\n\nLSTMs也有一个类似链结构，但是重复模块有一个复杂的结构。相对于单神经网络层，LSTM有4层，以一个非常特殊的方式交互。\n\n![3-2](/posts_res/2018-06-20-rnn1/3-2.jpg)\n\n不用担心其中的细节。我们将会一步步图解LSTM。现在让我们熟悉一下将用到的符号。\n\n![3-3](/posts_res/2018-06-20-rnn1/3-3.jpg)\n\n在上图中，每个线代表一个完整的向量，从一个节点输出到其他节点的输入，粉色的圈代表节点操作，\n比如向量加，而黄色框是学习到的神经网络层。线合并代表连接，线分叉代表复制。\n\n<br>\n\n**LSTMs背后的核心概念**\n\nLSTM的关键是Cell的状态，水平线穿过计算图的顶部。Cell状态有点像输送带。直接在整个链上运行，只有一些小的线性相互作用。 信息流畅地保持不变。\n\n![3-4](/posts_res/2018-06-20-rnn1/3-4.jpg)\n\nLSTMs没有能力去添加或者删除cell状态的信息，而是由称作门（gates)的结构进行调节。\n门是一个可以让信息选择性通过的结构。由一个$sigmoid$神经网络层和一个节点乘算法组成。\n\n![3-5](/posts_res/2018-06-20-rnn1/3-5.jpg)\n\n<br>\n\n**一步步了解LSTM**\n\nLSTM中，第一步是决定我们将丢弃什么信息，并把剩下的送入cell状态。这个决定由叫做**遗忘门层（forget gate layer）**的$sigmoid$层做出。\n它通过观察$h_{t-1}$和$x_t$，为cell状态$C_{t-1}$中的每个数输出一个 0 到 1 之间的数，1表示完全保留，0表示完全抛弃。\n\n让我们回到语言模型的例子。在这个预测下一个单词的问题中，cell状态也许会包含前面对象的属性，所以可以使用正确的代名词。当我们看到一个新的对象，我们希望忘记旧对象的属性。\n\n![3-6](/posts_res/2018-06-20-rnn1/3-6.jpg)\n\n下一步是来决定我们将保存什么新信息到cell状态中。这有两个部分: 首先，一个$sigmoid$层**（input gate layer, 输入门层）**决定哪个值将被更新;\n然后，一个$tanh$层创建一个新候选值的向量$ \\tilde{C}_t $，可以被加入到cell状态中。在下一步，我们将要合并这两步来创建一个状态的更新。\n\n在语言模型的例子里，我们想要添加新对象的属性到cell状态中来替换我们希望遗忘的旧对象的属性。\n\n![3-7](/posts_res/2018-06-20-rnn1/3-7.jpg)\n\n现在是时候来更新旧的cell状态$C_{t-1}$为新状态$C_t$。前面的步骤已经决定了应该做什么，我们只需要实施。\n\n我们**将旧状态乘$f_t$，忘记我们之前决定忘记的东西。然后我们加上$i_t * \\tilde{C}_t$。这是新的候选值，由我们决定更新每一个状态值的多少来缩放**。\n\n在语言模型的例子里，这是我们丢弃旧对象信息并添加新信息的地方。就如同我们在前一步决定的那样。\n\n![3-8](/posts_res/2018-06-20-rnn1/3-8.jpg)\n\n最终，我们需要决定输出什么**(output gate layer，输出门层)**。这输出将基于我们的cell状态，但会是一个过滤(filtered)后的版本。\n首先，我们运行一个$sigmoid$层来决定cell状态的哪些部分将被输出。然后，我们将cell状态通过$tanh$(将值转化为-1到1之间)并与$sigmoid$门的输出相乘，就可以只输出我们想要的部分。\n\n对于语言模型例子，因为只看到一个对象，在预测下一个词的时候，它也许想要输出一个关于动词的信息。\n比如，它也许输出这个对象是单数还是复数，因此我们知道接下来的是一个动词形式的。\n\n![3-9](/posts_res/2018-06-20-rnn1/3-9.jpg)\n\n\n--------\n\n### 4.LSTM的变种\n\n上文讲述了一个普通的LSTM。但不是所有的LSTMs都和上面一样。事实上，看起来几乎所有论文都用了一个有一点不一样的LSTM。区别很小，但是值得注意。\n\n<br>\n\n一个著名的LSTM变种，由[Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)提出，\n添加了窥视孔连接（peephole connections）。这意味着可以让门层查看cell状态(This means that we let the gate layers look at the cell state)。\n\n![4-1](/posts_res/2018-06-20-rnn1/4-1.jpg)\n\n上面的图添加了链接到所有门的窥孔，但很多论文只会添加其中的一些。\n\n<br>\n\n另一个变种是使用耦合的遗忘和输出门。相对于普通的LSTM分别决定添加和遗忘什么信息，这个变种同时作出决定，只在输入相同位置信息的时候遗忘当前的信息。\n只有在忘记一些值的时候才输入新值到状态中。\n\n![4-2](/posts_res/2018-06-20-rnn1/4-2.jpg)\n\n<br>\n\n一个有点戏剧化的LSTM变种是GRU（Gated Recurrent Unit），由 [Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf)介绍。\n它把遗忘和输入门结合成一个“更新门”，同时还合并了cell状态和隐藏状态，并有其他一些改变。这使得模型比标准LSTM要简洁，并越来越流行。\n\n![4-3](/posts_res/2018-06-20-rnn1/4-3.jpg)\n\n<br>\n\n这只是众多值得注意的LSTM变种中的几个。还有很多，如Depth Gated RNNs,[Yao, et al. (2015)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.03790v2.pdf)。\n同时还有一些完全不同的追踪长依赖的方式，比如Clockwork RNNs，[Koutnik, et al. (2014)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1402.3511v1.pdf).\n\n哪个变种是最好的？差别有必要吗？[Greff, et al. (2015)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.04069.pdf)对流行的变种做了一个总结，发现他们几乎一样。\n[Jozefowicz, et al. (2015)](https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)测试了超过一万种RNN结构，发现有一些在特定问题上比LSTM更好。\n\n\n----------\n\n### 5.结论\n\n早前提到了人们在RNN获得的显著成就。基本都是由LSTM实现的，他们对于大多数任务都工作的很好！\n\nLSTM的方程看起来很吓人，但是通过本文一步一步讲解，希望它们看起来更易懂。\n\nLSTM是我们可以用RNN完成的一大进步。很自然的，我们会想，还有另一大步吗？研究人员的共同观点是：是的！\n还有下一步这就是注意力（attention）！这个想法是让RNN的每一步从一些较大的信息合集中挑选信息。\n比如，您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个字。\n事实上[Xu, et al. (2015)](http://arxiv.org/pdf/1502.03044v2.pdf)做了这个，如果你想要探索更多关于注意力，这可能是一个有趣的起点！\n有一些真正令人兴奋的结果使用了注意力...\n\n注意力并不是RNN研究中唯一的突破口。例如Grid LSTMs，[Kalchbrenner, et al. (2015)](http://arxiv.org/pdf/1507.01526v1.pdf)看起来非常有意思。\n在生成模型中使用 RNN 进行工作，如[Gregor, et al. (2015)](http://arxiv.org/pdf/1502.04623.pdf), \n[Chung, et al. (2015)](http://arxiv.org/pdf/1506.02216v3.pdf), [Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf)。\n","source":"_posts/2018-06-20-rnn1.md","raw":"---\nlayout: post\ntitle: 递归神经网络RNN初识 - LSTM\ndate: 2018-06-20 12:10 +0800\ncategories: 深度学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: false\n---\n\n目录\n\n* 1.循环神经网络\n* 2.长期依赖的问题\n* 3.LSTM 网络\n* 4.LSTM的变种\n* 5.结论\n\n\n译自：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n----------\n\n### 1.循环神经网络\n\n人类不会每秒钟都从头开始思考。在阅读本文时，你可以根据以前单词的理解来理解每个单词。你不要把所有东西都扔掉，再从头开始思考。 你的想法有持久性。 \n在这方面，传统神经网络不能做到这一点，这是一个很大的缺点。 例如，想象一下，当你想要分析电影中剧情的发展。 传统神经网络是不能够利用电影中以前的事件来推理以后的事情的。 \n而循环神经网络解决了这个问题。 它们是具有循环的网络，允许信息影响持续存在。\n\n![1-1](/posts_res/2018-06-20-rnn1/1-1.jpg)\n\n在上图中，一组神经网络$A$输入$X_t$并输出一个值$h_t$。 循环允许信息从网络的一个步骤传递到下一个。\n这些循环使得循环神经网络显得很神秘。 然而，如果你仔细想想，它们并不与一般的神经网络完全不同。 \n一个循环神经网络可以被认为是同一个网络的多个副本，每一个都传递一个信息给后继者。 \n考虑如果我们展开循环会发生什么：\n\n![1-2](/posts_res/2018-06-20-rnn1/1-2.jpg)\n\n这种链状特征揭示了循环神经网络与序列和列表密切相关，它们是用于此类数据的神经网络的自然结构。\n而且他们确实得到应用！ 在过去几年里，将RNN应用于语音识别，语言建模，翻译，图像字幕等各种问题已经取得了令人难以置信的成功。\n我将讨论可以通过RNN实现的令人惊叹的专长，使Andrej Karpathy的优秀博客\n“[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)”成为可能。 \n但他们真的很神奇。这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，对于许多任务来说，它们比标准版本好得多。\n几乎所有令人兴奋的结果基于循环神经网络是通过他们实现的。这篇文章将探讨这些LSTM。\n\n\n---------\n\n### 2.长期依赖的问题\n\nRNNs吸引人的一点在于它能够将过去的信息连接到当前任务，例如使用先前的视频帧可以为当前帧的理解提供信息。如果RNNs可以做到这一点，他们将非常有用。\n但他们可以吗？这取决于不同的情况。有时候，我们只需要最近的信息来处理当前的任务。\n举个例子，输入法能够通过你输入的上一个词来预测你想要输入的下一个词。比如，我们想要预测“云彩在天上\"，我们不需要更多的信息就能够预测最后两个字。\n在这种情况下，如果相关信息与所需预测的信息间隔很小，则RNN可以学习使用过去的信息。\n\n![2-1](/posts_res/2018-06-20-rnn1/2-1.jpg)\n\n但有些情况下我们就需要更多的文本。比如预测“我来自法国...我法语很流利...”，最近的信息预测这是一个语言名词，但是当我们想确定具体是哪一种语言的时候，\n我们需要找到包含法国的文本，但这个文本很可能间隔非常远。很不幸，当间隔增加，RNNs就会更难以连接相关的信息。\n\n![2-2](/posts_res/2018-06-20-rnn1/2-2.jpg)\n\n理论上，RNNs完全有能力有能力处理这样的“长期依赖”。一个人可以仔细挑选参数来解决这种形式的问题。但实践却表明RNNs似乎不能学习。\n[Hochreiter(1991) German](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)深层次的揭露了这个问题，\n而[Bengio,et al.(1994)](https://link.zhihu.com/?target=http%3A//www-dsi.ing.unifi.it/%7Epaolo/ps/tnn-94-gradient.pdf)解释了一些基本的原因。\n\n但是，LSTMs没有这个问题！\n\n\n--------\n\n### 3.LSTM 网络\n\n长短记忆网络（Long Short Term Memory networks）通常称为“LSTMs\"-是一种特殊的RNN，有能力学习长期的依赖。\nLSTMs最初由[Hochreiter & Schmidhuber (1997)](https://link.zhihu.com/?target=http%3A//deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)提出，\n并在之后的工作中由许多人优化推广。LSTM在各种各样的问题上工作得很好，现在被广泛使用。\n\nLSTM专为解决长期依赖问题而设计。记忆长周期的信息是默认的行为而不是努力需要学习的东西。\n现有的循环神经网络都会有一个链来重复神经网络的模块。在标准的RNN中一般是一个非常简单的结构，如一个$tanh$层。\n\n![3-1](/posts_res/2018-06-20-rnn1/3-1.jpg)\n\nLSTMs也有一个类似链结构，但是重复模块有一个复杂的结构。相对于单神经网络层，LSTM有4层，以一个非常特殊的方式交互。\n\n![3-2](/posts_res/2018-06-20-rnn1/3-2.jpg)\n\n不用担心其中的细节。我们将会一步步图解LSTM。现在让我们熟悉一下将用到的符号。\n\n![3-3](/posts_res/2018-06-20-rnn1/3-3.jpg)\n\n在上图中，每个线代表一个完整的向量，从一个节点输出到其他节点的输入，粉色的圈代表节点操作，\n比如向量加，而黄色框是学习到的神经网络层。线合并代表连接，线分叉代表复制。\n\n<br>\n\n**LSTMs背后的核心概念**\n\nLSTM的关键是Cell的状态，水平线穿过计算图的顶部。Cell状态有点像输送带。直接在整个链上运行，只有一些小的线性相互作用。 信息流畅地保持不变。\n\n![3-4](/posts_res/2018-06-20-rnn1/3-4.jpg)\n\nLSTMs没有能力去添加或者删除cell状态的信息，而是由称作门（gates)的结构进行调节。\n门是一个可以让信息选择性通过的结构。由一个$sigmoid$神经网络层和一个节点乘算法组成。\n\n![3-5](/posts_res/2018-06-20-rnn1/3-5.jpg)\n\n<br>\n\n**一步步了解LSTM**\n\nLSTM中，第一步是决定我们将丢弃什么信息，并把剩下的送入cell状态。这个决定由叫做**遗忘门层（forget gate layer）**的$sigmoid$层做出。\n它通过观察$h_{t-1}$和$x_t$，为cell状态$C_{t-1}$中的每个数输出一个 0 到 1 之间的数，1表示完全保留，0表示完全抛弃。\n\n让我们回到语言模型的例子。在这个预测下一个单词的问题中，cell状态也许会包含前面对象的属性，所以可以使用正确的代名词。当我们看到一个新的对象，我们希望忘记旧对象的属性。\n\n![3-6](/posts_res/2018-06-20-rnn1/3-6.jpg)\n\n下一步是来决定我们将保存什么新信息到cell状态中。这有两个部分: 首先，一个$sigmoid$层**（input gate layer, 输入门层）**决定哪个值将被更新;\n然后，一个$tanh$层创建一个新候选值的向量$ \\tilde{C}_t $，可以被加入到cell状态中。在下一步，我们将要合并这两步来创建一个状态的更新。\n\n在语言模型的例子里，我们想要添加新对象的属性到cell状态中来替换我们希望遗忘的旧对象的属性。\n\n![3-7](/posts_res/2018-06-20-rnn1/3-7.jpg)\n\n现在是时候来更新旧的cell状态$C_{t-1}$为新状态$C_t$。前面的步骤已经决定了应该做什么，我们只需要实施。\n\n我们**将旧状态乘$f_t$，忘记我们之前决定忘记的东西。然后我们加上$i_t * \\tilde{C}_t$。这是新的候选值，由我们决定更新每一个状态值的多少来缩放**。\n\n在语言模型的例子里，这是我们丢弃旧对象信息并添加新信息的地方。就如同我们在前一步决定的那样。\n\n![3-8](/posts_res/2018-06-20-rnn1/3-8.jpg)\n\n最终，我们需要决定输出什么**(output gate layer，输出门层)**。这输出将基于我们的cell状态，但会是一个过滤(filtered)后的版本。\n首先，我们运行一个$sigmoid$层来决定cell状态的哪些部分将被输出。然后，我们将cell状态通过$tanh$(将值转化为-1到1之间)并与$sigmoid$门的输出相乘，就可以只输出我们想要的部分。\n\n对于语言模型例子，因为只看到一个对象，在预测下一个词的时候，它也许想要输出一个关于动词的信息。\n比如，它也许输出这个对象是单数还是复数，因此我们知道接下来的是一个动词形式的。\n\n![3-9](/posts_res/2018-06-20-rnn1/3-9.jpg)\n\n\n--------\n\n### 4.LSTM的变种\n\n上文讲述了一个普通的LSTM。但不是所有的LSTMs都和上面一样。事实上，看起来几乎所有论文都用了一个有一点不一样的LSTM。区别很小，但是值得注意。\n\n<br>\n\n一个著名的LSTM变种，由[Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)提出，\n添加了窥视孔连接（peephole connections）。这意味着可以让门层查看cell状态(This means that we let the gate layers look at the cell state)。\n\n![4-1](/posts_res/2018-06-20-rnn1/4-1.jpg)\n\n上面的图添加了链接到所有门的窥孔，但很多论文只会添加其中的一些。\n\n<br>\n\n另一个变种是使用耦合的遗忘和输出门。相对于普通的LSTM分别决定添加和遗忘什么信息，这个变种同时作出决定，只在输入相同位置信息的时候遗忘当前的信息。\n只有在忘记一些值的时候才输入新值到状态中。\n\n![4-2](/posts_res/2018-06-20-rnn1/4-2.jpg)\n\n<br>\n\n一个有点戏剧化的LSTM变种是GRU（Gated Recurrent Unit），由 [Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf)介绍。\n它把遗忘和输入门结合成一个“更新门”，同时还合并了cell状态和隐藏状态，并有其他一些改变。这使得模型比标准LSTM要简洁，并越来越流行。\n\n![4-3](/posts_res/2018-06-20-rnn1/4-3.jpg)\n\n<br>\n\n这只是众多值得注意的LSTM变种中的几个。还有很多，如Depth Gated RNNs,[Yao, et al. (2015)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.03790v2.pdf)。\n同时还有一些完全不同的追踪长依赖的方式，比如Clockwork RNNs，[Koutnik, et al. (2014)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1402.3511v1.pdf).\n\n哪个变种是最好的？差别有必要吗？[Greff, et al. (2015)](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.04069.pdf)对流行的变种做了一个总结，发现他们几乎一样。\n[Jozefowicz, et al. (2015)](https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)测试了超过一万种RNN结构，发现有一些在特定问题上比LSTM更好。\n\n\n----------\n\n### 5.结论\n\n早前提到了人们在RNN获得的显著成就。基本都是由LSTM实现的，他们对于大多数任务都工作的很好！\n\nLSTM的方程看起来很吓人，但是通过本文一步一步讲解，希望它们看起来更易懂。\n\nLSTM是我们可以用RNN完成的一大进步。很自然的，我们会想，还有另一大步吗？研究人员的共同观点是：是的！\n还有下一步这就是注意力（attention）！这个想法是让RNN的每一步从一些较大的信息合集中挑选信息。\n比如，您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个字。\n事实上[Xu, et al. (2015)](http://arxiv.org/pdf/1502.03044v2.pdf)做了这个，如果你想要探索更多关于注意力，这可能是一个有趣的起点！\n有一些真正令人兴奋的结果使用了注意力...\n\n注意力并不是RNN研究中唯一的突破口。例如Grid LSTMs，[Kalchbrenner, et al. (2015)](http://arxiv.org/pdf/1507.01526v1.pdf)看起来非常有意思。\n在生成模型中使用 RNN 进行工作，如[Gregor, et al. (2015)](http://arxiv.org/pdf/1502.04623.pdf), \n[Chung, et al. (2015)](http://arxiv.org/pdf/1506.02216v3.pdf), [Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf)。\n","slug":"rnn1","published":1,"updated":"2019-08-17T09:37:46.506Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw6002n2qwp3y2aw744","content":"<p>目录</p><ul>\n<li>1.循环神经网络</li>\n<li>2.长期依赖的问题</li>\n<li>3.LSTM 网络</li>\n<li>4.LSTM的变种</li>\n<li>5.结论</li>\n</ul><p>译自：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"noopener\">Understanding LSTM Networks</a></p><hr><h3 id=\"1-循环神经网络\"><a href=\"#1-循环神经网络\" class=\"headerlink\" title=\"1.循环神经网络\"></a>1.循环神经网络</h3><p>人类不会每秒钟都从头开始思考。在阅读本文时，你可以根据以前单词的理解来理解每个单词。你不要把所有东西都扔掉，再从头开始思考。 你的想法有持久性。 \n在这方面，传统神经网络不能做到这一点，这是一个很大的缺点。 例如，想象一下，当你想要分析电影中剧情的发展。 传统神经网络是不能够利用电影中以前的事件来推理以后的事情的。 \n而循环神经网络解决了这个问题。 它们是具有循环的网络，允许信息影响持续存在。</p><p><img src=\"/posts_res/2018-06-20-rnn1/1-1.jpg\" alt=\"1-1\"></p><p>在上图中，一组神经网络$A$输入$X_t$并输出一个值$h_t$。 循环允许信息从网络的一个步骤传递到下一个。\n这些循环使得循环神经网络显得很神秘。 然而，如果你仔细想想，它们并不与一般的神经网络完全不同。 \n一个循环神经网络可以被认为是同一个网络的多个副本，每一个都传递一个信息给后继者。 \n考虑如果我们展开循环会发生什么：</p><a id=\"more\"></a>\n\n\n\n\n\n\n<p><img src=\"/posts_res/2018-06-20-rnn1/1-2.jpg\" alt=\"1-2\"></p>\n<p>这种链状特征揭示了循环神经网络与序列和列表密切相关，它们是用于此类数据的神经网络的自然结构。\n而且他们确实得到应用！ 在过去几年里，将RNN应用于语音识别，语言建模，翻译，图像字幕等各种问题已经取得了令人难以置信的成功。\n我将讨论可以通过RNN实现的令人惊叹的专长，使Andrej Karpathy的优秀博客\n“<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\" rel=\"noopener\">The Unreasonable Effectiveness of Recurrent Neural Networks</a>”成为可能。 \n但他们真的很神奇。这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，对于许多任务来说，它们比标准版本好得多。\n几乎所有令人兴奋的结果基于循环神经网络是通过他们实现的。这篇文章将探讨这些LSTM。</p>\n<hr>\n<h3 id=\"2-长期依赖的问题\"><a href=\"#2-长期依赖的问题\" class=\"headerlink\" title=\"2.长期依赖的问题\"></a>2.长期依赖的问题</h3><p>RNNs吸引人的一点在于它能够将过去的信息连接到当前任务，例如使用先前的视频帧可以为当前帧的理解提供信息。如果RNNs可以做到这一点，他们将非常有用。\n但他们可以吗？这取决于不同的情况。有时候，我们只需要最近的信息来处理当前的任务。\n举个例子，输入法能够通过你输入的上一个词来预测你想要输入的下一个词。比如，我们想要预测“云彩在天上”，我们不需要更多的信息就能够预测最后两个字。\n在这种情况下，如果相关信息与所需预测的信息间隔很小，则RNN可以学习使用过去的信息。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/2-1.jpg\" alt=\"2-1\"></p>\n<p>但有些情况下我们就需要更多的文本。比如预测“我来自法国…我法语很流利…”，最近的信息预测这是一个语言名词，但是当我们想确定具体是哪一种语言的时候，\n我们需要找到包含法国的文本，但这个文本很可能间隔非常远。很不幸，当间隔增加，RNNs就会更难以连接相关的信息。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/2-2.jpg\" alt=\"2-2\"></p>\n<p>理论上，RNNs完全有能力有能力处理这样的“长期依赖”。一个人可以仔细挑选参数来解决这种形式的问题。但实践却表明RNNs似乎不能学习。\n<a href=\"http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\" target=\"_blank\" rel=\"noopener\">Hochreiter(1991) German</a>深层次的揭露了这个问题，\n而<a href=\"https://link.zhihu.com/?target=http%3A//www-dsi.ing.unifi.it/%7Epaolo/ps/tnn-94-gradient.pdf\" target=\"_blank\" rel=\"noopener\">Bengio,et al.(1994)</a>解释了一些基本的原因。</p>\n<p>但是，LSTMs没有这个问题！</p>\n<hr>\n<h3 id=\"3-LSTM-网络\"><a href=\"#3-LSTM-网络\" class=\"headerlink\" title=\"3.LSTM 网络\"></a>3.LSTM 网络</h3><p>长短记忆网络（Long Short Term Memory networks）通常称为“LSTMs”-是一种特殊的RNN，有能力学习长期的依赖。\nLSTMs最初由<a href=\"https://link.zhihu.com/?target=http%3A//deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\" target=\"_blank\" rel=\"noopener\">Hochreiter &amp; Schmidhuber (1997)</a>提出，\n并在之后的工作中由许多人优化推广。LSTM在各种各样的问题上工作得很好，现在被广泛使用。</p>\n<p>LSTM专为解决长期依赖问题而设计。记忆长周期的信息是默认的行为而不是努力需要学习的东西。\n现有的循环神经网络都会有一个链来重复神经网络的模块。在标准的RNN中一般是一个非常简单的结构，如一个$tanh$层。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-1.jpg\" alt=\"3-1\"></p>\n<p>LSTMs也有一个类似链结构，但是重复模块有一个复杂的结构。相对于单神经网络层，LSTM有4层，以一个非常特殊的方式交互。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-2.jpg\" alt=\"3-2\"></p>\n<p>不用担心其中的细节。我们将会一步步图解LSTM。现在让我们熟悉一下将用到的符号。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-3.jpg\" alt=\"3-3\"></p>\n<p>在上图中，每个线代表一个完整的向量，从一个节点输出到其他节点的输入，粉色的圈代表节点操作，\n比如向量加，而黄色框是学习到的神经网络层。线合并代表连接，线分叉代表复制。</p>\n<p><br></p>\n<p><strong>LSTMs背后的核心概念</strong></p>\n<p>LSTM的关键是Cell的状态，水平线穿过计算图的顶部。Cell状态有点像输送带。直接在整个链上运行，只有一些小的线性相互作用。 信息流畅地保持不变。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-4.jpg\" alt=\"3-4\"></p>\n<p>LSTMs没有能力去添加或者删除cell状态的信息，而是由称作门（gates)的结构进行调节。\n门是一个可以让信息选择性通过的结构。由一个$sigmoid$神经网络层和一个节点乘算法组成。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-5.jpg\" alt=\"3-5\"></p>\n<p><br></p>\n<p><strong>一步步了解LSTM</strong></p>\n<p>LSTM中，第一步是决定我们将丢弃什么信息，并把剩下的送入cell状态。这个决定由叫做<strong>遗忘门层（forget gate layer）</strong>的$sigmoid$层做出。\n它通过观察$h<em>{t-1}$和$x_t$，为cell状态$C</em>{t-1}$中的每个数输出一个 0 到 1 之间的数，1表示完全保留，0表示完全抛弃。</p>\n<p>让我们回到语言模型的例子。在这个预测下一个单词的问题中，cell状态也许会包含前面对象的属性，所以可以使用正确的代名词。当我们看到一个新的对象，我们希望忘记旧对象的属性。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-6.jpg\" alt=\"3-6\"></p>\n<p>下一步是来决定我们将保存什么新信息到cell状态中。这有两个部分: 首先，一个$sigmoid$层<strong>（input gate layer, 输入门层）</strong>决定哪个值将被更新;\n然后，一个$tanh$层创建一个新候选值的向量$ \\tilde{C}_t $，可以被加入到cell状态中。在下一步，我们将要合并这两步来创建一个状态的更新。</p>\n<p>在语言模型的例子里，我们想要添加新对象的属性到cell状态中来替换我们希望遗忘的旧对象的属性。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-7.jpg\" alt=\"3-7\"></p>\n<p>现在是时候来更新旧的cell状态$C_{t-1}$为新状态$C_t$。前面的步骤已经决定了应该做什么，我们只需要实施。</p>\n<p>我们<strong>将旧状态乘$f_t$，忘记我们之前决定忘记的东西。然后我们加上$i_t * \\tilde{C}_t$。这是新的候选值，由我们决定更新每一个状态值的多少来缩放</strong>。</p>\n<p>在语言模型的例子里，这是我们丢弃旧对象信息并添加新信息的地方。就如同我们在前一步决定的那样。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-8.jpg\" alt=\"3-8\"></p>\n<p>最终，我们需要决定输出什么<strong>(output gate layer，输出门层)</strong>。这输出将基于我们的cell状态，但会是一个过滤(filtered)后的版本。\n首先，我们运行一个$sigmoid$层来决定cell状态的哪些部分将被输出。然后，我们将cell状态通过$tanh$(将值转化为-1到1之间)并与$sigmoid$门的输出相乘，就可以只输出我们想要的部分。</p>\n<p>对于语言模型例子，因为只看到一个对象，在预测下一个词的时候，它也许想要输出一个关于动词的信息。\n比如，它也许输出这个对象是单数还是复数，因此我们知道接下来的是一个动词形式的。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-9.jpg\" alt=\"3-9\"></p>\n<hr>\n<h3 id=\"4-LSTM的变种\"><a href=\"#4-LSTM的变种\" class=\"headerlink\" title=\"4.LSTM的变种\"></a>4.LSTM的变种</h3><p>上文讲述了一个普通的LSTM。但不是所有的LSTMs都和上面一样。事实上，看起来几乎所有论文都用了一个有一点不一样的LSTM。区别很小，但是值得注意。</p>\n<p><br></p>\n<p>一个著名的LSTM变种，由<a href=\"ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf\" target=\"_blank\" rel=\"noopener\">Gers &amp; Schmidhuber (2000)</a>提出，\n添加了窥视孔连接（peephole connections）。这意味着可以让门层查看cell状态(This means that we let the gate layers look at the cell state)。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-1.jpg\" alt=\"4-1\"></p>\n<p>上面的图添加了链接到所有门的窥孔，但很多论文只会添加其中的一些。</p>\n<p><br></p>\n<p>另一个变种是使用耦合的遗忘和输出门。相对于普通的LSTM分别决定添加和遗忘什么信息，这个变种同时作出决定，只在输入相同位置信息的时候遗忘当前的信息。\n只有在忘记一些值的时候才输入新值到状态中。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-2.jpg\" alt=\"4-2\"></p>\n<p><br></p>\n<p>一个有点戏剧化的LSTM变种是GRU（Gated Recurrent Unit），由 <a href=\"http://arxiv.org/pdf/1406.1078v3.pdf\" target=\"_blank\" rel=\"noopener\">Cho, et al. (2014)</a>介绍。\n它把遗忘和输入门结合成一个“更新门”，同时还合并了cell状态和隐藏状态，并有其他一些改变。这使得模型比标准LSTM要简洁，并越来越流行。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-3.jpg\" alt=\"4-3\"></p>\n<p><br></p>\n<p>这只是众多值得注意的LSTM变种中的几个。还有很多，如Depth Gated RNNs,<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.03790v2.pdf\" target=\"_blank\" rel=\"noopener\">Yao, et al. (2015)</a>。\n同时还有一些完全不同的追踪长依赖的方式，比如Clockwork RNNs，<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1402.3511v1.pdf\" target=\"_blank\" rel=\"noopener\">Koutnik, et al. (2014)</a>.</p>\n<p>哪个变种是最好的？差别有必要吗？<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.04069.pdf\" target=\"_blank\" rel=\"noopener\">Greff, et al. (2015)</a>对流行的变种做了一个总结，发现他们几乎一样。\n<a href=\"https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\" target=\"_blank\" rel=\"noopener\">Jozefowicz, et al. (2015)</a>测试了超过一万种RNN结构，发现有一些在特定问题上比LSTM更好。</p>\n<hr>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5.结论\"></a>5.结论</h3><p>早前提到了人们在RNN获得的显著成就。基本都是由LSTM实现的，他们对于大多数任务都工作的很好！</p>\n<p>LSTM的方程看起来很吓人，但是通过本文一步一步讲解，希望它们看起来更易懂。</p>\n<p>LSTM是我们可以用RNN完成的一大进步。很自然的，我们会想，还有另一大步吗？研究人员的共同观点是：是的！\n还有下一步这就是注意力（attention）！这个想法是让RNN的每一步从一些较大的信息合集中挑选信息。\n比如，您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个字。\n事实上<a href=\"http://arxiv.org/pdf/1502.03044v2.pdf\" target=\"_blank\" rel=\"noopener\">Xu, et al. (2015)</a>做了这个，如果你想要探索更多关于注意力，这可能是一个有趣的起点！\n有一些真正令人兴奋的结果使用了注意力…</p>\n<p>注意力并不是RNN研究中唯一的突破口。例如Grid LSTMs，<a href=\"http://arxiv.org/pdf/1507.01526v1.pdf\" target=\"_blank\" rel=\"noopener\">Kalchbrenner, et al. (2015)</a>看起来非常有意思。\n在生成模型中使用 RNN 进行工作，如<a href=\"http://arxiv.org/pdf/1502.04623.pdf\" target=\"_blank\" rel=\"noopener\">Gregor, et al. (2015)</a>, \n<a href=\"http://arxiv.org/pdf/1506.02216v3.pdf\" target=\"_blank\" rel=\"noopener\">Chung, et al. (2015)</a>, <a href=\"http://arxiv.org/pdf/1411.7610v3.pdf\" target=\"_blank\" rel=\"noopener\">Bayer &amp; Osendorfer (2015)</a>。</p>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>1.循环神经网络</li>\n<li>2.长期依赖的问题</li>\n<li>3.LSTM 网络</li>\n<li>4.LSTM的变种</li>\n<li>5.结论</li>\n</ul><p>译自：<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"noopener\">Understanding LSTM Networks</a></p><hr><h3 id=\"1-循环神经网络\"><a href=\"#1-循环神经网络\" class=\"headerlink\" title=\"1.循环神经网络\"></a>1.循环神经网络</h3><p>人类不会每秒钟都从头开始思考。在阅读本文时，你可以根据以前单词的理解来理解每个单词。你不要把所有东西都扔掉，再从头开始思考。 你的想法有持久性。 \n在这方面，传统神经网络不能做到这一点，这是一个很大的缺点。 例如，想象一下，当你想要分析电影中剧情的发展。 传统神经网络是不能够利用电影中以前的事件来推理以后的事情的。 \n而循环神经网络解决了这个问题。 它们是具有循环的网络，允许信息影响持续存在。</p><p><img src=\"/posts_res/2018-06-20-rnn1/1-1.jpg\" alt=\"1-1\"></p><p>在上图中，一组神经网络$A$输入$X_t$并输出一个值$h_t$。 循环允许信息从网络的一个步骤传递到下一个。\n这些循环使得循环神经网络显得很神秘。 然而，如果你仔细想想，它们并不与一般的神经网络完全不同。 \n一个循环神经网络可以被认为是同一个网络的多个副本，每一个都传递一个信息给后继者。 \n考虑如果我们展开循环会发生什么：</p>","more":"\n\n\n\n\n\n\n<p><img src=\"/posts_res/2018-06-20-rnn1/1-2.jpg\" alt=\"1-2\"></p>\n<p>这种链状特征揭示了循环神经网络与序列和列表密切相关，它们是用于此类数据的神经网络的自然结构。\n而且他们确实得到应用！ 在过去几年里，将RNN应用于语音识别，语言建模，翻译，图像字幕等各种问题已经取得了令人难以置信的成功。\n我将讨论可以通过RNN实现的令人惊叹的专长，使Andrej Karpathy的优秀博客\n“<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\" rel=\"noopener\">The Unreasonable Effectiveness of Recurrent Neural Networks</a>”成为可能。 \n但他们真的很神奇。这些成功的关键在于使用“LSTM”，这是一种非常特殊的循环神经网络，对于许多任务来说，它们比标准版本好得多。\n几乎所有令人兴奋的结果基于循环神经网络是通过他们实现的。这篇文章将探讨这些LSTM。</p>\n<hr>\n<h3 id=\"2-长期依赖的问题\"><a href=\"#2-长期依赖的问题\" class=\"headerlink\" title=\"2.长期依赖的问题\"></a>2.长期依赖的问题</h3><p>RNNs吸引人的一点在于它能够将过去的信息连接到当前任务，例如使用先前的视频帧可以为当前帧的理解提供信息。如果RNNs可以做到这一点，他们将非常有用。\n但他们可以吗？这取决于不同的情况。有时候，我们只需要最近的信息来处理当前的任务。\n举个例子，输入法能够通过你输入的上一个词来预测你想要输入的下一个词。比如，我们想要预测“云彩在天上”，我们不需要更多的信息就能够预测最后两个字。\n在这种情况下，如果相关信息与所需预测的信息间隔很小，则RNN可以学习使用过去的信息。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/2-1.jpg\" alt=\"2-1\"></p>\n<p>但有些情况下我们就需要更多的文本。比如预测“我来自法国…我法语很流利…”，最近的信息预测这是一个语言名词，但是当我们想确定具体是哪一种语言的时候，\n我们需要找到包含法国的文本，但这个文本很可能间隔非常远。很不幸，当间隔增加，RNNs就会更难以连接相关的信息。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/2-2.jpg\" alt=\"2-2\"></p>\n<p>理论上，RNNs完全有能力有能力处理这样的“长期依赖”。一个人可以仔细挑选参数来解决这种形式的问题。但实践却表明RNNs似乎不能学习。\n<a href=\"http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\" target=\"_blank\" rel=\"noopener\">Hochreiter(1991) German</a>深层次的揭露了这个问题，\n而<a href=\"https://link.zhihu.com/?target=http%3A//www-dsi.ing.unifi.it/%7Epaolo/ps/tnn-94-gradient.pdf\" target=\"_blank\" rel=\"noopener\">Bengio,et al.(1994)</a>解释了一些基本的原因。</p>\n<p>但是，LSTMs没有这个问题！</p>\n<hr>\n<h3 id=\"3-LSTM-网络\"><a href=\"#3-LSTM-网络\" class=\"headerlink\" title=\"3.LSTM 网络\"></a>3.LSTM 网络</h3><p>长短记忆网络（Long Short Term Memory networks）通常称为“LSTMs”-是一种特殊的RNN，有能力学习长期的依赖。\nLSTMs最初由<a href=\"https://link.zhihu.com/?target=http%3A//deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\" target=\"_blank\" rel=\"noopener\">Hochreiter &amp; Schmidhuber (1997)</a>提出，\n并在之后的工作中由许多人优化推广。LSTM在各种各样的问题上工作得很好，现在被广泛使用。</p>\n<p>LSTM专为解决长期依赖问题而设计。记忆长周期的信息是默认的行为而不是努力需要学习的东西。\n现有的循环神经网络都会有一个链来重复神经网络的模块。在标准的RNN中一般是一个非常简单的结构，如一个$tanh$层。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-1.jpg\" alt=\"3-1\"></p>\n<p>LSTMs也有一个类似链结构，但是重复模块有一个复杂的结构。相对于单神经网络层，LSTM有4层，以一个非常特殊的方式交互。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-2.jpg\" alt=\"3-2\"></p>\n<p>不用担心其中的细节。我们将会一步步图解LSTM。现在让我们熟悉一下将用到的符号。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-3.jpg\" alt=\"3-3\"></p>\n<p>在上图中，每个线代表一个完整的向量，从一个节点输出到其他节点的输入，粉色的圈代表节点操作，\n比如向量加，而黄色框是学习到的神经网络层。线合并代表连接，线分叉代表复制。</p>\n<p><br></p>\n<p><strong>LSTMs背后的核心概念</strong></p>\n<p>LSTM的关键是Cell的状态，水平线穿过计算图的顶部。Cell状态有点像输送带。直接在整个链上运行，只有一些小的线性相互作用。 信息流畅地保持不变。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-4.jpg\" alt=\"3-4\"></p>\n<p>LSTMs没有能力去添加或者删除cell状态的信息，而是由称作门（gates)的结构进行调节。\n门是一个可以让信息选择性通过的结构。由一个$sigmoid$神经网络层和一个节点乘算法组成。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-5.jpg\" alt=\"3-5\"></p>\n<p><br></p>\n<p><strong>一步步了解LSTM</strong></p>\n<p>LSTM中，第一步是决定我们将丢弃什么信息，并把剩下的送入cell状态。这个决定由叫做<strong>遗忘门层（forget gate layer）</strong>的$sigmoid$层做出。\n它通过观察$h<em>{t-1}$和$x_t$，为cell状态$C</em>{t-1}$中的每个数输出一个 0 到 1 之间的数，1表示完全保留，0表示完全抛弃。</p>\n<p>让我们回到语言模型的例子。在这个预测下一个单词的问题中，cell状态也许会包含前面对象的属性，所以可以使用正确的代名词。当我们看到一个新的对象，我们希望忘记旧对象的属性。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-6.jpg\" alt=\"3-6\"></p>\n<p>下一步是来决定我们将保存什么新信息到cell状态中。这有两个部分: 首先，一个$sigmoid$层<strong>（input gate layer, 输入门层）</strong>决定哪个值将被更新;\n然后，一个$tanh$层创建一个新候选值的向量$ \\tilde{C}_t $，可以被加入到cell状态中。在下一步，我们将要合并这两步来创建一个状态的更新。</p>\n<p>在语言模型的例子里，我们想要添加新对象的属性到cell状态中来替换我们希望遗忘的旧对象的属性。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-7.jpg\" alt=\"3-7\"></p>\n<p>现在是时候来更新旧的cell状态$C_{t-1}$为新状态$C_t$。前面的步骤已经决定了应该做什么，我们只需要实施。</p>\n<p>我们<strong>将旧状态乘$f_t$，忘记我们之前决定忘记的东西。然后我们加上$i_t * \\tilde{C}_t$。这是新的候选值，由我们决定更新每一个状态值的多少来缩放</strong>。</p>\n<p>在语言模型的例子里，这是我们丢弃旧对象信息并添加新信息的地方。就如同我们在前一步决定的那样。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-8.jpg\" alt=\"3-8\"></p>\n<p>最终，我们需要决定输出什么<strong>(output gate layer，输出门层)</strong>。这输出将基于我们的cell状态，但会是一个过滤(filtered)后的版本。\n首先，我们运行一个$sigmoid$层来决定cell状态的哪些部分将被输出。然后，我们将cell状态通过$tanh$(将值转化为-1到1之间)并与$sigmoid$门的输出相乘，就可以只输出我们想要的部分。</p>\n<p>对于语言模型例子，因为只看到一个对象，在预测下一个词的时候，它也许想要输出一个关于动词的信息。\n比如，它也许输出这个对象是单数还是复数，因此我们知道接下来的是一个动词形式的。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/3-9.jpg\" alt=\"3-9\"></p>\n<hr>\n<h3 id=\"4-LSTM的变种\"><a href=\"#4-LSTM的变种\" class=\"headerlink\" title=\"4.LSTM的变种\"></a>4.LSTM的变种</h3><p>上文讲述了一个普通的LSTM。但不是所有的LSTMs都和上面一样。事实上，看起来几乎所有论文都用了一个有一点不一样的LSTM。区别很小，但是值得注意。</p>\n<p><br></p>\n<p>一个著名的LSTM变种，由<a href=\"ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf\" target=\"_blank\" rel=\"noopener\">Gers &amp; Schmidhuber (2000)</a>提出，\n添加了窥视孔连接（peephole connections）。这意味着可以让门层查看cell状态(This means that we let the gate layers look at the cell state)。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-1.jpg\" alt=\"4-1\"></p>\n<p>上面的图添加了链接到所有门的窥孔，但很多论文只会添加其中的一些。</p>\n<p><br></p>\n<p>另一个变种是使用耦合的遗忘和输出门。相对于普通的LSTM分别决定添加和遗忘什么信息，这个变种同时作出决定，只在输入相同位置信息的时候遗忘当前的信息。\n只有在忘记一些值的时候才输入新值到状态中。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-2.jpg\" alt=\"4-2\"></p>\n<p><br></p>\n<p>一个有点戏剧化的LSTM变种是GRU（Gated Recurrent Unit），由 <a href=\"http://arxiv.org/pdf/1406.1078v3.pdf\" target=\"_blank\" rel=\"noopener\">Cho, et al. (2014)</a>介绍。\n它把遗忘和输入门结合成一个“更新门”，同时还合并了cell状态和隐藏状态，并有其他一些改变。这使得模型比标准LSTM要简洁，并越来越流行。</p>\n<p><img src=\"/posts_res/2018-06-20-rnn1/4-3.jpg\" alt=\"4-3\"></p>\n<p><br></p>\n<p>这只是众多值得注意的LSTM变种中的几个。还有很多，如Depth Gated RNNs,<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.03790v2.pdf\" target=\"_blank\" rel=\"noopener\">Yao, et al. (2015)</a>。\n同时还有一些完全不同的追踪长依赖的方式，比如Clockwork RNNs，<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1402.3511v1.pdf\" target=\"_blank\" rel=\"noopener\">Koutnik, et al. (2014)</a>.</p>\n<p>哪个变种是最好的？差别有必要吗？<a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.04069.pdf\" target=\"_blank\" rel=\"noopener\">Greff, et al. (2015)</a>对流行的变种做了一个总结，发现他们几乎一样。\n<a href=\"https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\" target=\"_blank\" rel=\"noopener\">Jozefowicz, et al. (2015)</a>测试了超过一万种RNN结构，发现有一些在特定问题上比LSTM更好。</p>\n<hr>\n<h3 id=\"5-结论\"><a href=\"#5-结论\" class=\"headerlink\" title=\"5.结论\"></a>5.结论</h3><p>早前提到了人们在RNN获得的显著成就。基本都是由LSTM实现的，他们对于大多数任务都工作的很好！</p>\n<p>LSTM的方程看起来很吓人，但是通过本文一步一步讲解，希望它们看起来更易懂。</p>\n<p>LSTM是我们可以用RNN完成的一大进步。很自然的，我们会想，还有另一大步吗？研究人员的共同观点是：是的！\n还有下一步这就是注意力（attention）！这个想法是让RNN的每一步从一些较大的信息合集中挑选信息。\n比如，您使用RNN创建描述图像的标题，则可能会选择图像的一部分来查看其输出的每个字。\n事实上<a href=\"http://arxiv.org/pdf/1502.03044v2.pdf\" target=\"_blank\" rel=\"noopener\">Xu, et al. (2015)</a>做了这个，如果你想要探索更多关于注意力，这可能是一个有趣的起点！\n有一些真正令人兴奋的结果使用了注意力…</p>\n<p>注意力并不是RNN研究中唯一的突破口。例如Grid LSTMs，<a href=\"http://arxiv.org/pdf/1507.01526v1.pdf\" target=\"_blank\" rel=\"noopener\">Kalchbrenner, et al. (2015)</a>看起来非常有意思。\n在生成模型中使用 RNN 进行工作，如<a href=\"http://arxiv.org/pdf/1502.04623.pdf\" target=\"_blank\" rel=\"noopener\">Gregor, et al. (2015)</a>, \n<a href=\"http://arxiv.org/pdf/1506.02216v3.pdf\" target=\"_blank\" rel=\"noopener\">Chung, et al. (2015)</a>, <a href=\"http://arxiv.org/pdf/1411.7610v3.pdf\" target=\"_blank\" rel=\"noopener\">Bayer &amp; Osendorfer (2015)</a>。</p>\n"},{"layout":"post","title":"卷积神经网络CNN初识","date":"2018-06-28T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n目录\n\n* 1.引入\n* 2.卷积神经网络的结构\n* 3.卷积神经网络的正式表达\n* 4.理解卷积\n\n\n译自：[Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)，\n[Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n\n----------\n\n### 1.引入\n\n在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。\n\n简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 \n但是因为其具有的重复性，我们需要学习的参数相对来说非常少。\n\n![1-1](/posts_res/2018-06-28-cnn1/1-1.png)\n\n这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。\n这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数，\n并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。\n\n\n-------\n\n### 2.卷积神经网络的结构\n\n假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。\n假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。\n\n![2-1](/posts_res/2018-06-28-cnn1/2-1.png)\n\n引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出）\n\n![2-2](/posts_res/2018-06-28-cnn1/2-2.png)\n\n更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？\n\n我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性，\n所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。\n\n因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$\n\n![2-3](/posts_res/2018-06-28-cnn1/2-3.png)\n\n在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些\n（所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数）\n\n在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。\n\n![2-4](/posts_res/2018-06-28-cnn1/2-4.png)\n\n卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。\n\n如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。\n\n![2-5](/posts_res/2018-06-28-cnn1/2-5.png)\n\n卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。\n通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。\n比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。\n\n最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。\n\n最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。\n最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。\n由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。\n\n![2-6](/posts_res/2018-06-28-cnn1/2-6.png)\n\n至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。\n\n<br>\n\n在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。\n例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。\n\n![2-7](/posts_res/2018-06-28-cnn1/2-7.png)\n\n上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。\n\n![2-8](/posts_res/2018-06-28-cnn1/2-8.png)\n\n同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。\n这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。\n\n![2-9](/posts_res/2018-06-28-cnn1/2-9.png)\n\n<br>\n\n有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。\n\n上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。\n在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。\n\n例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。\n\n![2-10](/posts_res/2018-06-28-cnn1/2-10.png)\n\n在论文[Network in Network](http://arxiv.org/abs/1312.4400)中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点，\n由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。\n\n![2-11](/posts_res/2018-06-28-cnn1/2-11.png)\n\n这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。\n\n\n--------\n\n### 3.卷积神经网络的正式表达\n\n考虑一维的卷积层，其中输入为$\\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $\n\n![3-1](/posts_res/2018-06-28-cnn1/3-1.png)\n\n很容易可以用输入表示输出：\n\n$$ y_n = A( x_n, x_{n+1}, \\cdots ) $$\n\n如上图：\n\n$$\ny_0 = A(x_0, x_1) \\\\\ny_1 = A(x_1, x_2)\n$$\n\n相似地，如果我们考虑二维的卷积层，其中输入为$\\lbrace x_{n,m} \\rbrace$，输出为$ \\lbrace y_{n,m} \\rbrace $\n\n![3-2](/posts_res/2018-06-28-cnn1/3-2.png)\n\n同样地，我们可以用输入表示输出：\n\n$$ \ny_{n,m} = A \n\\begin{pmatrix}\nx_{n,m}, & x_{n+1, m}, & \\cdots , \\\\\nx_{n,m+1}, & x_{n+1, m+1}, & \\cdots , \\\\\n & \\cdots &\n\\end{pmatrix}\n$$\n\n例如：\n\n$$\ny_{0,0}=A\n\\begin{pmatrix}\nx_{0,0}, & x_{1,0}, \\\\\nx_{1,0}, & x_{1,1}, \\\\\n\\end{pmatrix}\ny_{1,0}=A\n\\begin{pmatrix}\nx_{1,0}, & x_{2,0}, \\\\\nx_{1,1}, & x_{2,1}, \\\\\n\\end{pmatrix}\n$$\n\n把这些和矩阵方程联合起来，得$A(x)$。\n\n$$ A(x) = \\sigma (W x + b) $$\n\n实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式--关于卷积的数学运算，这通常更有帮助。\n\n卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。\n它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。\n其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。\n最后，卷积将为我们提供一个非常不同的角度来解释卷积层。\n\n\n----------\n\n### 4.理解卷积\n\n上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？\n\n**球体下落的启示**\n\n试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。\n球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。\n那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？\n\n我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。\n球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同，\n那么$g$和$f$是两个不同的概率分布。\n\n![4-1](/posts_res/2018-06-28-cnn1/4-1.png)\n\n假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \\cdot f(b)$。\n从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\\cdot g(1)$。\n\n![4-2](/posts_res/2018-06-28-cnn1/4-2.png)\n\n然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \\cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。\n\n![4-3](/posts_res/2018-06-28-cnn1/4-3.png)\n\n上述两种可能各种的概率表示依次是$f(1) \\cdot g(2)$和$ f(0) \\cdot g(3)$。\n\n为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。\n\n$$\\cdots f(0) \\cdot g(3) + f(1) \\cdot g(2) + f(2) \\cdot g(1) \\cdots $$\n\n我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \\cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为：\n\n$$ \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下:\n\n$$ (f ∗ g) (c) = \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n如果用$b=c-a$替换，则有\n\n$$(f ∗ g) (c) = \\sum_a f(a) \\cdot g(c−a) $$\n\n这正是标准的卷积定义。\n\n更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$\n\n![4-4](/posts_res/2018-06-28-cnn1/4-4.png)\n\n为了完成卷积，我们考虑所有的中转位置。\n\n![4-5](/posts_res/2018-06-28-cnn1/4-5.png)\n\n<br>\n\n**卷积的可视化**\n\n有一个非常好的例子可以帮助我们更好地理解卷积的过程。\n\n首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。\n\n![4-6](/posts_res/2018-06-28-cnn1/4-6.png)\n\n设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？\n\n![4-7](/posts_res/2018-06-28-cnn1/4-7.png)\n\n第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $\n\n我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。\n\n![4-8](/posts_res/2018-06-28-cnn1/4-8.png)\n\n对所有的$a$求和，我们就得到了卷积的结果。\n这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。\n如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。\n\n具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值)\n\n![4-9](/posts_res/2018-06-28-cnn1/4-9.png)\n\n随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。\n\n![4-10](/posts_res/2018-06-28-cnn1/4-10.png)\n\n当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。\n下图，我们用两个箱型图变现了卷积的过程：\n\n![4-11](/posts_res/2018-06-28-cnn1/4-11.gif)\n\n从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。\n卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。\n采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。\n\n<br>\n\n**高维卷积**\n\n卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。\n\n![4-12](/posts_res/2018-06-28-cnn1/4-12.png)\n\n卷积过程还是和之前的一样\n\n$$ (f ∗ g) ( c ) = \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n只是这里的$a，b，c$是向量，更明确地表示是\n\n$$ (f ∗ g)(c_1, c_2) = \\sum_{a_1 + b_1 = c_1 \\\\ a_2 + b_2 = c_2} f ( a_1, a_2) \\cdot g(b_1, b_2 ) $$\n\n或者使用标准定义：\n\n$$ (f ∗ g ) ( c_1, c_2) = \\sum{a_1,a_2} f(a_1, a_2) \\cdot g(c_1−a_1, c_2−a_2) $$\n\n和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。\n\n二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。\n\n![4-13](/posts_res/2018-06-28-cnn1/4-13.png)\n\n核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。\n\n比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9\n\n![4-14](/posts_res/2018-06-28-cnn1/4-14.png)\n\n我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。\n也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。\n如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。\n\n![4-15](/posts_res/2018-06-28-cnn1/4-15.png)\n\ngimp documentation 中有很多[其他的例子](http://docs.gimp.org/en/plug-in-convmatrix.html)。\n\n<br>\n\n**卷积神经网络**\n\n那么，卷积操作和卷积神经网络是如何联系起来的呢？\n\n首先我们考虑一维卷积神经网络，其中输入为$ \\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $，\n\n![4-16](/posts_res/2018-06-28-cnn1/4-16.png)\n\n由图可得，输入层与输出层之间的函数关系为：\n\n$$ y_n = A ( x_n, x_{n+1}, \\cdots) $$\n\n通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成：\n\n$$ \\sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \\cdots + b) $$\n\n在这个公式中，$x_0, x_1, \\cdots $代表输入；$w_0, w_1, \\cdots $代表神经节点与输入之间连接的权重。\n这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。\n正是这些权重，决定了卷积神经网络可以为我们处理的任务。\n\n具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$：\n\n$$ y = \\sigma ( W x + b ) $$\n\n由此我们可以得到：\n\n$$\ny_0 = \\sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \\cdots) \\\\\ny_1 = \\sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \\cdots)\n$$\n\n参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。\n\n![4-17](/posts_res/2018-06-28-cnn1/4-17.png)\n\n上图描述的这种神经网络可以表达成如下等式：\n\n$$\ny_0 = \\sigma ( W_{0} x_0 + W_{1} x_1 - b) \\\\\ny_1 = \\sigma ( W_{0} x_1 + W_{1} x_2 - b)\n$$\n\n然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）：\n\n$$\nW = \\left[\n\\begin{array}{ccccc} \nW_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\nW_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\nW_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\nW_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n...     &   ...   &   ...   &  ...    & ...\\\\\n\\end{array}\n\\right]\n$$\n\n上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。\n\n$$\nW = \\left[\n\\begin{array}{ccccc} \nw_0 & w_1 &  0  &  0  & ...\\\\\n 0  & w_0 & w_1 &  0  & ...\\\\\n 0  &  0  & w_0 & w_1 & ...\\\\\n 0  &  0  &  0  & w_0 & ...\\\\\n... & ... & ... & ... & ...\\\\\n\\end{array}\n\\right]\n$$\n\n与上述矩阵的点乘，其实和卷积一个向量$ \\left[ \\cdots 0, w_1, w_0, 0 \\cdots \\right] $是一样的。\n当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。\n\n*那么二维的卷积层又是什么样呢？*\n\n![4-18](/posts_res/2018-06-28-cnn1/4-18.png)\n\n上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。\n\n回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。\n\n<br>\n\n**结语**\n\n在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？\n\n- 非常强大的语言来描述网络的连接。\n    - 到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。\n- 卷积运算具有显著的实现优势。\n    - 许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。\n- 实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。\n","source":"_posts/2018-06-28-cnn1.md","raw":"---\nlayout: post\ntitle: 卷积神经网络CNN初识\ndate: 2018-06-28 12:10 +0800\ncategories: 深度学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n目录\n\n* 1.引入\n* 2.卷积神经网络的结构\n* 3.卷积神经网络的正式表达\n* 4.理解卷积\n\n\n译自：[Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)，\n[Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n\n----------\n\n### 1.引入\n\n在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。\n\n简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 \n但是因为其具有的重复性，我们需要学习的参数相对来说非常少。\n\n![1-1](/posts_res/2018-06-28-cnn1/1-1.png)\n\n这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。\n这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数，\n并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。\n\n\n-------\n\n### 2.卷积神经网络的结构\n\n假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。\n假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。\n\n![2-1](/posts_res/2018-06-28-cnn1/2-1.png)\n\n引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出）\n\n![2-2](/posts_res/2018-06-28-cnn1/2-2.png)\n\n更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？\n\n我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性，\n所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。\n\n因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$\n\n![2-3](/posts_res/2018-06-28-cnn1/2-3.png)\n\n在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些\n（所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数）\n\n在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。\n\n![2-4](/posts_res/2018-06-28-cnn1/2-4.png)\n\n卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。\n\n如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。\n\n![2-5](/posts_res/2018-06-28-cnn1/2-5.png)\n\n卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。\n通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。\n比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。\n\n最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。\n\n最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。\n最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。\n由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。\n\n![2-6](/posts_res/2018-06-28-cnn1/2-6.png)\n\n至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。\n\n<br>\n\n在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。\n例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。\n\n![2-7](/posts_res/2018-06-28-cnn1/2-7.png)\n\n上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。\n\n![2-8](/posts_res/2018-06-28-cnn1/2-8.png)\n\n同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。\n这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。\n\n![2-9](/posts_res/2018-06-28-cnn1/2-9.png)\n\n<br>\n\n有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。\n\n上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。\n在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。\n\n例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。\n\n![2-10](/posts_res/2018-06-28-cnn1/2-10.png)\n\n在论文[Network in Network](http://arxiv.org/abs/1312.4400)中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点，\n由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。\n\n![2-11](/posts_res/2018-06-28-cnn1/2-11.png)\n\n这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。\n\n\n--------\n\n### 3.卷积神经网络的正式表达\n\n考虑一维的卷积层，其中输入为$\\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $\n\n![3-1](/posts_res/2018-06-28-cnn1/3-1.png)\n\n很容易可以用输入表示输出：\n\n$$ y_n = A( x_n, x_{n+1}, \\cdots ) $$\n\n如上图：\n\n$$\ny_0 = A(x_0, x_1) \\\\\ny_1 = A(x_1, x_2)\n$$\n\n相似地，如果我们考虑二维的卷积层，其中输入为$\\lbrace x_{n,m} \\rbrace$，输出为$ \\lbrace y_{n,m} \\rbrace $\n\n![3-2](/posts_res/2018-06-28-cnn1/3-2.png)\n\n同样地，我们可以用输入表示输出：\n\n$$ \ny_{n,m} = A \n\\begin{pmatrix}\nx_{n,m}, & x_{n+1, m}, & \\cdots , \\\\\nx_{n,m+1}, & x_{n+1, m+1}, & \\cdots , \\\\\n & \\cdots &\n\\end{pmatrix}\n$$\n\n例如：\n\n$$\ny_{0,0}=A\n\\begin{pmatrix}\nx_{0,0}, & x_{1,0}, \\\\\nx_{1,0}, & x_{1,1}, \\\\\n\\end{pmatrix}\ny_{1,0}=A\n\\begin{pmatrix}\nx_{1,0}, & x_{2,0}, \\\\\nx_{1,1}, & x_{2,1}, \\\\\n\\end{pmatrix}\n$$\n\n把这些和矩阵方程联合起来，得$A(x)$。\n\n$$ A(x) = \\sigma (W x + b) $$\n\n实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式--关于卷积的数学运算，这通常更有帮助。\n\n卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。\n它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。\n其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。\n最后，卷积将为我们提供一个非常不同的角度来解释卷积层。\n\n\n----------\n\n### 4.理解卷积\n\n上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？\n\n**球体下落的启示**\n\n试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。\n球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。\n那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？\n\n我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。\n球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同，\n那么$g$和$f$是两个不同的概率分布。\n\n![4-1](/posts_res/2018-06-28-cnn1/4-1.png)\n\n假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \\cdot f(b)$。\n从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\\cdot g(1)$。\n\n![4-2](/posts_res/2018-06-28-cnn1/4-2.png)\n\n然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \\cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。\n\n![4-3](/posts_res/2018-06-28-cnn1/4-3.png)\n\n上述两种可能各种的概率表示依次是$f(1) \\cdot g(2)$和$ f(0) \\cdot g(3)$。\n\n为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。\n\n$$\\cdots f(0) \\cdot g(3) + f(1) \\cdot g(2) + f(2) \\cdot g(1) \\cdots $$\n\n我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \\cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为：\n\n$$ \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下:\n\n$$ (f ∗ g) (c) = \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n如果用$b=c-a$替换，则有\n\n$$(f ∗ g) (c) = \\sum_a f(a) \\cdot g(c−a) $$\n\n这正是标准的卷积定义。\n\n更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$\n\n![4-4](/posts_res/2018-06-28-cnn1/4-4.png)\n\n为了完成卷积，我们考虑所有的中转位置。\n\n![4-5](/posts_res/2018-06-28-cnn1/4-5.png)\n\n<br>\n\n**卷积的可视化**\n\n有一个非常好的例子可以帮助我们更好地理解卷积的过程。\n\n首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。\n\n![4-6](/posts_res/2018-06-28-cnn1/4-6.png)\n\n设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？\n\n![4-7](/posts_res/2018-06-28-cnn1/4-7.png)\n\n第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $\n\n我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。\n\n![4-8](/posts_res/2018-06-28-cnn1/4-8.png)\n\n对所有的$a$求和，我们就得到了卷积的结果。\n这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。\n如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。\n\n具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值)\n\n![4-9](/posts_res/2018-06-28-cnn1/4-9.png)\n\n随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。\n\n![4-10](/posts_res/2018-06-28-cnn1/4-10.png)\n\n当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。\n下图，我们用两个箱型图变现了卷积的过程：\n\n![4-11](/posts_res/2018-06-28-cnn1/4-11.gif)\n\n从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。\n卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。\n采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。\n\n<br>\n\n**高维卷积**\n\n卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。\n\n![4-12](/posts_res/2018-06-28-cnn1/4-12.png)\n\n卷积过程还是和之前的一样\n\n$$ (f ∗ g) ( c ) = \\sum_{a+b=c} f(a) \\cdot g(b) $$\n\n只是这里的$a，b，c$是向量，更明确地表示是\n\n$$ (f ∗ g)(c_1, c_2) = \\sum_{a_1 + b_1 = c_1 \\\\ a_2 + b_2 = c_2} f ( a_1, a_2) \\cdot g(b_1, b_2 ) $$\n\n或者使用标准定义：\n\n$$ (f ∗ g ) ( c_1, c_2) = \\sum{a_1,a_2} f(a_1, a_2) \\cdot g(c_1−a_1, c_2−a_2) $$\n\n和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。\n\n二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。\n\n![4-13](/posts_res/2018-06-28-cnn1/4-13.png)\n\n核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。\n\n比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9\n\n![4-14](/posts_res/2018-06-28-cnn1/4-14.png)\n\n我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。\n也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。\n如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。\n\n![4-15](/posts_res/2018-06-28-cnn1/4-15.png)\n\ngimp documentation 中有很多[其他的例子](http://docs.gimp.org/en/plug-in-convmatrix.html)。\n\n<br>\n\n**卷积神经网络**\n\n那么，卷积操作和卷积神经网络是如何联系起来的呢？\n\n首先我们考虑一维卷积神经网络，其中输入为$ \\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $，\n\n![4-16](/posts_res/2018-06-28-cnn1/4-16.png)\n\n由图可得，输入层与输出层之间的函数关系为：\n\n$$ y_n = A ( x_n, x_{n+1}, \\cdots) $$\n\n通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成：\n\n$$ \\sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \\cdots + b) $$\n\n在这个公式中，$x_0, x_1, \\cdots $代表输入；$w_0, w_1, \\cdots $代表神经节点与输入之间连接的权重。\n这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。\n正是这些权重，决定了卷积神经网络可以为我们处理的任务。\n\n具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$：\n\n$$ y = \\sigma ( W x + b ) $$\n\n由此我们可以得到：\n\n$$\ny_0 = \\sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \\cdots) \\\\\ny_1 = \\sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \\cdots)\n$$\n\n参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。\n\n![4-17](/posts_res/2018-06-28-cnn1/4-17.png)\n\n上图描述的这种神经网络可以表达成如下等式：\n\n$$\ny_0 = \\sigma ( W_{0} x_0 + W_{1} x_1 - b) \\\\\ny_1 = \\sigma ( W_{0} x_1 + W_{1} x_2 - b)\n$$\n\n然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）：\n\n$$\nW = \\left[\n\\begin{array}{ccccc} \nW_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\nW_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\nW_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\nW_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n...     &   ...   &   ...   &  ...    & ...\\\\\n\\end{array}\n\\right]\n$$\n\n上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。\n\n$$\nW = \\left[\n\\begin{array}{ccccc} \nw_0 & w_1 &  0  &  0  & ...\\\\\n 0  & w_0 & w_1 &  0  & ...\\\\\n 0  &  0  & w_0 & w_1 & ...\\\\\n 0  &  0  &  0  & w_0 & ...\\\\\n... & ... & ... & ... & ...\\\\\n\\end{array}\n\\right]\n$$\n\n与上述矩阵的点乘，其实和卷积一个向量$ \\left[ \\cdots 0, w_1, w_0, 0 \\cdots \\right] $是一样的。\n当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。\n\n*那么二维的卷积层又是什么样呢？*\n\n![4-18](/posts_res/2018-06-28-cnn1/4-18.png)\n\n上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。\n\n回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。\n\n<br>\n\n**结语**\n\n在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？\n\n- 非常强大的语言来描述网络的连接。\n    - 到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。\n- 卷积运算具有显著的实现优势。\n    - 许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。\n- 实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。\n","slug":"cnn1","published":1,"updated":"2019-08-17T09:38:11.742Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnw8002r2qwpb7nzkbyg","content":"<p>目录</p><ul>\n<li>1.引入</li>\n<li>2.卷积神经网络的结构</li>\n<li>3.卷积神经网络的正式表达</li>\n<li>4.理解卷积</li>\n</ul><p>译自：<a href=\"http://colah.github.io/posts/2014-07-Conv-Nets-Modular/\" target=\"_blank\" rel=\"noopener\">Conv Nets: A Modular Perspective</a>，\n<a href=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/\" target=\"_blank\" rel=\"noopener\">Understanding Convolutions</a></p><hr><h3 id=\"1-引入\"><a href=\"#1-引入\" class=\"headerlink\" title=\"1.引入\"></a>1.引入</h3><p>在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。</p><p>简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 \n但是因为其具有的重复性，我们需要学习的参数相对来说非常少。</p><a id=\"more\"></a>\n\n\n\n\n\n<p><img src=\"/posts_res/2018-06-28-cnn1/1-1.png\" alt=\"1-1\"></p>\n<p>这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。\n这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数，\n并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。</p>\n<hr>\n<h3 id=\"2-卷积神经网络的结构\"><a href=\"#2-卷积神经网络的结构\" class=\"headerlink\" title=\"2.卷积神经网络的结构\"></a>2.卷积神经网络的结构</h3><p>假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。\n假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-1.png\" alt=\"2-1\"></p>\n<p>引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出）</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-2.png\" alt=\"2-2\"></p>\n<p>更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？</p>\n<p>我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性，\n所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。</p>\n<p>因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-3.png\" alt=\"2-3\"></p>\n<p>在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些\n（所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数）</p>\n<p>在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-4.png\" alt=\"2-4\"></p>\n<p>卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。</p>\n<p>如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-5.png\" alt=\"2-5\"></p>\n<p>卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。\n通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。\n比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。</p>\n<p>最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。</p>\n<p>最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。\n最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。\n由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-6.png\" alt=\"2-6\"></p>\n<p>至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。</p>\n<p><br></p>\n<p>在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。\n例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-7.png\" alt=\"2-7\"></p>\n<p>上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-8.png\" alt=\"2-8\"></p>\n<p>同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。\n这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-9.png\" alt=\"2-9\"></p>\n<p><br></p>\n<p>有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。</p>\n<p>上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。\n在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。</p>\n<p>例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-10.png\" alt=\"2-10\"></p>\n<p>在论文<a href=\"http://arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"noopener\">Network in Network</a>中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点，\n由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-11.png\" alt=\"2-11\"></p>\n<p>这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。</p>\n<hr>\n<h3 id=\"3-卷积神经网络的正式表达\"><a href=\"#3-卷积神经网络的正式表达\" class=\"headerlink\" title=\"3.卷积神经网络的正式表达\"></a>3.卷积神经网络的正式表达</h3><p>考虑一维的卷积层，其中输入为$\\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/3-1.png\" alt=\"3-1\"></p>\n<p>很容易可以用输入表示输出：</p>\n<script type=\"math/tex; mode=display\">y_n = A( x_n, x_{n+1}, \\cdots )</script><p>如上图：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = A(x_0, x_1) \\\\\ny_1 = A(x_1, x_2)</script><p>相似地，如果我们考虑二维的卷积层，其中输入为$\\lbrace x<em>{n,m} \\rbrace$，输出为$ \\lbrace y</em>{n,m} \\rbrace $</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/3-2.png\" alt=\"3-2\"></p>\n<p>同样地，我们可以用输入表示输出：</p>\n<script type=\"math/tex; mode=display\">\ny_{n,m} = A \n\\begin{pmatrix}\nx_{n,m}, & x_{n+1, m}, & \\cdots , \\\\\nx_{n,m+1}, & x_{n+1, m+1}, & \\cdots , \\\\\n & \\cdots &\n\\end{pmatrix}</script><p>例如：</p>\n<script type=\"math/tex; mode=display\">\ny_{0,0}=A\n\\begin{pmatrix}\nx_{0,0}, & x_{1,0}, \\\\\nx_{1,0}, & x_{1,1}, \\\\\n\\end{pmatrix}\ny_{1,0}=A\n\\begin{pmatrix}\nx_{1,0}, & x_{2,0}, \\\\\nx_{1,1}, & x_{2,1}, \\\\\n\\end{pmatrix}</script><p>把这些和矩阵方程联合起来，得$A(x)$。</p>\n<script type=\"math/tex; mode=display\">A(x) = \\sigma (W x + b)</script><p>实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式—关于卷积的数学运算，这通常更有帮助。</p>\n<p>卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。\n它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。\n其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。\n最后，卷积将为我们提供一个非常不同的角度来解释卷积层。</p>\n<hr>\n<h3 id=\"4-理解卷积\"><a href=\"#4-理解卷积\" class=\"headerlink\" title=\"4.理解卷积\"></a>4.理解卷积</h3><p>上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？</p>\n<p><strong>球体下落的启示</strong></p>\n<p>试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。\n球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。\n那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？</p>\n<p>我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。\n球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同，\n那么$g$和$f$是两个不同的概率分布。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-1.png\" alt=\"4-1\"></p>\n<p>假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \\cdot f(b)$。\n从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\\cdot g(1)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-2.png\" alt=\"4-2\"></p>\n<p>然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \\cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-3.png\" alt=\"4-3\"></p>\n<p>上述两种可能各种的概率表示依次是$f(1) \\cdot g(2)$和$ f(0) \\cdot g(3)$。</p>\n<p>为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。</p>\n<script type=\"math/tex; mode=display\">\\cdots f(0) \\cdot g(3) + f(1) \\cdot g(2) + f(2) \\cdot g(1) \\cdots</script><p>我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \\cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为：</p>\n<script type=\"math/tex; mode=display\">\\sum_{a+b=c} f(a) \\cdot g(b)</script><p>事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下:</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) (c) = \\sum_{a+b=c} f(a) \\cdot g(b)</script><p>如果用$b=c-a$替换，则有</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) (c) = \\sum_a f(a) \\cdot g(c−a)</script><p>这正是标准的卷积定义。</p>\n<p>更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-4.png\" alt=\"4-4\"></p>\n<p>为了完成卷积，我们考虑所有的中转位置。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-5.png\" alt=\"4-5\"></p>\n<p><br></p>\n<p><strong>卷积的可视化</strong></p>\n<p>有一个非常好的例子可以帮助我们更好地理解卷积的过程。</p>\n<p>首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-6.png\" alt=\"4-6\"></p>\n<p>设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-7.png\" alt=\"4-7\"></p>\n<p>第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $</p>\n<p>我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-8.png\" alt=\"4-8\"></p>\n<p>对所有的$a$求和，我们就得到了卷积的结果。\n这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。\n如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。</p>\n<p>具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值)</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-9.png\" alt=\"4-9\"></p>\n<p>随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-10.png\" alt=\"4-10\"></p>\n<p>当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。\n下图，我们用两个箱型图变现了卷积的过程：</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-11.gif\" alt=\"4-11\"></p>\n<p>从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。\n卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。\n采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。</p>\n<p><br></p>\n<p><strong>高维卷积</strong></p>\n<p>卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-12.png\" alt=\"4-12\"></p>\n<p>卷积过程还是和之前的一样</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) ( c ) = \\sum_{a+b=c} f(a) \\cdot g(b)</script><p>只是这里的$a，b，c$是向量，更明确地表示是</p>\n<script type=\"math/tex; mode=display\">(f ∗ g)(c_1, c_2) = \\sum_{a_1 + b_1 = c_1 \\\\ a_2 + b_2 = c_2} f ( a_1, a_2) \\cdot g(b_1, b_2 )</script><p>或者使用标准定义：</p>\n<script type=\"math/tex; mode=display\">(f ∗ g ) ( c_1, c_2) = \\sum{a_1,a_2} f(a_1, a_2) \\cdot g(c_1−a_1, c_2−a_2)</script><p>和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。</p>\n<p>二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-13.png\" alt=\"4-13\"></p>\n<p>核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。</p>\n<p>比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-14.png\" alt=\"4-14\"></p>\n<p>我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。\n也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。\n如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-15.png\" alt=\"4-15\"></p>\n<p>gimp documentation 中有很多<a href=\"http://docs.gimp.org/en/plug-in-convmatrix.html\" target=\"_blank\" rel=\"noopener\">其他的例子</a>。</p>\n<p><br></p>\n<p><strong>卷积神经网络</strong></p>\n<p>那么，卷积操作和卷积神经网络是如何联系起来的呢？</p>\n<p>首先我们考虑一维卷积神经网络，其中输入为$ \\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $，</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-16.png\" alt=\"4-16\"></p>\n<p>由图可得，输入层与输出层之间的函数关系为：</p>\n<script type=\"math/tex; mode=display\">y_n = A ( x_n, x_{n+1}, \\cdots)</script><p>通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成：</p>\n<script type=\"math/tex; mode=display\">\\sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \\cdots + b)</script><p>在这个公式中，$x_0, x_1, \\cdots $代表输入；$w_0, w_1, \\cdots $代表神经节点与输入之间连接的权重。\n这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。\n正是这些权重，决定了卷积神经网络可以为我们处理的任务。</p>\n<p>具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$：</p>\n<script type=\"math/tex; mode=display\">y = \\sigma ( W x + b )</script><p>由此我们可以得到：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = \\sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \\cdots) \\\\\ny_1 = \\sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \\cdots)</script><p>参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-17.png\" alt=\"4-17\"></p>\n<p>上图描述的这种神经网络可以表达成如下等式：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = \\sigma ( W_{0} x_0 + W_{1} x_1 - b) \\\\\ny_1 = \\sigma ( W_{0} x_1 + W_{1} x_2 - b)</script><p>然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）：</p>\n<script type=\"math/tex; mode=display\">\nW = \\left[\n\\begin{array}{ccccc} \nW_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\nW_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\nW_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\nW_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n...     &   ...   &   ...   &  ...    & ...\\\\\n\\end{array}\n\\right]</script><p>上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。</p>\n<script type=\"math/tex; mode=display\">\nW = \\left[\n\\begin{array}{ccccc} \nw_0 & w_1 &  0  &  0  & ...\\\\\n 0  & w_0 & w_1 &  0  & ...\\\\\n 0  &  0  & w_0 & w_1 & ...\\\\\n 0  &  0  &  0  & w_0 & ...\\\\\n... & ... & ... & ... & ...\\\\\n\\end{array}\n\\right]</script><p>与上述矩阵的点乘，其实和卷积一个向量$ \\left[ \\cdots 0, w_1, w_0, 0 \\cdots \\right] $是一样的。\n当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。</p>\n<p><em>那么二维的卷积层又是什么样呢？</em></p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-18.png\" alt=\"4-18\"></p>\n<p>上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。</p>\n<p>回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。</p>\n<p><br></p>\n<p><strong>结语</strong></p>\n<p>在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？</p>\n<ul>\n<li>非常强大的语言来描述网络的连接。<ul>\n<li>到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。</li>\n</ul>\n</li>\n<li>卷积运算具有显著的实现优势。<ul>\n<li>许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。</li>\n</ul>\n</li>\n<li>实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>1.引入</li>\n<li>2.卷积神经网络的结构</li>\n<li>3.卷积神经网络的正式表达</li>\n<li>4.理解卷积</li>\n</ul><p>译自：<a href=\"http://colah.github.io/posts/2014-07-Conv-Nets-Modular/\" target=\"_blank\" rel=\"noopener\">Conv Nets: A Modular Perspective</a>，\n<a href=\"https://colah.github.io/posts/2014-07-Understanding-Convolutions/\" target=\"_blank\" rel=\"noopener\">Understanding Convolutions</a></p><hr><h3 id=\"1-引入\"><a href=\"#1-引入\" class=\"headerlink\" title=\"1.引入\"></a>1.引入</h3><p>在最近几年间，深度神经网络为模式识别领域的很多难题带来了突破性的进展，比如计算机视觉和语音识别领域。这些突破性进展其实都来源于一种特别的神经网络，我们称之为卷积神经网络。</p><p>简单来说，卷积神经网络可以认为是由若干个相同的神经节点构成的网络。卷积神经网络具有大量节点并且具有宏大的模型结构，有时特征向量空间的维度非常庞大－－这些特征向量由来描述一个神经网络的行为。 \n但是因为其具有的重复性，我们需要学习的参数相对来说非常少。</p>","more":"\n\n\n\n\n\n<p><img src=\"/posts_res/2018-06-28-cnn1/1-1.png\" alt=\"1-1\"></p>\n<p>这种结构上的重复性，也体现在计算机科学和数学领域。例如当我们编写代码时，我们事先声明一个函数，然后反复调用它－－而不是在每一个需要使用的地方重新编写代码。\n这种重复使用，使得我们的程序更加简洁，也避免了错误的发生。相应的，当我们构建一个卷积神经网络时，可以通过学习得到一个神经节点的参数，\n并把以这个节点为模板复制出若干个一致的节点组成网络，这使得整个网络参数的学习非常简便。</p>\n<hr>\n<h3 id=\"2-卷积神经网络的结构\"><a href=\"#2-卷积神经网络的结构\" class=\"headerlink\" title=\"2.卷积神经网络的结构\"></a>2.卷积神经网络的结构</h3><p>假设你现在想用一个神经网络来观察声音的采样信号，并且预测接下来是否会继续出现采样值。或者你也可以用它来对人说话的声音做更多的分析。\n假设我们已经得到了如下图的这样一个声音信号的时间采样序列，采样周期为一个固定值（意味着每个采样点间的时间间隔相同）。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-1.png\" alt=\"2-1\"></p>\n<p>引入神经网络最简单的方法，就是把所有采样点连接到一个全连接层上，每个采样点对应一个独立的神经节点（下图中神经节点并未画出）</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-2.png\" alt=\"2-2\"></p>\n<p>更加复杂的一种方式，引入了输入层中的一种非常有用的对称性。我们关注数据的多方面特性：一定间隔内声音的频率如何变化？增大还是减小？</p>\n<p>我们关注这些性质在全部时间轴上的变化，例如初始的频率值，中间部分的频率值，数据集末尾的频率值。这些其实都是仅与数据本身有关的特性，\n所以在整个时间轴上分布不会有太大变化，因此我们可以通过观察一小段时间范围内采样结果来观察这些性质。</p>\n<p>因此我们可以构建一组神经节点$A$，在一小段时间间隔内，将输入全部连到一个节点上，由若干个节点覆盖整个采样时间范围。然后再将这样一个神经节点层连接到一个全连接层$F$</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-3.png\" alt=\"2-3\"></p>\n<p>在上面这个例子中，每个节点只覆盖一小段时间间隔（具体来说就是两个采样周期）。这并不是实际应用的情况，通常来说卷积的时间窗会更长一些\n（所谓卷积窗就是每个神经节点覆盖的时间间隔，也可以表达为每个神经节点所连接的输入个数）</p>\n<p>在下图中，我们将窗长增加到了3，然而这也不符合实际应用的情况，只是为了帮助理解卷积窗长度的变化方式。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-4.png\" alt=\"2-4\"></p>\n<p>卷积层的一个非常好的性质就是它们是可以相互组合的。你可以将一个卷积层的输出作为另一个卷积层的输入。数据流每经历一层卷积，我们就能从中提取出一些高维的更加抽象的特征。</p>\n<p>如下图所示的例子中，我们加入了一组新的神经节点$B$，这些节点构成了位于最顶端的卷积层。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-5.png\" alt=\"2-5\"></p>\n<p>卷积层之间通常包含有一个池化层，也可以称为最大池化层，这种方法应用的十分广泛。\n通常情况下，宏观来看，我们并不关注某一个特性在时间轴上出现的精确位置。\n比如一个频率变化出现的时间点提前或滞后一点点对实验结果是没有什么影响的。</p>\n<p>最大池化层的作用，就是提取出最大的若干个特征值。通过一个筛选窗，将较大值取出。这一层的输出会告诉我们特征的有效值，但不会包含其在时间轴上的具体信息。</p>\n<p>最大池化层可以看作是一个“拉远”操作，想象你正使用一个相机，当你把镜头拉远，你就可以看到更大范围上的东西。\n最大池化层的作用也类似于此，它使得接下来的卷积层能够使用更大时间间隔上的特征值，也就是说，某一个小间隔上的特征值被提取出来作为相邻几个小间隔的代表。\n由此构成了针对一个相对较大的时间间隔的表达。这使得我们可以省略掉数据在较小范围内的一些变化。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-6.png\" alt=\"2-6\"></p>\n<p>至此，我们已经介绍了一维的卷积层。其实卷积神经网络也可以应用到高维的情形当中。实际上，最有名的卷积神经网络的应用就是将二维的CNN应用在图像识别当中。</p>\n<p><br></p>\n<p>在此二维的网络中，我们所覆盖的最小单位不再是时间间隔，而是像素点。$A$（即前文提到的神经节点）被用于提取每个像素点的特征。\n例如可以用于提取一幅图像的轮廓。也可以用于检测图像的类别，或者对两种颜色进行比较。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-7.png\" alt=\"2-7\"></p>\n<p>上图揭示了一层二维神经网络的结构。我们也可以像在一维空间中那样，连接若干层二维神经网络。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-8.png\" alt=\"2-8\"></p>\n<p>同理在二维神经网络中我们也可以加入池化层。在这种情况下，我们就是提取了某一个或几个像素上的特征值作为一片区域的特征值。\n这样做的最大好处是，处理图像时，我们不再关心某个片段在整幅图像上的具体位置（具体到像素级别），而是可以通过其周围的几个像素上的特征值来观察某一特征。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-9.png\" alt=\"2-9\"></p>\n<p><br></p>\n<p>有时我们也会用到三维的卷积神经网络，比如视频数据的处理或者立体分析的过程（例如3D扫描）。这种更高维度的网络非常复杂，它们并没有被广泛应用并且很难表现出来。</p>\n<p>上文中，我们提到$A$是一组神经节点。接下来我们具体来看一下到底$A$是什么样的结构。\n在传统的卷积神经网络中，$A$是一系列相互平行的节点，它们接受相同的输入，计算不同的特征值。</p>\n<p>例如在二维网络中，一个神经节点用来获取水平的轮廓，另一个可能用来获取垂直的轮廓，还有一个用来进行红绿颜色的对比。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-10.png\" alt=\"2-10\"></p>\n<p>在论文<a href=\"http://arxiv.org/abs/1312.4400\" target=\"_blank\" rel=\"noopener\">Network in Network</a>中，作者提出了一种新的”Mlpconv“网络结构。在这种模型中，$A$代表了多层神经节点，\n由最后一层输出其所连接的若干输入的高维特征值。这篇文章提到，此模型可以达到一个非常好的效果，在现有数据的基础上达到了最优的效果。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/2-11.png\" alt=\"2-11\"></p>\n<p>这提醒着我们，卷积神经网络已经发展到了一定的阶段，不再是简单的模型能够概括的，我们需要更加深入的思考创新的方向。</p>\n<hr>\n<h3 id=\"3-卷积神经网络的正式表达\"><a href=\"#3-卷积神经网络的正式表达\" class=\"headerlink\" title=\"3.卷积神经网络的正式表达\"></a>3.卷积神经网络的正式表达</h3><p>考虑一维的卷积层，其中输入为$\\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/3-1.png\" alt=\"3-1\"></p>\n<p>很容易可以用输入表示输出：</p>\n<script type=\"math/tex; mode=display\">y_n = A( x_n, x_{n+1}, \\cdots )</script><p>如上图：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = A(x_0, x_1) \\\\\ny_1 = A(x_1, x_2)</script><p>相似地，如果我们考虑二维的卷积层，其中输入为$\\lbrace x<em>{n,m} \\rbrace$，输出为$ \\lbrace y</em>{n,m} \\rbrace $</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/3-2.png\" alt=\"3-2\"></p>\n<p>同样地，我们可以用输入表示输出：</p>\n<script type=\"math/tex; mode=display\">\ny_{n,m} = A \n\\begin{pmatrix}\nx_{n,m}, & x_{n+1, m}, & \\cdots , \\\\\nx_{n,m+1}, & x_{n+1, m+1}, & \\cdots , \\\\\n & \\cdots &\n\\end{pmatrix}</script><p>例如：</p>\n<script type=\"math/tex; mode=display\">\ny_{0,0}=A\n\\begin{pmatrix}\nx_{0,0}, & x_{1,0}, \\\\\nx_{1,0}, & x_{1,1}, \\\\\n\\end{pmatrix}\ny_{1,0}=A\n\\begin{pmatrix}\nx_{1,0}, & x_{2,0}, \\\\\nx_{1,1}, & x_{2,1}, \\\\\n\\end{pmatrix}</script><p>把这些和矩阵方程联合起来，得$A(x)$。</p>\n<script type=\"math/tex; mode=display\">A(x) = \\sigma (W x + b)</script><p>实际上，这通常不是考虑卷积神经网络的最佳方法，有一个替代的公式—关于卷积的数学运算，这通常更有帮助。</p>\n<p>卷积运算是一个强大的工具。在数学中，它出现在不同的背景下，从偏微分方程的研究到概率论。卷积在物理科学中非常重要，部分原因是它在偏微分方程中的作用。\n它在计算机图形学和信号处理等许多应用领域也起着重要的作用。对我们来说，卷积会带来很多好处。首先，它将允许我们创建卷积层的更有效的实现，这比单纯的透视图所建议的要有效得多。\n其次，它将从我们的公式中去除很多混乱，目前的公式可能看起来还不混乱，但那只是因为我们还没有进入棘手的情况。\n最后，卷积将为我们提供一个非常不同的角度来解释卷积层。</p>\n<hr>\n<h3 id=\"4-理解卷积\"><a href=\"#4-理解卷积\" class=\"headerlink\" title=\"4.理解卷积\"></a>4.理解卷积</h3><p>上面在没有涉及任何数学知识情况下，我们对卷积神经网络有了初步了解。为了更深入地理解卷积神经网络，我们需要明白，何为卷积？</p>\n<p><strong>球体下落的启示</strong></p>\n<p>试想一下，我们从某一高度释放球让它做自由落体运动，并假设球在地面上只做一维运动（某一条线上）。\n球第一次释放并且在地面上静止后，在静止处上方我们以另一高度让球做第二次下落运动。\n那么，球最后停止的位置和最开始释放处水平距离为c的可能性有多大呢？</p>\n<p>我们把这过程分解一下。第一次下落后，球停止在离释放点$a$单位距离的可能性是$f(a)$, $f$是概率分布。\n球第一次下落并静止后，我们将球拿到静止处上方以另一高度让球第二次下落，球最终停止处与第二次释放处距离是$b$单位距离的可能性是$g(b)$, 如果两次释放高度不同，\n那么$g$和$f$是两个不同的概率分布。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-1.png\" alt=\"4-1\"></p>\n<p>假设第一次下落球在地面上运动了距离$a$，第二次下落球在地面上运动了距离$b$，而两次下落球在水平面上运动总距离是$c$，则有$a+b=c$，这种情况发生的概率可以简单表示成$f(a) \\cdot f(b)$。\n从离散的角度来看这个例子，假设总运动距离$c=3$，第一次滚动距离$a=2$，那么第二次滚动距离$b$必须是$1$，所有概率表示为$f(2)\\cdot g(1)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-2.png\" alt=\"4-2\"></p>\n<p>然而，这不是$c=3$的唯一可能，我们也可以让第一次运动$a=1$，第二次运动$b=2$。或者$a=0，b=3 \\cdots $ 只要两次运动距离和$c=3$，$a$和$b$有无限种可能的组合。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-3.png\" alt=\"4-3\"></p>\n<p>上述两种可能各种的概率表示依次是$f(1) \\cdot g(2)$和$ f(0) \\cdot g(3)$。</p>\n<p>为了得到总运动距离为$c$的所有可能结果，我们不能只考虑距离为$c$一种可能。取而代之，我们考虑把$c$分割成$a$和$b$的所有情况，并将每种情况概率相加。</p>\n<script type=\"math/tex; mode=display\">\\cdots f(0) \\cdot g(3) + f(1) \\cdot g(2) + f(2) \\cdot g(1) \\cdots</script><p>我们知道对每种$a+b=c$的情况，其概率可表示为$f(a) \\cdot g(b)$。因此，我们可以对每种$a+b=c$的情况做概率求和得到总概率为：</p>\n<script type=\"math/tex; mode=display\">\\sum_{a+b=c} f(a) \\cdot g(b)</script><p>事实上，这就是一个卷积过程！特别地，$f$和$g$的卷积，对$c$的评估定义如下:</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) (c) = \\sum_{a+b=c} f(a) \\cdot g(b)</script><p>如果用$b=c-a$替换，则有</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) (c) = \\sum_a f(a) \\cdot g(c−a)</script><p>这正是标准的卷积定义。</p>\n<p>更具体地说，我们可以从球可能停止的位置来思考。球第一次下落后，球停止在中转位置距离为$a$处的概率是$f(a)$,如果球第一次停在了距离$a$处，那么球最终停在距离$c$处的概率是$g(c-a)$</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-4.png\" alt=\"4-4\"></p>\n<p>为了完成卷积，我们考虑所有的中转位置。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-5.png\" alt=\"4-5\"></p>\n<p><br></p>\n<p><strong>卷积的可视化</strong></p>\n<p>有一个非常好的例子可以帮助我们更好地理解卷积的过程。</p>\n<p>首先，基于对下图的观察。我们假设一个小球运动到距离其初始位置$x$单位的概率为$f(x)$。然后，紧接着我们定义小球从距离其初始位置$x$的点运动到初始位置的概率为 $f(−x)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-6.png\" alt=\"4-6\"></p>\n<p>设想，小球第二次下落后的位置为$c$，并且已知小球第一次下落的过渡位置为$a$，这一概率是多少呢？</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-7.png\" alt=\"4-7\"></p>\n<p>第一次下落后距离为$a$则第二次落到$c$的概率是: $ g( − ( a − c )) = g ( c − a ) $</p>\n<p>我们考虑，已知每次二段下落后距离为$c$的情况下，第一次下落后到达一个过渡位置的概率，即我们之前提到的第一次下落位于$a$的概率$f(a)$。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-8.png\" alt=\"4-8\"></p>\n<p>对所有的$a$求和，我们就得到了卷积的结果。\n这种表示法使得我们可以在一幅图里面形象地对一个值为$c$的卷积结果进行估计。\n如果我们只关注坐标轴的一侧（因为坐标轴是对称的），当$c$的位置在一侧改变时，我们可以对其卷积的结果进行估计，这有助于我们整体把握卷积的概念。</p>\n<p>具体来说，我们可以看到$c$点的位置与原始位置分布在一条线上时卷积的结果达到峰值。(此时卷积的式子中求和项数达到最大值)</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-9.png\" alt=\"4-9\"></p>\n<p>随着$c$点的位置沿着坐标轴远离，我们可以看到结果中的项数逐渐减少，直到最右端时达到最小。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-10.png\" alt=\"4-10\"></p>\n<p>当我们把这种思路下的结果用动图表示出来，我们就要可以直观的看到卷积的结果了。\n下图，我们用两个箱型图变现了卷积的过程：</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-11.gif\" alt=\"4-11\"></p>\n<p>从这种角度看待卷积，我们发现CNN的概念变得更加直观。接下来我们一起思考一种非概率的模型。\n卷积操作有时也被应用在音频处理上。比如，我们经常使用两个冲击作为采样函数。\n采样的结果中，在第一个冲击处得到输出，经过一个时延后，在第二个冲击处再次得到一个采样值作为第一次采样结果的延迟输出。</p>\n<p><br></p>\n<p><strong>高维卷积</strong></p>\n<p>卷积可以一般化，我们可以在更高的维度使用它。我们还以球体下落为例，只是现在，球在水平面上的运动是不是一维的，而是二维的。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-12.png\" alt=\"4-12\"></p>\n<p>卷积过程还是和之前的一样</p>\n<script type=\"math/tex; mode=display\">(f ∗ g) ( c ) = \\sum_{a+b=c} f(a) \\cdot g(b)</script><p>只是这里的$a，b，c$是向量，更明确地表示是</p>\n<script type=\"math/tex; mode=display\">(f ∗ g)(c_1, c_2) = \\sum_{a_1 + b_1 = c_1 \\\\ a_2 + b_2 = c_2} f ( a_1, a_2) \\cdot g(b_1, b_2 )</script><p>或者使用标准定义：</p>\n<script type=\"math/tex; mode=display\">(f ∗ g ) ( c_1, c_2) = \\sum{a_1,a_2} f(a_1, a_2) \\cdot g(c_1−a_1, c_2−a_2)</script><p>和一维卷积类似，我们可以把二维卷积看成一个函数在另一函数上移动，相乘并相加。</p>\n<p>二维卷积一个常见的应用是图像处理。把图像当做二维函数。许多图像转换方法都是图像函数和一个名为核函数的本地函数卷积的过程。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-13.png\" alt=\"4-13\"></p>\n<p>核在图像上每个位置点上移动，并计算被其覆盖住的像素点的加权和得到新的像素（核上的像素值是权值）。</p>\n<p>比如，通过一个 3 x 3 宫格来做像素平均能使图像变模糊。此时，核中方格中每个像素点值为1/9</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-14.png\" alt=\"4-14\"></p>\n<p>我们还能探测图像的边界信息。此时核中两个相邻的像素点值为 -1 和 1，其他地方像素值为0。\n也就是说，当核与其覆盖的图像像素相乘时，我们让相邻的像素点值相减。\n如果相邻的像素点类似，那么相减得到值约为0,；然而在垂直于边界方向处，相邻像素点差异很大。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-15.png\" alt=\"4-15\"></p>\n<p>gimp documentation 中有很多<a href=\"http://docs.gimp.org/en/plug-in-convmatrix.html\" target=\"_blank\" rel=\"noopener\">其他的例子</a>。</p>\n<p><br></p>\n<p><strong>卷积神经网络</strong></p>\n<p>那么，卷积操作和卷积神经网络是如何联系起来的呢？</p>\n<p>首先我们考虑一维卷积神经网络，其中输入为$ \\lbrace x_n \\rbrace$，输出为$ \\lbrace y_n \\rbrace $，</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-16.png\" alt=\"4-16\"></p>\n<p>由图可得，输入层与输出层之间的函数关系为：</p>\n<script type=\"math/tex; mode=display\">y_n = A ( x_n, x_{n+1}, \\cdots)</script><p>通常来说，$A$表示一个多重的神经网络层。但是为了简便，我们先讨论一重网络的情况。回想一下，神经网络中一个神经节点可以表示成：</p>\n<script type=\"math/tex; mode=display\">\\sigma ( w_0 x_0 + w_1 x_1 + w_2 x_2 \\cdots + b)</script><p>在这个公式中，$x_0, x_1, \\cdots $代表输入；$w_0, w_1, \\cdots $代表神经节点与输入之间连接的权重。\n这些权重就是神经网络节点的主要特征，它们决定了神经网络节点的行为。当我们说两个节点一致实际上就是说这两个节点的权重向量完全一致。\n正是这些权重，决定了卷积神经网络可以为我们处理的任务。</p>\n<p>具体来说，我们将整个神经网络层表达成一个式子，而不是分开表示。方法就是使用一个权重矩阵$W$：</p>\n<script type=\"math/tex; mode=display\">y = \\sigma ( W x + b )</script><p>由此我们可以得到：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = \\sigma ( W_{0,0} x_0 + W_{0,1} x_1 + W_{0,2} x_2 \\cdots) \\\\\ny_1 = \\sigma ( W_{1,0} x_0 + W_{1,1} x_1 + W_{1,2} x_2 \\cdots)</script><p>参数矩阵的每一行，描述了神经节点如何与输入相连。我们再回到卷积层，因为这一层中的神经节点有时是一致的，相应的，很多权值也会反复在权值矩阵中出现。</p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-17.png\" alt=\"4-17\"></p>\n<p>上图描述的这种神经网络可以表达成如下等式：</p>\n<script type=\"math/tex; mode=display\">\ny_0 = \\sigma ( W_{0} x_0 + W_{1} x_1 - b) \\\\\ny_1 = \\sigma ( W_{0} x_1 + W_{1} x_2 - b)</script><p>然而，更加普遍的情况是，各个神经节点的表示是不同的（即它们的权值向量是不同的）：</p>\n<script type=\"math/tex; mode=display\">\nW = \\left[\n\\begin{array}{ccccc} \nW_{0,0} & W_{0,1} & W_{0,2} & W_{0,3} & ...\\\\\nW_{1,0} & W_{1,1} & W_{1,2} & W_{1,3} & ...\\\\\nW_{2,0} & W_{2,1} & W_{2,2} & W_{2,3} & ...\\\\\nW_{3,0} & W_{3,1} & W_{3,2} & W_{3,3} & ...\\\\\n...     &   ...   &   ...   &  ...    & ...\\\\\n\\end{array}\n\\right]</script><p>上述矩阵所描述的卷积层是和我们之前看的是不同的。大多数情况下，权值会已一个短序列形式重复出现。同时，因为神经节点并不会与所有输入相连，参数矩阵大多是稀疏的。</p>\n<script type=\"math/tex; mode=display\">\nW = \\left[\n\\begin{array}{ccccc} \nw_0 & w_1 &  0  &  0  & ...\\\\\n 0  & w_0 & w_1 &  0  & ...\\\\\n 0  &  0  & w_0 & w_1 & ...\\\\\n 0  &  0  &  0  & w_0 & ...\\\\\n... & ... & ... & ... & ...\\\\\n\\end{array}\n\\right]</script><p>与上述矩阵的点乘，其实和卷积一个向量$ \\left[ \\cdots 0, w_1, w_0, 0 \\cdots \\right] $是一样的。\n当这个向量（可以理解为一个卷积窗）滑动到输入层的不同位置时，它可以代表相应位置上的神经节点。</p>\n<p><em>那么二维的卷积层又是什么样呢？</em></p>\n<p><img src=\"/posts_res/2018-06-28-cnn1/4-18.png\" alt=\"4-18\"></p>\n<p>上图可以理解为，卷积窗由一个向量变成了一个矩阵，在二维的输入空间滑动，在对应位置表示对应的神经节点。</p>\n<p>回想我们上文提到的，用卷积操作获取一个图片的轮廓，就是通过在每一个像素点上滑动核（即二维卷积窗），由此实现遍历了每一个像素的卷积操作。</p>\n<p><br></p>\n<p><strong>结语</strong></p>\n<p>在这片博文中，我们介绍了很多个数学架构，导致我们忘了我们的目的。在概率论和计算机学中，卷积操作是一个非常有用的工具。然而在神经网络中引入卷积可以带来什么好处呢？</p>\n<ul>\n<li>非常强大的语言来描述网络的连接。<ul>\n<li>到目前为止，我们所处理的示例还不够复杂，无法使这个好处变得清晰，但是复杂的操作可以让我们摆脱大量令人不快的bookkeeping。</li>\n</ul>\n</li>\n<li>卷积运算具有显著的实现优势。<ul>\n<li>许多库提供了高效的卷积例程。此外，虽然卷积看起来是一种$O(n^2)^操作，但使用一些相当深入的数学见解，可以创建^O(nlog(n))^实现。</li>\n</ul>\n</li>\n<li>实际上，在gpu上使用高效的并行卷积实现对于计算机视觉的最新进展是至关重要的。</li>\n</ul>\n"},{"layout":"post","title":"LINE Tutorial","date":"2018-07-13T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n目录\n\n* 写作动机\n* 引子\n* 问题定义\n* LINE:大规模信息网络嵌入\n\n\n-------\n\n### 写作动机\n\n当前大多数图形嵌入方法不能对包含数百万个节点的真实信息网络进行扩展，分析大型信息网络在学术界和行业中一直受到越来越多的关注。\n而现在的大多数嵌入方法在小型网络中适用性非常不错，但当网络包含数百万乃至数百亿节点时，就看起来并不那么有效，其时间复杂度至少是节点数的二次方。\n最重要的是，它们着重于关注节点之间的一阶相似性，及两点之间是否直接相连，而忽略了其二阶相似性(即拥有许多共同的邻节点)。\n因此LINE模型就是为了在信息网络嵌入至低维空间时保留其一阶相似以及二阶相似。\n\n\n-------\n\n### 引子\n\n下图展示了一个说明性例子。因为顶点6和7之间的边的权值很大，即6和7有非常高的一阶近似，它们应该在嵌入领域中彼此紧密相连。\n另一方面，虽然顶点5和6之间的顶点没有链接，但是它们有很多共同的邻域，即它们有很高的二阶近似和两者的表示会是十分的接近。\n\n![neighbors](/posts_res/2018-07-13-LINE/1.png)\n\n\n--------\n\n### 问题定义\n\n- 1.信息网络的定义\n    - 信息网络定义为$G=(V,E)$，其中$V$是顶点集合，顶点表示数据对象，$E$是顶点之间的边缘的集合，每条边表示两个数据对象之间的关系。每条边$e\\in E$表示为有序对$e=(u,v)$，并且与权重$W_{uv} > 0$相关联，权重表示关系的强度。如果$G$是无向的，我们有$(u,v)\\equiv (v,u)$和$W_{uv}\\equiv W_{vu}$;如果G是有向的，我们有$(u,v)\\ne (v,u)$和$W_{uv}\\ne W_{vu}$。关于边权重问题，本次论文中只研究非负权重的情况。\n- 2.一阶相似性的定义\n    - 网络中的一阶相似性是两个顶点之间的局部点对的邻近度。对于由边缘$(u,v)$链接的每对顶点，该边缘的权重$W_{uv}$表示$u$和$v$之间的一阶相似性，如果在$u$和$v$之间没有观察到边缘，它们的一阶相似性为$0$。\n- 3.二阶相似度的定义\n    - 二阶相似度指的是一对顶点之间的接近程度$(u,v)$在网络中是其邻域网络结构之间的相似性。数学上，让$p_{u} =(w_{u,1},...,w_{u, \\| V \\| })$表示一阶附近$u$与所有其他的顶点，那么$u$和$v$之间的二阶相似性由$p_{u}$和$p_{v}$之间的相似性来决定。如果没有一个顶点同时和$u$与$v$链接，那么$u$和$v$的二阶相似性是0。\n- 4.大规模信息网络嵌入的定义\n    - 给定大网络$G=(V,E)$，大规模信息网络嵌入是将每个顶点$v\\in V$表示为低维空间$R^{d}$中的向量，学习一个函数$f_{G} : V \\rightarrow R^{d}$，其中$d\\ll \\| V \\|$. 在空间$R^{d}$中，顶点之间的一阶相似性和二阶相似性都被保留。\n\n\n--------\n\n### LINE:大规模信息网络嵌入\n\n前文已经提到了信息网络的两大相似性，LINE模型也是由此展开的。\n\n- **LINE模型的一阶相似性**：对每个无向边$(i,j)$，定义顶点$v_{i}$和$v_{j}$的联合概率分布为：\n\n$$p_1(v_i, v_j) = \\frac{1}{1+ exp( - \\vec{u}_i^T \\cdot \\vec{u}_j )} $$\n\n$u_{i}\\in {R}^{d}$是顶点$v_{i}$的低维向量表示，为保持其一阶相似性，一种直接方法是最小化以下的目标函数：\n\n$$ O_1 = d( \\hat{p}_1 (\\cdot, \\cdot), p_1 (\\cdot, \\cdot) ) $$\n\n$d(\\cdot , \\cdot)$为两种分布之间的距离，$p(,)$为空间$V\\times V$上的一个分布，\n其经验分布可以定义为 $ \\hat{p} _{1} (i,j) = \\frac{ w _{i,j} }{ W } $，其中$ W = \\sum _{(i,j) \\in E} w _{ij} $。\n我们选择尽量减少两个概率分布的KL 散度。将$d(,)$替换为 KL 散度并省略一些常数，我们得到︰\n\n$$ O_1 = - \\sum_{(i,j) \\in E} w_{i,j} log p_1 (v_i, v_j) $$\n\n需要注意的是，*一阶相似度仅适用于无向图,而不适用于有向图*。\n\n\n- **LINE模型的二阶相似性**：二阶相似性假定与其他顶点共享邻居顶点的两个点彼此相似(无向有向均可)，一个向量$u^{} $和$u^{'}$ 分别表示顶点本身和其他顶点的特定“上下文”，意为二阶相似。对于每个有向边$(i,j)$，我们首先定义由生成“上下文”的概率：\n\n$$ p_2(v_j | v_i) = \\frac{exp(\\vec{u}_j^{'T} \\cdot \\vec{u}_i)}{\\sum_{k=1}^{|V|} exp(\\vec{u}_k^{'T} \\cdot \\vec{u}_i)} $$\n\n可以看到，上式其实是一个条件分布，我们取$i$为研究对象，$p(,v_{i} )$，在降维之后使其接近与经验分布$\\hat{p}_{2} $。因此最小化以下目标函数：\n\n$$ O_2 = \\sum_{i \\in V} \\lambda_i d ( \\hat{p}_2 (\\cdot | v_i), p_2(\\cdot | v_i) ) $$\n\n$d(,)$上文已经说明，$\\lambda _{i}$ 来表示网络中顶点$i$的声望,本文中即是顶点$i$的度数，因此二阶相似性的计算公式为：\n\n$$ O_2 = - \\sum_{(i,j) \\in E} w_{i,j} log p_2(v_j | v_i) $$\n\n将上述的$O_{1}$ 以及$O_{2}$ 进行训练，再进行综合。\n\n\n- **模型优化**：$O_{2}$ 的计算代价十分的昂贵，因此优化时使用了负采样方法，为每条边指定了一个目标函数：\n\n$$ log \\sigma (\\vec{u}_j^{'T} \\cdot \\vec{u}_i) + \\sum_{i=1}^K E_{v_n \\sim P_n (v)} \\left[ log \\sigma (- \\vec{u}_n^{'T} \\cdot \\vec{u}_i) \\right] $$\n\n其中 $\\sigma (x) = 1 / (1+exp(−x))$ 是 sigmoid 函数。这个模型的第一项是观察的边，模型的第二项是从噪声分布中得出的负样例的边 和 K 是负样例的边的数量。\n这里设$ P_n (v) \\propto d^{3/4}_v $，其中$ d_v $是顶点$ v $的 out-degree。\n\n对于目标函数$O_1$，存在一个非常简单的解：$u_{ik} = \\infty$ ，对 $i=1,…, \\| V \\| $和 $k=1,…,d$。为了避免这个简单的解，这里仍然可以利用负采样的方法，只需要将$ \\vec{v}_j^{'T} $改为$\\vec{v}_j^T$。\n\n这里采样异步随机梯度算法（ASGD）来优化上式。在每一步，ASGD 算法采样一个 mini-batch 的边并更新参数。如果采样了一个边 $(i,j)$，顶点 $i$ 的嵌入向量 $\\vec{u}_i$ 梯度会被计算为：\n\n$$ \\frac{\\partial O_2}{\\partial \\vec{u}_i} = w_{i,j} \\cdot \\frac{\\partial log p_2 (v_j | v_i)}{\\partial \\vec{u}_i} $$\n\n注意这个梯度是与边的权值相乘，当边的权值有很大的方差的时候会出现问题。例如，在此共现网络中，有些单词共现了很多次但有些词却共现很少次。\n在这样的网络中，梯度的比例是发散的，很难找到一个好的学习速率。如果根据小权重的边选择一个大的学习速率，那么在有较大权重的边上的梯度将会爆炸，\n如果根据具有较大权重的边来选择学习速率，那么梯度将会变得太小。因此边缘采样同样要优化。从起始边缘采样并将采样的边缘作为二进制边缘，其中采样概率与原始边缘的权重成比例。\n\n\n-------\n\n>\n1. [LINE：Large-scale Information Network Embedding](https://arxiv.org/pdf/1503.03578.pdf)\n2. [LINE:Large-scale Information Network Embedding阅读笔记](https://zhuanlan.zhihu.com/p/27037042)\n3. [论文 LINE：Large-scale Information Network Embedding 阅读笔记](https://huangzhanpeng.github.io/2018/03/01/LINE%EF%BC%9ALarge-scale-Information-Network-Embedding/)\n\n","source":"_posts/2018-07-13-LINE.md","raw":"---\nlayout: post\ntitle: LINE Tutorial\ndate: 2018-07-13 12:10 +0800\ncategories: 深度学习\ntags:\n- Embedding\n- 神经网络\nmathjax: true\ncopyright: false\n---\n\n目录\n\n* 写作动机\n* 引子\n* 问题定义\n* LINE:大规模信息网络嵌入\n\n\n-------\n\n### 写作动机\n\n当前大多数图形嵌入方法不能对包含数百万个节点的真实信息网络进行扩展，分析大型信息网络在学术界和行业中一直受到越来越多的关注。\n而现在的大多数嵌入方法在小型网络中适用性非常不错，但当网络包含数百万乃至数百亿节点时，就看起来并不那么有效，其时间复杂度至少是节点数的二次方。\n最重要的是，它们着重于关注节点之间的一阶相似性，及两点之间是否直接相连，而忽略了其二阶相似性(即拥有许多共同的邻节点)。\n因此LINE模型就是为了在信息网络嵌入至低维空间时保留其一阶相似以及二阶相似。\n\n\n-------\n\n### 引子\n\n下图展示了一个说明性例子。因为顶点6和7之间的边的权值很大，即6和7有非常高的一阶近似，它们应该在嵌入领域中彼此紧密相连。\n另一方面，虽然顶点5和6之间的顶点没有链接，但是它们有很多共同的邻域，即它们有很高的二阶近似和两者的表示会是十分的接近。\n\n![neighbors](/posts_res/2018-07-13-LINE/1.png)\n\n\n--------\n\n### 问题定义\n\n- 1.信息网络的定义\n    - 信息网络定义为$G=(V,E)$，其中$V$是顶点集合，顶点表示数据对象，$E$是顶点之间的边缘的集合，每条边表示两个数据对象之间的关系。每条边$e\\in E$表示为有序对$e=(u,v)$，并且与权重$W_{uv} > 0$相关联，权重表示关系的强度。如果$G$是无向的，我们有$(u,v)\\equiv (v,u)$和$W_{uv}\\equiv W_{vu}$;如果G是有向的，我们有$(u,v)\\ne (v,u)$和$W_{uv}\\ne W_{vu}$。关于边权重问题，本次论文中只研究非负权重的情况。\n- 2.一阶相似性的定义\n    - 网络中的一阶相似性是两个顶点之间的局部点对的邻近度。对于由边缘$(u,v)$链接的每对顶点，该边缘的权重$W_{uv}$表示$u$和$v$之间的一阶相似性，如果在$u$和$v$之间没有观察到边缘，它们的一阶相似性为$0$。\n- 3.二阶相似度的定义\n    - 二阶相似度指的是一对顶点之间的接近程度$(u,v)$在网络中是其邻域网络结构之间的相似性。数学上，让$p_{u} =(w_{u,1},...,w_{u, \\| V \\| })$表示一阶附近$u$与所有其他的顶点，那么$u$和$v$之间的二阶相似性由$p_{u}$和$p_{v}$之间的相似性来决定。如果没有一个顶点同时和$u$与$v$链接，那么$u$和$v$的二阶相似性是0。\n- 4.大规模信息网络嵌入的定义\n    - 给定大网络$G=(V,E)$，大规模信息网络嵌入是将每个顶点$v\\in V$表示为低维空间$R^{d}$中的向量，学习一个函数$f_{G} : V \\rightarrow R^{d}$，其中$d\\ll \\| V \\|$. 在空间$R^{d}$中，顶点之间的一阶相似性和二阶相似性都被保留。\n\n\n--------\n\n### LINE:大规模信息网络嵌入\n\n前文已经提到了信息网络的两大相似性，LINE模型也是由此展开的。\n\n- **LINE模型的一阶相似性**：对每个无向边$(i,j)$，定义顶点$v_{i}$和$v_{j}$的联合概率分布为：\n\n$$p_1(v_i, v_j) = \\frac{1}{1+ exp( - \\vec{u}_i^T \\cdot \\vec{u}_j )} $$\n\n$u_{i}\\in {R}^{d}$是顶点$v_{i}$的低维向量表示，为保持其一阶相似性，一种直接方法是最小化以下的目标函数：\n\n$$ O_1 = d( \\hat{p}_1 (\\cdot, \\cdot), p_1 (\\cdot, \\cdot) ) $$\n\n$d(\\cdot , \\cdot)$为两种分布之间的距离，$p(,)$为空间$V\\times V$上的一个分布，\n其经验分布可以定义为 $ \\hat{p} _{1} (i,j) = \\frac{ w _{i,j} }{ W } $，其中$ W = \\sum _{(i,j) \\in E} w _{ij} $。\n我们选择尽量减少两个概率分布的KL 散度。将$d(,)$替换为 KL 散度并省略一些常数，我们得到︰\n\n$$ O_1 = - \\sum_{(i,j) \\in E} w_{i,j} log p_1 (v_i, v_j) $$\n\n需要注意的是，*一阶相似度仅适用于无向图,而不适用于有向图*。\n\n\n- **LINE模型的二阶相似性**：二阶相似性假定与其他顶点共享邻居顶点的两个点彼此相似(无向有向均可)，一个向量$u^{} $和$u^{'}$ 分别表示顶点本身和其他顶点的特定“上下文”，意为二阶相似。对于每个有向边$(i,j)$，我们首先定义由生成“上下文”的概率：\n\n$$ p_2(v_j | v_i) = \\frac{exp(\\vec{u}_j^{'T} \\cdot \\vec{u}_i)}{\\sum_{k=1}^{|V|} exp(\\vec{u}_k^{'T} \\cdot \\vec{u}_i)} $$\n\n可以看到，上式其实是一个条件分布，我们取$i$为研究对象，$p(,v_{i} )$，在降维之后使其接近与经验分布$\\hat{p}_{2} $。因此最小化以下目标函数：\n\n$$ O_2 = \\sum_{i \\in V} \\lambda_i d ( \\hat{p}_2 (\\cdot | v_i), p_2(\\cdot | v_i) ) $$\n\n$d(,)$上文已经说明，$\\lambda _{i}$ 来表示网络中顶点$i$的声望,本文中即是顶点$i$的度数，因此二阶相似性的计算公式为：\n\n$$ O_2 = - \\sum_{(i,j) \\in E} w_{i,j} log p_2(v_j | v_i) $$\n\n将上述的$O_{1}$ 以及$O_{2}$ 进行训练，再进行综合。\n\n\n- **模型优化**：$O_{2}$ 的计算代价十分的昂贵，因此优化时使用了负采样方法，为每条边指定了一个目标函数：\n\n$$ log \\sigma (\\vec{u}_j^{'T} \\cdot \\vec{u}_i) + \\sum_{i=1}^K E_{v_n \\sim P_n (v)} \\left[ log \\sigma (- \\vec{u}_n^{'T} \\cdot \\vec{u}_i) \\right] $$\n\n其中 $\\sigma (x) = 1 / (1+exp(−x))$ 是 sigmoid 函数。这个模型的第一项是观察的边，模型的第二项是从噪声分布中得出的负样例的边 和 K 是负样例的边的数量。\n这里设$ P_n (v) \\propto d^{3/4}_v $，其中$ d_v $是顶点$ v $的 out-degree。\n\n对于目标函数$O_1$，存在一个非常简单的解：$u_{ik} = \\infty$ ，对 $i=1,…, \\| V \\| $和 $k=1,…,d$。为了避免这个简单的解，这里仍然可以利用负采样的方法，只需要将$ \\vec{v}_j^{'T} $改为$\\vec{v}_j^T$。\n\n这里采样异步随机梯度算法（ASGD）来优化上式。在每一步，ASGD 算法采样一个 mini-batch 的边并更新参数。如果采样了一个边 $(i,j)$，顶点 $i$ 的嵌入向量 $\\vec{u}_i$ 梯度会被计算为：\n\n$$ \\frac{\\partial O_2}{\\partial \\vec{u}_i} = w_{i,j} \\cdot \\frac{\\partial log p_2 (v_j | v_i)}{\\partial \\vec{u}_i} $$\n\n注意这个梯度是与边的权值相乘，当边的权值有很大的方差的时候会出现问题。例如，在此共现网络中，有些单词共现了很多次但有些词却共现很少次。\n在这样的网络中，梯度的比例是发散的，很难找到一个好的学习速率。如果根据小权重的边选择一个大的学习速率，那么在有较大权重的边上的梯度将会爆炸，\n如果根据具有较大权重的边来选择学习速率，那么梯度将会变得太小。因此边缘采样同样要优化。从起始边缘采样并将采样的边缘作为二进制边缘，其中采样概率与原始边缘的权重成比例。\n\n\n-------\n\n>\n1. [LINE：Large-scale Information Network Embedding](https://arxiv.org/pdf/1503.03578.pdf)\n2. [LINE:Large-scale Information Network Embedding阅读笔记](https://zhuanlan.zhihu.com/p/27037042)\n3. [论文 LINE：Large-scale Information Network Embedding 阅读笔记](https://huangzhanpeng.github.io/2018/03/01/LINE%EF%BC%9ALarge-scale-Information-Network-Embedding/)\n\n","slug":"LINE","published":1,"updated":"2019-08-17T09:38:51.517Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwc002v2qwps8j75kx3","content":"<p>目录</p><ul>\n<li>写作动机</li>\n<li>引子</li>\n<li>问题定义</li>\n<li>LINE:大规模信息网络嵌入</li>\n</ul><hr><h3 id=\"写作动机\"><a href=\"#写作动机\" class=\"headerlink\" title=\"写作动机\"></a>写作动机</h3><p>当前大多数图形嵌入方法不能对包含数百万个节点的真实信息网络进行扩展，分析大型信息网络在学术界和行业中一直受到越来越多的关注。\n而现在的大多数嵌入方法在小型网络中适用性非常不错，但当网络包含数百万乃至数百亿节点时，就看起来并不那么有效，其时间复杂度至少是节点数的二次方。\n最重要的是，它们着重于关注节点之间的一阶相似性，及两点之间是否直接相连，而忽略了其二阶相似性(即拥有许多共同的邻节点)。\n因此LINE模型就是为了在信息网络嵌入至低维空间时保留其一阶相似以及二阶相似。</p><hr><h3 id=\"引子\"><a href=\"#引子\" class=\"headerlink\" title=\"引子\"></a>引子</h3><p>下图展示了一个说明性例子。因为顶点6和7之间的边的权值很大，即6和7有非常高的一阶近似，它们应该在嵌入领域中彼此紧密相连。\n另一方面，虽然顶点5和6之间的顶点没有链接，但是它们有很多共同的邻域，即它们有很高的二阶近似和两者的表示会是十分的接近。</p><a id=\"more\"></a>\n\n\n\n\n\n<p><img src=\"/posts_res/2018-07-13-LINE/1.png\" alt=\"neighbors\"></p>\n<hr>\n<h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><ul>\n<li>1.信息网络的定义<ul>\n<li>信息网络定义为$G=(V,E)$，其中$V$是顶点集合，顶点表示数据对象，$E$是顶点之间的边缘的集合，每条边表示两个数据对象之间的关系。每条边$e\\in E$表示为有序对$e=(u,v)$，并且与权重$W<em>{uv} &gt; 0$相关联，权重表示关系的强度。如果$G$是无向的，我们有$(u,v)\\equiv (v,u)$和$W</em>{uv}\\equiv W<em>{vu}$;如果G是有向的，我们有$(u,v)\\ne (v,u)$和$W</em>{uv}\\ne W_{vu}$。关于边权重问题，本次论文中只研究非负权重的情况。</li>\n</ul>\n</li>\n<li>2.一阶相似性的定义<ul>\n<li>网络中的一阶相似性是两个顶点之间的局部点对的邻近度。对于由边缘$(u,v)$链接的每对顶点，该边缘的权重$W_{uv}$表示$u$和$v$之间的一阶相似性，如果在$u$和$v$之间没有观察到边缘，它们的一阶相似性为$0$。</li>\n</ul>\n</li>\n<li>3.二阶相似度的定义<ul>\n<li>二阶相似度指的是一对顶点之间的接近程度$(u,v)$在网络中是其邻域网络结构之间的相似性。数学上，让$p<em>{u} =(w</em>{u,1},…,w<em>{u, | V | })$表示一阶附近$u$与所有其他的顶点，那么$u$和$v$之间的二阶相似性由$p</em>{u}$和$p_{v}$之间的相似性来决定。如果没有一个顶点同时和$u$与$v$链接，那么$u$和$v$的二阶相似性是0。</li>\n</ul>\n</li>\n<li>4.大规模信息网络嵌入的定义<ul>\n<li>给定大网络$G=(V,E)$，大规模信息网络嵌入是将每个顶点$v\\in V$表示为低维空间$R^{d}$中的向量，学习一个函数$f_{G} : V \\rightarrow R^{d}$，其中$d\\ll | V |$. 在空间$R^{d}$中，顶点之间的一阶相似性和二阶相似性都被保留。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"LINE-大规模信息网络嵌入\"><a href=\"#LINE-大规模信息网络嵌入\" class=\"headerlink\" title=\"LINE:大规模信息网络嵌入\"></a>LINE:大规模信息网络嵌入</h3><p>前文已经提到了信息网络的两大相似性，LINE模型也是由此展开的。</p>\n<ul>\n<li><strong>LINE模型的一阶相似性</strong>：对每个无向边$(i,j)$，定义顶点$v<em>{i}$和$v</em>{j}$的联合概率分布为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">p_1(v_i, v_j) = \\frac{1}{1+ exp( - \\vec{u}_i^T \\cdot \\vec{u}_j )}</script><p>$u<em>{i}\\in {R}^{d}$是顶点$v</em>{i}$的低维向量表示，为保持其一阶相似性，一种直接方法是最小化以下的目标函数：</p>\n<script type=\"math/tex; mode=display\">O_1 = d( \\hat{p}_1 (\\cdot, \\cdot), p_1 (\\cdot, \\cdot) )</script><p>$d(\\cdot , \\cdot)$为两种分布之间的距离，$p(,)$为空间$V\\times V$上的一个分布，\n其经验分布可以定义为 $ \\hat{p} <em>{1} (i,j) = \\frac{ w </em>{i,j} }{ W } $，其中$ W = \\sum <em>{(i,j) \\in E} w </em>{ij} $。\n我们选择尽量减少两个概率分布的KL 散度。将$d(,)$替换为 KL 散度并省略一些常数，我们得到︰</p>\n<script type=\"math/tex; mode=display\">O_1 = - \\sum_{(i,j) \\in E} w_{i,j} log p_1 (v_i, v_j)</script><p>需要注意的是，<em>一阶相似度仅适用于无向图,而不适用于有向图</em>。</p>\n<ul>\n<li><strong>LINE模型的二阶相似性</strong>：二阶相似性假定与其他顶点共享邻居顶点的两个点彼此相似(无向有向均可)，一个向量$u^{} $和$u^{‘}$ 分别表示顶点本身和其他顶点的特定“上下文”，意为二阶相似。对于每个有向边$(i,j)$，我们首先定义由生成“上下文”的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">p_2(v_j | v_i) = \\frac{exp(\\vec{u}_j^{'T} \\cdot \\vec{u}_i)}{\\sum_{k=1}^{|V|} exp(\\vec{u}_k^{'T} \\cdot \\vec{u}_i)}</script><p>可以看到，上式其实是一个条件分布，我们取$i$为研究对象，$p(,v<em>{i} )$，在降维之后使其接近与经验分布$\\hat{p}</em>{2} $。因此最小化以下目标函数：</p>\n<script type=\"math/tex; mode=display\">O_2 = \\sum_{i \\in V} \\lambda_i d ( \\hat{p}_2 (\\cdot | v_i), p_2(\\cdot | v_i) )</script><p>$d(,)$上文已经说明，$\\lambda _{i}$ 来表示网络中顶点$i$的声望,本文中即是顶点$i$的度数，因此二阶相似性的计算公式为：</p>\n<script type=\"math/tex; mode=display\">O_2 = - \\sum_{(i,j) \\in E} w_{i,j} log p_2(v_j | v_i)</script><p>将上述的$O<em>{1}$ 以及$O</em>{2}$ 进行训练，再进行综合。</p>\n<ul>\n<li><strong>模型优化</strong>：$O_{2}$ 的计算代价十分的昂贵，因此优化时使用了负采样方法，为每条边指定了一个目标函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">log \\sigma (\\vec{u}_j^{'T} \\cdot \\vec{u}_i) + \\sum_{i=1}^K E_{v_n \\sim P_n (v)} \\left[ log \\sigma (- \\vec{u}_n^{'T} \\cdot \\vec{u}_i) \\right]</script><p>其中 $\\sigma (x) = 1 / (1+exp(−x))$ 是 sigmoid 函数。这个模型的第一项是观察的边，模型的第二项是从噪声分布中得出的负样例的边 和 K 是负样例的边的数量。\n这里设$ P_n (v) \\propto d^{3/4}_v $，其中$ d_v $是顶点$ v $的 out-degree。</p>\n<p>对于目标函数$O<em>1$，存在一个非常简单的解：$u</em>{ik} = \\infty$ ，对 $i=1,…, | V | $和 $k=1,…,d$。为了避免这个简单的解，这里仍然可以利用负采样的方法，只需要将$ \\vec{v}_j^{‘T} $改为$\\vec{v}_j^T$。</p>\n<p>这里采样异步随机梯度算法（ASGD）来优化上式。在每一步，ASGD 算法采样一个 mini-batch 的边并更新参数。如果采样了一个边 $(i,j)$，顶点 $i$ 的嵌入向量 $\\vec{u}_i$ 梯度会被计算为：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial O_2}{\\partial \\vec{u}_i} = w_{i,j} \\cdot \\frac{\\partial log p_2 (v_j | v_i)}{\\partial \\vec{u}_i}</script><p>注意这个梯度是与边的权值相乘，当边的权值有很大的方差的时候会出现问题。例如，在此共现网络中，有些单词共现了很多次但有些词却共现很少次。\n在这样的网络中，梯度的比例是发散的，很难找到一个好的学习速率。如果根据小权重的边选择一个大的学习速率，那么在有较大权重的边上的梯度将会爆炸，\n如果根据具有较大权重的边来选择学习速率，那么梯度将会变得太小。因此边缘采样同样要优化。从起始边缘采样并将采样的边缘作为二进制边缘，其中采样概率与原始边缘的权重成比例。</p>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1503.03578.pdf\" target=\"_blank\" rel=\"noopener\">LINE：Large-scale Information Network Embedding</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/27037042\" target=\"_blank\" rel=\"noopener\">LINE:Large-scale Information Network Embedding阅读笔记</a></li>\n<li><a href=\"https://huangzhanpeng.github.io/2018/03/01/LINE%EF%BC%9ALarge-scale-Information-Network-Embedding/\" target=\"_blank\" rel=\"noopener\">论文 LINE：Large-scale Information Network Embedding 阅读笔记</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>写作动机</li>\n<li>引子</li>\n<li>问题定义</li>\n<li>LINE:大规模信息网络嵌入</li>\n</ul><hr><h3 id=\"写作动机\"><a href=\"#写作动机\" class=\"headerlink\" title=\"写作动机\"></a>写作动机</h3><p>当前大多数图形嵌入方法不能对包含数百万个节点的真实信息网络进行扩展，分析大型信息网络在学术界和行业中一直受到越来越多的关注。\n而现在的大多数嵌入方法在小型网络中适用性非常不错，但当网络包含数百万乃至数百亿节点时，就看起来并不那么有效，其时间复杂度至少是节点数的二次方。\n最重要的是，它们着重于关注节点之间的一阶相似性，及两点之间是否直接相连，而忽略了其二阶相似性(即拥有许多共同的邻节点)。\n因此LINE模型就是为了在信息网络嵌入至低维空间时保留其一阶相似以及二阶相似。</p><hr><h3 id=\"引子\"><a href=\"#引子\" class=\"headerlink\" title=\"引子\"></a>引子</h3><p>下图展示了一个说明性例子。因为顶点6和7之间的边的权值很大，即6和7有非常高的一阶近似，它们应该在嵌入领域中彼此紧密相连。\n另一方面，虽然顶点5和6之间的顶点没有链接，但是它们有很多共同的邻域，即它们有很高的二阶近似和两者的表示会是十分的接近。</p>","more":"\n\n\n\n\n\n<p><img src=\"/posts_res/2018-07-13-LINE/1.png\" alt=\"neighbors\"></p>\n<hr>\n<h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><ul>\n<li>1.信息网络的定义<ul>\n<li>信息网络定义为$G=(V,E)$，其中$V$是顶点集合，顶点表示数据对象，$E$是顶点之间的边缘的集合，每条边表示两个数据对象之间的关系。每条边$e\\in E$表示为有序对$e=(u,v)$，并且与权重$W<em>{uv} &gt; 0$相关联，权重表示关系的强度。如果$G$是无向的，我们有$(u,v)\\equiv (v,u)$和$W</em>{uv}\\equiv W<em>{vu}$;如果G是有向的，我们有$(u,v)\\ne (v,u)$和$W</em>{uv}\\ne W_{vu}$。关于边权重问题，本次论文中只研究非负权重的情况。</li>\n</ul>\n</li>\n<li>2.一阶相似性的定义<ul>\n<li>网络中的一阶相似性是两个顶点之间的局部点对的邻近度。对于由边缘$(u,v)$链接的每对顶点，该边缘的权重$W_{uv}$表示$u$和$v$之间的一阶相似性，如果在$u$和$v$之间没有观察到边缘，它们的一阶相似性为$0$。</li>\n</ul>\n</li>\n<li>3.二阶相似度的定义<ul>\n<li>二阶相似度指的是一对顶点之间的接近程度$(u,v)$在网络中是其邻域网络结构之间的相似性。数学上，让$p<em>{u} =(w</em>{u,1},…,w<em>{u, | V | })$表示一阶附近$u$与所有其他的顶点，那么$u$和$v$之间的二阶相似性由$p</em>{u}$和$p_{v}$之间的相似性来决定。如果没有一个顶点同时和$u$与$v$链接，那么$u$和$v$的二阶相似性是0。</li>\n</ul>\n</li>\n<li>4.大规模信息网络嵌入的定义<ul>\n<li>给定大网络$G=(V,E)$，大规模信息网络嵌入是将每个顶点$v\\in V$表示为低维空间$R^{d}$中的向量，学习一个函数$f_{G} : V \\rightarrow R^{d}$，其中$d\\ll | V |$. 在空间$R^{d}$中，顶点之间的一阶相似性和二阶相似性都被保留。</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"LINE-大规模信息网络嵌入\"><a href=\"#LINE-大规模信息网络嵌入\" class=\"headerlink\" title=\"LINE:大规模信息网络嵌入\"></a>LINE:大规模信息网络嵌入</h3><p>前文已经提到了信息网络的两大相似性，LINE模型也是由此展开的。</p>\n<ul>\n<li><strong>LINE模型的一阶相似性</strong>：对每个无向边$(i,j)$，定义顶点$v<em>{i}$和$v</em>{j}$的联合概率分布为：</li>\n</ul>\n<script type=\"math/tex; mode=display\">p_1(v_i, v_j) = \\frac{1}{1+ exp( - \\vec{u}_i^T \\cdot \\vec{u}_j )}</script><p>$u<em>{i}\\in {R}^{d}$是顶点$v</em>{i}$的低维向量表示，为保持其一阶相似性，一种直接方法是最小化以下的目标函数：</p>\n<script type=\"math/tex; mode=display\">O_1 = d( \\hat{p}_1 (\\cdot, \\cdot), p_1 (\\cdot, \\cdot) )</script><p>$d(\\cdot , \\cdot)$为两种分布之间的距离，$p(,)$为空间$V\\times V$上的一个分布，\n其经验分布可以定义为 $ \\hat{p} <em>{1} (i,j) = \\frac{ w </em>{i,j} }{ W } $，其中$ W = \\sum <em>{(i,j) \\in E} w </em>{ij} $。\n我们选择尽量减少两个概率分布的KL 散度。将$d(,)$替换为 KL 散度并省略一些常数，我们得到︰</p>\n<script type=\"math/tex; mode=display\">O_1 = - \\sum_{(i,j) \\in E} w_{i,j} log p_1 (v_i, v_j)</script><p>需要注意的是，<em>一阶相似度仅适用于无向图,而不适用于有向图</em>。</p>\n<ul>\n<li><strong>LINE模型的二阶相似性</strong>：二阶相似性假定与其他顶点共享邻居顶点的两个点彼此相似(无向有向均可)，一个向量$u^{} $和$u^{‘}$ 分别表示顶点本身和其他顶点的特定“上下文”，意为二阶相似。对于每个有向边$(i,j)$，我们首先定义由生成“上下文”的概率：</li>\n</ul>\n<script type=\"math/tex; mode=display\">p_2(v_j | v_i) = \\frac{exp(\\vec{u}_j^{'T} \\cdot \\vec{u}_i)}{\\sum_{k=1}^{|V|} exp(\\vec{u}_k^{'T} \\cdot \\vec{u}_i)}</script><p>可以看到，上式其实是一个条件分布，我们取$i$为研究对象，$p(,v<em>{i} )$，在降维之后使其接近与经验分布$\\hat{p}</em>{2} $。因此最小化以下目标函数：</p>\n<script type=\"math/tex; mode=display\">O_2 = \\sum_{i \\in V} \\lambda_i d ( \\hat{p}_2 (\\cdot | v_i), p_2(\\cdot | v_i) )</script><p>$d(,)$上文已经说明，$\\lambda _{i}$ 来表示网络中顶点$i$的声望,本文中即是顶点$i$的度数，因此二阶相似性的计算公式为：</p>\n<script type=\"math/tex; mode=display\">O_2 = - \\sum_{(i,j) \\in E} w_{i,j} log p_2(v_j | v_i)</script><p>将上述的$O<em>{1}$ 以及$O</em>{2}$ 进行训练，再进行综合。</p>\n<ul>\n<li><strong>模型优化</strong>：$O_{2}$ 的计算代价十分的昂贵，因此优化时使用了负采样方法，为每条边指定了一个目标函数：</li>\n</ul>\n<script type=\"math/tex; mode=display\">log \\sigma (\\vec{u}_j^{'T} \\cdot \\vec{u}_i) + \\sum_{i=1}^K E_{v_n \\sim P_n (v)} \\left[ log \\sigma (- \\vec{u}_n^{'T} \\cdot \\vec{u}_i) \\right]</script><p>其中 $\\sigma (x) = 1 / (1+exp(−x))$ 是 sigmoid 函数。这个模型的第一项是观察的边，模型的第二项是从噪声分布中得出的负样例的边 和 K 是负样例的边的数量。\n这里设$ P_n (v) \\propto d^{3/4}_v $，其中$ d_v $是顶点$ v $的 out-degree。</p>\n<p>对于目标函数$O<em>1$，存在一个非常简单的解：$u</em>{ik} = \\infty$ ，对 $i=1,…, | V | $和 $k=1,…,d$。为了避免这个简单的解，这里仍然可以利用负采样的方法，只需要将$ \\vec{v}_j^{‘T} $改为$\\vec{v}_j^T$。</p>\n<p>这里采样异步随机梯度算法（ASGD）来优化上式。在每一步，ASGD 算法采样一个 mini-batch 的边并更新参数。如果采样了一个边 $(i,j)$，顶点 $i$ 的嵌入向量 $\\vec{u}_i$ 梯度会被计算为：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial O_2}{\\partial \\vec{u}_i} = w_{i,j} \\cdot \\frac{\\partial log p_2 (v_j | v_i)}{\\partial \\vec{u}_i}</script><p>注意这个梯度是与边的权值相乘，当边的权值有很大的方差的时候会出现问题。例如，在此共现网络中，有些单词共现了很多次但有些词却共现很少次。\n在这样的网络中，梯度的比例是发散的，很难找到一个好的学习速率。如果根据小权重的边选择一个大的学习速率，那么在有较大权重的边上的梯度将会爆炸，\n如果根据具有较大权重的边来选择学习速率，那么梯度将会变得太小。因此边缘采样同样要优化。从起始边缘采样并将采样的边缘作为二进制边缘，其中采样概率与原始边缘的权重成比例。</p>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1503.03578.pdf\" target=\"_blank\" rel=\"noopener\">LINE：Large-scale Information Network Embedding</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/27037042\" target=\"_blank\" rel=\"noopener\">LINE:Large-scale Information Network Embedding阅读笔记</a></li>\n<li><a href=\"https://huangzhanpeng.github.io/2018/03/01/LINE%EF%BC%9ALarge-scale-Information-Network-Embedding/\" target=\"_blank\" rel=\"noopener\">论文 LINE：Large-scale Information Network Embedding 阅读笔记</a></li>\n</ol>\n"},{"layout":"post","title":"Doc2Vec Tutorial","date":"2018-07-08T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n目录\n\n* gensim简单使用\n* Doc2Vec(PV-DM)\n* Doc2Vec(PV-DBOW)\n\n\n---------\n\n### gensim简单使用\n\n模型参数说明：\n1. dm=1 PV-DM  dm=0 PV-DBOW。\n2. size 所得向量的维度。\n3. window 上下文词语离当前词语的最大距离。\n4. alpha 初始学习率，在训练中会下降到min_alpha。\n5. min_count 词频小于min_count的词会被忽略。\n6. max_vocab_size 最大词汇表size，每一百万词会需要1GB的内存，默认没有限制。\n7. sample 下采样比例。\n8. iter 在整个语料上的迭代次数(epochs)，推荐10到20。\n9. hs=1 hierarchical softmax ，hs=0(default) negative sampling。\n10. dm_mean=0(default) 上下文向量取综合，dm_mean=1 上下文向量取均值。\n11. dbow_words:1训练词向量，0只训练doc向量。\n\n```python\n# coding: utf-8\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\n\ndata = [\"I love machine learning. Its awesome.\",\n        \"I love coding in python\",\n        \"I love building chatbots\",\n        \"they chat amagingly well\"]\ntagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[i]) for i, _d in enumerate(data)]\n\n# for item in tagged_data:\n#     print(item)\n# TaggedDocument(['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], ['0'])\n# TaggedDocument(['i', 'love', 'coding', 'in', 'python'], ['1'])\n# TaggedDocument(['i', 'love', 'building', 'chatbots'], ['2'])\n# TaggedDocument(['they', 'chat', 'amagingly', 'well'], ['3'])\n\nmax_epochs = 100\nvec_size = 10\nalpha = 0.025\nmodel = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\nmodel.build_vocab(tagged_data)\nfor epoch in range(max_epochs):\n    print(\"iteration {0}\".format(epoch))\n    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)\n    model.alpha -= 0.0002\n    model.min_alpha = model.alpha\n\nmodel.save(\"d2v.model\")\nprint(\"Model Saved!\")\n\n\nmodel = Doc2Vec.load(\"d2v.model\")\ntest_data = word_tokenize(\"I love chatbots\".lower())\nv1 = model.infer_vector(test_data)\nprint(\"V1_infer:\", v1, end=\"\\n\\n\")\nsimilar_doc = model.docvecs.most_similar(1)\nprint(similar_doc, end=\"\\n\\n\")\n\nfor i in range(len(data)):\n    print(i, model.docvecs[i], end=\"\\n\\n\")\n\n\"\"\"\nV1_infer: [ 0.07642581  0.11099993 -0.02696064 -0.06895891  0.01907274 -0.08622721\n -0.00581482 -0.08242869 -0.02741096 -0.05718143]\n\n[(0, 0.9964085817337036), (2, 0.9903678894042969), (3, 0.985236406326294)]\n\n0 [ 1.22332275  0.93302435  0.0843188  -0.40848678 -0.26203951 -0.54852372\n -0.07185869 -0.72306669 -0.635378   -0.05744991]\n\n1 [ 0.83533287  0.64208162  0.04288336 -0.28720176 -0.17140444 -0.43564293\n -0.0435797  -0.45327306 -0.46411869 -0.12211297]\n\n2 [ 0.71188128  0.49025729 -0.03114104 -0.2172543  -0.18351653 -0.35265383\n -0.09494802 -0.47745392 -0.33393192 -0.1065111 ]\n\n3 [ 0.86559838  0.77232999 -0.0108105  -0.179581   -0.10455605 -0.41468951\n -0.11108498 -0.59402496 -0.59637135 -0.22117028]\n\"\"\"\n```\n\n> [DOC2VEC gensim tutorial](https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5)\n> [使用gensim的doc2vec生成文档向量](https://blog.csdn.net/weixin_39837402/article/details/80254868)\n\n-----------\n\n**需要说明的一点是这里的Paragraph Vector不是真的段落向量的意思，它可以根据需要的不同进行变化，可以是短语、句子甚至是文档。** \n\n### Doc2Vec(PV-DM)\n\nPV-DM在模型的输入层新增了一个Paragraph id，用于表征输入上下文所在的Paragraph。\n例如如果需要训练得到句子向量，那么Paragraph id即为语料库中的每个句子的表示。\nParagraph id其实也是一个向量，具有和词向量一样的维度，但是它们来自不同的向量空间，D和W，也就是来自于两个不同的矩阵。\n剩下的思路和CBOW模型基本一样。在模型中值得注意的一点是，在同一个Paragraph中，进行窗口滑动时，Paragraph id是不变的。\n\nParagraph id本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。\n\n- 模型的具体步骤如下：\n  - 每个段落都映射到一个唯一的向量，由矩阵$D$中的一列表示，每个词也被映射到一个唯一的向量，表示为$W$ ;\n  - 对**当前段落向量**和**当前上下文**所有词向量一起进行取平均值或连接操作，生成的向量用于输入到softmax层，以预测上下文中的下一个词: $$y=b+Uh(w_{t-k}, \\dots, w_{t+k}; W; D)$$ \n- 这个段落向量可以被认为是另一个词。可以将它理解为一种记忆单元，记住**当前上下文所缺失的内容**或段落的**主题** ；\n- 矩阵$D$ 和$W$ 的区别:\n  - 通过当前段落的index，对$D$ 进行Lookup得到的段落向量，对于当前段落的所有上下文是共享的，但是其他段落的上下文并不会影响它的值，也就是说它**不会跨段落(not across paragraphs)** ；\n  - 当时词向量矩阵$W$对于所有段落、所有上下文都是共享的。\n\n![pv-dm](/posts_res/2018-07-08-doc2vectutorial/1.png)\n\n\n--------\n\n### Doc2Vec(PV-DBOW)\n\n模型希望通过输入一个Paragraph id来预测该Paragraph中的单词的概率，和Skip-gram模型非常的类似。\n\n- PV-DBOW模型的输入忽略了的上下文单词，但是关注模型从输出的段落中预测从段落中随机抽取的单词；\n- PV-DBOW模型和训练词向量的**Skip-gram模型**非常相似。\n\n![pv-dbow](/posts_res/2018-07-08-doc2vectutorial/2.png)\n\n\n--------\n\n### Doc2Vec的特点\n\n- 可以从未标记的数据中学习，在没有足够多带标记的数据上仍工作良好；\n- 继承了词向量的词的语义（semantics）的特点；\n- 会考虑词的顺序（至少在某个小上下文中会考虑）\n\n\n----------\n\n> \n1. [Distributed representations of sentences and documents](https://arxiv.org/pdf/1405.4053.pdf)\n2. [Distributed representations of sentences and documents总结 - paperweekly](https://www.paperweekly.site/papers/notes/135)\n3. [用 Doc2Vec 得到文档／段落／句子的向量表达](https://blog.csdn.net/aliceyangxi1987/article/details/75097598)\n4. [关于Gensim的初次见面 和 Doc2vec 的模型训练](https://blog.csdn.net/qq_36472696/article/details/77871723)\n5. [论文笔记：Distributed Representations of Sentences and Documents](https://github.com/llhthinker/NLP-Papers/blob/master/distributed%20representations/2017-11/Distributed%20Representations%20of%20Sentences%20and%20Documents/note.md)\n6. [paragraph2vec介绍](http://d0evi1.com/paragraph2vec/)\n","source":"_posts/2018-07-08-doc2vectutorial.md","raw":"---\nlayout: post\ntitle: Doc2Vec Tutorial\ndate: 2018-07-08 12:10 +0800\ncategories: 深度学习\ntags:\n- Embedding\n- 神经网络\nmathjax: true\ncopyright: false\n---\n\n目录\n\n* gensim简单使用\n* Doc2Vec(PV-DM)\n* Doc2Vec(PV-DBOW)\n\n\n---------\n\n### gensim简单使用\n\n模型参数说明：\n1. dm=1 PV-DM  dm=0 PV-DBOW。\n2. size 所得向量的维度。\n3. window 上下文词语离当前词语的最大距离。\n4. alpha 初始学习率，在训练中会下降到min_alpha。\n5. min_count 词频小于min_count的词会被忽略。\n6. max_vocab_size 最大词汇表size，每一百万词会需要1GB的内存，默认没有限制。\n7. sample 下采样比例。\n8. iter 在整个语料上的迭代次数(epochs)，推荐10到20。\n9. hs=1 hierarchical softmax ，hs=0(default) negative sampling。\n10. dm_mean=0(default) 上下文向量取综合，dm_mean=1 上下文向量取均值。\n11. dbow_words:1训练词向量，0只训练doc向量。\n\n```python\n# coding: utf-8\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\n\ndata = [\"I love machine learning. Its awesome.\",\n        \"I love coding in python\",\n        \"I love building chatbots\",\n        \"they chat amagingly well\"]\ntagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[i]) for i, _d in enumerate(data)]\n\n# for item in tagged_data:\n#     print(item)\n# TaggedDocument(['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], ['0'])\n# TaggedDocument(['i', 'love', 'coding', 'in', 'python'], ['1'])\n# TaggedDocument(['i', 'love', 'building', 'chatbots'], ['2'])\n# TaggedDocument(['they', 'chat', 'amagingly', 'well'], ['3'])\n\nmax_epochs = 100\nvec_size = 10\nalpha = 0.025\nmodel = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\nmodel.build_vocab(tagged_data)\nfor epoch in range(max_epochs):\n    print(\"iteration {0}\".format(epoch))\n    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)\n    model.alpha -= 0.0002\n    model.min_alpha = model.alpha\n\nmodel.save(\"d2v.model\")\nprint(\"Model Saved!\")\n\n\nmodel = Doc2Vec.load(\"d2v.model\")\ntest_data = word_tokenize(\"I love chatbots\".lower())\nv1 = model.infer_vector(test_data)\nprint(\"V1_infer:\", v1, end=\"\\n\\n\")\nsimilar_doc = model.docvecs.most_similar(1)\nprint(similar_doc, end=\"\\n\\n\")\n\nfor i in range(len(data)):\n    print(i, model.docvecs[i], end=\"\\n\\n\")\n\n\"\"\"\nV1_infer: [ 0.07642581  0.11099993 -0.02696064 -0.06895891  0.01907274 -0.08622721\n -0.00581482 -0.08242869 -0.02741096 -0.05718143]\n\n[(0, 0.9964085817337036), (2, 0.9903678894042969), (3, 0.985236406326294)]\n\n0 [ 1.22332275  0.93302435  0.0843188  -0.40848678 -0.26203951 -0.54852372\n -0.07185869 -0.72306669 -0.635378   -0.05744991]\n\n1 [ 0.83533287  0.64208162  0.04288336 -0.28720176 -0.17140444 -0.43564293\n -0.0435797  -0.45327306 -0.46411869 -0.12211297]\n\n2 [ 0.71188128  0.49025729 -0.03114104 -0.2172543  -0.18351653 -0.35265383\n -0.09494802 -0.47745392 -0.33393192 -0.1065111 ]\n\n3 [ 0.86559838  0.77232999 -0.0108105  -0.179581   -0.10455605 -0.41468951\n -0.11108498 -0.59402496 -0.59637135 -0.22117028]\n\"\"\"\n```\n\n> [DOC2VEC gensim tutorial](https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5)\n> [使用gensim的doc2vec生成文档向量](https://blog.csdn.net/weixin_39837402/article/details/80254868)\n\n-----------\n\n**需要说明的一点是这里的Paragraph Vector不是真的段落向量的意思，它可以根据需要的不同进行变化，可以是短语、句子甚至是文档。** \n\n### Doc2Vec(PV-DM)\n\nPV-DM在模型的输入层新增了一个Paragraph id，用于表征输入上下文所在的Paragraph。\n例如如果需要训练得到句子向量，那么Paragraph id即为语料库中的每个句子的表示。\nParagraph id其实也是一个向量，具有和词向量一样的维度，但是它们来自不同的向量空间，D和W，也就是来自于两个不同的矩阵。\n剩下的思路和CBOW模型基本一样。在模型中值得注意的一点是，在同一个Paragraph中，进行窗口滑动时，Paragraph id是不变的。\n\nParagraph id本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。\n\n- 模型的具体步骤如下：\n  - 每个段落都映射到一个唯一的向量，由矩阵$D$中的一列表示，每个词也被映射到一个唯一的向量，表示为$W$ ;\n  - 对**当前段落向量**和**当前上下文**所有词向量一起进行取平均值或连接操作，生成的向量用于输入到softmax层，以预测上下文中的下一个词: $$y=b+Uh(w_{t-k}, \\dots, w_{t+k}; W; D)$$ \n- 这个段落向量可以被认为是另一个词。可以将它理解为一种记忆单元，记住**当前上下文所缺失的内容**或段落的**主题** ；\n- 矩阵$D$ 和$W$ 的区别:\n  - 通过当前段落的index，对$D$ 进行Lookup得到的段落向量，对于当前段落的所有上下文是共享的，但是其他段落的上下文并不会影响它的值，也就是说它**不会跨段落(not across paragraphs)** ；\n  - 当时词向量矩阵$W$对于所有段落、所有上下文都是共享的。\n\n![pv-dm](/posts_res/2018-07-08-doc2vectutorial/1.png)\n\n\n--------\n\n### Doc2Vec(PV-DBOW)\n\n模型希望通过输入一个Paragraph id来预测该Paragraph中的单词的概率，和Skip-gram模型非常的类似。\n\n- PV-DBOW模型的输入忽略了的上下文单词，但是关注模型从输出的段落中预测从段落中随机抽取的单词；\n- PV-DBOW模型和训练词向量的**Skip-gram模型**非常相似。\n\n![pv-dbow](/posts_res/2018-07-08-doc2vectutorial/2.png)\n\n\n--------\n\n### Doc2Vec的特点\n\n- 可以从未标记的数据中学习，在没有足够多带标记的数据上仍工作良好；\n- 继承了词向量的词的语义（semantics）的特点；\n- 会考虑词的顺序（至少在某个小上下文中会考虑）\n\n\n----------\n\n> \n1. [Distributed representations of sentences and documents](https://arxiv.org/pdf/1405.4053.pdf)\n2. [Distributed representations of sentences and documents总结 - paperweekly](https://www.paperweekly.site/papers/notes/135)\n3. [用 Doc2Vec 得到文档／段落／句子的向量表达](https://blog.csdn.net/aliceyangxi1987/article/details/75097598)\n4. [关于Gensim的初次见面 和 Doc2vec 的模型训练](https://blog.csdn.net/qq_36472696/article/details/77871723)\n5. [论文笔记：Distributed Representations of Sentences and Documents](https://github.com/llhthinker/NLP-Papers/blob/master/distributed%20representations/2017-11/Distributed%20Representations%20of%20Sentences%20and%20Documents/note.md)\n6. [paragraph2vec介绍](http://d0evi1.com/paragraph2vec/)\n","slug":"doc2vectutorial","published":1,"updated":"2019-08-17T09:38:31.598Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwe002y2qwp3ppsh7f2","content":"<p>目录</p><ul>\n<li>gensim简单使用</li>\n<li>Doc2Vec(PV-DM)</li>\n<li>Doc2Vec(PV-DBOW)</li>\n</ul><hr><h3 id=\"gensim简单使用\"><a href=\"#gensim简单使用\" class=\"headerlink\" title=\"gensim简单使用\"></a>gensim简单使用</h3><p>模型参数说明：</p><ol>\n<li>dm=1 PV-DM  dm=0 PV-DBOW。</li>\n<li>size 所得向量的维度。</li>\n<li>window 上下文词语离当前词语的最大距离。</li>\n<li>alpha 初始学习率，在训练中会下降到min_alpha。</li>\n<li>min_count 词频小于min_count的词会被忽略。</li>\n<li>max_vocab_size 最大词汇表size，每一百万词会需要1GB的内存，默认没有限制。</li>\n<li>sample 下采样比例。</li>\n<li>iter 在整个语料上的迭代次数(epochs)，推荐10到20。</li>\n<li>hs=1 hierarchical softmax ，hs=0(default) negative sampling。</li>\n<li>dm_mean=0(default) 上下文向量取综合，dm_mean=1 上下文向量取均值。</li>\n<li>dbow_words:1训练词向量，0只训练doc向量。</li>\n</ol><a id=\"more\"></a>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> gensim.models.doc2vec <span class=\"keyword\">import</span> Doc2Vec, TaggedDocument</span><br><span class=\"line\"><span class=\"keyword\">from</span> nltk.tokenize <span class=\"keyword\">import</span> word_tokenize</span><br><span class=\"line\"></span><br><span class=\"line\">data = [<span class=\"string\">\"I love machine learning. Its awesome.\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"I love coding in python\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"I love building chatbots\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"they chat amagingly well\"</span>]</span><br><span class=\"line\">tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[i]) <span class=\"keyword\">for</span> i, _d <span class=\"keyword\">in</span> enumerate(data)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># for item in tagged_data:</span></span><br><span class=\"line\"><span class=\"comment\">#     print(item)</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], ['0'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'coding', 'in', 'python'], ['1'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'building', 'chatbots'], ['2'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['they', 'chat', 'amagingly', 'well'], ['3'])</span></span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">100</span></span><br><span class=\"line\">vec_size = <span class=\"number\">10</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.025</span></span><br><span class=\"line\">model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=<span class=\"number\">0.00025</span>, min_count=<span class=\"number\">1</span>, dm=<span class=\"number\">1</span>)</span><br><span class=\"line\">model.build_vocab(tagged_data)</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(max_epochs):</span><br><span class=\"line\">    print(<span class=\"string\">\"iteration &#123;0&#125;\"</span>.format(epoch))</span><br><span class=\"line\">    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)</span><br><span class=\"line\">    model.alpha -= <span class=\"number\">0.0002</span></span><br><span class=\"line\">    model.min_alpha = model.alpha</span><br><span class=\"line\"></span><br><span class=\"line\">model.save(<span class=\"string\">\"d2v.model\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"Model Saved!\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model = Doc2Vec.load(<span class=\"string\">\"d2v.model\"</span>)</span><br><span class=\"line\">test_data = word_tokenize(<span class=\"string\">\"I love chatbots\"</span>.lower())</span><br><span class=\"line\">v1 = model.infer_vector(test_data)</span><br><span class=\"line\">print(<span class=\"string\">\"V1_infer:\"</span>, v1, end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\">similar_doc = model.docvecs.most_similar(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(similar_doc, end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(data)):</span><br><span class=\"line\">    print(i, model.docvecs[i], end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">V1_infer: [ 0.07642581  0.11099993 -0.02696064 -0.06895891  0.01907274 -0.08622721</span></span><br><span class=\"line\"><span class=\"string\"> -0.00581482 -0.08242869 -0.02741096 -0.05718143]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">[(0, 0.9964085817337036), (2, 0.9903678894042969), (3, 0.985236406326294)]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">0 [ 1.22332275  0.93302435  0.0843188  -0.40848678 -0.26203951 -0.54852372</span></span><br><span class=\"line\"><span class=\"string\"> -0.07185869 -0.72306669 -0.635378   -0.05744991]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">1 [ 0.83533287  0.64208162  0.04288336 -0.28720176 -0.17140444 -0.43564293</span></span><br><span class=\"line\"><span class=\"string\"> -0.0435797  -0.45327306 -0.46411869 -0.12211297]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">2 [ 0.71188128  0.49025729 -0.03114104 -0.2172543  -0.18351653 -0.35265383</span></span><br><span class=\"line\"><span class=\"string\"> -0.09494802 -0.47745392 -0.33393192 -0.1065111 ]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">3 [ 0.86559838  0.77232999 -0.0108105  -0.179581   -0.10455605 -0.41468951</span></span><br><span class=\"line\"><span class=\"string\"> -0.11108498 -0.59402496 -0.59637135 -0.22117028]</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><a href=\"https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\" target=\"_blank\" rel=\"noopener\">DOC2VEC gensim tutorial</a>\n<a href=\"https://blog.csdn.net/weixin_39837402/article/details/80254868\" target=\"_blank\" rel=\"noopener\">使用gensim的doc2vec生成文档向量</a></p>\n</blockquote>\n<hr>\n<p><strong>需要说明的一点是这里的Paragraph Vector不是真的段落向量的意思，它可以根据需要的不同进行变化，可以是短语、句子甚至是文档。</strong> </p>\n<h3 id=\"Doc2Vec-PV-DM\"><a href=\"#Doc2Vec-PV-DM\" class=\"headerlink\" title=\"Doc2Vec(PV-DM)\"></a>Doc2Vec(PV-DM)</h3><p>PV-DM在模型的输入层新增了一个Paragraph id，用于表征输入上下文所在的Paragraph。\n例如如果需要训练得到句子向量，那么Paragraph id即为语料库中的每个句子的表示。\nParagraph id其实也是一个向量，具有和词向量一样的维度，但是它们来自不同的向量空间，D和W，也就是来自于两个不同的矩阵。\n剩下的思路和CBOW模型基本一样。在模型中值得注意的一点是，在同一个Paragraph中，进行窗口滑动时，Paragraph id是不变的。</p>\n<p>Paragraph id本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。</p>\n<ul>\n<li>模型的具体步骤如下：<ul>\n<li>每个段落都映射到一个唯一的向量，由矩阵$D$中的一列表示，每个词也被映射到一个唯一的向量，表示为$W$ ;</li>\n<li>对<strong>当前段落向量</strong>和<strong>当前上下文</strong>所有词向量一起进行取平均值或连接操作，生成的向量用于输入到softmax层，以预测上下文中的下一个词: <script type=\"math/tex\">y=b+Uh(w_{t-k}, \\dots, w_{t+k}; W; D)</script> </li>\n</ul>\n</li>\n<li>这个段落向量可以被认为是另一个词。可以将它理解为一种记忆单元，记住<strong>当前上下文所缺失的内容</strong>或段落的<strong>主题</strong> ；</li>\n<li>矩阵$D$ 和$W$ 的区别:<ul>\n<li>通过当前段落的index，对$D$ 进行Lookup得到的段落向量，对于当前段落的所有上下文是共享的，但是其他段落的上下文并不会影响它的值，也就是说它<strong>不会跨段落(not across paragraphs)</strong> ；</li>\n<li>当时词向量矩阵$W$对于所有段落、所有上下文都是共享的。</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-07-08-doc2vectutorial/1.png\" alt=\"pv-dm\"></p>\n<hr>\n<h3 id=\"Doc2Vec-PV-DBOW\"><a href=\"#Doc2Vec-PV-DBOW\" class=\"headerlink\" title=\"Doc2Vec(PV-DBOW)\"></a>Doc2Vec(PV-DBOW)</h3><p>模型希望通过输入一个Paragraph id来预测该Paragraph中的单词的概率，和Skip-gram模型非常的类似。</p>\n<ul>\n<li>PV-DBOW模型的输入忽略了的上下文单词，但是关注模型从输出的段落中预测从段落中随机抽取的单词；</li>\n<li>PV-DBOW模型和训练词向量的<strong>Skip-gram模型</strong>非常相似。</li>\n</ul>\n<p><img src=\"/posts_res/2018-07-08-doc2vectutorial/2.png\" alt=\"pv-dbow\"></p>\n<hr>\n<h3 id=\"Doc2Vec的特点\"><a href=\"#Doc2Vec的特点\" class=\"headerlink\" title=\"Doc2Vec的特点\"></a>Doc2Vec的特点</h3><ul>\n<li>可以从未标记的数据中学习，在没有足够多带标记的数据上仍工作良好；</li>\n<li>继承了词向量的词的语义（semantics）的特点；</li>\n<li>会考虑词的顺序（至少在某个小上下文中会考虑）</li>\n</ul>\n<hr>\n<blockquote>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1405.4053.pdf\" target=\"_blank\" rel=\"noopener\">Distributed representations of sentences and documents</a></li>\n<li><a href=\"https://www.paperweekly.site/papers/notes/135\" target=\"_blank\" rel=\"noopener\">Distributed representations of sentences and documents总结 - paperweekly</a></li>\n<li><a href=\"https://blog.csdn.net/aliceyangxi1987/article/details/75097598\" target=\"_blank\" rel=\"noopener\">用 Doc2Vec 得到文档／段落／句子的向量表达</a></li>\n<li><a href=\"https://blog.csdn.net/qq_36472696/article/details/77871723\" target=\"_blank\" rel=\"noopener\">关于Gensim的初次见面 和 Doc2vec 的模型训练</a></li>\n<li><a href=\"https://github.com/llhthinker/NLP-Papers/blob/master/distributed%20representations/2017-11/Distributed%20Representations%20of%20Sentences%20and%20Documents/note.md\" target=\"_blank\" rel=\"noopener\">论文笔记：Distributed Representations of Sentences and Documents</a></li>\n<li><a href=\"http://d0evi1.com/paragraph2vec/\" target=\"_blank\" rel=\"noopener\">paragraph2vec介绍</a></li>\n</ol>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>gensim简单使用</li>\n<li>Doc2Vec(PV-DM)</li>\n<li>Doc2Vec(PV-DBOW)</li>\n</ul><hr><h3 id=\"gensim简单使用\"><a href=\"#gensim简单使用\" class=\"headerlink\" title=\"gensim简单使用\"></a>gensim简单使用</h3><p>模型参数说明：</p><ol>\n<li>dm=1 PV-DM  dm=0 PV-DBOW。</li>\n<li>size 所得向量的维度。</li>\n<li>window 上下文词语离当前词语的最大距离。</li>\n<li>alpha 初始学习率，在训练中会下降到min_alpha。</li>\n<li>min_count 词频小于min_count的词会被忽略。</li>\n<li>max_vocab_size 最大词汇表size，每一百万词会需要1GB的内存，默认没有限制。</li>\n<li>sample 下采样比例。</li>\n<li>iter 在整个语料上的迭代次数(epochs)，推荐10到20。</li>\n<li>hs=1 hierarchical softmax ，hs=0(default) negative sampling。</li>\n<li>dm_mean=0(default) 上下文向量取综合，dm_mean=1 上下文向量取均值。</li>\n<li>dbow_words:1训练词向量，0只训练doc向量。</li>\n</ol>","more":"\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># coding: utf-8</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> gensim.models.doc2vec <span class=\"keyword\">import</span> Doc2Vec, TaggedDocument</span><br><span class=\"line\"><span class=\"keyword\">from</span> nltk.tokenize <span class=\"keyword\">import</span> word_tokenize</span><br><span class=\"line\"></span><br><span class=\"line\">data = [<span class=\"string\">\"I love machine learning. Its awesome.\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"I love coding in python\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"I love building chatbots\"</span>,</span><br><span class=\"line\">        <span class=\"string\">\"they chat amagingly well\"</span>]</span><br><span class=\"line\">tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[i]) <span class=\"keyword\">for</span> i, _d <span class=\"keyword\">in</span> enumerate(data)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># for item in tagged_data:</span></span><br><span class=\"line\"><span class=\"comment\">#     print(item)</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], ['0'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'coding', 'in', 'python'], ['1'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['i', 'love', 'building', 'chatbots'], ['2'])</span></span><br><span class=\"line\"><span class=\"comment\"># TaggedDocument(['they', 'chat', 'amagingly', 'well'], ['3'])</span></span><br><span class=\"line\"></span><br><span class=\"line\">max_epochs = <span class=\"number\">100</span></span><br><span class=\"line\">vec_size = <span class=\"number\">10</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.025</span></span><br><span class=\"line\">model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=<span class=\"number\">0.00025</span>, min_count=<span class=\"number\">1</span>, dm=<span class=\"number\">1</span>)</span><br><span class=\"line\">model.build_vocab(tagged_data)</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(max_epochs):</span><br><span class=\"line\">    print(<span class=\"string\">\"iteration &#123;0&#125;\"</span>.format(epoch))</span><br><span class=\"line\">    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)</span><br><span class=\"line\">    model.alpha -= <span class=\"number\">0.0002</span></span><br><span class=\"line\">    model.min_alpha = model.alpha</span><br><span class=\"line\"></span><br><span class=\"line\">model.save(<span class=\"string\">\"d2v.model\"</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"Model Saved!\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">model = Doc2Vec.load(<span class=\"string\">\"d2v.model\"</span>)</span><br><span class=\"line\">test_data = word_tokenize(<span class=\"string\">\"I love chatbots\"</span>.lower())</span><br><span class=\"line\">v1 = model.infer_vector(test_data)</span><br><span class=\"line\">print(<span class=\"string\">\"V1_infer:\"</span>, v1, end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\">similar_doc = model.docvecs.most_similar(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(similar_doc, end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(data)):</span><br><span class=\"line\">    print(i, model.docvecs[i], end=<span class=\"string\">\"\\n\\n\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">V1_infer: [ 0.07642581  0.11099993 -0.02696064 -0.06895891  0.01907274 -0.08622721</span></span><br><span class=\"line\"><span class=\"string\"> -0.00581482 -0.08242869 -0.02741096 -0.05718143]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">[(0, 0.9964085817337036), (2, 0.9903678894042969), (3, 0.985236406326294)]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">0 [ 1.22332275  0.93302435  0.0843188  -0.40848678 -0.26203951 -0.54852372</span></span><br><span class=\"line\"><span class=\"string\"> -0.07185869 -0.72306669 -0.635378   -0.05744991]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">1 [ 0.83533287  0.64208162  0.04288336 -0.28720176 -0.17140444 -0.43564293</span></span><br><span class=\"line\"><span class=\"string\"> -0.0435797  -0.45327306 -0.46411869 -0.12211297]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">2 [ 0.71188128  0.49025729 -0.03114104 -0.2172543  -0.18351653 -0.35265383</span></span><br><span class=\"line\"><span class=\"string\"> -0.09494802 -0.47745392 -0.33393192 -0.1065111 ]</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">3 [ 0.86559838  0.77232999 -0.0108105  -0.179581   -0.10455605 -0.41468951</span></span><br><span class=\"line\"><span class=\"string\"> -0.11108498 -0.59402496 -0.59637135 -0.22117028]</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p><a href=\"https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\" target=\"_blank\" rel=\"noopener\">DOC2VEC gensim tutorial</a>\n<a href=\"https://blog.csdn.net/weixin_39837402/article/details/80254868\" target=\"_blank\" rel=\"noopener\">使用gensim的doc2vec生成文档向量</a></p>\n</blockquote>\n<hr>\n<p><strong>需要说明的一点是这里的Paragraph Vector不是真的段落向量的意思，它可以根据需要的不同进行变化，可以是短语、句子甚至是文档。</strong> </p>\n<h3 id=\"Doc2Vec-PV-DM\"><a href=\"#Doc2Vec-PV-DM\" class=\"headerlink\" title=\"Doc2Vec(PV-DM)\"></a>Doc2Vec(PV-DM)</h3><p>PV-DM在模型的输入层新增了一个Paragraph id，用于表征输入上下文所在的Paragraph。\n例如如果需要训练得到句子向量，那么Paragraph id即为语料库中的每个句子的表示。\nParagraph id其实也是一个向量，具有和词向量一样的维度，但是它们来自不同的向量空间，D和W，也就是来自于两个不同的矩阵。\n剩下的思路和CBOW模型基本一样。在模型中值得注意的一点是，在同一个Paragraph中，进行窗口滑动时，Paragraph id是不变的。</p>\n<p>Paragraph id本质上就是一个word，只是这个word唯一代表了这个paragraph，丰富了context vector。</p>\n<ul>\n<li>模型的具体步骤如下：<ul>\n<li>每个段落都映射到一个唯一的向量，由矩阵$D$中的一列表示，每个词也被映射到一个唯一的向量，表示为$W$ ;</li>\n<li>对<strong>当前段落向量</strong>和<strong>当前上下文</strong>所有词向量一起进行取平均值或连接操作，生成的向量用于输入到softmax层，以预测上下文中的下一个词: <script type=\"math/tex\">y=b+Uh(w_{t-k}, \\dots, w_{t+k}; W; D)</script> </li>\n</ul>\n</li>\n<li>这个段落向量可以被认为是另一个词。可以将它理解为一种记忆单元，记住<strong>当前上下文所缺失的内容</strong>或段落的<strong>主题</strong> ；</li>\n<li>矩阵$D$ 和$W$ 的区别:<ul>\n<li>通过当前段落的index，对$D$ 进行Lookup得到的段落向量，对于当前段落的所有上下文是共享的，但是其他段落的上下文并不会影响它的值，也就是说它<strong>不会跨段落(not across paragraphs)</strong> ；</li>\n<li>当时词向量矩阵$W$对于所有段落、所有上下文都是共享的。</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-07-08-doc2vectutorial/1.png\" alt=\"pv-dm\"></p>\n<hr>\n<h3 id=\"Doc2Vec-PV-DBOW\"><a href=\"#Doc2Vec-PV-DBOW\" class=\"headerlink\" title=\"Doc2Vec(PV-DBOW)\"></a>Doc2Vec(PV-DBOW)</h3><p>模型希望通过输入一个Paragraph id来预测该Paragraph中的单词的概率，和Skip-gram模型非常的类似。</p>\n<ul>\n<li>PV-DBOW模型的输入忽略了的上下文单词，但是关注模型从输出的段落中预测从段落中随机抽取的单词；</li>\n<li>PV-DBOW模型和训练词向量的<strong>Skip-gram模型</strong>非常相似。</li>\n</ul>\n<p><img src=\"/posts_res/2018-07-08-doc2vectutorial/2.png\" alt=\"pv-dbow\"></p>\n<hr>\n<h3 id=\"Doc2Vec的特点\"><a href=\"#Doc2Vec的特点\" class=\"headerlink\" title=\"Doc2Vec的特点\"></a>Doc2Vec的特点</h3><ul>\n<li>可以从未标记的数据中学习，在没有足够多带标记的数据上仍工作良好；</li>\n<li>继承了词向量的词的语义（semantics）的特点；</li>\n<li>会考虑词的顺序（至少在某个小上下文中会考虑）</li>\n</ul>\n<hr>\n<blockquote>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1405.4053.pdf\" target=\"_blank\" rel=\"noopener\">Distributed representations of sentences and documents</a></li>\n<li><a href=\"https://www.paperweekly.site/papers/notes/135\" target=\"_blank\" rel=\"noopener\">Distributed representations of sentences and documents总结 - paperweekly</a></li>\n<li><a href=\"https://blog.csdn.net/aliceyangxi1987/article/details/75097598\" target=\"_blank\" rel=\"noopener\">用 Doc2Vec 得到文档／段落／句子的向量表达</a></li>\n<li><a href=\"https://blog.csdn.net/qq_36472696/article/details/77871723\" target=\"_blank\" rel=\"noopener\">关于Gensim的初次见面 和 Doc2vec 的模型训练</a></li>\n<li><a href=\"https://github.com/llhthinker/NLP-Papers/blob/master/distributed%20representations/2017-11/Distributed%20Representations%20of%20Sentences%20and%20Documents/note.md\" target=\"_blank\" rel=\"noopener\">论文笔记：Distributed Representations of Sentences and Documents</a></li>\n<li><a href=\"http://d0evi1.com/paragraph2vec/\" target=\"_blank\" rel=\"noopener\">paragraph2vec介绍</a></li>\n</ol>\n</blockquote>\n"},{"layout":"post","title":"SVD&PCA&LDA降维","date":"2018-05-18T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n目录\n\n* SVD 奇异值分解 [无监督]\n* PCA 主成份分析 [无监督]\n* LDA 线性判别分析 [有监督]\n* PCA与LDA的异同\n\n-------\n\n### 1. SVD 奇异值分解(Singular Value Decomposition)\n\n不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。\n\n#### 1.1 特征值与特征向量\n\n特征值和特征向量的定义如下：\n\\\\[\nAx = \\lambda x\n\\\\]\n\n其中``A``是一个``n x n``的矩阵，\\\\(x\\\\)是一个``n维向量``，则我们\\\\(\\lambda\\\\)是矩阵``A``的一个特征值，而``x``是矩阵``A``的特征值\\\\( \\lambda \\\\)所对应的特征向量。\n\n根据特征值和特征向量，可以将矩阵``A``特征分解。如果我们求出了矩阵``A``的``n``个特征值\\\\( \\lambda\\_1 \\leq \\lambda\\_2 \\leq ... \\leq \\lambda\\_n \\\\)以及这``n``个特征值所对应的特征向量\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_n \\rbrace \\\\)，那么矩阵``A``就可以用下式的特征分解表示：\n\\\\[\nA = W \\Sigma W^{-1}\n\\\\]\n\n其中``W``是这``n``个特征向量所组成的``n x n``维矩阵，而\\\\( \\Sigma \\\\)为这``n``个特征值为主对角线的``n x n``维矩阵。\n\n一般我们会把``W``的这``n``个特征向量标准化，即满足 \\\\( \\|\\| w\\_i \\|\\|\\_2=1 \\\\), 或者说 \\\\( w^T\\_i w\\_i=1 \\\\)，此时``W``的``n``个特征向量为标准正交基，满足\\\\( W^T W=I \\\\)，即\\\\( W^T=W^{−1}\\\\), 也就是说``W``为酉矩阵。\n\n这样我们的特征分解表达式可以写成\n\\\\[\nA=WΣW^T\n\\\\]\n\n注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。\n\n\n#### 1.2 SVD的定义\n\nSVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\\\\(A\\\\)是一个``m × n``的矩阵，那么我们定义矩阵\\\\(A\\\\)的SVD为：\n\\\\[\nA=UΣV^T\n\\\\]\n\n其中\\\\(U\\\\)是一个``m × m``的矩阵，\\\\(\\Sigma\\\\)是一个``m × n``的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\\\\(V\\\\)是一个``n × n``的矩阵。\\\\(U\\\\)和\\\\(V\\\\)都是酉矩阵，即满足\\\\( U^TU=I,V^TV=I\\\\)。下图可以很形象的看出上面SVD的定义：\n\n![svd](/posts_res/2018-05-18-dimreduction/1-1.png)\n\n如何求出SVD分解后的\\\\( U, \\Sigma, V \\\\) 这三个矩阵呢？\n\n如果将\\\\(A^T\\\\)和\\\\(A\\\\)做矩阵乘法，那么会得到``n × n``的一个方阵\\\\(A^TA\\\\)。既然\\\\(A^TA\\\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\\\[\n(A^TA)v\\_i=\\lambda\\_i v\\_i\n\\\\]\n\n这样就可以得到矩阵\\\\(A^TA\\\\)的``n``个特征值和对应的``n``个特征向量\\\\(v\\\\)了。将\\\\(A^TA\\\\)的所有特征向量张成一个``n × n``的矩阵\\\\(V\\\\)，就是SVD公式里面的\\\\(V\\\\)矩阵了。一般将\\\\(V\\\\)中的每个特征向量叫做\\\\(A\\\\)的右奇异向量。\n\n如果将\\\\(A\\\\)和\\\\(A^T\\\\)做矩阵乘法，那么会得到``m × m``的一个方阵\\\\(AA^T\\\\)。既然\\\\(AA^T\\\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\\\[\n(AA^T)u\\_i=\\lambda\\_iu\\_i\n\\\\]\n\n这样就可以得到矩阵\\\\(AA^T\\\\)的``m``个特征值和对应的``m``个特征向量\\\\(u\\\\)了。将\\\\(AA^T\\\\)的所有特征向量张成一个``m × m``的矩阵\\\\(U\\\\)，就是SVD公式里面的\\\\(U\\\\)矩阵了。一般我们将\\\\(U\\\\)中的每个特征向量叫做\\\\(A\\\\)的左奇异向量。\n\n\\\\(U\\\\)和\\\\(V\\\\)都求出来了，现在就剩下奇异值矩阵\\\\( \\Sigma \\\\)没有求出了。由于\\\\( \\Sigma \\\\)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\\\\( \\sigma \\\\)就可以了。\n\n注意到:\n\\\\[\nA=UΣV^T \\Rightarrow AV=UΣV^TV \\Rightarrow AV=UΣ \\Rightarrow Av\\_i=\\sigma\\_iu\\_i \\Rightarrow \\sigma\\_i= \\frac{Av\\_i}{u\\_i}\n\\\\]\n\n这样可以求出每个奇异值，进而求出奇异值矩阵\\\\( \\Sigma \\\\)。\n\n上面还有一个问题没有讲，就是说\\\\(A^TA\\\\)的特征向量组成的就是我们SVD中的\\\\(V\\\\)矩阵，而\\\\(AA^T\\\\)的特征向量组成的就是我们SVD中的\\\\(U\\\\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\\\\(V\\\\)矩阵的证明为例。\n\\\\[\nA=UΣV^T \\Rightarrow A^T=VΣ^TU^T \\Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T\n\\\\]\n\n上式证明使用了:\\\\( U^TU=I,Σ^TΣ=Σ^2\\\\)，可以看出\\\\( A^TA \\\\)的特征向量组成的的确就是我们SVD中的\\\\(V\\\\)矩阵。类似的方法可以得到\\\\(AA^T\\\\)的特征向量组成的就是我们SVD中的\\\\(U\\\\)矩阵。\n\n进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：\n\\\\[\n\\sigma\\_i=\\sqrt{\\lambda\\_i}\n\\\\]\n\n这样也就是说，可以不用\\\\( \\sigma\\_i= Av\\_i / u\\_i \\\\)来计算奇异值，也可以通过求出\\\\( A^TA \\\\)的特征值取平方根来求奇异值。\n\n\n#### 1.3 SVD的性质\n\n对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：\n\\\\[\nA\\_{m×n}=U\\_{m×m} \\Sigma\\_{m×n} V^T\\_{n×n} \\sim U\\_{m×k} \\Sigma\\_{k×k} V^T\\_{k×n}\n\\\\]\n\n其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\\\\( U\\_{m×k}, \\Sigma\\_{k×k}, V^T\\_{k×n} \\\\)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。\n\n![property](/posts_res/2018-05-18-dimreduction/1-2.png)\n\n由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。\n\n\n>\n[奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)\n\n------\n\n### 2. PCA 主成份分析\n\n**事先声明**\n\n当 ``X = 样本数量 * 每个样本的维度`` 时， 协方差矩阵为$ X^T X $；  \n当 ``X = 每个样本的维度 * 样本数量`` 时， 协方差矩阵为$ X X^T $；\n<br>\n\n#### 2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)\n\n假设 m 个 n 维数据\\\\( (x^{(1)}, x^{(2)},...,x^{(m)}) \\\\)都已经进行了中心化，即 \\\\( \\sum\\_{i=1}^m x^{(i)}=0 \\\\)。经过投影变换后得到的新坐标系为\\\\( \\lbrace w\\_1,w\\_2,...,w\\_n \\rbrace \\\\)，其中\\\\(w\\\\)是标准正交基，即\\\\( \\|\\| w \\|\\|^2 = 1, w^T\\_iw\\_j=0 \\\\)。\n\n如果将数据从 n 维降到 n' 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_{n′} \\rbrace \\\\)，样本点\\\\( x^{(i)} \\\\)在 n' 维坐标系中的投影为：\\\\( z^{(i)} = ( z^{(i)}\\_1,z^{(i)}\\_2, ..., z^{(i)}\\_{n′}) \\\\)。其中，\\\\( z^{(i)}\\_j = w^T\\_j x^{(i)} \\\\)是\\\\( x^{(i)} \\\\)在低维坐标系里第 j 维的坐标。\n\n如果用\\\\( z^{(i)} \\\\)来恢复原始数据\\\\( x^{(i)} \\\\)，则得到的恢复数据 \\\\( \\bar{x}^{(i)} = \\sum\\_{j=1}^{n′} z^{(i)}\\_j w\\_j = Wz^{(i)} \\\\)，其中，\\\\(W\\\\)为标准正交基组成的矩阵。\n\n现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式：\n\\\\[\n\\sum\\_{i=1}^m \\|\\| \\bar{x}^{(i)} − x^{(i)} \\|\\|^2\\_2\n\\\\]\n\n将这个式子进行整理，可以得到:\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum\\_{i=1}^m \\|\\| \\bar{x}^{(i)} − x^{(i)} \\|\\|^2\\_2 \n& = \\sum\\_{i=1}^m \\|\\| Wz^{(i)} − x^{(i)} \\|\\|^2\\_2 \\\\\\\n& = \\sum\\_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \\sum\\_{i=1}^m (Wz^{(i)})^T x^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = \\sum\\_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum\\_{i=1}^m z^{(i)T}W^T x^{(i)} + \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\\\\n& = \\sum\\_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum\\_{i=1}^m z^{(i)T}z^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = − \\sum\\_{i=1}^m z^{(i)T}z^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = − tr( W^T ( \\sum\\_{i=1}^m x^{(i)} x^{(i)T} ) W) + \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\\\\n& = − tr( W^T X X^T W) + \\sum\\_{i=1}^m x^{(i)T} x^{(i)}\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n* 第1个等式用到了\\\\( \\bar{x}^{(i)} = W z^{(i)} \\\\)\n* 第2个等式用到了平方和展开\n* 第3个等式用到了矩阵转置公式\\\\( (AB)^T= B^T A^T \\\\) 和 \\\\( W^T W =I \\\\)\n* 第4个等式用到了\\\\( z^{(i)} = W^T x^{(i)} \\\\)\n* 第5个等式合并同类项\n* 第6个等式用到了\\\\( z^{(i)} = W^T x^{(i)} \\\\)和矩阵的迹\n* 第7个等式将代数和表达为矩阵形式\n\n注意到\\\\( \\sum\\_{i=1}^m x^{(i)} x^{(i)T} \\\\)是数据集的协方差矩阵，\\\\(W\\\\)的每一个向量\\\\(w\\_j\\\\)是标准正交基。而\\\\( \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\) 是一个常量。最小化上式等价于：\n\\\\[\n\\arg\\min\\_W - tr( W^T X X^T W ) \\quad \\quad s.t. W^TW = I\n\\\\]\n\n这个最小化不难，直接观察也可以发现最小值对应的\\\\(W\\\\)由协方差矩阵\\\\(XX^T\\\\)最大的 n' 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到\n\\\\[\nJ(W) = −tr( W^T X X^T W) + \\lambda ( W^T W − I )\n\\\\]\n\n对\\\\( W \\\\)求导有\\\\( − X X^T W + \\lambda W = 0 \\\\)，整理下即为：\n\\\\[\nXX^T W = \\lambda W\n\\\\]\n\n这样可以更清楚的看出，\\\\(W\\\\)为\\\\(X X^T \\\\)的 n' 个特征向量组成的矩阵，而\\\\( \\lambda \\\\) 为 \\\\( XX^T \\\\) 的特征值。当我们将数据集从 n 维降到 n' 维时，需要找到最大的 n' 个特征值对应的特征向量。这 n' 个特征向量组成的矩阵\\\\( W\\\\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\\\\( z^{(i)} = W^T x^{(i)} \\\\)，就可以把原始数据集降维到最小投影距离的 n' 维数据集。\n　\n\n#### 2.2 PCA的推导:基于最大投影方差\n\n假设 m 个 n 维数据\\\\( (x^{(1)}, x^{(2)}, ..., x^{(m)} ) \\\\)都已经进行了中心化，即\\\\( \\sum\\_{i=1}^m x^{(i)} = 0 \\\\)。经过投影变换后得到的新坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_n \\rbrace \\\\)，其中\\\\( w\\\\) 是标准正交基，即\\\\( \\|\\| w \\|\\|^2 = 1, w^T\\_i w\\_j = 0\\\\)。\n\n如果将数据从 n 维降到 n' 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_{n′} \\rbrace \\\\)，样本点\\\\( x^{(i)} \\\\) 在 n' 维坐标系中的投影为：\\\\( z^{(i)} = ( z^{(i)}\\_1, z^{(i)}\\_2, ..., z^{(i)}\\_{n′}) \\\\) 。其中，\\\\( z^{(i)}\\_j = w^T\\_j x^{(i)} \\\\)是 \\\\( x^{(i)} \\\\) 在低维坐标系里第 j 维的坐标。\n\n对于任意一个样本\\\\( x^{(i)} \\\\)，在新的坐标系中的投影为\\\\( W^T x^{(i)} \\\\)，在新坐标系中的投影方差为\\\\( W^T x^{(i)} x^{(i)T}W \\\\)，要使所有的样本的投影方差和最大，也就是最大化\\\\( \\sum\\_{i=1}^m W^T x^{(i)} x^{(i)T} W \\\\)，即：\n\\\\[\n\\arg\\max\\_W tr(W^T X X^T W) \\quad \\quad s.t. W^TW=I\n\\\\]\n\n观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。\n\n利用拉格朗日函数可以得到\n\\\\[\nJ(W) = tr( W^T X X^T W) + \\lambda (W^T W − I)\n\\\\]\n\n对\\\\( W \\\\) 求导有 \\\\( XX^T W + \\lambda W = 0 \\\\)，整理下即为：\n\\\\[\nXX^TW = (−\\lambda)W\n\\\\]\n\n和上面一样可以看出，\\\\(W\\\\)为\\\\(XX^T\\\\)的 n' 个特征向量组成的矩阵，而 \\\\( −\\lambda \\\\) 为\\\\( XX^T\\\\) 的特征值。将数据集从 n 维降到 n' 维时，需要找到最大的 n' 个特征值对应的特征向量。这 n' 个特征向量组成的矩阵\\\\( W \\\\)即为需要的矩阵。对于原始数据集，只需要用\\\\( z^{(i)} = W^T x^{(i)} \\\\)，就可以把原始数据集降维到最小投影距离的 n' 维数据集。\n\n\n#### 2.3 PCA算法流程\n\n输入: n 维样本集 \\\\( D = ( x^{(1)}, x^{(2)}, ..., x^{(m)} ) \\\\)，要降维到的维数 n'。\n\n输出: 降维后的样本集 \\\\( D′ \\\\)\n\n1. 对所有的样本进行中心化： \\\\( x^{(i)} = x^{(i)} − \\frac{1}{m} \\sum\\_{j=1}^m x^{(j)}  \\\\)\n2. 计算样本的协方差矩阵\\\\( XX^T \\\\)\n3. 对矩阵\\\\( XX^T \\\\)进行特征值分解\n4. 取出最大的 n' 个特征值对应的特征向量\\\\( (w\\_1, w\\_2, ..., w\\_{n′}) \\\\)，将所有的特征向量标准化后，组成特征向量矩阵\\\\(W\\\\)\n5. 对样本集中的每一个样本\\\\( x^{(i)}\\\\)，转化为新的样本\\\\( z^{(i)} = W^T x^{(i)} \\\\)\n6. 得到输出样本集 \\\\( D′=(z^{(1)}, z^{(2)}, ..., z^{(m)} ) \\\\)\n\n有时候，我们不指定降维后的 n' 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \\\\( \\lambda\\_1 \\geq \\lambda\\_2 \\geq ... \\geq \\lambda\\_n \\\\)，则 n' 可以通过下式得到:\n\\\\[\n\\sum\\_{i=1}^{n′} \\lambda\\_i / \\sum\\_{i=1}^n \\lambda\\_i \\geq t\n\\\\]\n\n\n#### 2.4 PCA实例\n\n假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下：\n\n```\n(2.5, 2.4), (0.5, 0.7), (2.2, 2.9), (1.9, 2.2), (3.1, 3.0), \n(2.3, 2.7), (2  , 1.6), (1  , 1.1), (1.5, 1.6), (1.1, 0.9)\n```\n　\n首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为\n\n```\n(0.69, 0.49), (-1.31, -1.21), ( 0.39,  0.99), ( 0.09,  0.29), ( 1.29,  1.09), \n(0.49, 0.79), ( 0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)\n```\n\n现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为：\n\\\\[\nXX^T = \n\\left( \n\\begin{matrix}\ncov(x1,x1) & cov(x2,x1) \\\\\\ cov(x1,x2) & cov(x2,x2) \n\\end{matrix}\n\\right)\n\\\\]\n\n对于上述数据，求出协方差矩阵为：\n\\\\[\nXX^T = \n\\left(\n\\begin{matrix}\n0.616555556 & 0.615444444 \\\\\\ 0.615444444 & 0.716555556\n\\end{matrix}\n\\right)\n\\\\]\n\n求出特征值为 \\\\(（0.0490834, 1.28402771）\\\\)，对应的特征向量分别为：\\\\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \\\\)，\n由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \\\\( (0.6778734, -0.73517866)^T \\\\)。\n则 \\\\( W=(0.6778734, -0.73517866)^T \\\\)。\n\n对所有的数据集进行投影\\\\( z^{(i)} = W^T x^{(i)} \\\\)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为：\n```\n   0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 \n  -0.24863317  0.35670133  0.04641726  0.01776463  0.26124033\n```\n\n>下面是例子的Python代码\n\n```python\nimport numpy as np\n\noriginalData = np.array([\n\t[ 2.5,  0.5,  2.2,  1.9,  3.1,  2.3,  2. ,  1. ,  1.5,  1.1,],\n\t[ 2.4,  0.7,  2.9,  2.2,  3. ,  2.7,  1.6,  1.1,  1.6,  0.9]])\n\nmean = np.mean(originalData, axis=1)\nprint(\"均值：\")\nprint(mean) # [ 1.81  1.91]\n\npreData = (originalData.T - mean).T\nprint(\"中心化后数据：\")\nprint(preData)\n# [[ 0.69 -1.31  0.39  0.09  1.29  0.49  0.19 -0.81 -0.31 -0.71]\n#  [ 0.49 -1.21  0.99  0.29  1.09  0.79 -0.31 -0.81 -0.31 -1.01]]\n\ncov = np.cov(preData)\nprint(\"协方差矩阵：\")\nprint(cov)\n# [[ 0.61655556  0.61544444]\n#  [ 0.61544444  0.71655556]]\n\nfeature_value, feature_vector = np.linalg.eig(cov)\nprint(\"特征值：\")\nprint(feature_value) # [ 0.0490834   1.28402771]\nprint(\"特征向量:\")\nprint(feature_vector)\n# [[-0.73517866 -0.6778734 ]\n#  [ 0.6778734  -0.73517866]]\n\nmaxIndex = 1\nW = feature_vector[maxIndex, :]\nredData = W.T.dot(preData)\nprint(\"降维后数据：\")\nprint(redData)\n# [ 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 -0.24863317\n#   0.35670133  0.04641726  0.01776463  0.26124033]\n```\n\n\n\n\n#### 2.5 PCA总结\n\n作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。\n\nPCA算法的主要优点有：\n\n* 仅仅需要以方差衡量信息量，不受数据集以外的因素影响\n* 各主成分之间正交，可消除原始数据成分间的相互影响的因素\n* 计算方法简单，主要运算是特征值分解，易于实现\n\nPCA算法的主要缺点有：\n\n* 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强\n* 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响\n\n\n#### 2.6 PCA实现方面\n\nPCA降维需要找到样本协方差矩阵\\\\( X^TX \\\\)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。\n可以看出，在这个过程中需要先求出协方差矩阵\\\\( X^TX\\\\)，当样本数多样本特征数也多的时候，这个计算量是很大的。\n\n注意到SVD也可以得到协方差矩阵\\\\( X^TX \\\\)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\\\\(X^TX\\\\)，也能求出右奇异矩阵\\\\( V \\\\)。\n也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。\n实际上，[scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。\n\n*另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？*\n\n假设样本是 m×n 的矩阵 \\\\(X\\\\)，如果通过SVD找到了矩阵\\\\( XX^T \\\\)最大的 d 个特征向量张成的 m×d 维矩阵\\\\(U\\\\)，则如果进行如下处理：\n\\\\[\nX′\\_{d×n} = U^T\\_{d×m} X\\_{m×n}\n\\\\]\n\n可以得到一个 d×n 的矩阵\\\\( X' \\\\)，这个矩阵和原来的 m×n 维样本矩阵\\\\( X \\\\)相比，行数从 m 减到了 k ，可见对行数进行了压缩。\n也就是说，**左奇异矩阵可以用于行数的压缩**；相对的，**右奇异矩阵可以用于列数即特征维度的压缩**，也就是PCA降维。\n\n\n>\n[主成分分析（PCA）原理总结](http://www.cnblogs.com/pinard/p/6239403.html)\n\n-----\n\n### 3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)\n\nLDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。\n\nLDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。\n我们的目标就是找到一条直线，尽可能满足上面的要求。\n\n来看一个例子：两类会这样被降维\n\n![lda](/posts_res/2018-05-18-dimreduction/3-1.jpg)\n\n设数据集\\\\( D=\\lbrace (x\\_i, y\\_i) \\rbrace^m\\_{i=1} \\\\)，投影向量为\\\\(w\\\\)，则点\\\\(x\\_i\\\\)经过投影后为\\\\( y=w^Tx\\_i\\\\)，投影前的样本中心点为\\\\( u \\\\)，投影后的中心点为\\\\( \\bar{u} = w^T u\\\\)。\n\n希望投影后不同类别的样本尽量离得较远：使用度量值\n\\\\[\n\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2\n\\\\]\n\n同时希望投影后相同类别的样本之间尽量离得较近：使用度量值\n\\\\[\n\\sum\\_{y \\in Y\\_i} (y - \\hat{u}\\_i)^2\n\\\\]\n\n这个值其实就是投影后样本的**方差**乘以此类样本集合中样本的数量。\n\n所以总的优化目标函数为：\n\\\\[\nJ(W) = \\frac{\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2}{\\sum\\_{y\\in Y\\_i} ( y - \\hat{u}\\_i )^2} = \\frac{\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2}{\\sum\\_0 ( y - \\hat{u}\\_0 )^2 + \\sum\\_1 (y - \\hat{u}\\_1)^2}\n\\\\]\n\n目标\\\\(J(W)\\\\)当然是越大越好；\n\n定义类内散度矩阵为: \\\\( S\\_w = \\sum\\_0 + \\sum\\_1 = \\sum\\_{X\\_0} (x-u\\_0)(x-u\\_0)^T + \\sum\\_{X\\_1} (x-u\\_1)(x-u\\_1)^T \\\\)\n\n定义类间散度矩阵：\\\\( S\\_b = (u\\_0 - u\\_1)(u\\_0 - u\\_1)^T \\\\)\n\n分子：\\\\( \\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2 = w^T (u\\_0 - u\\_1)(u\\_0 - u\\_1)^T w = w^TS\\_bw \\\\)\n\n分母：\\\\( \\sum\\_0 (y-\\hat{u}\\_0)^2 + \\sum\\_1 (y-\\hat{u}\\_1)^2 = w^T S\\_w w \\\\)\n\n所以\\\\( J(w)= w^TS\\_bw / w^T S\\_w w \\\\)，因为向量\\\\( w \\\\)的长度成比例改变不影响\\\\( J(W) \\\\)的取值，所以我们令\\\\( w^TS\\_ww=1 \\\\)，那么原优化目标就变为\n\\\\[\n\\min\\_{w} J(W) = - w^T S\\_b w, \\quad \\quad s.t. \\quad w^TS\\_ww = 1\n\\\\]\n\n这里直接使用拉格朗日乘子法就可以了，解得\n\\\\[\nS\\_b w = \\lambda w S\\_w\n\\\\]\n\n因为\\\\( S\\_bw = (u\\_0-u\\_1)(u\\_0-u\\_1)^Tw = (u\\_0 - u\\_1) \\lambda\\_t \\\\)，\n所以\\\\( (u\\_0 - u\\_1)\\lambda\\_t = \\lambda w S\\_w \\\\)，可以得到\n\\\\[\nw = S\\_w^{-1} (u\\_0 - u\\_1)\n\\\\]\n\n\n**LDA局限性：**\n\n1. 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出\n2. LDA不适合对非高斯分布的样本进行降维\n3. LDA在样本分类信息依赖方差而不是均值时，效果不好\n4. LDA可能过度拟合数据\n\n\n### 4. PCA与LDA的异同\n\n* **出发思想不同。**PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。\n\n* **学习模式不同。**PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。\n\n* **降维后可用维度数量不同。**LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。\n\n* **投影的坐标系不一定相同。**PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。\n\n\n![pca&lda](/posts_res/2018-05-18-dimreduction/4-1.jpg)\n\n上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。\n\n\n>\n[LDA与PCA都是常用的降维方法，二者的区别](https://blog.csdn.net/dongyanwen6036/article/details/78311071)\n","source":"_posts/2018-05-18-dimreduction.md","raw":"---\nlayout: post\ntitle: SVD&PCA&LDA降维\ndate: 2018-05-18 12:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\n---\n\n\n目录\n\n* SVD 奇异值分解 [无监督]\n* PCA 主成份分析 [无监督]\n* LDA 线性判别分析 [有监督]\n* PCA与LDA的异同\n\n-------\n\n### 1. SVD 奇异值分解(Singular Value Decomposition)\n\n不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。\n\n#### 1.1 特征值与特征向量\n\n特征值和特征向量的定义如下：\n\\\\[\nAx = \\lambda x\n\\\\]\n\n其中``A``是一个``n x n``的矩阵，\\\\(x\\\\)是一个``n维向量``，则我们\\\\(\\lambda\\\\)是矩阵``A``的一个特征值，而``x``是矩阵``A``的特征值\\\\( \\lambda \\\\)所对应的特征向量。\n\n根据特征值和特征向量，可以将矩阵``A``特征分解。如果我们求出了矩阵``A``的``n``个特征值\\\\( \\lambda\\_1 \\leq \\lambda\\_2 \\leq ... \\leq \\lambda\\_n \\\\)以及这``n``个特征值所对应的特征向量\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_n \\rbrace \\\\)，那么矩阵``A``就可以用下式的特征分解表示：\n\\\\[\nA = W \\Sigma W^{-1}\n\\\\]\n\n其中``W``是这``n``个特征向量所组成的``n x n``维矩阵，而\\\\( \\Sigma \\\\)为这``n``个特征值为主对角线的``n x n``维矩阵。\n\n一般我们会把``W``的这``n``个特征向量标准化，即满足 \\\\( \\|\\| w\\_i \\|\\|\\_2=1 \\\\), 或者说 \\\\( w^T\\_i w\\_i=1 \\\\)，此时``W``的``n``个特征向量为标准正交基，满足\\\\( W^T W=I \\\\)，即\\\\( W^T=W^{−1}\\\\), 也就是说``W``为酉矩阵。\n\n这样我们的特征分解表达式可以写成\n\\\\[\nA=WΣW^T\n\\\\]\n\n注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。\n\n\n#### 1.2 SVD的定义\n\nSVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\\\\(A\\\\)是一个``m × n``的矩阵，那么我们定义矩阵\\\\(A\\\\)的SVD为：\n\\\\[\nA=UΣV^T\n\\\\]\n\n其中\\\\(U\\\\)是一个``m × m``的矩阵，\\\\(\\Sigma\\\\)是一个``m × n``的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\\\\(V\\\\)是一个``n × n``的矩阵。\\\\(U\\\\)和\\\\(V\\\\)都是酉矩阵，即满足\\\\( U^TU=I,V^TV=I\\\\)。下图可以很形象的看出上面SVD的定义：\n\n![svd](/posts_res/2018-05-18-dimreduction/1-1.png)\n\n如何求出SVD分解后的\\\\( U, \\Sigma, V \\\\) 这三个矩阵呢？\n\n如果将\\\\(A^T\\\\)和\\\\(A\\\\)做矩阵乘法，那么会得到``n × n``的一个方阵\\\\(A^TA\\\\)。既然\\\\(A^TA\\\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\\\[\n(A^TA)v\\_i=\\lambda\\_i v\\_i\n\\\\]\n\n这样就可以得到矩阵\\\\(A^TA\\\\)的``n``个特征值和对应的``n``个特征向量\\\\(v\\\\)了。将\\\\(A^TA\\\\)的所有特征向量张成一个``n × n``的矩阵\\\\(V\\\\)，就是SVD公式里面的\\\\(V\\\\)矩阵了。一般将\\\\(V\\\\)中的每个特征向量叫做\\\\(A\\\\)的右奇异向量。\n\n如果将\\\\(A\\\\)和\\\\(A^T\\\\)做矩阵乘法，那么会得到``m × m``的一个方阵\\\\(AA^T\\\\)。既然\\\\(AA^T\\\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\\\[\n(AA^T)u\\_i=\\lambda\\_iu\\_i\n\\\\]\n\n这样就可以得到矩阵\\\\(AA^T\\\\)的``m``个特征值和对应的``m``个特征向量\\\\(u\\\\)了。将\\\\(AA^T\\\\)的所有特征向量张成一个``m × m``的矩阵\\\\(U\\\\)，就是SVD公式里面的\\\\(U\\\\)矩阵了。一般我们将\\\\(U\\\\)中的每个特征向量叫做\\\\(A\\\\)的左奇异向量。\n\n\\\\(U\\\\)和\\\\(V\\\\)都求出来了，现在就剩下奇异值矩阵\\\\( \\Sigma \\\\)没有求出了。由于\\\\( \\Sigma \\\\)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\\\\( \\sigma \\\\)就可以了。\n\n注意到:\n\\\\[\nA=UΣV^T \\Rightarrow AV=UΣV^TV \\Rightarrow AV=UΣ \\Rightarrow Av\\_i=\\sigma\\_iu\\_i \\Rightarrow \\sigma\\_i= \\frac{Av\\_i}{u\\_i}\n\\\\]\n\n这样可以求出每个奇异值，进而求出奇异值矩阵\\\\( \\Sigma \\\\)。\n\n上面还有一个问题没有讲，就是说\\\\(A^TA\\\\)的特征向量组成的就是我们SVD中的\\\\(V\\\\)矩阵，而\\\\(AA^T\\\\)的特征向量组成的就是我们SVD中的\\\\(U\\\\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\\\\(V\\\\)矩阵的证明为例。\n\\\\[\nA=UΣV^T \\Rightarrow A^T=VΣ^TU^T \\Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T\n\\\\]\n\n上式证明使用了:\\\\( U^TU=I,Σ^TΣ=Σ^2\\\\)，可以看出\\\\( A^TA \\\\)的特征向量组成的的确就是我们SVD中的\\\\(V\\\\)矩阵。类似的方法可以得到\\\\(AA^T\\\\)的特征向量组成的就是我们SVD中的\\\\(U\\\\)矩阵。\n\n进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：\n\\\\[\n\\sigma\\_i=\\sqrt{\\lambda\\_i}\n\\\\]\n\n这样也就是说，可以不用\\\\( \\sigma\\_i= Av\\_i / u\\_i \\\\)来计算奇异值，也可以通过求出\\\\( A^TA \\\\)的特征值取平方根来求奇异值。\n\n\n#### 1.3 SVD的性质\n\n对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：\n\\\\[\nA\\_{m×n}=U\\_{m×m} \\Sigma\\_{m×n} V^T\\_{n×n} \\sim U\\_{m×k} \\Sigma\\_{k×k} V^T\\_{k×n}\n\\\\]\n\n其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\\\\( U\\_{m×k}, \\Sigma\\_{k×k}, V^T\\_{k×n} \\\\)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。\n\n![property](/posts_res/2018-05-18-dimreduction/1-2.png)\n\n由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。\n\n\n>\n[奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)\n\n------\n\n### 2. PCA 主成份分析\n\n**事先声明**\n\n当 ``X = 样本数量 * 每个样本的维度`` 时， 协方差矩阵为$ X^T X $；  \n当 ``X = 每个样本的维度 * 样本数量`` 时， 协方差矩阵为$ X X^T $；\n<br>\n\n#### 2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)\n\n假设 m 个 n 维数据\\\\( (x^{(1)}, x^{(2)},...,x^{(m)}) \\\\)都已经进行了中心化，即 \\\\( \\sum\\_{i=1}^m x^{(i)}=0 \\\\)。经过投影变换后得到的新坐标系为\\\\( \\lbrace w\\_1,w\\_2,...,w\\_n \\rbrace \\\\)，其中\\\\(w\\\\)是标准正交基，即\\\\( \\|\\| w \\|\\|^2 = 1, w^T\\_iw\\_j=0 \\\\)。\n\n如果将数据从 n 维降到 n' 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_{n′} \\rbrace \\\\)，样本点\\\\( x^{(i)} \\\\)在 n' 维坐标系中的投影为：\\\\( z^{(i)} = ( z^{(i)}\\_1,z^{(i)}\\_2, ..., z^{(i)}\\_{n′}) \\\\)。其中，\\\\( z^{(i)}\\_j = w^T\\_j x^{(i)} \\\\)是\\\\( x^{(i)} \\\\)在低维坐标系里第 j 维的坐标。\n\n如果用\\\\( z^{(i)} \\\\)来恢复原始数据\\\\( x^{(i)} \\\\)，则得到的恢复数据 \\\\( \\bar{x}^{(i)} = \\sum\\_{j=1}^{n′} z^{(i)}\\_j w\\_j = Wz^{(i)} \\\\)，其中，\\\\(W\\\\)为标准正交基组成的矩阵。\n\n现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式：\n\\\\[\n\\sum\\_{i=1}^m \\|\\| \\bar{x}^{(i)} − x^{(i)} \\|\\|^2\\_2\n\\\\]\n\n将这个式子进行整理，可以得到:\n\\\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum\\_{i=1}^m \\|\\| \\bar{x}^{(i)} − x^{(i)} \\|\\|^2\\_2 \n& = \\sum\\_{i=1}^m \\|\\| Wz^{(i)} − x^{(i)} \\|\\|^2\\_2 \\\\\\\n& = \\sum\\_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \\sum\\_{i=1}^m (Wz^{(i)})^T x^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = \\sum\\_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum\\_{i=1}^m z^{(i)T}W^T x^{(i)} + \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\\\\n& = \\sum\\_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum\\_{i=1}^m z^{(i)T}z^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = − \\sum\\_{i=1}^m z^{(i)T}z^{(i)} + \\sum\\_{i=1}^m x^{(i)T}x^{(i)} \\\\\\\n& = − tr( W^T ( \\sum\\_{i=1}^m x^{(i)} x^{(i)T} ) W) + \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\\\\n& = − tr( W^T X X^T W) + \\sum\\_{i=1}^m x^{(i)T} x^{(i)}\n\\end{aligned}\n\\end{equation}\n\\\\]\n\n* 第1个等式用到了\\\\( \\bar{x}^{(i)} = W z^{(i)} \\\\)\n* 第2个等式用到了平方和展开\n* 第3个等式用到了矩阵转置公式\\\\( (AB)^T= B^T A^T \\\\) 和 \\\\( W^T W =I \\\\)\n* 第4个等式用到了\\\\( z^{(i)} = W^T x^{(i)} \\\\)\n* 第5个等式合并同类项\n* 第6个等式用到了\\\\( z^{(i)} = W^T x^{(i)} \\\\)和矩阵的迹\n* 第7个等式将代数和表达为矩阵形式\n\n注意到\\\\( \\sum\\_{i=1}^m x^{(i)} x^{(i)T} \\\\)是数据集的协方差矩阵，\\\\(W\\\\)的每一个向量\\\\(w\\_j\\\\)是标准正交基。而\\\\( \\sum\\_{i=1}^m x^{(i)T} x^{(i)} \\\\) 是一个常量。最小化上式等价于：\n\\\\[\n\\arg\\min\\_W - tr( W^T X X^T W ) \\quad \\quad s.t. W^TW = I\n\\\\]\n\n这个最小化不难，直接观察也可以发现最小值对应的\\\\(W\\\\)由协方差矩阵\\\\(XX^T\\\\)最大的 n' 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到\n\\\\[\nJ(W) = −tr( W^T X X^T W) + \\lambda ( W^T W − I )\n\\\\]\n\n对\\\\( W \\\\)求导有\\\\( − X X^T W + \\lambda W = 0 \\\\)，整理下即为：\n\\\\[\nXX^T W = \\lambda W\n\\\\]\n\n这样可以更清楚的看出，\\\\(W\\\\)为\\\\(X X^T \\\\)的 n' 个特征向量组成的矩阵，而\\\\( \\lambda \\\\) 为 \\\\( XX^T \\\\) 的特征值。当我们将数据集从 n 维降到 n' 维时，需要找到最大的 n' 个特征值对应的特征向量。这 n' 个特征向量组成的矩阵\\\\( W\\\\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\\\\( z^{(i)} = W^T x^{(i)} \\\\)，就可以把原始数据集降维到最小投影距离的 n' 维数据集。\n　\n\n#### 2.2 PCA的推导:基于最大投影方差\n\n假设 m 个 n 维数据\\\\( (x^{(1)}, x^{(2)}, ..., x^{(m)} ) \\\\)都已经进行了中心化，即\\\\( \\sum\\_{i=1}^m x^{(i)} = 0 \\\\)。经过投影变换后得到的新坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_n \\rbrace \\\\)，其中\\\\( w\\\\) 是标准正交基，即\\\\( \\|\\| w \\|\\|^2 = 1, w^T\\_i w\\_j = 0\\\\)。\n\n如果将数据从 n 维降到 n' 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\\\( \\lbrace w\\_1, w\\_2, ..., w\\_{n′} \\rbrace \\\\)，样本点\\\\( x^{(i)} \\\\) 在 n' 维坐标系中的投影为：\\\\( z^{(i)} = ( z^{(i)}\\_1, z^{(i)}\\_2, ..., z^{(i)}\\_{n′}) \\\\) 。其中，\\\\( z^{(i)}\\_j = w^T\\_j x^{(i)} \\\\)是 \\\\( x^{(i)} \\\\) 在低维坐标系里第 j 维的坐标。\n\n对于任意一个样本\\\\( x^{(i)} \\\\)，在新的坐标系中的投影为\\\\( W^T x^{(i)} \\\\)，在新坐标系中的投影方差为\\\\( W^T x^{(i)} x^{(i)T}W \\\\)，要使所有的样本的投影方差和最大，也就是最大化\\\\( \\sum\\_{i=1}^m W^T x^{(i)} x^{(i)T} W \\\\)，即：\n\\\\[\n\\arg\\max\\_W tr(W^T X X^T W) \\quad \\quad s.t. W^TW=I\n\\\\]\n\n观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。\n\n利用拉格朗日函数可以得到\n\\\\[\nJ(W) = tr( W^T X X^T W) + \\lambda (W^T W − I)\n\\\\]\n\n对\\\\( W \\\\) 求导有 \\\\( XX^T W + \\lambda W = 0 \\\\)，整理下即为：\n\\\\[\nXX^TW = (−\\lambda)W\n\\\\]\n\n和上面一样可以看出，\\\\(W\\\\)为\\\\(XX^T\\\\)的 n' 个特征向量组成的矩阵，而 \\\\( −\\lambda \\\\) 为\\\\( XX^T\\\\) 的特征值。将数据集从 n 维降到 n' 维时，需要找到最大的 n' 个特征值对应的特征向量。这 n' 个特征向量组成的矩阵\\\\( W \\\\)即为需要的矩阵。对于原始数据集，只需要用\\\\( z^{(i)} = W^T x^{(i)} \\\\)，就可以把原始数据集降维到最小投影距离的 n' 维数据集。\n\n\n#### 2.3 PCA算法流程\n\n输入: n 维样本集 \\\\( D = ( x^{(1)}, x^{(2)}, ..., x^{(m)} ) \\\\)，要降维到的维数 n'。\n\n输出: 降维后的样本集 \\\\( D′ \\\\)\n\n1. 对所有的样本进行中心化： \\\\( x^{(i)} = x^{(i)} − \\frac{1}{m} \\sum\\_{j=1}^m x^{(j)}  \\\\)\n2. 计算样本的协方差矩阵\\\\( XX^T \\\\)\n3. 对矩阵\\\\( XX^T \\\\)进行特征值分解\n4. 取出最大的 n' 个特征值对应的特征向量\\\\( (w\\_1, w\\_2, ..., w\\_{n′}) \\\\)，将所有的特征向量标准化后，组成特征向量矩阵\\\\(W\\\\)\n5. 对样本集中的每一个样本\\\\( x^{(i)}\\\\)，转化为新的样本\\\\( z^{(i)} = W^T x^{(i)} \\\\)\n6. 得到输出样本集 \\\\( D′=(z^{(1)}, z^{(2)}, ..., z^{(m)} ) \\\\)\n\n有时候，我们不指定降维后的 n' 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \\\\( \\lambda\\_1 \\geq \\lambda\\_2 \\geq ... \\geq \\lambda\\_n \\\\)，则 n' 可以通过下式得到:\n\\\\[\n\\sum\\_{i=1}^{n′} \\lambda\\_i / \\sum\\_{i=1}^n \\lambda\\_i \\geq t\n\\\\]\n\n\n#### 2.4 PCA实例\n\n假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下：\n\n```\n(2.5, 2.4), (0.5, 0.7), (2.2, 2.9), (1.9, 2.2), (3.1, 3.0), \n(2.3, 2.7), (2  , 1.6), (1  , 1.1), (1.5, 1.6), (1.1, 0.9)\n```\n　\n首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为\n\n```\n(0.69, 0.49), (-1.31, -1.21), ( 0.39,  0.99), ( 0.09,  0.29), ( 1.29,  1.09), \n(0.49, 0.79), ( 0.19, -0.31), (-0.81, -0.81), (-0.31, -0.31), (-0.71, -1.01)\n```\n\n现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为：\n\\\\[\nXX^T = \n\\left( \n\\begin{matrix}\ncov(x1,x1) & cov(x2,x1) \\\\\\ cov(x1,x2) & cov(x2,x2) \n\\end{matrix}\n\\right)\n\\\\]\n\n对于上述数据，求出协方差矩阵为：\n\\\\[\nXX^T = \n\\left(\n\\begin{matrix}\n0.616555556 & 0.615444444 \\\\\\ 0.615444444 & 0.716555556\n\\end{matrix}\n\\right)\n\\\\]\n\n求出特征值为 \\\\(（0.0490834, 1.28402771）\\\\)，对应的特征向量分别为：\\\\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \\\\)，\n由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \\\\( (0.6778734, -0.73517866)^T \\\\)。\n则 \\\\( W=(0.6778734, -0.73517866)^T \\\\)。\n\n对所有的数据集进行投影\\\\( z^{(i)} = W^T x^{(i)} \\\\)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为：\n```\n   0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 \n  -0.24863317  0.35670133  0.04641726  0.01776463  0.26124033\n```\n\n>下面是例子的Python代码\n\n```python\nimport numpy as np\n\noriginalData = np.array([\n\t[ 2.5,  0.5,  2.2,  1.9,  3.1,  2.3,  2. ,  1. ,  1.5,  1.1,],\n\t[ 2.4,  0.7,  2.9,  2.2,  3. ,  2.7,  1.6,  1.1,  1.6,  0.9]])\n\nmean = np.mean(originalData, axis=1)\nprint(\"均值：\")\nprint(mean) # [ 1.81  1.91]\n\npreData = (originalData.T - mean).T\nprint(\"中心化后数据：\")\nprint(preData)\n# [[ 0.69 -1.31  0.39  0.09  1.29  0.49  0.19 -0.81 -0.31 -0.71]\n#  [ 0.49 -1.21  0.99  0.29  1.09  0.79 -0.31 -0.81 -0.31 -1.01]]\n\ncov = np.cov(preData)\nprint(\"协方差矩阵：\")\nprint(cov)\n# [[ 0.61655556  0.61544444]\n#  [ 0.61544444  0.71655556]]\n\nfeature_value, feature_vector = np.linalg.eig(cov)\nprint(\"特征值：\")\nprint(feature_value) # [ 0.0490834   1.28402771]\nprint(\"特征向量:\")\nprint(feature_vector)\n# [[-0.73517866 -0.6778734 ]\n#  [ 0.6778734  -0.73517866]]\n\nmaxIndex = 1\nW = feature_vector[maxIndex, :]\nredData = W.T.dot(preData)\nprint(\"降维后数据：\")\nprint(redData)\n# [ 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 -0.24863317\n#   0.35670133  0.04641726  0.01776463  0.26124033]\n```\n\n\n\n\n#### 2.5 PCA总结\n\n作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。\n\nPCA算法的主要优点有：\n\n* 仅仅需要以方差衡量信息量，不受数据集以外的因素影响\n* 各主成分之间正交，可消除原始数据成分间的相互影响的因素\n* 计算方法简单，主要运算是特征值分解，易于实现\n\nPCA算法的主要缺点有：\n\n* 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强\n* 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响\n\n\n#### 2.6 PCA实现方面\n\nPCA降维需要找到样本协方差矩阵\\\\( X^TX \\\\)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。\n可以看出，在这个过程中需要先求出协方差矩阵\\\\( X^TX\\\\)，当样本数多样本特征数也多的时候，这个计算量是很大的。\n\n注意到SVD也可以得到协方差矩阵\\\\( X^TX \\\\)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\\\\(X^TX\\\\)，也能求出右奇异矩阵\\\\( V \\\\)。\n也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。\n实际上，[scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。\n\n*另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？*\n\n假设样本是 m×n 的矩阵 \\\\(X\\\\)，如果通过SVD找到了矩阵\\\\( XX^T \\\\)最大的 d 个特征向量张成的 m×d 维矩阵\\\\(U\\\\)，则如果进行如下处理：\n\\\\[\nX′\\_{d×n} = U^T\\_{d×m} X\\_{m×n}\n\\\\]\n\n可以得到一个 d×n 的矩阵\\\\( X' \\\\)，这个矩阵和原来的 m×n 维样本矩阵\\\\( X \\\\)相比，行数从 m 减到了 k ，可见对行数进行了压缩。\n也就是说，**左奇异矩阵可以用于行数的压缩**；相对的，**右奇异矩阵可以用于列数即特征维度的压缩**，也就是PCA降维。\n\n\n>\n[主成分分析（PCA）原理总结](http://www.cnblogs.com/pinard/p/6239403.html)\n\n-----\n\n### 3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)\n\nLDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。\n\nLDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。\n我们的目标就是找到一条直线，尽可能满足上面的要求。\n\n来看一个例子：两类会这样被降维\n\n![lda](/posts_res/2018-05-18-dimreduction/3-1.jpg)\n\n设数据集\\\\( D=\\lbrace (x\\_i, y\\_i) \\rbrace^m\\_{i=1} \\\\)，投影向量为\\\\(w\\\\)，则点\\\\(x\\_i\\\\)经过投影后为\\\\( y=w^Tx\\_i\\\\)，投影前的样本中心点为\\\\( u \\\\)，投影后的中心点为\\\\( \\bar{u} = w^T u\\\\)。\n\n希望投影后不同类别的样本尽量离得较远：使用度量值\n\\\\[\n\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2\n\\\\]\n\n同时希望投影后相同类别的样本之间尽量离得较近：使用度量值\n\\\\[\n\\sum\\_{y \\in Y\\_i} (y - \\hat{u}\\_i)^2\n\\\\]\n\n这个值其实就是投影后样本的**方差**乘以此类样本集合中样本的数量。\n\n所以总的优化目标函数为：\n\\\\[\nJ(W) = \\frac{\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2}{\\sum\\_{y\\in Y\\_i} ( y - \\hat{u}\\_i )^2} = \\frac{\\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2}{\\sum\\_0 ( y - \\hat{u}\\_0 )^2 + \\sum\\_1 (y - \\hat{u}\\_1)^2}\n\\\\]\n\n目标\\\\(J(W)\\\\)当然是越大越好；\n\n定义类内散度矩阵为: \\\\( S\\_w = \\sum\\_0 + \\sum\\_1 = \\sum\\_{X\\_0} (x-u\\_0)(x-u\\_0)^T + \\sum\\_{X\\_1} (x-u\\_1)(x-u\\_1)^T \\\\)\n\n定义类间散度矩阵：\\\\( S\\_b = (u\\_0 - u\\_1)(u\\_0 - u\\_1)^T \\\\)\n\n分子：\\\\( \\|\\| \\hat{u}\\_0 - \\hat{u}\\_1 \\|\\|^2\\_2 = w^T (u\\_0 - u\\_1)(u\\_0 - u\\_1)^T w = w^TS\\_bw \\\\)\n\n分母：\\\\( \\sum\\_0 (y-\\hat{u}\\_0)^2 + \\sum\\_1 (y-\\hat{u}\\_1)^2 = w^T S\\_w w \\\\)\n\n所以\\\\( J(w)= w^TS\\_bw / w^T S\\_w w \\\\)，因为向量\\\\( w \\\\)的长度成比例改变不影响\\\\( J(W) \\\\)的取值，所以我们令\\\\( w^TS\\_ww=1 \\\\)，那么原优化目标就变为\n\\\\[\n\\min\\_{w} J(W) = - w^T S\\_b w, \\quad \\quad s.t. \\quad w^TS\\_ww = 1\n\\\\]\n\n这里直接使用拉格朗日乘子法就可以了，解得\n\\\\[\nS\\_b w = \\lambda w S\\_w\n\\\\]\n\n因为\\\\( S\\_bw = (u\\_0-u\\_1)(u\\_0-u\\_1)^Tw = (u\\_0 - u\\_1) \\lambda\\_t \\\\)，\n所以\\\\( (u\\_0 - u\\_1)\\lambda\\_t = \\lambda w S\\_w \\\\)，可以得到\n\\\\[\nw = S\\_w^{-1} (u\\_0 - u\\_1)\n\\\\]\n\n\n**LDA局限性：**\n\n1. 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出\n2. LDA不适合对非高斯分布的样本进行降维\n3. LDA在样本分类信息依赖方差而不是均值时，效果不好\n4. LDA可能过度拟合数据\n\n\n### 4. PCA与LDA的异同\n\n* **出发思想不同。**PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。\n\n* **学习模式不同。**PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。\n\n* **降维后可用维度数量不同。**LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。\n\n* **投影的坐标系不一定相同。**PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。\n\n\n![pca&lda](/posts_res/2018-05-18-dimreduction/4-1.jpg)\n\n上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。\n\n\n>\n[LDA与PCA都是常用的降维方法，二者的区别](https://blog.csdn.net/dongyanwen6036/article/details/78311071)\n","slug":"dimreduction","published":1,"updated":"2019-08-17T09:36:36.860Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwf00322qwpi6um87kt","content":"<p>目录</p><ul>\n<li>SVD 奇异值分解 [无监督]</li>\n<li>PCA 主成份分析 [无监督]</li>\n<li>LDA 线性判别分析 [有监督]</li>\n<li>PCA与LDA的异同</li>\n</ul><hr><h3 id=\"1-SVD-奇异值分解-Singular-Value-Decomposition\"><a href=\"#1-SVD-奇异值分解-Singular-Value-Decomposition\" class=\"headerlink\" title=\"1. SVD 奇异值分解(Singular Value Decomposition)\"></a>1. SVD 奇异值分解(Singular Value Decomposition)</h3><p>不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。</p><h4 id=\"1-1-特征值与特征向量\"><a href=\"#1-1-特征值与特征向量\" class=\"headerlink\" title=\"1.1 特征值与特征向量\"></a>1.1 特征值与特征向量</h4><p>特征值和特征向量的定义如下：\n\\[\nAx = \\lambda x\n\\]</p><p>其中<code>A</code>是一个<code>n x n</code>的矩阵，\\(x\\)是一个<code>n维向量</code>，则我们\\(\\lambda\\)是矩阵<code>A</code>的一个特征值，而<code>x</code>是矩阵<code>A</code>的特征值\\( \\lambda \\)所对应的特征向量。</p><p>根据特征值和特征向量，可以将矩阵<code>A</code>特征分解。如果我们求出了矩阵<code>A</code>的<code>n</code>个特征值\\( \\lambda_1 \\leq \\lambda_2 \\leq … \\leq \\lambda_n \\)以及这<code>n</code>个特征值所对应的特征向量\\( \\lbrace w_1, w_2, …, w_n \\rbrace \\)，那么矩阵<code>A</code>就可以用下式的特征分解表示：\n\\[\nA = W \\Sigma W^{-1}\n\\]</p><a id=\"more\"></a>\n\n\n\n\n\n\n<p>其中<code>W</code>是这<code>n</code>个特征向量所组成的<code>n x n</code>维矩阵，而\\( \\Sigma \\)为这<code>n</code>个特征值为主对角线的<code>n x n</code>维矩阵。</p>\n<p>一般我们会把<code>W</code>的这<code>n</code>个特征向量标准化，即满足 \\( || w_i ||_2=1 \\), 或者说 \\( w^T_i w_i=1 \\)，此时<code>W</code>的<code>n</code>个特征向量为标准正交基，满足\\( W^T W=I \\)，即\\( W^T=W^{−1}\\), 也就是说<code>W</code>为酉矩阵。</p>\n<p>这样我们的特征分解表达式可以写成\n\\[\nA=WΣW^T\n\\]</p>\n<p>注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。</p>\n<h4 id=\"1-2-SVD的定义\"><a href=\"#1-2-SVD的定义\" class=\"headerlink\" title=\"1.2 SVD的定义\"></a>1.2 SVD的定义</h4><p>SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\\(A\\)是一个<code>m × n</code>的矩阵，那么我们定义矩阵\\(A\\)的SVD为：\n\\[\nA=UΣV^T\n\\]</p>\n<p>其中\\(U\\)是一个<code>m × m</code>的矩阵，\\(\\Sigma\\)是一个<code>m × n</code>的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\\(V\\)是一个<code>n × n</code>的矩阵。\\(U\\)和\\(V\\)都是酉矩阵，即满足\\( U^TU=I,V^TV=I\\)。下图可以很形象的看出上面SVD的定义：</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/1-1.png\" alt=\"svd\"></p>\n<p>如何求出SVD分解后的\\( U, \\Sigma, V \\) 这三个矩阵呢？</p>\n<p>如果将\\(A^T\\)和\\(A\\)做矩阵乘法，那么会得到<code>n × n</code>的一个方阵\\(A^TA\\)。既然\\(A^TA\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\[\n(A^TA)v_i=\\lambda_i v_i\n\\]</p>\n<p>这样就可以得到矩阵\\(A^TA\\)的<code>n</code>个特征值和对应的<code>n</code>个特征向量\\(v\\)了。将\\(A^TA\\)的所有特征向量张成一个<code>n × n</code>的矩阵\\(V\\)，就是SVD公式里面的\\(V\\)矩阵了。一般将\\(V\\)中的每个特征向量叫做\\(A\\)的右奇异向量。</p>\n<p>如果将\\(A\\)和\\(A^T\\)做矩阵乘法，那么会得到<code>m × m</code>的一个方阵\\(AA^T\\)。既然\\(AA^T\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\[\n(AA^T)u_i=\\lambda_iu_i\n\\]</p>\n<p>这样就可以得到矩阵\\(AA^T\\)的<code>m</code>个特征值和对应的<code>m</code>个特征向量\\(u\\)了。将\\(AA^T\\)的所有特征向量张成一个<code>m × m</code>的矩阵\\(U\\)，就是SVD公式里面的\\(U\\)矩阵了。一般我们将\\(U\\)中的每个特征向量叫做\\(A\\)的左奇异向量。</p>\n<p>\\(U\\)和\\(V\\)都求出来了，现在就剩下奇异值矩阵\\( \\Sigma \\)没有求出了。由于\\( \\Sigma \\)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\\( \\sigma \\)就可以了。</p>\n<p>注意到:\n\\[\nA=UΣV^T \\Rightarrow AV=UΣV^TV \\Rightarrow AV=UΣ \\Rightarrow Av_i=\\sigma_iu_i \\Rightarrow \\sigma_i= \\frac{Av_i}{u_i}\n\\]</p>\n<p>这样可以求出每个奇异值，进而求出奇异值矩阵\\( \\Sigma \\)。</p>\n<p>上面还有一个问题没有讲，就是说\\(A^TA\\)的特征向量组成的就是我们SVD中的\\(V\\)矩阵，而\\(AA^T\\)的特征向量组成的就是我们SVD中的\\(U\\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\\(V\\)矩阵的证明为例。\n\\[\nA=UΣV^T \\Rightarrow A^T=VΣ^TU^T \\Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T\n\\]</p>\n<p>上式证明使用了:\\( U^TU=I,Σ^TΣ=Σ^2\\)，可以看出\\( A^TA \\)的特征向量组成的的确就是我们SVD中的\\(V\\)矩阵。类似的方法可以得到\\(AA^T\\)的特征向量组成的就是我们SVD中的\\(U\\)矩阵。</p>\n<p>进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：\n\\[\n\\sigma_i=\\sqrt{\\lambda_i}\n\\]</p>\n<p>这样也就是说，可以不用\\( \\sigma_i= Av_i / u_i \\)来计算奇异值，也可以通过求出\\( A^TA \\)的特征值取平方根来求奇异值。</p>\n<h4 id=\"1-3-SVD的性质\"><a href=\"#1-3-SVD的性质\" class=\"headerlink\" title=\"1.3 SVD的性质\"></a>1.3 SVD的性质</h4><p>对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：\n\\[\nA_{m×n}=U_{m×m} \\Sigma_{m×n} V^T_{n×n} \\sim U_{m×k} \\Sigma_{k×k} V^T_{k×n}\n\\]</p>\n<p>其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\\( U_{m×k}, \\Sigma_{k×k}, V^T_{k×n} \\)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/1-2.png\" alt=\"property\"></p>\n<p>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。</p>\n<p>&gt;\n<a href=\"https://www.cnblogs.com/pinard/p/6251584.html\" target=\"_blank\" rel=\"noopener\">奇异值分解(SVD)原理与在降维中的应用</a></p>\n<hr>\n<h3 id=\"2-PCA-主成份分析\"><a href=\"#2-PCA-主成份分析\" class=\"headerlink\" title=\"2. PCA 主成份分析\"></a>2. PCA 主成份分析</h3><p><strong>事先声明</strong></p>\n<p>当 <code>X = 样本数量 * 每个样本的维度</code> 时， 协方差矩阵为$ X^T X $；<br>当 <code>X = 每个样本的维度 * 样本数量</code> 时， 协方差矩阵为$ X X^T $；\n<br></p>\n<h4 id=\"2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近\"><a href=\"#2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近\" class=\"headerlink\" title=\"2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)\"></a>2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)</h4><p>假设 m 个 n 维数据\\( (x^{(1)}, x^{(2)},…,x^{(m)}) \\)都已经进行了中心化，即 \\( \\sum_{i=1}^m x^{(i)}=0 \\)。经过投影变换后得到的新坐标系为\\( \\lbrace w_1,w_2,…,w_n \\rbrace \\)，其中\\(w\\)是标准正交基，即\\( || w ||^2 = 1, w^T_iw_j=0 \\)。</p>\n<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\( \\lbrace w_1, w_2, …, w_{n′} \\rbrace \\)，样本点\\( x^{(i)} \\)在 n’ 维坐标系中的投影为：\\( z^{(i)} = ( z^{(i)}_1,z^{(i)}_2, …, z^{(i)}_{n′}) \\)。其中，\\( z^{(i)}_j = w^T_j x^{(i)} \\)是\\( x^{(i)} \\)在低维坐标系里第 j 维的坐标。</p>\n<p>如果用\\( z^{(i)} \\)来恢复原始数据\\( x^{(i)} \\)，则得到的恢复数据 \\( \\bar{x}^{(i)} = \\sum_{j=1}^{n′} z^{(i)}_j w_j = Wz^{(i)} \\)，其中，\\(W\\)为标准正交基组成的矩阵。</p>\n<p>现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式：\n\\[\n\\sum_{i=1}^m || \\bar{x}^{(i)} − x^{(i)} ||^2_2\n\\]</p>\n<p>将这个式子进行整理，可以得到:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^m || \\bar{x}^{(i)} − x^{(i)} ||^2_2 \n&amp; = \\sum_{i=1}^m || Wz^{(i)} − x^{(i)} ||^2_2 \\\\\n&amp; = \\sum_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \\sum_{i=1}^m (Wz^{(i)})^T x^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = \\sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum_{i=1}^m z^{(i)T}W^T x^{(i)} + \\sum_{i=1}^m x^{(i)T} x^{(i)} \\\\\n&amp; = \\sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum_{i=1}^m z^{(i)T}z^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = − \\sum_{i=1}^m z^{(i)T}z^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = − tr( W^T ( \\sum_{i=1}^m x^{(i)} x^{(i)T} ) W) + \\sum_{i=1}^m x^{(i)T} x^{(i)} \\\\\n&amp; = − tr( W^T X X^T W) + \\sum_{i=1}^m x^{(i)T} x^{(i)}\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<ul>\n<li>第1个等式用到了\\( \\bar{x}^{(i)} = W z^{(i)} \\)</li>\n<li>第2个等式用到了平方和展开</li>\n<li>第3个等式用到了矩阵转置公式\\( (AB)^T= B^T A^T \\) 和 \\( W^T W =I \\)</li>\n<li>第4个等式用到了\\( z^{(i)} = W^T x^{(i)} \\)</li>\n<li>第5个等式合并同类项</li>\n<li>第6个等式用到了\\( z^{(i)} = W^T x^{(i)} \\)和矩阵的迹</li>\n<li>第7个等式将代数和表达为矩阵形式</li>\n</ul>\n<p>注意到\\( \\sum_{i=1}^m x^{(i)} x^{(i)T} \\)是数据集的协方差矩阵，\\(W\\)的每一个向量\\(w_j\\)是标准正交基。而\\( \\sum_{i=1}^m x^{(i)T} x^{(i)} \\) 是一个常量。最小化上式等价于：\n\\[\n\\arg\\min_W - tr( W^T X X^T W ) \\quad \\quad s.t. W^TW = I\n\\]</p>\n<p>这个最小化不难，直接观察也可以发现最小值对应的\\(W\\)由协方差矩阵\\(XX^T\\)最大的 n’ 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到\n\\[\nJ(W) = −tr( W^T X X^T W) + \\lambda ( W^T W − I )\n\\]</p>\n<p>对\\( W \\)求导有\\( − X X^T W + \\lambda W = 0 \\)，整理下即为：\n\\[\nXX^T W = \\lambda W\n\\]</p>\n<p>这样可以更清楚的看出，\\(W\\)为\\(X X^T \\)的 n’ 个特征向量组成的矩阵，而\\( \\lambda \\) 为 \\( XX^T \\) 的特征值。当我们将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\\( W\\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\\( z^{(i)} = W^T x^{(i)} \\)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。\n　</p>\n<h4 id=\"2-2-PCA的推导-基于最大投影方差\"><a href=\"#2-2-PCA的推导-基于最大投影方差\" class=\"headerlink\" title=\"2.2 PCA的推导:基于最大投影方差\"></a>2.2 PCA的推导:基于最大投影方差</h4><p>假设 m 个 n 维数据\\( (x^{(1)}, x^{(2)}, …, x^{(m)} ) \\)都已经进行了中心化，即\\( \\sum_{i=1}^m x^{(i)} = 0 \\)。经过投影变换后得到的新坐标系为\\( \\lbrace w_1, w_2, …, w_n \\rbrace \\)，其中\\( w\\) 是标准正交基，即\\( || w ||^2 = 1, w^T_i w_j = 0\\)。</p>\n<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\( \\lbrace w_1, w_2, …, w_{n′} \\rbrace \\)，样本点\\( x^{(i)} \\) 在 n’ 维坐标系中的投影为：\\( z^{(i)} = ( z^{(i)}_1, z^{(i)}_2, …, z^{(i)}_{n′}) \\) 。其中，\\( z^{(i)}_j = w^T_j x^{(i)} \\)是 \\( x^{(i)} \\) 在低维坐标系里第 j 维的坐标。</p>\n<p>对于任意一个样本\\( x^{(i)} \\)，在新的坐标系中的投影为\\( W^T x^{(i)} \\)，在新坐标系中的投影方差为\\( W^T x^{(i)} x^{(i)T}W \\)，要使所有的样本的投影方差和最大，也就是最大化\\( \\sum_{i=1}^m W^T x^{(i)} x^{(i)T} W \\)，即：\n\\[\n\\arg\\max_W tr(W^T X X^T W) \\quad \\quad s.t. W^TW=I\n\\]</p>\n<p>观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。</p>\n<p>利用拉格朗日函数可以得到\n\\[\nJ(W) = tr( W^T X X^T W) + \\lambda (W^T W − I)\n\\]</p>\n<p>对\\( W \\) 求导有 \\( XX^T W + \\lambda W = 0 \\)，整理下即为：\n\\[\nXX^TW = (−\\lambda)W\n\\]</p>\n<p>和上面一样可以看出，\\(W\\)为\\(XX^T\\)的 n’ 个特征向量组成的矩阵，而 \\( −\\lambda \\) 为\\( XX^T\\) 的特征值。将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\\( W \\)即为需要的矩阵。对于原始数据集，只需要用\\( z^{(i)} = W^T x^{(i)} \\)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。</p>\n<h4 id=\"2-3-PCA算法流程\"><a href=\"#2-3-PCA算法流程\" class=\"headerlink\" title=\"2.3 PCA算法流程\"></a>2.3 PCA算法流程</h4><p>输入: n 维样本集 \\( D = ( x^{(1)}, x^{(2)}, …, x^{(m)} ) \\)，要降维到的维数 n’。</p>\n<p>输出: 降维后的样本集 \\( D′ \\)</p>\n<ol>\n<li>对所有的样本进行中心化： \\( x^{(i)} = x^{(i)} − \\frac{1}{m} \\sum_{j=1}^m x^{(j)}  \\)</li>\n<li>计算样本的协方差矩阵\\( XX^T \\)</li>\n<li>对矩阵\\( XX^T \\)进行特征值分解</li>\n<li>取出最大的 n’ 个特征值对应的特征向量\\( (w_1, w_2, …, w_{n′}) \\)，将所有的特征向量标准化后，组成特征向量矩阵\\(W\\)</li>\n<li>对样本集中的每一个样本\\( x^{(i)}\\)，转化为新的样本\\( z^{(i)} = W^T x^{(i)} \\)</li>\n<li>得到输出样本集 \\( D′=(z^{(1)}, z^{(2)}, …, z^{(m)} ) \\)</li>\n</ol>\n<p>有时候，我们不指定降维后的 n’ 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \\( \\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_n \\)，则 n’ 可以通过下式得到:\n\\[\n\\sum_{i=1}^{n′} \\lambda_i / \\sum_{i=1}^n \\lambda_i \\geq t\n\\]</p>\n<h4 id=\"2-4-PCA实例\"><a href=\"#2-4-PCA实例\" class=\"headerlink\" title=\"2.4 PCA实例\"></a>2.4 PCA实例</h4><p>假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下：</p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">2.5</span>, <span class=\"number\">2.4</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.7</span>), (<span class=\"number\">2.2</span>, <span class=\"number\">2.9</span>), (<span class=\"number\">1.9</span>, <span class=\"number\">2.2</span>), (<span class=\"number\">3.1</span>, <span class=\"number\">3.0</span>), </span><br><span class=\"line\">(<span class=\"number\">2.3</span>, <span class=\"number\">2.7</span>), (<span class=\"number\">2</span>  , <span class=\"number\">1.6</span>), (<span class=\"number\">1</span>  , <span class=\"number\">1.1</span>), (<span class=\"number\">1.5</span>, <span class=\"number\">1.6</span>), (<span class=\"number\">1.1</span>, <span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure>\n<p>　\n首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(0<span class=\"selector-class\">.69</span>, 0<span class=\"selector-class\">.49</span>), (<span class=\"selector-tag\">-1</span><span class=\"selector-class\">.31</span>, <span class=\"selector-tag\">-1</span><span class=\"selector-class\">.21</span>), ( 0<span class=\"selector-class\">.39</span>,  0<span class=\"selector-class\">.99</span>), ( 0<span class=\"selector-class\">.09</span>,  0<span class=\"selector-class\">.29</span>), ( 1<span class=\"selector-class\">.29</span>,  1<span class=\"selector-class\">.09</span>), </span><br><span class=\"line\">(0<span class=\"selector-class\">.49</span>, 0<span class=\"selector-class\">.79</span>), ( 0<span class=\"selector-class\">.19</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.81</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.81</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.71</span>, <span class=\"selector-tag\">-1</span><span class=\"selector-class\">.01</span>)</span><br></pre></td></tr></table></figure>\n<p>现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为：\n\\[\nXX^T = \n\\left( \n\\begin{matrix}\ncov(x1,x1) &amp; cov(x2,x1) \\\\ cov(x1,x2) &amp; cov(x2,x2) \n\\end{matrix}\n\\right)\n\\]</p>\n<p>对于上述数据，求出协方差矩阵为：\n\\[\nXX^T = \n\\left(\n\\begin{matrix}\n0.616555556 &amp; 0.615444444 \\\\ 0.615444444 &amp; 0.716555556\n\\end{matrix}\n\\right)\n\\]</p>\n<p>求出特征值为 \\(（0.0490834, 1.28402771）\\)，对应的特征向量分别为：\\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \\)，\n由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \\( (0.6778734, -0.73517866)^T \\)。\n则 \\( W=(0.6778734, -0.73517866)^T \\)。</p>\n<p>对所有的数据集进行投影\\( z^{(i)} = W^T x^{(i)} \\)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为：\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> 0<span class=\"selector-class\">.1074951</span>   0<span class=\"selector-class\">.00155202</span> <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.46345624</span> <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.1521932</span>   0<span class=\"selector-class\">.07311195</span> </span><br><span class=\"line\"><span class=\"selector-tag\">-0</span><span class=\"selector-class\">.24863317</span>  0<span class=\"selector-class\">.35670133</span>  0<span class=\"selector-class\">.04641726</span>  0<span class=\"selector-class\">.01776463</span>  0<span class=\"selector-class\">.26124033</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>下面是例子的Python代码</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">originalData = np.array([</span><br><span class=\"line\">\t[ <span class=\"number\">2.5</span>,  <span class=\"number\">0.5</span>,  <span class=\"number\">2.2</span>,  <span class=\"number\">1.9</span>,  <span class=\"number\">3.1</span>,  <span class=\"number\">2.3</span>,  <span class=\"number\">2.</span> ,  <span class=\"number\">1.</span> ,  <span class=\"number\">1.5</span>,  <span class=\"number\">1.1</span>,],</span><br><span class=\"line\">\t[ <span class=\"number\">2.4</span>,  <span class=\"number\">0.7</span>,  <span class=\"number\">2.9</span>,  <span class=\"number\">2.2</span>,  <span class=\"number\">3.</span> ,  <span class=\"number\">2.7</span>,  <span class=\"number\">1.6</span>,  <span class=\"number\">1.1</span>,  <span class=\"number\">1.6</span>,  <span class=\"number\">0.9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">mean = np.mean(originalData, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"均值：\"</span>)</span><br><span class=\"line\">print(mean) <span class=\"comment\"># [ 1.81  1.91]</span></span><br><span class=\"line\"></span><br><span class=\"line\">preData = (originalData.T - mean).T</span><br><span class=\"line\">print(<span class=\"string\">\"中心化后数据：\"</span>)</span><br><span class=\"line\">print(preData)</span><br><span class=\"line\"><span class=\"comment\"># [[ 0.69 -1.31  0.39  0.09  1.29  0.49  0.19 -0.81 -0.31 -0.71]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.49 -1.21  0.99  0.29  1.09  0.79 -0.31 -0.81 -0.31 -1.01]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">cov = np.cov(preData)</span><br><span class=\"line\">print(<span class=\"string\">\"协方差矩阵：\"</span>)</span><br><span class=\"line\">print(cov)</span><br><span class=\"line\"><span class=\"comment\"># [[ 0.61655556  0.61544444]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.61544444  0.71655556]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">feature_value, feature_vector = np.linalg.eig(cov)</span><br><span class=\"line\">print(<span class=\"string\">\"特征值：\"</span>)</span><br><span class=\"line\">print(feature_value) <span class=\"comment\"># [ 0.0490834   1.28402771]</span></span><br><span class=\"line\">print(<span class=\"string\">\"特征向量:\"</span>)</span><br><span class=\"line\">print(feature_vector)</span><br><span class=\"line\"><span class=\"comment\"># [[-0.73517866 -0.6778734 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.6778734  -0.73517866]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">maxIndex = <span class=\"number\">1</span></span><br><span class=\"line\">W = feature_vector[maxIndex, :]</span><br><span class=\"line\">redData = W.T.dot(preData)</span><br><span class=\"line\">print(<span class=\"string\">\"降维后数据：\"</span>)</span><br><span class=\"line\">print(redData)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 -0.24863317</span></span><br><span class=\"line\"><span class=\"comment\">#   0.35670133  0.04641726  0.01776463  0.26124033]</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"2-5-PCA总结\"><a href=\"#2-5-PCA总结\" class=\"headerlink\" title=\"2.5 PCA总结\"></a>2.5 PCA总结</h4><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p>\n<p>PCA算法的主要优点有：</p>\n<ul>\n<li>仅仅需要以方差衡量信息量，不受数据集以外的因素影响</li>\n<li>各主成分之间正交，可消除原始数据成分间的相互影响的因素</li>\n<li>计算方法简单，主要运算是特征值分解，易于实现</li>\n</ul>\n<p>PCA算法的主要缺点有：</p>\n<ul>\n<li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强</li>\n<li>方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响</li>\n</ul>\n<h4 id=\"2-6-PCA实现方面\"><a href=\"#2-6-PCA实现方面\" class=\"headerlink\" title=\"2.6 PCA实现方面\"></a>2.6 PCA实现方面</h4><p>PCA降维需要找到样本协方差矩阵\\( X^TX \\)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。\n可以看出，在这个过程中需要先求出协方差矩阵\\( X^TX\\)，当样本数多样本特征数也多的时候，这个计算量是很大的。</p>\n<p>注意到SVD也可以得到协方差矩阵\\( X^TX \\)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\\(X^TX\\)，也能求出右奇异矩阵\\( V \\)。\n也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。\n实际上，<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" target=\"_blank\" rel=\"noopener\">scikit-learn</a>的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。</p>\n<p><em>另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？</em></p>\n<p>假设样本是 m×n 的矩阵 \\(X\\)，如果通过SVD找到了矩阵\\( XX^T \\)最大的 d 个特征向量张成的 m×d 维矩阵\\(U\\)，则如果进行如下处理：\n\\[\nX′_{d×n} = U^T_{d×m} X_{m×n}\n\\]</p>\n<p>可以得到一个 d×n 的矩阵\\( X’ \\)，这个矩阵和原来的 m×n 维样本矩阵\\( X \\)相比，行数从 m 减到了 k ，可见对行数进行了压缩。\n也就是说，<strong>左奇异矩阵可以用于行数的压缩</strong>；相对的，<strong>右奇异矩阵可以用于列数即特征维度的压缩</strong>，也就是PCA降维。</p>\n<p>&gt;\n<a href=\"http://www.cnblogs.com/pinard/p/6239403.html\" target=\"_blank\" rel=\"noopener\">主成分分析（PCA）原理总结</a></p>\n<hr>\n<h3 id=\"3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant\"><a href=\"#3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant\" class=\"headerlink\" title=\"3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)\"></a>3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)</h3><p>LDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。</p>\n<p>LDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。\n我们的目标就是找到一条直线，尽可能满足上面的要求。</p>\n<p>来看一个例子：两类会这样被降维</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/3-1.jpg\" alt=\"lda\"></p>\n<p>设数据集\\( D=\\lbrace (x_i, y_i) \\rbrace^m_{i=1} \\)，投影向量为\\(w\\)，则点\\(x_i\\)经过投影后为\\( y=w^Tx_i\\)，投影前的样本中心点为\\( u \\)，投影后的中心点为\\( \\bar{u} = w^T u\\)。</p>\n<p>希望投影后不同类别的样本尽量离得较远：使用度量值\n\\[\n|| \\hat{u}_0 - \\hat{u}_1 ||^2_2\n\\]</p>\n<p>同时希望投影后相同类别的样本之间尽量离得较近：使用度量值\n\\[\n\\sum_{y \\in Y_i} (y - \\hat{u}_i)^2\n\\]</p>\n<p>这个值其实就是投影后样本的<strong>方差</strong>乘以此类样本集合中样本的数量。</p>\n<p>所以总的优化目标函数为：\n\\[\nJ(W) = \\frac{|| \\hat{u}_0 - \\hat{u}_1 ||^2_2}{\\sum_{y\\in Y_i} ( y - \\hat{u}_i )^2} = \\frac{|| \\hat{u}_0 - \\hat{u}_1 ||^2_2}{\\sum_0 ( y - \\hat{u}_0 )^2 + \\sum_1 (y - \\hat{u}_1)^2}\n\\]</p>\n<p>目标\\(J(W)\\)当然是越大越好；</p>\n<p>定义类内散度矩阵为: \\( S_w = \\sum_0 + \\sum_1 = \\sum_{X_0} (x-u_0)(x-u_0)^T + \\sum_{X_1} (x-u_1)(x-u_1)^T \\)</p>\n<p>定义类间散度矩阵：\\( S_b = (u_0 - u_1)(u_0 - u_1)^T \\)</p>\n<p>分子：\\( || \\hat{u}_0 - \\hat{u}_1 ||^2_2 = w^T (u_0 - u_1)(u_0 - u_1)^T w = w^TS_bw \\)</p>\n<p>分母：\\( \\sum_0 (y-\\hat{u}_0)^2 + \\sum_1 (y-\\hat{u}_1)^2 = w^T S_w w \\)</p>\n<p>所以\\( J(w)= w^TS_bw / w^T S_w w \\)，因为向量\\( w \\)的长度成比例改变不影响\\( J(W) \\)的取值，所以我们令\\( w^TS_ww=1 \\)，那么原优化目标就变为\n\\[\n\\min_{w} J(W) = - w^T S_b w, \\quad \\quad s.t. \\quad w^TS_ww = 1\n\\]</p>\n<p>这里直接使用拉格朗日乘子法就可以了，解得\n\\[\nS_b w = \\lambda w S_w\n\\]</p>\n<p>因为\\( S_bw = (u_0-u_1)(u_0-u_1)^Tw = (u_0 - u_1) \\lambda_t \\)，\n所以\\( (u_0 - u_1)\\lambda_t = \\lambda w S_w \\)，可以得到\n\\[\nw = S_w^{-1} (u_0 - u_1)\n\\]</p>\n<p><strong>LDA局限性：</strong></p>\n<ol>\n<li>当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出</li>\n<li>LDA不适合对非高斯分布的样本进行降维</li>\n<li>LDA在样本分类信息依赖方差而不是均值时，效果不好</li>\n<li>LDA可能过度拟合数据</li>\n</ol>\n<h3 id=\"4-PCA与LDA的异同\"><a href=\"#4-PCA与LDA的异同\" class=\"headerlink\" title=\"4. PCA与LDA的异同\"></a>4. PCA与LDA的异同</h3><ul>\n<li><p><strong>出发思想不同。</strong>PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。</p>\n</li>\n<li><p><strong>学习模式不同。</strong>PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。</p>\n</li>\n<li><p><strong>降维后可用维度数量不同。</strong>LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。</p>\n</li>\n<li><p><strong>投影的坐标系不一定相同。</strong>PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。</p>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/4-1.jpg\" alt=\"pca&amp;lda\"></p>\n<p>上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。</p>\n<p>&gt;\n<a href=\"https://blog.csdn.net/dongyanwen6036/article/details/78311071\" target=\"_blank\" rel=\"noopener\">LDA与PCA都是常用的降维方法，二者的区别</a></p>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>SVD 奇异值分解 [无监督]</li>\n<li>PCA 主成份分析 [无监督]</li>\n<li>LDA 线性判别分析 [有监督]</li>\n<li>PCA与LDA的异同</li>\n</ul><hr><h3 id=\"1-SVD-奇异值分解-Singular-Value-Decomposition\"><a href=\"#1-SVD-奇异值分解-Singular-Value-Decomposition\" class=\"headerlink\" title=\"1. SVD 奇异值分解(Singular Value Decomposition)\"></a>1. SVD 奇异值分解(Singular Value Decomposition)</h3><p>不仅可以用于降维算法中的特征分解，也可以用于推荐系统、自言语言处理等领域，是很多机器学习算法的基石。</p><h4 id=\"1-1-特征值与特征向量\"><a href=\"#1-1-特征值与特征向量\" class=\"headerlink\" title=\"1.1 特征值与特征向量\"></a>1.1 特征值与特征向量</h4><p>特征值和特征向量的定义如下：\n\\[\nAx = \\lambda x\n\\]</p><p>其中<code>A</code>是一个<code>n x n</code>的矩阵，\\(x\\)是一个<code>n维向量</code>，则我们\\(\\lambda\\)是矩阵<code>A</code>的一个特征值，而<code>x</code>是矩阵<code>A</code>的特征值\\( \\lambda \\)所对应的特征向量。</p><p>根据特征值和特征向量，可以将矩阵<code>A</code>特征分解。如果我们求出了矩阵<code>A</code>的<code>n</code>个特征值\\( \\lambda_1 \\leq \\lambda_2 \\leq … \\leq \\lambda_n \\)以及这<code>n</code>个特征值所对应的特征向量\\( \\lbrace w_1, w_2, …, w_n \\rbrace \\)，那么矩阵<code>A</code>就可以用下式的特征分解表示：\n\\[\nA = W \\Sigma W^{-1}\n\\]</p>","more":"\n\n\n\n\n\n\n<p>其中<code>W</code>是这<code>n</code>个特征向量所组成的<code>n x n</code>维矩阵，而\\( \\Sigma \\)为这<code>n</code>个特征值为主对角线的<code>n x n</code>维矩阵。</p>\n<p>一般我们会把<code>W</code>的这<code>n</code>个特征向量标准化，即满足 \\( || w_i ||_2=1 \\), 或者说 \\( w^T_i w_i=1 \\)，此时<code>W</code>的<code>n</code>个特征向量为标准正交基，满足\\( W^T W=I \\)，即\\( W^T=W^{−1}\\), 也就是说<code>W</code>为酉矩阵。</p>\n<p>这样我们的特征分解表达式可以写成\n\\[\nA=WΣW^T\n\\]</p>\n<p>注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，这时就要使用到SVD了。</p>\n<h4 id=\"1-2-SVD的定义\"><a href=\"#1-2-SVD的定义\" class=\"headerlink\" title=\"1.2 SVD的定义\"></a>1.2 SVD的定义</h4><p>SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵\\(A\\)是一个<code>m × n</code>的矩阵，那么我们定义矩阵\\(A\\)的SVD为：\n\\[\nA=UΣV^T\n\\]</p>\n<p>其中\\(U\\)是一个<code>m × m</code>的矩阵，\\(\\Sigma\\)是一个<code>m × n</code>的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，\\(V\\)是一个<code>n × n</code>的矩阵。\\(U\\)和\\(V\\)都是酉矩阵，即满足\\( U^TU=I,V^TV=I\\)。下图可以很形象的看出上面SVD的定义：</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/1-1.png\" alt=\"svd\"></p>\n<p>如何求出SVD分解后的\\( U, \\Sigma, V \\) 这三个矩阵呢？</p>\n<p>如果将\\(A^T\\)和\\(A\\)做矩阵乘法，那么会得到<code>n × n</code>的一个方阵\\(A^TA\\)。既然\\(A^TA\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\[\n(A^TA)v_i=\\lambda_i v_i\n\\]</p>\n<p>这样就可以得到矩阵\\(A^TA\\)的<code>n</code>个特征值和对应的<code>n</code>个特征向量\\(v\\)了。将\\(A^TA\\)的所有特征向量张成一个<code>n × n</code>的矩阵\\(V\\)，就是SVD公式里面的\\(V\\)矩阵了。一般将\\(V\\)中的每个特征向量叫做\\(A\\)的右奇异向量。</p>\n<p>如果将\\(A\\)和\\(A^T\\)做矩阵乘法，那么会得到<code>m × m</code>的一个方阵\\(AA^T\\)。既然\\(AA^T\\)是方阵，那么就可以进行特征分解，得到的特征值和特征向量满足下式：\n\\[\n(AA^T)u_i=\\lambda_iu_i\n\\]</p>\n<p>这样就可以得到矩阵\\(AA^T\\)的<code>m</code>个特征值和对应的<code>m</code>个特征向量\\(u\\)了。将\\(AA^T\\)的所有特征向量张成一个<code>m × m</code>的矩阵\\(U\\)，就是SVD公式里面的\\(U\\)矩阵了。一般我们将\\(U\\)中的每个特征向量叫做\\(A\\)的左奇异向量。</p>\n<p>\\(U\\)和\\(V\\)都求出来了，现在就剩下奇异值矩阵\\( \\Sigma \\)没有求出了。由于\\( \\Sigma \\)除了对角线上是奇异值其他位置都是0，那只需要求出每个奇异值\\( \\sigma \\)就可以了。</p>\n<p>注意到:\n\\[\nA=UΣV^T \\Rightarrow AV=UΣV^TV \\Rightarrow AV=UΣ \\Rightarrow Av_i=\\sigma_iu_i \\Rightarrow \\sigma_i= \\frac{Av_i}{u_i}\n\\]</p>\n<p>这样可以求出每个奇异值，进而求出奇异值矩阵\\( \\Sigma \\)。</p>\n<p>上面还有一个问题没有讲，就是说\\(A^TA\\)的特征向量组成的就是我们SVD中的\\(V\\)矩阵，而\\(AA^T\\)的特征向量组成的就是我们SVD中的\\(U\\)矩阵，这有什么根据吗？这个其实很容易证明，我们以\\(V\\)矩阵的证明为例。\n\\[\nA=UΣV^T \\Rightarrow A^T=VΣ^TU^T \\Rightarrow A^TA= VΣ^TU^TUΣV^T= VΣ^2V^T\n\\]</p>\n<p>上式证明使用了:\\( U^TU=I,Σ^TΣ=Σ^2\\)，可以看出\\( A^TA \\)的特征向量组成的的确就是我们SVD中的\\(V\\)矩阵。类似的方法可以得到\\(AA^T\\)的特征向量组成的就是我们SVD中的\\(U\\)矩阵。</p>\n<p>进一步还可以看出特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：\n\\[\n\\sigma_i=\\sqrt{\\lambda_i}\n\\]</p>\n<p>这样也就是说，可以不用\\( \\sigma_i= Av_i / u_i \\)来计算奇异值，也可以通过求出\\( A^TA \\)的特征值取平方根来求奇异值。</p>\n<h4 id=\"1-3-SVD的性质\"><a href=\"#1-3-SVD的性质\" class=\"headerlink\" title=\"1.3 SVD的性质\"></a>1.3 SVD的性质</h4><p>对于奇异值,它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。也就是说，也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。也就是说：\n\\[\nA_{m×n}=U_{m×m} \\Sigma_{m×n} V^T_{n×n} \\sim U_{m×k} \\Sigma_{k×k} V^T_{k×n}\n\\]</p>\n<p>其中 k 要比 n 小很多，也就是一个大的矩阵 A 可以用三个小的矩阵\\( U_{m×k}, \\Sigma_{k×k}, V^T_{k×n} \\)来表示。如下图所示，现在我们的矩阵 A 只需要灰色的部分的三个小矩阵就可以近似描述了。</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/1-2.png\" alt=\"property\"></p>\n<p>由于这个重要的性质，SVD可以用于PCA降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP中的算法，比如潜在语义索引(LSI)。</p>\n<p>&gt;\n<a href=\"https://www.cnblogs.com/pinard/p/6251584.html\" target=\"_blank\" rel=\"noopener\">奇异值分解(SVD)原理与在降维中的应用</a></p>\n<hr>\n<h3 id=\"2-PCA-主成份分析\"><a href=\"#2-PCA-主成份分析\" class=\"headerlink\" title=\"2. PCA 主成份分析\"></a>2. PCA 主成份分析</h3><p><strong>事先声明</strong></p>\n<p>当 <code>X = 样本数量 * 每个样本的维度</code> 时， 协方差矩阵为$ X^T X $；<br>当 <code>X = 每个样本的维度 * 样本数量</code> 时， 协方差矩阵为$ X X^T $；\n<br></p>\n<h4 id=\"2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近\"><a href=\"#2-1-PCA推导-基于小于投影距离-样本到超平面距离足够近\" class=\"headerlink\" title=\"2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)\"></a>2.1 PCA推导:基于小于投影距离(样本到超平面距离足够近)</h4><p>假设 m 个 n 维数据\\( (x^{(1)}, x^{(2)},…,x^{(m)}) \\)都已经进行了中心化，即 \\( \\sum_{i=1}^m x^{(i)}=0 \\)。经过投影变换后得到的新坐标系为\\( \\lbrace w_1,w_2,…,w_n \\rbrace \\)，其中\\(w\\)是标准正交基，即\\( || w ||^2 = 1, w^T_iw_j=0 \\)。</p>\n<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\( \\lbrace w_1, w_2, …, w_{n′} \\rbrace \\)，样本点\\( x^{(i)} \\)在 n’ 维坐标系中的投影为：\\( z^{(i)} = ( z^{(i)}_1,z^{(i)}_2, …, z^{(i)}_{n′}) \\)。其中，\\( z^{(i)}_j = w^T_j x^{(i)} \\)是\\( x^{(i)} \\)在低维坐标系里第 j 维的坐标。</p>\n<p>如果用\\( z^{(i)} \\)来恢复原始数据\\( x^{(i)} \\)，则得到的恢复数据 \\( \\bar{x}^{(i)} = \\sum_{j=1}^{n′} z^{(i)}_j w_j = Wz^{(i)} \\)，其中，\\(W\\)为标准正交基组成的矩阵。</p>\n<p>现在考虑整个样本集，希望所有的样本到这个超平面的距离足够近，即最小化下式：\n\\[\n\\sum_{i=1}^m || \\bar{x}^{(i)} − x^{(i)} ||^2_2\n\\]</p>\n<p>将这个式子进行整理，可以得到:\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\sum_{i=1}^m || \\bar{x}^{(i)} − x^{(i)} ||^2_2 \n&amp; = \\sum_{i=1}^m || Wz^{(i)} − x^{(i)} ||^2_2 \\\\\n&amp; = \\sum_{i=1}^m (Wz^{(i)})^T (Wz^{(i)}) − 2 \\sum_{i=1}^m (Wz^{(i)})^T x^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = \\sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum_{i=1}^m z^{(i)T}W^T x^{(i)} + \\sum_{i=1}^m x^{(i)T} x^{(i)} \\\\\n&amp; = \\sum_{i=1}^m z^{(i)T}z^{(i)} − 2 \\sum_{i=1}^m z^{(i)T}z^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = − \\sum_{i=1}^m z^{(i)T}z^{(i)} + \\sum_{i=1}^m x^{(i)T}x^{(i)} \\\\\n&amp; = − tr( W^T ( \\sum_{i=1}^m x^{(i)} x^{(i)T} ) W) + \\sum_{i=1}^m x^{(i)T} x^{(i)} \\\\\n&amp; = − tr( W^T X X^T W) + \\sum_{i=1}^m x^{(i)T} x^{(i)}\n\\end{aligned}\n\\end{equation}\n\\]</p>\n<ul>\n<li>第1个等式用到了\\( \\bar{x}^{(i)} = W z^{(i)} \\)</li>\n<li>第2个等式用到了平方和展开</li>\n<li>第3个等式用到了矩阵转置公式\\( (AB)^T= B^T A^T \\) 和 \\( W^T W =I \\)</li>\n<li>第4个等式用到了\\( z^{(i)} = W^T x^{(i)} \\)</li>\n<li>第5个等式合并同类项</li>\n<li>第6个等式用到了\\( z^{(i)} = W^T x^{(i)} \\)和矩阵的迹</li>\n<li>第7个等式将代数和表达为矩阵形式</li>\n</ul>\n<p>注意到\\( \\sum_{i=1}^m x^{(i)} x^{(i)T} \\)是数据集的协方差矩阵，\\(W\\)的每一个向量\\(w_j\\)是标准正交基。而\\( \\sum_{i=1}^m x^{(i)T} x^{(i)} \\) 是一个常量。最小化上式等价于：\n\\[\n\\arg\\min_W - tr( W^T X X^T W ) \\quad \\quad s.t. W^TW = I\n\\]</p>\n<p>这个最小化不难，直接观察也可以发现最小值对应的\\(W\\)由协方差矩阵\\(XX^T\\)最大的 n’ 个特征值对应的特征向量组成。当然用数学推导也很容易。利用拉格朗日函数可以得到\n\\[\nJ(W) = −tr( W^T X X^T W) + \\lambda ( W^T W − I )\n\\]</p>\n<p>对\\( W \\)求导有\\( − X X^T W + \\lambda W = 0 \\)，整理下即为：\n\\[\nXX^T W = \\lambda W\n\\]</p>\n<p>这样可以更清楚的看出，\\(W\\)为\\(X X^T \\)的 n’ 个特征向量组成的矩阵，而\\( \\lambda \\) 为 \\( XX^T \\) 的特征值。当我们将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\\( W\\) 即为我们需要的矩阵。对于原始数据集，我们只需要用\\( z^{(i)} = W^T x^{(i)} \\)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。\n　</p>\n<h4 id=\"2-2-PCA的推导-基于最大投影方差\"><a href=\"#2-2-PCA的推导-基于最大投影方差\" class=\"headerlink\" title=\"2.2 PCA的推导:基于最大投影方差\"></a>2.2 PCA的推导:基于最大投影方差</h4><p>假设 m 个 n 维数据\\( (x^{(1)}, x^{(2)}, …, x^{(m)} ) \\)都已经进行了中心化，即\\( \\sum_{i=1}^m x^{(i)} = 0 \\)。经过投影变换后得到的新坐标系为\\( \\lbrace w_1, w_2, …, w_n \\rbrace \\)，其中\\( w\\) 是标准正交基，即\\( || w ||^2 = 1, w^T_i w_j = 0\\)。</p>\n<p>如果将数据从 n 维降到 n’ 维，即丢弃新坐标系中的部分坐标，则新的坐标系为\\( \\lbrace w_1, w_2, …, w_{n′} \\rbrace \\)，样本点\\( x^{(i)} \\) 在 n’ 维坐标系中的投影为：\\( z^{(i)} = ( z^{(i)}_1, z^{(i)}_2, …, z^{(i)}_{n′}) \\) 。其中，\\( z^{(i)}_j = w^T_j x^{(i)} \\)是 \\( x^{(i)} \\) 在低维坐标系里第 j 维的坐标。</p>\n<p>对于任意一个样本\\( x^{(i)} \\)，在新的坐标系中的投影为\\( W^T x^{(i)} \\)，在新坐标系中的投影方差为\\( W^T x^{(i)} x^{(i)T}W \\)，要使所有的样本的投影方差和最大，也就是最大化\\( \\sum_{i=1}^m W^T x^{(i)} x^{(i)T} W \\)，即：\n\\[\n\\arg\\max_W tr(W^T X X^T W) \\quad \\quad s.t. W^TW=I\n\\]</p>\n<p>观察上一节的基于最小投影距离的优化目标，可以发现完全一样，只是一个是加负号的最小化，一个是最大化。</p>\n<p>利用拉格朗日函数可以得到\n\\[\nJ(W) = tr( W^T X X^T W) + \\lambda (W^T W − I)\n\\]</p>\n<p>对\\( W \\) 求导有 \\( XX^T W + \\lambda W = 0 \\)，整理下即为：\n\\[\nXX^TW = (−\\lambda)W\n\\]</p>\n<p>和上面一样可以看出，\\(W\\)为\\(XX^T\\)的 n’ 个特征向量组成的矩阵，而 \\( −\\lambda \\) 为\\( XX^T\\) 的特征值。将数据集从 n 维降到 n’ 维时，需要找到最大的 n’ 个特征值对应的特征向量。这 n’ 个特征向量组成的矩阵\\( W \\)即为需要的矩阵。对于原始数据集，只需要用\\( z^{(i)} = W^T x^{(i)} \\)，就可以把原始数据集降维到最小投影距离的 n’ 维数据集。</p>\n<h4 id=\"2-3-PCA算法流程\"><a href=\"#2-3-PCA算法流程\" class=\"headerlink\" title=\"2.3 PCA算法流程\"></a>2.3 PCA算法流程</h4><p>输入: n 维样本集 \\( D = ( x^{(1)}, x^{(2)}, …, x^{(m)} ) \\)，要降维到的维数 n’。</p>\n<p>输出: 降维后的样本集 \\( D′ \\)</p>\n<ol>\n<li>对所有的样本进行中心化： \\( x^{(i)} = x^{(i)} − \\frac{1}{m} \\sum_{j=1}^m x^{(j)}  \\)</li>\n<li>计算样本的协方差矩阵\\( XX^T \\)</li>\n<li>对矩阵\\( XX^T \\)进行特征值分解</li>\n<li>取出最大的 n’ 个特征值对应的特征向量\\( (w_1, w_2, …, w_{n′}) \\)，将所有的特征向量标准化后，组成特征向量矩阵\\(W\\)</li>\n<li>对样本集中的每一个样本\\( x^{(i)}\\)，转化为新的样本\\( z^{(i)} = W^T x^{(i)} \\)</li>\n<li>得到输出样本集 \\( D′=(z^{(1)}, z^{(2)}, …, z^{(m)} ) \\)</li>\n</ol>\n<p>有时候，我们不指定降维后的 n’ 的值，而是换种方式，指定一个降维到的主成分比重阈值 t 。这个阈值 t 在（0,1] 之间。假如我们的n个特征值为 \\( \\lambda_1 \\geq \\lambda_2 \\geq … \\geq \\lambda_n \\)，则 n’ 可以通过下式得到:\n\\[\n\\sum_{i=1}^{n′} \\lambda_i / \\sum_{i=1}^n \\lambda_i \\geq t\n\\]</p>\n<h4 id=\"2-4-PCA实例\"><a href=\"#2-4-PCA实例\" class=\"headerlink\" title=\"2.4 PCA实例\"></a>2.4 PCA实例</h4><p>假设我们的数据集有 10 个二维数据需要用PCA降到1维特征，数据如下：</p>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">2.5</span>, <span class=\"number\">2.4</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.7</span>), (<span class=\"number\">2.2</span>, <span class=\"number\">2.9</span>), (<span class=\"number\">1.9</span>, <span class=\"number\">2.2</span>), (<span class=\"number\">3.1</span>, <span class=\"number\">3.0</span>), </span><br><span class=\"line\">(<span class=\"number\">2.3</span>, <span class=\"number\">2.7</span>), (<span class=\"number\">2</span>  , <span class=\"number\">1.6</span>), (<span class=\"number\">1</span>  , <span class=\"number\">1.1</span>), (<span class=\"number\">1.5</span>, <span class=\"number\">1.6</span>), (<span class=\"number\">1.1</span>, <span class=\"number\">0.9</span>)</span><br></pre></td></tr></table></figure>\n<p>　\n首先对样本中心化，这里样本的均值为 (1.81, 1.91), 所有的样本减去这个均值后，即中心化后的数据集为</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(0<span class=\"selector-class\">.69</span>, 0<span class=\"selector-class\">.49</span>), (<span class=\"selector-tag\">-1</span><span class=\"selector-class\">.31</span>, <span class=\"selector-tag\">-1</span><span class=\"selector-class\">.21</span>), ( 0<span class=\"selector-class\">.39</span>,  0<span class=\"selector-class\">.99</span>), ( 0<span class=\"selector-class\">.09</span>,  0<span class=\"selector-class\">.29</span>), ( 1<span class=\"selector-class\">.29</span>,  1<span class=\"selector-class\">.09</span>), </span><br><span class=\"line\">(0<span class=\"selector-class\">.49</span>, 0<span class=\"selector-class\">.79</span>), ( 0<span class=\"selector-class\">.19</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.81</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.81</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>, <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.31</span>), (<span class=\"selector-tag\">-0</span><span class=\"selector-class\">.71</span>, <span class=\"selector-tag\">-1</span><span class=\"selector-class\">.01</span>)</span><br></pre></td></tr></table></figure>\n<p>现在开始求样本的协方差矩阵，由于数据是二维的，则协方差矩阵为：\n\\[\nXX^T = \n\\left( \n\\begin{matrix}\ncov(x1,x1) &amp; cov(x2,x1) \\\\ cov(x1,x2) &amp; cov(x2,x2) \n\\end{matrix}\n\\right)\n\\]</p>\n<p>对于上述数据，求出协方差矩阵为：\n\\[\nXX^T = \n\\left(\n\\begin{matrix}\n0.616555556 &amp; 0.615444444 \\\\ 0.615444444 &amp; 0.716555556\n\\end{matrix}\n\\right)\n\\]</p>\n<p>求出特征值为 \\(（0.0490834, 1.28402771）\\)，对应的特征向量分别为：\\( (-0.73517866, -0.6778734)^T, (0.6778734, -0.73517866)^T \\)，\n由于最大的 k=1 个特征值为 1.28402771，对于的k=1个特征向量为 \\( (0.6778734, -0.73517866)^T \\)。\n则 \\( W=(0.6778734, -0.73517866)^T \\)。</p>\n<p>对所有的数据集进行投影\\( z^{(i)} = W^T x^{(i)} \\)[乘的是中心化后的x]，得到PCA降维后的 10 个一维数据集为：\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> 0<span class=\"selector-class\">.1074951</span>   0<span class=\"selector-class\">.00155202</span> <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.46345624</span> <span class=\"selector-tag\">-0</span><span class=\"selector-class\">.1521932</span>   0<span class=\"selector-class\">.07311195</span> </span><br><span class=\"line\"><span class=\"selector-tag\">-0</span><span class=\"selector-class\">.24863317</span>  0<span class=\"selector-class\">.35670133</span>  0<span class=\"selector-class\">.04641726</span>  0<span class=\"selector-class\">.01776463</span>  0<span class=\"selector-class\">.26124033</span></span><br></pre></td></tr></table></figure></p>\n<blockquote>\n<p>下面是例子的Python代码</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">originalData = np.array([</span><br><span class=\"line\">\t[ <span class=\"number\">2.5</span>,  <span class=\"number\">0.5</span>,  <span class=\"number\">2.2</span>,  <span class=\"number\">1.9</span>,  <span class=\"number\">3.1</span>,  <span class=\"number\">2.3</span>,  <span class=\"number\">2.</span> ,  <span class=\"number\">1.</span> ,  <span class=\"number\">1.5</span>,  <span class=\"number\">1.1</span>,],</span><br><span class=\"line\">\t[ <span class=\"number\">2.4</span>,  <span class=\"number\">0.7</span>,  <span class=\"number\">2.9</span>,  <span class=\"number\">2.2</span>,  <span class=\"number\">3.</span> ,  <span class=\"number\">2.7</span>,  <span class=\"number\">1.6</span>,  <span class=\"number\">1.1</span>,  <span class=\"number\">1.6</span>,  <span class=\"number\">0.9</span>]])</span><br><span class=\"line\"></span><br><span class=\"line\">mean = np.mean(originalData, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">print(<span class=\"string\">\"均值：\"</span>)</span><br><span class=\"line\">print(mean) <span class=\"comment\"># [ 1.81  1.91]</span></span><br><span class=\"line\"></span><br><span class=\"line\">preData = (originalData.T - mean).T</span><br><span class=\"line\">print(<span class=\"string\">\"中心化后数据：\"</span>)</span><br><span class=\"line\">print(preData)</span><br><span class=\"line\"><span class=\"comment\"># [[ 0.69 -1.31  0.39  0.09  1.29  0.49  0.19 -0.81 -0.31 -0.71]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.49 -1.21  0.99  0.29  1.09  0.79 -0.31 -0.81 -0.31 -1.01]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">cov = np.cov(preData)</span><br><span class=\"line\">print(<span class=\"string\">\"协方差矩阵：\"</span>)</span><br><span class=\"line\">print(cov)</span><br><span class=\"line\"><span class=\"comment\"># [[ 0.61655556  0.61544444]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.61544444  0.71655556]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">feature_value, feature_vector = np.linalg.eig(cov)</span><br><span class=\"line\">print(<span class=\"string\">\"特征值：\"</span>)</span><br><span class=\"line\">print(feature_value) <span class=\"comment\"># [ 0.0490834   1.28402771]</span></span><br><span class=\"line\">print(<span class=\"string\">\"特征向量:\"</span>)</span><br><span class=\"line\">print(feature_vector)</span><br><span class=\"line\"><span class=\"comment\"># [[-0.73517866 -0.6778734 ]</span></span><br><span class=\"line\"><span class=\"comment\">#  [ 0.6778734  -0.73517866]]</span></span><br><span class=\"line\"></span><br><span class=\"line\">maxIndex = <span class=\"number\">1</span></span><br><span class=\"line\">W = feature_vector[maxIndex, :]</span><br><span class=\"line\">redData = W.T.dot(preData)</span><br><span class=\"line\">print(<span class=\"string\">\"降维后数据：\"</span>)</span><br><span class=\"line\">print(redData)</span><br><span class=\"line\"><span class=\"comment\"># [ 0.1074951   0.00155202 -0.46345624 -0.1521932   0.07311195 -0.24863317</span></span><br><span class=\"line\"><span class=\"comment\">#   0.35670133  0.04641726  0.01776463  0.26124033]</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"2-5-PCA总结\"><a href=\"#2-5-PCA总结\" class=\"headerlink\" title=\"2.5 PCA总结\"></a>2.5 PCA总结</h4><p>作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，为解决非线性降维的KPCA、为解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p>\n<p>PCA算法的主要优点有：</p>\n<ul>\n<li>仅仅需要以方差衡量信息量，不受数据集以外的因素影响</li>\n<li>各主成分之间正交，可消除原始数据成分间的相互影响的因素</li>\n<li>计算方法简单，主要运算是特征值分解，易于实现</li>\n</ul>\n<p>PCA算法的主要缺点有：</p>\n<ul>\n<li>主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强</li>\n<li>方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响</li>\n</ul>\n<h4 id=\"2-6-PCA实现方面\"><a href=\"#2-6-PCA实现方面\" class=\"headerlink\" title=\"2.6 PCA实现方面\"></a>2.6 PCA实现方面</h4><p>PCA降维需要找到样本协方差矩阵\\( X^TX \\)的最大的 d 个特征向量，然后用这最大的 d 个特征向量张成的矩阵来做低维投影降维。\n可以看出，在这个过程中需要先求出协方差矩阵\\( X^TX\\)，当样本数多样本特征数也多的时候，这个计算量是很大的。</p>\n<p>注意到SVD也可以得到协方差矩阵\\( X^TX \\)最大的 d 个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵\\(X^TX\\)，也能求出右奇异矩阵\\( V \\)。\n也就是说，PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。\n实际上，<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\" target=\"_blank\" rel=\"noopener\">scikit-learn</a>的PCA算法的背后真正的实现就是用的SVD，而不是暴力特征分解。</p>\n<p><em>另一方面，注意到PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？</em></p>\n<p>假设样本是 m×n 的矩阵 \\(X\\)，如果通过SVD找到了矩阵\\( XX^T \\)最大的 d 个特征向量张成的 m×d 维矩阵\\(U\\)，则如果进行如下处理：\n\\[\nX′_{d×n} = U^T_{d×m} X_{m×n}\n\\]</p>\n<p>可以得到一个 d×n 的矩阵\\( X’ \\)，这个矩阵和原来的 m×n 维样本矩阵\\( X \\)相比，行数从 m 减到了 k ，可见对行数进行了压缩。\n也就是说，<strong>左奇异矩阵可以用于行数的压缩</strong>；相对的，<strong>右奇异矩阵可以用于列数即特征维度的压缩</strong>，也就是PCA降维。</p>\n<p>&gt;\n<a href=\"http://www.cnblogs.com/pinard/p/6239403.html\" target=\"_blank\" rel=\"noopener\">主成分分析（PCA）原理总结</a></p>\n<hr>\n<h3 id=\"3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant\"><a href=\"#3-LDA-线性判别分析-Linear-Discriminant-Analysis-Fisher-Linear-Discriminant\" class=\"headerlink\" title=\"3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)\"></a>3. LDA 线性判别分析(Linear Discriminant Analysis | Fisher Linear Discriminant)</h3><p>LDA是一种有监督的线性降维算法，与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。</p>\n<p>LDA将训练集中的点映射到一条直线上，（1）使得相同类别中的点尽可能靠在一起，（2）属于不同类别的点尽可能离得比较远。\n我们的目标就是找到一条直线，尽可能满足上面的要求。</p>\n<p>来看一个例子：两类会这样被降维</p>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/3-1.jpg\" alt=\"lda\"></p>\n<p>设数据集\\( D=\\lbrace (x_i, y_i) \\rbrace^m_{i=1} \\)，投影向量为\\(w\\)，则点\\(x_i\\)经过投影后为\\( y=w^Tx_i\\)，投影前的样本中心点为\\( u \\)，投影后的中心点为\\( \\bar{u} = w^T u\\)。</p>\n<p>希望投影后不同类别的样本尽量离得较远：使用度量值\n\\[\n|| \\hat{u}_0 - \\hat{u}_1 ||^2_2\n\\]</p>\n<p>同时希望投影后相同类别的样本之间尽量离得较近：使用度量值\n\\[\n\\sum_{y \\in Y_i} (y - \\hat{u}_i)^2\n\\]</p>\n<p>这个值其实就是投影后样本的<strong>方差</strong>乘以此类样本集合中样本的数量。</p>\n<p>所以总的优化目标函数为：\n\\[\nJ(W) = \\frac{|| \\hat{u}_0 - \\hat{u}_1 ||^2_2}{\\sum_{y\\in Y_i} ( y - \\hat{u}_i )^2} = \\frac{|| \\hat{u}_0 - \\hat{u}_1 ||^2_2}{\\sum_0 ( y - \\hat{u}_0 )^2 + \\sum_1 (y - \\hat{u}_1)^2}\n\\]</p>\n<p>目标\\(J(W)\\)当然是越大越好；</p>\n<p>定义类内散度矩阵为: \\( S_w = \\sum_0 + \\sum_1 = \\sum_{X_0} (x-u_0)(x-u_0)^T + \\sum_{X_1} (x-u_1)(x-u_1)^T \\)</p>\n<p>定义类间散度矩阵：\\( S_b = (u_0 - u_1)(u_0 - u_1)^T \\)</p>\n<p>分子：\\( || \\hat{u}_0 - \\hat{u}_1 ||^2_2 = w^T (u_0 - u_1)(u_0 - u_1)^T w = w^TS_bw \\)</p>\n<p>分母：\\( \\sum_0 (y-\\hat{u}_0)^2 + \\sum_1 (y-\\hat{u}_1)^2 = w^T S_w w \\)</p>\n<p>所以\\( J(w)= w^TS_bw / w^T S_w w \\)，因为向量\\( w \\)的长度成比例改变不影响\\( J(W) \\)的取值，所以我们令\\( w^TS_ww=1 \\)，那么原优化目标就变为\n\\[\n\\min_{w} J(W) = - w^T S_b w, \\quad \\quad s.t. \\quad w^TS_ww = 1\n\\]</p>\n<p>这里直接使用拉格朗日乘子法就可以了，解得\n\\[\nS_b w = \\lambda w S_w\n\\]</p>\n<p>因为\\( S_bw = (u_0-u_1)(u_0-u_1)^Tw = (u_0 - u_1) \\lambda_t \\)，\n所以\\( (u_0 - u_1)\\lambda_t = \\lambda w S_w \\)，可以得到\n\\[\nw = S_w^{-1} (u_0 - u_1)\n\\]</p>\n<p><strong>LDA局限性：</strong></p>\n<ol>\n<li>当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出</li>\n<li>LDA不适合对非高斯分布的样本进行降维</li>\n<li>LDA在样本分类信息依赖方差而不是均值时，效果不好</li>\n<li>LDA可能过度拟合数据</li>\n</ol>\n<h3 id=\"4-PCA与LDA的异同\"><a href=\"#4-PCA与LDA的异同\" class=\"headerlink\" title=\"4. PCA与LDA的异同\"></a>4. PCA与LDA的异同</h3><ul>\n<li><p><strong>出发思想不同。</strong>PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向(在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好)；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。</p>\n</li>\n<li><p><strong>学习模式不同。</strong>PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。</p>\n</li>\n<li><p><strong>降维后可用维度数量不同。</strong>LDA降维后最多可生成 C-1 维子空间(分类标签数-1)，因此LDA与原始维度 N 数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。</p>\n</li>\n<li><p><strong>投影的坐标系不一定相同。</strong>PCA投影的坐标系都是正交的；LDA关注分类能力，不保证投影到的坐标系是正交的。</p>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-05-18-dimreduction/4-1.jpg\" alt=\"pca&amp;lda\"></p>\n<p>上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便(降低了维数并能最大限度的保持原有信息)，但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了(在低维上就可以区分，减少了运算量)。</p>\n<p>&gt;\n<a href=\"https://blog.csdn.net/dongyanwen6036/article/details/78311071\" target=\"_blank\" rel=\"noopener\">LDA与PCA都是常用的降维方法，二者的区别</a></p>\n"},{"layout":"post","title":"机器学习评价指标","date":"2018-09-29T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n\n- 混淆矩阵 Confusion Matrix\n\n<table>\n<tr>\n <th></th>\n <th>实际负例</th>\n <th>实际正例</th>\n</tr>\n\n<tr>\n <td>预测负例</td>\n <td>TN  (将负例预测为负例个数)</td>\n <td>FN  (将正例预测为负例个数)</td>\n</tr>\n\n<tr>\n <td>实际正例</td>\n <td>FP  (将负例预测为正例个数)</td>\n <td>TP  (将正例预测为正例个数)</td>\n</tr>\n</table>\n\n\n- 召回率 Recall\n\n$$ Recall = \\frac{TP}{TP + FN} $$\n\n\n- 精确率 Precision\n\n$$ Precision = \\frac{TP}{TP + FP} $$\n\n\n- 准确率 Accuracy\n\n$$ Accuracy = \\frac{TP + FN}{TP + TN + FP + FN} $$\n\n\n- 调和均值 F-score\n\n$$ F-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall} $$\n\n\n- True Positive Rate (TPR)\n    - 代表将正例分对的概率\n\n$$ TPR = \\frac{TP}{TP + FN} $$\n\n- False Positive Rate (FPR)\n    - 代表将负例错分为正例的概率\n\n$$ FPR = \\frac{FP}{FP + TN} $$\n\n\n- 受试者曲线 ROC\n    - 在ROC空间中，每个点的横坐标是 **FPR**，纵坐标是 **TPR**，这也就描绘了分类器在 TP(真正的正例) 和 FP(错误的正例) 间的 trade-off。\n    - ROC的主要分析工具是一个画在ROC空间的曲线(ROC curve)，对于二分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正例或负例（比如大于阈值划分为正类）。\n    - 因此可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。\n    - ROC curve经过(0,0)、(1,1)，实际上(0,0)和(1,1)连线形成的ROC curve实际上代表的是一个随机分类器；一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方。\n\n\n- 受试者曲线下面积 AUC\n    - 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。\n    - 于是 AUC (Area Under roc Curve)就出现了。AUC的值就是处于ROC curve下方的那部分面积的大小；通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。\n\n","source":"_posts/2018-09-29-machinelearningevaluate.md","raw":"---\nlayout: post\ntitle: 机器学习评价指标\ndate: 2018-09-29 12:10 +0800\ncategories: 基础知识\ntags:\n- 评估方法\nmathjax: true\ncopyright: false\n---\n\n\n- 混淆矩阵 Confusion Matrix\n\n<table>\n<tr>\n <th></th>\n <th>实际负例</th>\n <th>实际正例</th>\n</tr>\n\n<tr>\n <td>预测负例</td>\n <td>TN  (将负例预测为负例个数)</td>\n <td>FN  (将正例预测为负例个数)</td>\n</tr>\n\n<tr>\n <td>实际正例</td>\n <td>FP  (将负例预测为正例个数)</td>\n <td>TP  (将正例预测为正例个数)</td>\n</tr>\n</table>\n\n\n- 召回率 Recall\n\n$$ Recall = \\frac{TP}{TP + FN} $$\n\n\n- 精确率 Precision\n\n$$ Precision = \\frac{TP}{TP + FP} $$\n\n\n- 准确率 Accuracy\n\n$$ Accuracy = \\frac{TP + FN}{TP + TN + FP + FN} $$\n\n\n- 调和均值 F-score\n\n$$ F-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall} $$\n\n\n- True Positive Rate (TPR)\n    - 代表将正例分对的概率\n\n$$ TPR = \\frac{TP}{TP + FN} $$\n\n- False Positive Rate (FPR)\n    - 代表将负例错分为正例的概率\n\n$$ FPR = \\frac{FP}{FP + TN} $$\n\n\n- 受试者曲线 ROC\n    - 在ROC空间中，每个点的横坐标是 **FPR**，纵坐标是 **TPR**，这也就描绘了分类器在 TP(真正的正例) 和 FP(错误的正例) 间的 trade-off。\n    - ROC的主要分析工具是一个画在ROC空间的曲线(ROC curve)，对于二分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正例或负例（比如大于阈值划分为正类）。\n    - 因此可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。\n    - ROC curve经过(0,0)、(1,1)，实际上(0,0)和(1,1)连线形成的ROC curve实际上代表的是一个随机分类器；一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方。\n\n\n- 受试者曲线下面积 AUC\n    - 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。\n    - 于是 AUC (Area Under roc Curve)就出现了。AUC的值就是处于ROC curve下方的那部分面积的大小；通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。\n\n","slug":"machinelearningevaluate","published":1,"updated":"2019-08-17T09:39:45.865Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwg00352qwpybnxardx","content":"<ul>\n<li>混淆矩阵 Confusion Matrix</li>\n</ul><table>\n<tr>\n <th></th>\n <th>实际负例</th>\n <th>实际正例</th>\n</tr>\n\n<tr>\n <td>预测负例</td>\n <td>TN  (将负例预测为负例个数)</td>\n <td>FN  (将正例预测为负例个数)</td>\n</tr>\n\n<tr>\n <td>实际正例</td>\n <td>FP  (将负例预测为正例个数)</td>\n <td>TP  (将正例预测为正例个数)</td>\n</tr>\n</table><ul>\n<li>召回率 Recall</li>\n</ul><script type=\"math/tex; mode=display\">Recall = \\frac{TP}{TP + FN}</script><ul>\n<li>精确率 Precision</li>\n</ul><script type=\"math/tex; mode=display\">Precision = \\frac{TP}{TP + FP}</script><ul>\n<li>准确率 Accuracy</li>\n</ul><script type=\"math/tex; mode=display\">Accuracy = \\frac{TP + FN}{TP + TN + FP + FN}</script><ul>\n<li>调和均值 F-score</li>\n</ul><script type=\"math/tex; mode=display\">F-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}</script><a id=\"more\"></a>\n\n\n\n\n\n\n\n<ul>\n<li>True Positive Rate (TPR)<ul>\n<li>代表将正例分对的概率</li>\n</ul>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">TPR = \\frac{TP}{TP + FN}</script><ul>\n<li>False Positive Rate (FPR)<ul>\n<li>代表将负例错分为正例的概率</li>\n</ul>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">FPR = \\frac{FP}{FP + TN}</script><ul>\n<li>受试者曲线 ROC<ul>\n<li>在ROC空间中，每个点的横坐标是 <strong>FPR</strong>，纵坐标是 <strong>TPR</strong>，这也就描绘了分类器在 TP(真正的正例) 和 FP(错误的正例) 间的 trade-off。</li>\n<li>ROC的主要分析工具是一个画在ROC空间的曲线(ROC curve)，对于二分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正例或负例（比如大于阈值划分为正类）。</li>\n<li>因此可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。</li>\n<li>ROC curve经过(0,0)、(1,1)，实际上(0,0)和(1,1)连线形成的ROC curve实际上代表的是一个随机分类器；一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方。</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>受试者曲线下面积 AUC<ul>\n<li>用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。</li>\n<li>于是 AUC (Area Under roc Curve)就出现了。AUC的值就是处于ROC curve下方的那部分面积的大小；通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<ul>\n<li>混淆矩阵 Confusion Matrix</li>\n</ul><table>\n<tr>\n <th></th>\n <th>实际负例</th>\n <th>实际正例</th>\n</tr>\n\n<tr>\n <td>预测负例</td>\n <td>TN  (将负例预测为负例个数)</td>\n <td>FN  (将正例预测为负例个数)</td>\n</tr>\n\n<tr>\n <td>实际正例</td>\n <td>FP  (将负例预测为正例个数)</td>\n <td>TP  (将正例预测为正例个数)</td>\n</tr>\n</table><ul>\n<li>召回率 Recall</li>\n</ul><script type=\"math/tex; mode=display\">Recall = \\frac{TP}{TP + FN}</script><ul>\n<li>精确率 Precision</li>\n</ul><script type=\"math/tex; mode=display\">Precision = \\frac{TP}{TP + FP}</script><ul>\n<li>准确率 Accuracy</li>\n</ul><script type=\"math/tex; mode=display\">Accuracy = \\frac{TP + FN}{TP + TN + FP + FN}</script><ul>\n<li>调和均值 F-score</li>\n</ul><script type=\"math/tex; mode=display\">F-score = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}</script>","more":"\n\n\n\n\n\n\n\n<ul>\n<li>True Positive Rate (TPR)<ul>\n<li>代表将正例分对的概率</li>\n</ul>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">TPR = \\frac{TP}{TP + FN}</script><ul>\n<li>False Positive Rate (FPR)<ul>\n<li>代表将负例错分为正例的概率</li>\n</ul>\n</li>\n</ul>\n<script type=\"math/tex; mode=display\">FPR = \\frac{FP}{FP + TN}</script><ul>\n<li>受试者曲线 ROC<ul>\n<li>在ROC空间中，每个点的横坐标是 <strong>FPR</strong>，纵坐标是 <strong>TPR</strong>，这也就描绘了分类器在 TP(真正的正例) 和 FP(错误的正例) 间的 trade-off。</li>\n<li>ROC的主要分析工具是一个画在ROC空间的曲线(ROC curve)，对于二分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正例或负例（比如大于阈值划分为正类）。</li>\n<li>因此可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。</li>\n<li>ROC curve经过(0,0)、(1,1)，实际上(0,0)和(1,1)连线形成的ROC curve实际上代表的是一个随机分类器；一般情况下，这个曲线都应该处于(0,0)和(1,1)连线的上方。</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>受试者曲线下面积 AUC<ul>\n<li>用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。</li>\n<li>于是 AUC (Area Under roc Curve)就出现了。AUC的值就是处于ROC curve下方的那部分面积的大小；通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。</li>\n</ul>\n</li>\n</ul>\n"},{"layout":"post","title":"递归神经网络 - GRU","date":"2018-10-17T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n### GRU神经单元\n\nGRU神经单元与[LSTM神经单元](/2018/06/rnn1/)及其相似，可以二者相互对比着学习。\nGRU可以看成是LSTM的变种，GRU把LSTM中的遗忘门(forget gate)和输入门(input gate)用更新门(update gate)来替代。\n把cell state$C_t$和隐状态$h_t$进行合并，在计算当前时刻新信息的方法和LSTM有所不同。\n\n![GRU结构图](/posts_res/2018-10-17-rnn2/gru.jpg)\n\n**重置门**\n\n$$ r_t = \\sigma (W_r x_t + U_t h_{t-1} ) $$\n\n**更新门**\n\n$$ z_t = \\sigma ( W_z x_t + U_z h_{t-1} ) $$\n\n**更新状态**\n\n计算候选隐藏层(candidate hidden layer)$\\hat{h}_t$，这个时候候选隐藏层和LSTM中的$\\hat{C}_t$是类似的。\n可以看成是当前时刻的新信息，其中$r_t$用来控制需要保留多少之前的记忆，如果$r_t$为$0$，那么$\\hat{h}_t$只包含当前词的信息。\n\n$$ \\hat{h}_t = tanh ( W x_t + U (r_t \\odot h_{t-1} ) $$\n\n最后$z_t$控制需要从前一时刻的隐藏层$h_{t−1}$中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$\\hat{h}_t$，最后得到$h_t$，直接得到最后输出的隐藏层信息，这里与LSTM的区别是GRU中没有output gate。\n\n$$ h_t = (1-z_t) h_{t-1} + z_t \\hat{h}_y $$\n\n\n如果reset gate接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关的信息；\n\nupdate gate控制当前时刻的隐藏层输出$h_t$需要保留多少之前的隐藏层信息，若$z_t$接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。\n\n一般来说那些具有短距离依赖的单元reset gate比较活跃（如果$r_t$为1，而$z_t$为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元update gate比较活跃。\n\n\n>\n1. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)\n2. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)\n","source":"_posts/2018-10-17-rnn2.md","raw":"---\nlayout: post\ntitle: 递归神经网络 - GRU\ndate: 2018-10-17 12:10 +0800\ncategories: 深度学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: false\n---\n\n### GRU神经单元\n\nGRU神经单元与[LSTM神经单元](/2018/06/rnn1/)及其相似，可以二者相互对比着学习。\nGRU可以看成是LSTM的变种，GRU把LSTM中的遗忘门(forget gate)和输入门(input gate)用更新门(update gate)来替代。\n把cell state$C_t$和隐状态$h_t$进行合并，在计算当前时刻新信息的方法和LSTM有所不同。\n\n![GRU结构图](/posts_res/2018-10-17-rnn2/gru.jpg)\n\n**重置门**\n\n$$ r_t = \\sigma (W_r x_t + U_t h_{t-1} ) $$\n\n**更新门**\n\n$$ z_t = \\sigma ( W_z x_t + U_z h_{t-1} ) $$\n\n**更新状态**\n\n计算候选隐藏层(candidate hidden layer)$\\hat{h}_t$，这个时候候选隐藏层和LSTM中的$\\hat{C}_t$是类似的。\n可以看成是当前时刻的新信息，其中$r_t$用来控制需要保留多少之前的记忆，如果$r_t$为$0$，那么$\\hat{h}_t$只包含当前词的信息。\n\n$$ \\hat{h}_t = tanh ( W x_t + U (r_t \\odot h_{t-1} ) $$\n\n最后$z_t$控制需要从前一时刻的隐藏层$h_{t−1}$中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$\\hat{h}_t$，最后得到$h_t$，直接得到最后输出的隐藏层信息，这里与LSTM的区别是GRU中没有output gate。\n\n$$ h_t = (1-z_t) h_{t-1} + z_t \\hat{h}_y $$\n\n\n如果reset gate接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关的信息；\n\nupdate gate控制当前时刻的隐藏层输出$h_t$需要保留多少之前的隐藏层信息，若$z_t$接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。\n\n一般来说那些具有短距离依赖的单元reset gate比较活跃（如果$r_t$为1，而$z_t$为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元update gate比较活跃。\n\n\n>\n1. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)\n2. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)\n","slug":"rnn2","published":1,"updated":"2019-08-17T09:40:10.283Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwh00392qwpelcgbfmy","content":"<h3 id=\"GRU神经单元\"><a href=\"#GRU神经单元\" class=\"headerlink\" title=\"GRU神经单元\"></a>GRU神经单元</h3><p>GRU神经单元与<a href=\"/2018/06/rnn1/\">LSTM神经单元</a>及其相似，可以二者相互对比着学习。\nGRU可以看成是LSTM的变种，GRU把LSTM中的遗忘门(forget gate)和输入门(input gate)用更新门(update gate)来替代。\n把cell state$C_t$和隐状态$h_t$进行合并，在计算当前时刻新信息的方法和LSTM有所不同。</p><p><img src=\"/posts_res/2018-10-17-rnn2/gru.jpg\" alt=\"GRU结构图\"></p><p><strong>重置门</strong></p><script type=\"math/tex; mode=display\">r_t = \\sigma (W_r x_t + U_t h_{t-1} )</script><p><strong>更新门</strong></p><script type=\"math/tex; mode=display\">z_t = \\sigma ( W_z x_t + U_z h_{t-1} )</script><p><strong>更新状态</strong></p><p>计算候选隐藏层(candidate hidden layer)$\\hat{h}_t$，这个时候候选隐藏层和LSTM中的$\\hat{C}_t$是类似的。\n可以看成是当前时刻的新信息，其中$r_t$用来控制需要保留多少之前的记忆，如果$r_t$为$0$，那么$\\hat{h}_t$只包含当前词的信息。</p><a id=\"more\"></a>\n\n\n\n\n\n<script type=\"math/tex; mode=display\">\\hat{h}_t = tanh ( W x_t + U (r_t \\odot h_{t-1} )</script><p>最后$z<em>t$控制需要从前一时刻的隐藏层$h</em>{t−1}$中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$\\hat{h}_t$，最后得到$h_t$，直接得到最后输出的隐藏层信息，这里与LSTM的区别是GRU中没有output gate。</p>\n<script type=\"math/tex; mode=display\">h_t = (1-z_t) h_{t-1} + z_t \\hat{h}_y</script><p>如果reset gate接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关的信息；</p>\n<p>update gate控制当前时刻的隐藏层输出$h_t$需要保留多少之前的隐藏层信息，若$z_t$接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。</p>\n<p>一般来说那些具有短距离依赖的单元reset gate比较活跃（如果$r_t$为1，而$z_t$为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元update gate比较活跃。</p>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1412.3555.pdf\" target=\"_blank\" rel=\"noopener\">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li>\n<li><a href=\"https://arxiv.org/pdf/1406.1078.pdf\" target=\"_blank\" rel=\"noopener\">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"GRU神经单元\"><a href=\"#GRU神经单元\" class=\"headerlink\" title=\"GRU神经单元\"></a>GRU神经单元</h3><p>GRU神经单元与<a href=\"/2018/06/rnn1/\">LSTM神经单元</a>及其相似，可以二者相互对比着学习。\nGRU可以看成是LSTM的变种，GRU把LSTM中的遗忘门(forget gate)和输入门(input gate)用更新门(update gate)来替代。\n把cell state$C_t$和隐状态$h_t$进行合并，在计算当前时刻新信息的方法和LSTM有所不同。</p><p><img src=\"/posts_res/2018-10-17-rnn2/gru.jpg\" alt=\"GRU结构图\"></p><p><strong>重置门</strong></p><script type=\"math/tex; mode=display\">r_t = \\sigma (W_r x_t + U_t h_{t-1} )</script><p><strong>更新门</strong></p><script type=\"math/tex; mode=display\">z_t = \\sigma ( W_z x_t + U_z h_{t-1} )</script><p><strong>更新状态</strong></p><p>计算候选隐藏层(candidate hidden layer)$\\hat{h}_t$，这个时候候选隐藏层和LSTM中的$\\hat{C}_t$是类似的。\n可以看成是当前时刻的新信息，其中$r_t$用来控制需要保留多少之前的记忆，如果$r_t$为$0$，那么$\\hat{h}_t$只包含当前词的信息。</p>","more":"\n\n\n\n\n\n<script type=\"math/tex; mode=display\">\\hat{h}_t = tanh ( W x_t + U (r_t \\odot h_{t-1} )</script><p>最后$z<em>t$控制需要从前一时刻的隐藏层$h</em>{t−1}$中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$\\hat{h}_t$，最后得到$h_t$，直接得到最后输出的隐藏层信息，这里与LSTM的区别是GRU中没有output gate。</p>\n<script type=\"math/tex; mode=display\">h_t = (1-z_t) h_{t-1} + z_t \\hat{h}_y</script><p>如果reset gate接近0，那么之前的隐藏层信息就会丢弃，允许模型丢弃一些和未来无关的信息；</p>\n<p>update gate控制当前时刻的隐藏层输出$h_t$需要保留多少之前的隐藏层信息，若$z_t$接近1相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。</p>\n<p>一般来说那些具有短距离依赖的单元reset gate比较活跃（如果$r_t$为1，而$z_t$为0那么相当于变成了一个标准的RNN，能处理短距离依赖），具有长距离依赖的单元update gate比较活跃。</p>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://arxiv.org/pdf/1412.3555.pdf\" target=\"_blank\" rel=\"noopener\">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li>\n<li><a href=\"https://arxiv.org/pdf/1406.1078.pdf\" target=\"_blank\" rel=\"noopener\">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></li>\n</ol>\n"},{"layout":"post","title":"K-Means聚类","date":"2018-10-09T02:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n\n### 1. 距离量度\n\n#### 1.1 两点之间的距离\n\n- 欧式距离\n    - $$ d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} $$\n- 曼哈顿距离\n    - $$ d = | x_1 - x_1 | + | y_1 - y_2 | $$\n- 切比雪夫距离\n    - $$ d = max(| x_1 - x_2 |, | y_1 - y_2 |) $$\n- 余弦距离\n    - $$ cos \\theta = \\frac{x_1 x_2 + y_1 y_2}{\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}} $$\n- Jaccard相似系数\n    - $$ J(A,B) = \\frac{| A \\cap B |}{| A \\cup B |} $$\n- 相关系数\n    - $$ \\rho_{XY} = \\frac{Cov(X, Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}} = \\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)} \\sqrt{D(Y)}} $$\n\n\n#### 1.2 两个类别之间的距离\n\n- 单连接聚类 Single-linkage clustering\n    - 一个类的所有成员到另一个类的所有成员之间的最短两个之间的距离\n- 全连接聚类 Complete-linkage clustering\n    - 两个类中最远的两个点之间的距离\n- 平均连接聚类 Average-linkage clustering\n    - 两个类中的点两两的距离求平均\n\n\n------------\n\n### 2. K-Means聚类\n\n算法思想：\n\n- 选择K个点作为初始质心\n- repeat\n    - 将每个点指派到最近的质心，形成K个簇\n    - 重新计算每个簇的质心\n- until 簇不发生变化或达到最大迭代次数\n\n这里的重新计算每个簇的质心，如何计算是根据目标函数得来的，因此在开始时需要考虑距离度量和目标函数。\n\n<br>\n\n考虑欧几里得距离的数据，使用**误差平方和**（Sum of the Squared Error, SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。\n\n$$ SSE = \\sum_{i=1}^K \\sum_{x \\in c_i} dist(c_i, x)^2 $$\n\n其中$K$表示$K$个聚类中心，$c_i$表示第几个中心，$dist$表示的是欧式距离。\n\n在更新质心时使用所有点的平均值，为什么呢？这里是由SSE决定的。\n\n对第$k$个质心$c_k$求解，最小化式(7)：即对SSE求导并令导数等于零，求解$c_k$，如下：\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial c_k} SSE\n&= \\frac{\\partial}{\\partial c_k} \\sum_{i=1}^K \\sum_{x \\in c_i} (c_i - x)^2 \\\\\n&= \\sum_{i=1}^K \\sum_{x \\in c_i} \\frac{\\partial}{\\partial c_k} (c_i - x)^2 \\\\\n&= \\sum_{x \\in c_k} 2 (c_k - x_k) \\\\\n&= 0\n\\end{aligned}\n\\end{equation}\n$$\n\n$$\n\\sum_{x \\in c_k} 2(c_k - x_k) = 0 \\Rightarrow m_k c_k = \\sum_{c \\in c_k} x_k \\Rightarrow c_k = \\frac{1}{m_k} \\sum_{x \\in c_k} x_k\n$$\n\n这样，正如前面所述，簇的最小化SSE的最佳质心是簇中各点的均值。\n\n\n----------\n\n### 3. K-Means算法的优缺点\n\n- 优点\n    - 易实现\n- 缺点\n    - K值需要预先给定\n    - 对初始选取的聚类中心点敏感\n    - 可能收敛到局部最小值\n    - 在大规模数据收敛慢\n\n进阶学习 [bisecting K-means]()，[DBSCAN]()\n\n\n-----------\n\n### 4. 算法实现\n\n[kmeans.py](/posts_res/2018-10-09-kmeans/kmeans.py)\n\n#### 4.1 运行结果：\n\n![res.jpg](/posts_res/2018-10-09-kmeans/res.jpg)\n\n#### 4.2 代码\n\n```python3\n# coding:utf-8\n\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\n\ndef kmeans(data, k=2):\n    def _distance(p1, p2):\n        tmp = np.sum((p1 - p2) ** 2)\n        return np.sqrt(tmp)\n\n    def _rand_center(data, k):\n        \"\"\"Generate k center within the range of data set.\"\"\"\n        n = data.shape[1]  # features\n        centroids = np.zeros((k, n))  # init with (0,0)....\n        for i in range(n):\n            dmin, dmax = np.min(data[:, i]), np.max(data[:, i])\n            centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k)\n        return centroids\n\n    def _converged(centroids1, centroids2):\n        # if centroids not changed, we say 'converged'\n        set1 = set([tuple(c) for c in centroids1])\n        set2 = set([tuple(c) for c in centroids2])\n        return (set1 == set2)\n\n    n = data.shape[0]  # number of entries\n    centroids = _rand_center(data, k)\n    label = np.zeros(n, dtype=np.int)  # track the nearest centroid\n    assement = np.zeros(n)  # for the assement of our model\n    converged = False\n\n    while not converged:\n        old_centroids = np.copy(centroids)\n        for i in range(n):\n            # determine the nearest centroid and track it with label\n            min_dist, min_index = np.inf, -1\n            for j in range(k):\n                dist = _distance(data[i], centroids[j])\n                if dist < min_dist:\n                    min_dist, min_index = dist, j\n                    label[i] = j\n            assement[i] = _distance(data[i], centroids[label[i]]) ** 2\n\n        # update centroid\n        for m in range(k):\n            centroids[m] = np.mean(data[label == m], axis=0)\n        converged = _converged(old_centroids, centroids)\n    return centroids, label, np.sum(assement)\n\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\ndata = X[:, [1, 3]]  # 为了便于可视化，只取两个维度\n# plt.scatter(data[:,0],data[:,1]);\n\nbest_assement = np.inf\nbest_centroids = None\nbest_label = None\n\nfor i in range(10):\n    centroids, label, assement = kmeans(data, 2)\n    if assement < best_assement:\n        best_assement = assement\n        best_centroids = centroids\n        best_label = label\n\ndata0 = data[best_label == 0]\ndata1 = data[best_label == 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1.scatter(data[:, 0], data[:, 1], c='c', s=30, marker='o')\nax2.scatter(data0[:, 0], data0[:, 1], c='r')\nax2.scatter(data1[:, 0], data1[:, 1], c='c')\nax2.scatter(centroids[:, 0], centroids[:, 1], c='b', s=120, marker='o')\nplt.show()\n```\n\n","source":"_posts/2018-10-09-kmeans.md","raw":"---\nlayout: post\ntitle: K-Means聚类\ndate: 2018-10-09 10:10 +0800\ncategories: 机器学习\ntags:\n- 模型算法\nmathjax: true\ncopyright: true\n---\n\n\n\n### 1. 距离量度\n\n#### 1.1 两点之间的距离\n\n- 欧式距离\n    - $$ d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} $$\n- 曼哈顿距离\n    - $$ d = | x_1 - x_1 | + | y_1 - y_2 | $$\n- 切比雪夫距离\n    - $$ d = max(| x_1 - x_2 |, | y_1 - y_2 |) $$\n- 余弦距离\n    - $$ cos \\theta = \\frac{x_1 x_2 + y_1 y_2}{\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}} $$\n- Jaccard相似系数\n    - $$ J(A,B) = \\frac{| A \\cap B |}{| A \\cup B |} $$\n- 相关系数\n    - $$ \\rho_{XY} = \\frac{Cov(X, Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}} = \\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)} \\sqrt{D(Y)}} $$\n\n\n#### 1.2 两个类别之间的距离\n\n- 单连接聚类 Single-linkage clustering\n    - 一个类的所有成员到另一个类的所有成员之间的最短两个之间的距离\n- 全连接聚类 Complete-linkage clustering\n    - 两个类中最远的两个点之间的距离\n- 平均连接聚类 Average-linkage clustering\n    - 两个类中的点两两的距离求平均\n\n\n------------\n\n### 2. K-Means聚类\n\n算法思想：\n\n- 选择K个点作为初始质心\n- repeat\n    - 将每个点指派到最近的质心，形成K个簇\n    - 重新计算每个簇的质心\n- until 簇不发生变化或达到最大迭代次数\n\n这里的重新计算每个簇的质心，如何计算是根据目标函数得来的，因此在开始时需要考虑距离度量和目标函数。\n\n<br>\n\n考虑欧几里得距离的数据，使用**误差平方和**（Sum of the Squared Error, SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。\n\n$$ SSE = \\sum_{i=1}^K \\sum_{x \\in c_i} dist(c_i, x)^2 $$\n\n其中$K$表示$K$个聚类中心，$c_i$表示第几个中心，$dist$表示的是欧式距离。\n\n在更新质心时使用所有点的平均值，为什么呢？这里是由SSE决定的。\n\n对第$k$个质心$c_k$求解，最小化式(7)：即对SSE求导并令导数等于零，求解$c_k$，如下：\n\n$$\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial c_k} SSE\n&= \\frac{\\partial}{\\partial c_k} \\sum_{i=1}^K \\sum_{x \\in c_i} (c_i - x)^2 \\\\\n&= \\sum_{i=1}^K \\sum_{x \\in c_i} \\frac{\\partial}{\\partial c_k} (c_i - x)^2 \\\\\n&= \\sum_{x \\in c_k} 2 (c_k - x_k) \\\\\n&= 0\n\\end{aligned}\n\\end{equation}\n$$\n\n$$\n\\sum_{x \\in c_k} 2(c_k - x_k) = 0 \\Rightarrow m_k c_k = \\sum_{c \\in c_k} x_k \\Rightarrow c_k = \\frac{1}{m_k} \\sum_{x \\in c_k} x_k\n$$\n\n这样，正如前面所述，簇的最小化SSE的最佳质心是簇中各点的均值。\n\n\n----------\n\n### 3. K-Means算法的优缺点\n\n- 优点\n    - 易实现\n- 缺点\n    - K值需要预先给定\n    - 对初始选取的聚类中心点敏感\n    - 可能收敛到局部最小值\n    - 在大规模数据收敛慢\n\n进阶学习 [bisecting K-means]()，[DBSCAN]()\n\n\n-----------\n\n### 4. 算法实现\n\n[kmeans.py](/posts_res/2018-10-09-kmeans/kmeans.py)\n\n#### 4.1 运行结果：\n\n![res.jpg](/posts_res/2018-10-09-kmeans/res.jpg)\n\n#### 4.2 代码\n\n```python3\n# coding:utf-8\n\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\nimport numpy as np\n\ndef kmeans(data, k=2):\n    def _distance(p1, p2):\n        tmp = np.sum((p1 - p2) ** 2)\n        return np.sqrt(tmp)\n\n    def _rand_center(data, k):\n        \"\"\"Generate k center within the range of data set.\"\"\"\n        n = data.shape[1]  # features\n        centroids = np.zeros((k, n))  # init with (0,0)....\n        for i in range(n):\n            dmin, dmax = np.min(data[:, i]), np.max(data[:, i])\n            centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k)\n        return centroids\n\n    def _converged(centroids1, centroids2):\n        # if centroids not changed, we say 'converged'\n        set1 = set([tuple(c) for c in centroids1])\n        set2 = set([tuple(c) for c in centroids2])\n        return (set1 == set2)\n\n    n = data.shape[0]  # number of entries\n    centroids = _rand_center(data, k)\n    label = np.zeros(n, dtype=np.int)  # track the nearest centroid\n    assement = np.zeros(n)  # for the assement of our model\n    converged = False\n\n    while not converged:\n        old_centroids = np.copy(centroids)\n        for i in range(n):\n            # determine the nearest centroid and track it with label\n            min_dist, min_index = np.inf, -1\n            for j in range(k):\n                dist = _distance(data[i], centroids[j])\n                if dist < min_dist:\n                    min_dist, min_index = dist, j\n                    label[i] = j\n            assement[i] = _distance(data[i], centroids[label[i]]) ** 2\n\n        # update centroid\n        for m in range(k):\n            centroids[m] = np.mean(data[label == m], axis=0)\n        converged = _converged(old_centroids, centroids)\n    return centroids, label, np.sum(assement)\n\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\ndata = X[:, [1, 3]]  # 为了便于可视化，只取两个维度\n# plt.scatter(data[:,0],data[:,1]);\n\nbest_assement = np.inf\nbest_centroids = None\nbest_label = None\n\nfor i in range(10):\n    centroids, label, assement = kmeans(data, 2)\n    if assement < best_assement:\n        best_assement = assement\n        best_centroids = centroids\n        best_label = label\n\ndata0 = data[best_label == 0]\ndata1 = data[best_label == 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\nax1.scatter(data[:, 0], data[:, 1], c='c', s=30, marker='o')\nax2.scatter(data0[:, 0], data0[:, 1], c='r')\nax2.scatter(data1[:, 0], data1[:, 1], c='c')\nax2.scatter(centroids[:, 0], centroids[:, 1], c='b', s=120, marker='o')\nplt.show()\n```\n\n","slug":"kmeans","published":1,"updated":"2019-08-17T09:39:59.950Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwi003c2qwpcbop0pal","content":"<h3 id=\"1-距离量度\"><a href=\"#1-距离量度\" class=\"headerlink\" title=\"1. 距离量度\"></a>1. 距离量度</h3><h4 id=\"1-1-两点之间的距离\"><a href=\"#1-1-两点之间的距离\" class=\"headerlink\" title=\"1.1 两点之间的距离\"></a>1.1 两点之间的距离</h4><ul>\n<li>欧式距离<ul>\n<li><script type=\"math/tex; mode=display\">d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}</script></li>\n</ul>\n</li>\n<li>曼哈顿距离<ul>\n<li><script type=\"math/tex; mode=display\">d = | x_1 - x_1 | + | y_1 - y_2 |</script></li>\n</ul>\n</li>\n<li>切比雪夫距离<ul>\n<li><script type=\"math/tex; mode=display\">d = max(| x_1 - x_2 |, | y_1 - y_2 |)</script></li>\n</ul>\n</li>\n<li>余弦距离<ul>\n<li><script type=\"math/tex; mode=display\">cos \\theta = \\frac{x_1 x_2 + y_1 y_2}{\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}}</script></li>\n</ul>\n</li>\n<li>Jaccard相似系数<ul>\n<li><script type=\"math/tex; mode=display\">J(A,B) = \\frac{| A \\cap B |}{| A \\cup B |}</script></li>\n</ul>\n</li>\n<li>相关系数<ul>\n<li><script type=\"math/tex; mode=display\">\\rho_{XY} = \\frac{Cov(X, Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}} = \\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)} \\sqrt{D(Y)}}</script></li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n<h4 id=\"1-2-两个类别之间的距离\"><a href=\"#1-2-两个类别之间的距离\" class=\"headerlink\" title=\"1.2 两个类别之间的距离\"></a>1.2 两个类别之间的距离</h4><ul>\n<li>单连接聚类 Single-linkage clustering<ul>\n<li>一个类的所有成员到另一个类的所有成员之间的最短两个之间的距离</li>\n</ul>\n</li>\n<li>全连接聚类 Complete-linkage clustering<ul>\n<li>两个类中最远的两个点之间的距离</li>\n</ul>\n</li>\n<li>平均连接聚类 Average-linkage clustering<ul>\n<li>两个类中的点两两的距离求平均</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"2-K-Means聚类\"><a href=\"#2-K-Means聚类\" class=\"headerlink\" title=\"2. K-Means聚类\"></a>2. K-Means聚类</h3><p>算法思想：</p>\n<ul>\n<li>选择K个点作为初始质心</li>\n<li>repeat<ul>\n<li>将每个点指派到最近的质心，形成K个簇</li>\n<li>重新计算每个簇的质心</li>\n</ul>\n</li>\n<li>until 簇不发生变化或达到最大迭代次数</li>\n</ul>\n<p>这里的重新计算每个簇的质心，如何计算是根据目标函数得来的，因此在开始时需要考虑距离度量和目标函数。</p>\n<p><br></p>\n<p>考虑欧几里得距离的数据，使用<strong>误差平方和</strong>（Sum of the Squared Error, SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。</p>\n<script type=\"math/tex; mode=display\">SSE = \\sum_{i=1}^K \\sum_{x \\in c_i} dist(c_i, x)^2</script><p>其中$K$表示$K$个聚类中心，$c_i$表示第几个中心，$dist$表示的是欧式距离。</p>\n<p>在更新质心时使用所有点的平均值，为什么呢？这里是由SSE决定的。</p>\n<p>对第$k$个质心$c_k$求解，最小化式(7)：即对SSE求导并令导数等于零，求解$c_k$，如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial c_k} SSE\n&= \\frac{\\partial}{\\partial c_k} \\sum_{i=1}^K \\sum_{x \\in c_i} (c_i - x)^2 \\\\\n&= \\sum_{i=1}^K \\sum_{x \\in c_i} \\frac{\\partial}{\\partial c_k} (c_i - x)^2 \\\\\n&= \\sum_{x \\in c_k} 2 (c_k - x_k) \\\\\n&= 0\n\\end{aligned}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n\\sum_{x \\in c_k} 2(c_k - x_k) = 0 \\Rightarrow m_k c_k = \\sum_{c \\in c_k} x_k \\Rightarrow c_k = \\frac{1}{m_k} \\sum_{x \\in c_k} x_k</script><p>这样，正如前面所述，簇的最小化SSE的最佳质心是簇中各点的均值。</p>\n<hr>\n<h3 id=\"3-K-Means算法的优缺点\"><a href=\"#3-K-Means算法的优缺点\" class=\"headerlink\" title=\"3. K-Means算法的优缺点\"></a>3. K-Means算法的优缺点</h3><ul>\n<li>优点<ul>\n<li>易实现</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>K值需要预先给定</li>\n<li>对初始选取的聚类中心点敏感</li>\n<li>可能收敛到局部最小值</li>\n<li>在大规模数据收敛慢</li>\n</ul>\n</li>\n</ul>\n<p>进阶学习 <a href>bisecting K-means</a>，<a href>DBSCAN</a></p>\n<hr>\n<h3 id=\"4-算法实现\"><a href=\"#4-算法实现\" class=\"headerlink\" title=\"4. 算法实现\"></a>4. 算法实现</h3><p><a href=\"/posts_res/2018-10-09-kmeans/kmeans.py\">kmeans.py</a></p>\n<h4 id=\"4-1-运行结果：\"><a href=\"#4-1-运行结果：\" class=\"headerlink\" title=\"4.1 运行结果：\"></a>4.1 运行结果：</h4><p><img src=\"/posts_res/2018-10-09-kmeans/res.jpg\" alt=\"res.jpg\"></p>\n<h4 id=\"4-2-代码\"><a href=\"#4-2-代码\" class=\"headerlink\" title=\"4.2 代码\"></a>4.2 代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from matplotlib import pyplot as plt</span><br><span class=\"line\">from sklearn import datasets</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">def kmeans(data, k=2):</span><br><span class=\"line\">    def _distance(p1, p2):</span><br><span class=\"line\">        tmp = np.sum((p1 - p2) ** 2)</span><br><span class=\"line\">        return np.sqrt(tmp)</span><br><span class=\"line\"></span><br><span class=\"line\">    def _rand_center(data, k):</span><br><span class=\"line\">        &quot;&quot;&quot;Generate k center within the range of data set.&quot;&quot;&quot;</span><br><span class=\"line\">        n = data.shape[1]  # features</span><br><span class=\"line\">        centroids = np.zeros((k, n))  # init with (0,0)....</span><br><span class=\"line\">        for i in range(n):</span><br><span class=\"line\">            dmin, dmax = np.min(data[:, i]), np.max(data[:, i])</span><br><span class=\"line\">            centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k)</span><br><span class=\"line\">        return centroids</span><br><span class=\"line\"></span><br><span class=\"line\">    def _converged(centroids1, centroids2):</span><br><span class=\"line\">        # if centroids not changed, we say &apos;converged&apos;</span><br><span class=\"line\">        set1 = set([tuple(c) for c in centroids1])</span><br><span class=\"line\">        set2 = set([tuple(c) for c in centroids2])</span><br><span class=\"line\">        return (set1 == set2)</span><br><span class=\"line\"></span><br><span class=\"line\">    n = data.shape[0]  # number of entries</span><br><span class=\"line\">    centroids = _rand_center(data, k)</span><br><span class=\"line\">    label = np.zeros(n, dtype=np.int)  # track the nearest centroid</span><br><span class=\"line\">    assement = np.zeros(n)  # for the assement of our model</span><br><span class=\"line\">    converged = False</span><br><span class=\"line\"></span><br><span class=\"line\">    while not converged:</span><br><span class=\"line\">        old_centroids = np.copy(centroids)</span><br><span class=\"line\">        for i in range(n):</span><br><span class=\"line\">            # determine the nearest centroid and track it with label</span><br><span class=\"line\">            min_dist, min_index = np.inf, -1</span><br><span class=\"line\">            for j in range(k):</span><br><span class=\"line\">                dist = _distance(data[i], centroids[j])</span><br><span class=\"line\">                if dist &lt; min_dist:</span><br><span class=\"line\">                    min_dist, min_index = dist, j</span><br><span class=\"line\">                    label[i] = j</span><br><span class=\"line\">            assement[i] = _distance(data[i], centroids[label[i]]) ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">        # update centroid</span><br><span class=\"line\">        for m in range(k):</span><br><span class=\"line\">            centroids[m] = np.mean(data[label == m], axis=0)</span><br><span class=\"line\">        converged = _converged(old_centroids, centroids)</span><br><span class=\"line\">    return centroids, label, np.sum(assement)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X, y = iris.data, iris.target</span><br><span class=\"line\">data = X[:, [1, 3]]  # 为了便于可视化，只取两个维度</span><br><span class=\"line\"># plt.scatter(data[:,0],data[:,1]);</span><br><span class=\"line\"></span><br><span class=\"line\">best_assement = np.inf</span><br><span class=\"line\">best_centroids = None</span><br><span class=\"line\">best_label = None</span><br><span class=\"line\"></span><br><span class=\"line\">for i in range(10):</span><br><span class=\"line\">    centroids, label, assement = kmeans(data, 2)</span><br><span class=\"line\">    if assement &lt; best_assement:</span><br><span class=\"line\">        best_assement = assement</span><br><span class=\"line\">        best_centroids = centroids</span><br><span class=\"line\">        best_label = label</span><br><span class=\"line\"></span><br><span class=\"line\">data0 = data[best_label == 0]</span><br><span class=\"line\">data1 = data[best_label == 1]</span><br><span class=\"line\"></span><br><span class=\"line\">fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))</span><br><span class=\"line\">ax1.scatter(data[:, 0], data[:, 1], c=&apos;c&apos;, s=30, marker=&apos;o&apos;)</span><br><span class=\"line\">ax2.scatter(data0[:, 0], data0[:, 1], c=&apos;r&apos;)</span><br><span class=\"line\">ax2.scatter(data1[:, 0], data1[:, 1], c=&apos;c&apos;)</span><br><span class=\"line\">ax2.scatter(centroids[:, 0], centroids[:, 1], c=&apos;b&apos;, s=120, marker=&apos;o&apos;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h3 id=\"1-距离量度\"><a href=\"#1-距离量度\" class=\"headerlink\" title=\"1. 距离量度\"></a>1. 距离量度</h3><h4 id=\"1-1-两点之间的距离\"><a href=\"#1-1-两点之间的距离\" class=\"headerlink\" title=\"1.1 两点之间的距离\"></a>1.1 两点之间的距离</h4><ul>\n<li>欧式距离<ul>\n<li><script type=\"math/tex; mode=display\">d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}</script></li>\n</ul>\n</li>\n<li>曼哈顿距离<ul>\n<li><script type=\"math/tex; mode=display\">d = | x_1 - x_1 | + | y_1 - y_2 |</script></li>\n</ul>\n</li>\n<li>切比雪夫距离<ul>\n<li><script type=\"math/tex; mode=display\">d = max(| x_1 - x_2 |, | y_1 - y_2 |)</script></li>\n</ul>\n</li>\n<li>余弦距离<ul>\n<li><script type=\"math/tex; mode=display\">cos \\theta = \\frac{x_1 x_2 + y_1 y_2}{\\sqrt{x_1^2 + y_1^2} \\sqrt{x_2^2 + y_2^2}}</script></li>\n</ul>\n</li>\n<li>Jaccard相似系数<ul>\n<li><script type=\"math/tex; mode=display\">J(A,B) = \\frac{| A \\cap B |}{| A \\cup B |}</script></li>\n</ul>\n</li>\n<li>相关系数<ul>\n<li><script type=\"math/tex; mode=display\">\\rho_{XY} = \\frac{Cov(X, Y)}{\\sqrt{D(X)} \\sqrt{D(Y)}} = \\frac{E((X-EX)(Y-EY))}{\\sqrt{D(X)} \\sqrt{D(Y)}}</script></li>\n</ul>\n</li>\n</ul>","more":"\n<h4 id=\"1-2-两个类别之间的距离\"><a href=\"#1-2-两个类别之间的距离\" class=\"headerlink\" title=\"1.2 两个类别之间的距离\"></a>1.2 两个类别之间的距离</h4><ul>\n<li>单连接聚类 Single-linkage clustering<ul>\n<li>一个类的所有成员到另一个类的所有成员之间的最短两个之间的距离</li>\n</ul>\n</li>\n<li>全连接聚类 Complete-linkage clustering<ul>\n<li>两个类中最远的两个点之间的距离</li>\n</ul>\n</li>\n<li>平均连接聚类 Average-linkage clustering<ul>\n<li>两个类中的点两两的距离求平均</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"2-K-Means聚类\"><a href=\"#2-K-Means聚类\" class=\"headerlink\" title=\"2. K-Means聚类\"></a>2. K-Means聚类</h3><p>算法思想：</p>\n<ul>\n<li>选择K个点作为初始质心</li>\n<li>repeat<ul>\n<li>将每个点指派到最近的质心，形成K个簇</li>\n<li>重新计算每个簇的质心</li>\n</ul>\n</li>\n<li>until 簇不发生变化或达到最大迭代次数</li>\n</ul>\n<p>这里的重新计算每个簇的质心，如何计算是根据目标函数得来的，因此在开始时需要考虑距离度量和目标函数。</p>\n<p><br></p>\n<p>考虑欧几里得距离的数据，使用<strong>误差平方和</strong>（Sum of the Squared Error, SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。</p>\n<script type=\"math/tex; mode=display\">SSE = \\sum_{i=1}^K \\sum_{x \\in c_i} dist(c_i, x)^2</script><p>其中$K$表示$K$个聚类中心，$c_i$表示第几个中心，$dist$表示的是欧式距离。</p>\n<p>在更新质心时使用所有点的平均值，为什么呢？这里是由SSE决定的。</p>\n<p>对第$k$个质心$c_k$求解，最小化式(7)：即对SSE求导并令导数等于零，求解$c_k$，如下：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\begin{aligned}\n\\frac{\\partial}{\\partial c_k} SSE\n&= \\frac{\\partial}{\\partial c_k} \\sum_{i=1}^K \\sum_{x \\in c_i} (c_i - x)^2 \\\\\n&= \\sum_{i=1}^K \\sum_{x \\in c_i} \\frac{\\partial}{\\partial c_k} (c_i - x)^2 \\\\\n&= \\sum_{x \\in c_k} 2 (c_k - x_k) \\\\\n&= 0\n\\end{aligned}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n\\sum_{x \\in c_k} 2(c_k - x_k) = 0 \\Rightarrow m_k c_k = \\sum_{c \\in c_k} x_k \\Rightarrow c_k = \\frac{1}{m_k} \\sum_{x \\in c_k} x_k</script><p>这样，正如前面所述，簇的最小化SSE的最佳质心是簇中各点的均值。</p>\n<hr>\n<h3 id=\"3-K-Means算法的优缺点\"><a href=\"#3-K-Means算法的优缺点\" class=\"headerlink\" title=\"3. K-Means算法的优缺点\"></a>3. K-Means算法的优缺点</h3><ul>\n<li>优点<ul>\n<li>易实现</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>K值需要预先给定</li>\n<li>对初始选取的聚类中心点敏感</li>\n<li>可能收敛到局部最小值</li>\n<li>在大规模数据收敛慢</li>\n</ul>\n</li>\n</ul>\n<p>进阶学习 <a href>bisecting K-means</a>，<a href>DBSCAN</a></p>\n<hr>\n<h3 id=\"4-算法实现\"><a href=\"#4-算法实现\" class=\"headerlink\" title=\"4. 算法实现\"></a>4. 算法实现</h3><p><a href=\"/posts_res/2018-10-09-kmeans/kmeans.py\">kmeans.py</a></p>\n<h4 id=\"4-1-运行结果：\"><a href=\"#4-1-运行结果：\" class=\"headerlink\" title=\"4.1 运行结果：\"></a>4.1 运行结果：</h4><p><img src=\"/posts_res/2018-10-09-kmeans/res.jpg\" alt=\"res.jpg\"></p>\n<h4 id=\"4-2-代码\"><a href=\"#4-2-代码\" class=\"headerlink\" title=\"4.2 代码\"></a>4.2 代码</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># coding:utf-8</span><br><span class=\"line\"></span><br><span class=\"line\">from matplotlib import pyplot as plt</span><br><span class=\"line\">from sklearn import datasets</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\"></span><br><span class=\"line\">def kmeans(data, k=2):</span><br><span class=\"line\">    def _distance(p1, p2):</span><br><span class=\"line\">        tmp = np.sum((p1 - p2) ** 2)</span><br><span class=\"line\">        return np.sqrt(tmp)</span><br><span class=\"line\"></span><br><span class=\"line\">    def _rand_center(data, k):</span><br><span class=\"line\">        &quot;&quot;&quot;Generate k center within the range of data set.&quot;&quot;&quot;</span><br><span class=\"line\">        n = data.shape[1]  # features</span><br><span class=\"line\">        centroids = np.zeros((k, n))  # init with (0,0)....</span><br><span class=\"line\">        for i in range(n):</span><br><span class=\"line\">            dmin, dmax = np.min(data[:, i]), np.max(data[:, i])</span><br><span class=\"line\">            centroids[:, i] = dmin + (dmax - dmin) * np.random.rand(k)</span><br><span class=\"line\">        return centroids</span><br><span class=\"line\"></span><br><span class=\"line\">    def _converged(centroids1, centroids2):</span><br><span class=\"line\">        # if centroids not changed, we say &apos;converged&apos;</span><br><span class=\"line\">        set1 = set([tuple(c) for c in centroids1])</span><br><span class=\"line\">        set2 = set([tuple(c) for c in centroids2])</span><br><span class=\"line\">        return (set1 == set2)</span><br><span class=\"line\"></span><br><span class=\"line\">    n = data.shape[0]  # number of entries</span><br><span class=\"line\">    centroids = _rand_center(data, k)</span><br><span class=\"line\">    label = np.zeros(n, dtype=np.int)  # track the nearest centroid</span><br><span class=\"line\">    assement = np.zeros(n)  # for the assement of our model</span><br><span class=\"line\">    converged = False</span><br><span class=\"line\"></span><br><span class=\"line\">    while not converged:</span><br><span class=\"line\">        old_centroids = np.copy(centroids)</span><br><span class=\"line\">        for i in range(n):</span><br><span class=\"line\">            # determine the nearest centroid and track it with label</span><br><span class=\"line\">            min_dist, min_index = np.inf, -1</span><br><span class=\"line\">            for j in range(k):</span><br><span class=\"line\">                dist = _distance(data[i], centroids[j])</span><br><span class=\"line\">                if dist &lt; min_dist:</span><br><span class=\"line\">                    min_dist, min_index = dist, j</span><br><span class=\"line\">                    label[i] = j</span><br><span class=\"line\">            assement[i] = _distance(data[i], centroids[label[i]]) ** 2</span><br><span class=\"line\"></span><br><span class=\"line\">        # update centroid</span><br><span class=\"line\">        for m in range(k):</span><br><span class=\"line\">            centroids[m] = np.mean(data[label == m], axis=0)</span><br><span class=\"line\">        converged = _converged(old_centroids, centroids)</span><br><span class=\"line\">    return centroids, label, np.sum(assement)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X, y = iris.data, iris.target</span><br><span class=\"line\">data = X[:, [1, 3]]  # 为了便于可视化，只取两个维度</span><br><span class=\"line\"># plt.scatter(data[:,0],data[:,1]);</span><br><span class=\"line\"></span><br><span class=\"line\">best_assement = np.inf</span><br><span class=\"line\">best_centroids = None</span><br><span class=\"line\">best_label = None</span><br><span class=\"line\"></span><br><span class=\"line\">for i in range(10):</span><br><span class=\"line\">    centroids, label, assement = kmeans(data, 2)</span><br><span class=\"line\">    if assement &lt; best_assement:</span><br><span class=\"line\">        best_assement = assement</span><br><span class=\"line\">        best_centroids = centroids</span><br><span class=\"line\">        best_label = label</span><br><span class=\"line\"></span><br><span class=\"line\">data0 = data[best_label == 0]</span><br><span class=\"line\">data1 = data[best_label == 1]</span><br><span class=\"line\"></span><br><span class=\"line\">fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))</span><br><span class=\"line\">ax1.scatter(data[:, 0], data[:, 1], c=&apos;c&apos;, s=30, marker=&apos;o&apos;)</span><br><span class=\"line\">ax2.scatter(data0[:, 0], data0[:, 1], c=&apos;r&apos;)</span><br><span class=\"line\">ax2.scatter(data1[:, 0], data1[:, 1], c=&apos;c&apos;)</span><br><span class=\"line\">ax2.scatter(centroids[:, 0], centroids[:, 1], c=&apos;b&apos;, s=120, marker=&apos;o&apos;)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n"},{"title":"hexo常用命令","date":"2018-11-26T01:32:16.000Z","mathjax":true,"copyright":false,"_content":"\n### 配置环境\n\n安装[git](https://git-scm.com/downloads)和[node.js](https://nodejs.org/en/)\n\n### 安装hexo\n\n```shell\nnpm install hexo -g     #安装  \nnpm update hexo -g      #升级  \nhexo init               #初始化\n```\n\n\n### 简写\n\n```shell\nhexo n \"我的博客\" == hexo new \"我的博客\"    #新建文章\nhexo p == hexo publish\nhexo g == hexo generate     #生成\nhexo s == hexo server       #启动服务预览\nhexo d == hexo deploy       #部署\n```\n\n\n### 安装插件\n\n```shell\n安装插件：npm install 插件名 –save\n卸载插件：npm uninstall 插件名\n更新插件和博客框架：npm update\n```\n\n\n### 服务器\n\n```shell\nhexo server     #Hexo 会监视文件变动并自动更新，您无须重启服务器。\nhexo server -s  #静态模式\nhexo server -p 5000         #更改端口\nhexo server -i 192.168.1.1  #自定义 IP\n\nhexo clean      #清除缓存 网页正常情况下可以忽略此条命令\nhexo g          #生成静态网页\nhexo d          #开始部署\n```\n\n\n### 监视文件变动\n\n```shell\nhexo generate #使用 Hexo 生成静态文件快速而且简单\nhexo generate --watch #监视文件变动\n```\n\n\n### 完成后部署\n\n```shell\n两个命令的作用是相同的:\nhexo generate --deploy == hexo g -d\nhexo deploy --generate == hexo d -g\n```\n\n\n### 草稿\n\n```shell\nhexo publish [layout] <title>\n```\n\n\n### 模版\n\n```shell\nhexo new \"postName\"         #新建文章\nhexo new page \"pageName\"    #新建页面\nhexo generate               #生成静态页面至public目录\nhexo server                 #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）\nhexo deploy                 #将.deploy目录部署到GitHub\n\nhexo new [layout] <title>\nhexo new draft \"My Draft\"\n```\n\n\n### 推送到服务器上\n\n```shell\nhexo n #写文章\nhexo g #生成\nhexo d #部署 #可与hexo g合并为 hexo d -g\n```\n\n\n### 部署类型设置git\n\n站点配置文件中\n\n```shell\n# Deployment\n## Docs: http://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repository: git@github.com:***/***.github.io.git\n  branch: master\n```\n","source":"_posts/2018-11-26-hexo常用命令.md","raw":"---\ntitle: hexo常用命令\ndate: 2018-11-26 09:32:16\ncategories: 小工具\nmathjax: true\ncopyright: false\n---\n\n### 配置环境\n\n安装[git](https://git-scm.com/downloads)和[node.js](https://nodejs.org/en/)\n\n### 安装hexo\n\n```shell\nnpm install hexo -g     #安装  \nnpm update hexo -g      #升级  \nhexo init               #初始化\n```\n\n\n### 简写\n\n```shell\nhexo n \"我的博客\" == hexo new \"我的博客\"    #新建文章\nhexo p == hexo publish\nhexo g == hexo generate     #生成\nhexo s == hexo server       #启动服务预览\nhexo d == hexo deploy       #部署\n```\n\n\n### 安装插件\n\n```shell\n安装插件：npm install 插件名 –save\n卸载插件：npm uninstall 插件名\n更新插件和博客框架：npm update\n```\n\n\n### 服务器\n\n```shell\nhexo server     #Hexo 会监视文件变动并自动更新，您无须重启服务器。\nhexo server -s  #静态模式\nhexo server -p 5000         #更改端口\nhexo server -i 192.168.1.1  #自定义 IP\n\nhexo clean      #清除缓存 网页正常情况下可以忽略此条命令\nhexo g          #生成静态网页\nhexo d          #开始部署\n```\n\n\n### 监视文件变动\n\n```shell\nhexo generate #使用 Hexo 生成静态文件快速而且简单\nhexo generate --watch #监视文件变动\n```\n\n\n### 完成后部署\n\n```shell\n两个命令的作用是相同的:\nhexo generate --deploy == hexo g -d\nhexo deploy --generate == hexo d -g\n```\n\n\n### 草稿\n\n```shell\nhexo publish [layout] <title>\n```\n\n\n### 模版\n\n```shell\nhexo new \"postName\"         #新建文章\nhexo new page \"pageName\"    #新建页面\nhexo generate               #生成静态页面至public目录\nhexo server                 #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）\nhexo deploy                 #将.deploy目录部署到GitHub\n\nhexo new [layout] <title>\nhexo new draft \"My Draft\"\n```\n\n\n### 推送到服务器上\n\n```shell\nhexo n #写文章\nhexo g #生成\nhexo d #部署 #可与hexo g合并为 hexo d -g\n```\n\n\n### 部署类型设置git\n\n站点配置文件中\n\n```shell\n# Deployment\n## Docs: http://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repository: git@github.com:***/***.github.io.git\n  branch: master\n```\n","slug":"hexo常用命令","published":1,"updated":"2019-08-17T09:41:03.145Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnwk003g2qwpgdfke1c9","content":"<h3 id=\"配置环境\"><a href=\"#配置环境\" class=\"headerlink\" title=\"配置环境\"></a>配置环境</h3><p>安装<a href=\"https://git-scm.com/downloads\" target=\"_blank\" rel=\"noopener\">git</a>和<a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">node.js</a></p><h3 id=\"安装hexo\"><a href=\"#安装hexo\" class=\"headerlink\" title=\"安装hexo\"></a>安装hexo</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo -g     #安装  </span><br><span class=\"line\">npm update hexo -g      #升级  </span><br><span class=\"line\">hexo init               #初始化</span><br></pre></td></tr></table></figure><h3 id=\"简写\"><a href=\"#简写\" class=\"headerlink\" title=\"简写\"></a>简写</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo n \"我的博客\" == hexo new \"我的博客\"    #新建文章</span><br><span class=\"line\">hexo p == hexo publish</span><br><span class=\"line\">hexo g == hexo generate     #生成</span><br><span class=\"line\">hexo s == hexo server       #启动服务预览</span><br><span class=\"line\">hexo d == hexo deploy       #部署</span><br></pre></td></tr></table></figure><h3 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">安装插件：npm install 插件名 –save</span><br><span class=\"line\">卸载插件：npm uninstall 插件名</span><br><span class=\"line\">更新插件和博客框架：npm update</span><br></pre></td></tr></table></figure><a id=\"more\"></a>\n\n\n\n<h3 id=\"服务器\"><a href=\"#服务器\" class=\"headerlink\" title=\"服务器\"></a>服务器</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo server     #Hexo 会监视文件变动并自动更新，您无须重启服务器。</span><br><span class=\"line\">hexo server -s  #静态模式</span><br><span class=\"line\">hexo server -p 5000         #更改端口</span><br><span class=\"line\">hexo server -i 192.168.1.1  #自定义 IP</span><br><span class=\"line\"></span><br><span class=\"line\">hexo clean      #清除缓存 网页正常情况下可以忽略此条命令</span><br><span class=\"line\">hexo g          #生成静态网页</span><br><span class=\"line\">hexo d          #开始部署</span><br></pre></td></tr></table></figure>\n<h3 id=\"监视文件变动\"><a href=\"#监视文件变动\" class=\"headerlink\" title=\"监视文件变动\"></a>监视文件变动</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo generate #使用 Hexo 生成静态文件快速而且简单</span><br><span class=\"line\">hexo generate --watch #监视文件变动</span><br></pre></td></tr></table></figure>\n<h3 id=\"完成后部署\"><a href=\"#完成后部署\" class=\"headerlink\" title=\"完成后部署\"></a>完成后部署</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">两个命令的作用是相同的:</span><br><span class=\"line\">hexo generate --deploy == hexo g -d</span><br><span class=\"line\">hexo deploy --generate == hexo d -g</span><br></pre></td></tr></table></figure>\n<h3 id=\"草稿\"><a href=\"#草稿\" class=\"headerlink\" title=\"草稿\"></a>草稿</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo publish [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"模版\"><a href=\"#模版\" class=\"headerlink\" title=\"模版\"></a>模版</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new \"postName\"         #新建文章</span><br><span class=\"line\">hexo new page \"pageName\"    #新建页面</span><br><span class=\"line\">hexo generate               #生成静态页面至public目录</span><br><span class=\"line\">hexo server                 #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）</span><br><span class=\"line\">hexo deploy                 #将.deploy目录部署到GitHub</span><br><span class=\"line\"></span><br><span class=\"line\">hexo new [layout] &lt;title&gt;</span><br><span class=\"line\">hexo new draft \"My Draft\"</span><br></pre></td></tr></table></figure>\n<h3 id=\"推送到服务器上\"><a href=\"#推送到服务器上\" class=\"headerlink\" title=\"推送到服务器上\"></a>推送到服务器上</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo n #写文章</span><br><span class=\"line\">hexo g #生成</span><br><span class=\"line\">hexo d #部署 #可与hexo g合并为 hexo d -g</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署类型设置git\"><a href=\"#部署类型设置git\" class=\"headerlink\" title=\"部署类型设置git\"></a>部署类型设置git</h3><p>站点配置文件中</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> Deployment</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"><span class=\"comment\"># Docs: http://hexo.io/docs/deployment.html</span></span></span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: git@github.com:***/***.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"<h3 id=\"配置环境\"><a href=\"#配置环境\" class=\"headerlink\" title=\"配置环境\"></a>配置环境</h3><p>安装<a href=\"https://git-scm.com/downloads\" target=\"_blank\" rel=\"noopener\">git</a>和<a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">node.js</a></p><h3 id=\"安装hexo\"><a href=\"#安装hexo\" class=\"headerlink\" title=\"安装hexo\"></a>安装hexo</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install hexo -g     #安装  </span><br><span class=\"line\">npm update hexo -g      #升级  </span><br><span class=\"line\">hexo init               #初始化</span><br></pre></td></tr></table></figure><h3 id=\"简写\"><a href=\"#简写\" class=\"headerlink\" title=\"简写\"></a>简写</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo n \"我的博客\" == hexo new \"我的博客\"    #新建文章</span><br><span class=\"line\">hexo p == hexo publish</span><br><span class=\"line\">hexo g == hexo generate     #生成</span><br><span class=\"line\">hexo s == hexo server       #启动服务预览</span><br><span class=\"line\">hexo d == hexo deploy       #部署</span><br></pre></td></tr></table></figure><h3 id=\"安装插件\"><a href=\"#安装插件\" class=\"headerlink\" title=\"安装插件\"></a>安装插件</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">安装插件：npm install 插件名 –save</span><br><span class=\"line\">卸载插件：npm uninstall 插件名</span><br><span class=\"line\">更新插件和博客框架：npm update</span><br></pre></td></tr></table></figure>","more":"\n\n\n\n<h3 id=\"服务器\"><a href=\"#服务器\" class=\"headerlink\" title=\"服务器\"></a>服务器</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo server     #Hexo 会监视文件变动并自动更新，您无须重启服务器。</span><br><span class=\"line\">hexo server -s  #静态模式</span><br><span class=\"line\">hexo server -p 5000         #更改端口</span><br><span class=\"line\">hexo server -i 192.168.1.1  #自定义 IP</span><br><span class=\"line\"></span><br><span class=\"line\">hexo clean      #清除缓存 网页正常情况下可以忽略此条命令</span><br><span class=\"line\">hexo g          #生成静态网页</span><br><span class=\"line\">hexo d          #开始部署</span><br></pre></td></tr></table></figure>\n<h3 id=\"监视文件变动\"><a href=\"#监视文件变动\" class=\"headerlink\" title=\"监视文件变动\"></a>监视文件变动</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo generate #使用 Hexo 生成静态文件快速而且简单</span><br><span class=\"line\">hexo generate --watch #监视文件变动</span><br></pre></td></tr></table></figure>\n<h3 id=\"完成后部署\"><a href=\"#完成后部署\" class=\"headerlink\" title=\"完成后部署\"></a>完成后部署</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">两个命令的作用是相同的:</span><br><span class=\"line\">hexo generate --deploy == hexo g -d</span><br><span class=\"line\">hexo deploy --generate == hexo d -g</span><br></pre></td></tr></table></figure>\n<h3 id=\"草稿\"><a href=\"#草稿\" class=\"headerlink\" title=\"草稿\"></a>草稿</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo publish [layout] &lt;title&gt;</span><br></pre></td></tr></table></figure>\n<h3 id=\"模版\"><a href=\"#模版\" class=\"headerlink\" title=\"模版\"></a>模版</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new \"postName\"         #新建文章</span><br><span class=\"line\">hexo new page \"pageName\"    #新建页面</span><br><span class=\"line\">hexo generate               #生成静态页面至public目录</span><br><span class=\"line\">hexo server                 #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）</span><br><span class=\"line\">hexo deploy                 #将.deploy目录部署到GitHub</span><br><span class=\"line\"></span><br><span class=\"line\">hexo new [layout] &lt;title&gt;</span><br><span class=\"line\">hexo new draft \"My Draft\"</span><br></pre></td></tr></table></figure>\n<h3 id=\"推送到服务器上\"><a href=\"#推送到服务器上\" class=\"headerlink\" title=\"推送到服务器上\"></a>推送到服务器上</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo n #写文章</span><br><span class=\"line\">hexo g #生成</span><br><span class=\"line\">hexo d #部署 #可与hexo g合并为 hexo d -g</span><br></pre></td></tr></table></figure>\n<h3 id=\"部署类型设置git\"><a href=\"#部署类型设置git\" class=\"headerlink\" title=\"部署类型设置git\"></a>部署类型设置git</h3><p>站点配置文件中</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"> Deployment</span></span><br><span class=\"line\"><span class=\"meta\">#</span><span class=\"bash\"><span class=\"comment\"># Docs: http://hexo.io/docs/deployment.html</span></span></span><br><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repository: git@github.com:***/***.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n"},{"layout":"post","title":"RNN梯度消失、梯度爆炸&LSTM解决梯度消失的办法","date":"2018-07-30T04:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n\n目录\n\n* RNN梯度消失和梯度爆炸的原因\n* LSTM解决梯度消失问题\n\n\n-----------\n\n### RNN梯度消失和梯度爆炸的原因\n\n经典的RNN结构如下图所示：\n\n![traditional_rnn](/posts_res/2018-07-30-rnngradientdisappear/1.jpg)\n\n假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：\n\n$$S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2}$$\n\n$$S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}$$\n\n$$S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}$$\n\n假设在$t=3$时刻，损失函数为 \n\n$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}$$\n\n则对于一次训练任务的损失函数为 $L=\\sum_{t=0}^{T}{L_{t}}$ ，即每一时刻损失值的累加。\n\n使用随机梯度下降法训练RNN其实就是对 $W_{x}  、 W_{s} 、 W_{o}$ 以及 $b_{1}、 b_{2}$ 求偏导，并不断调整它们以使$L$尽可能达到最小的过程。\n\n现在假设我们我们的时间序列只有三段，$t1，t2，t3$。\n\n我们只对t3时刻的 $W_{x}、W_{s}、W_{0}$ 求偏导（其他时刻类似）：\n\n$$\\frac{\\partial{L_{3}}}{\\partial{W_{0}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{W_{o}}}$$\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{x}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{x}}}\n$$\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n可以看出对于 $W_{0}$ 求偏导并没有长期依赖，但是对于 $W_{x}、W_{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S_{t}$ 随着时间序列向前传播，而 $S_{t}$ 又是 $W_{x}、W_{s}$ 的函数。\n\n根据上述求偏导的过程，我们可以得出任意时刻对 $W_{x}、W_{s}$ 求偏导的公式：\n\n$$\n\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}\n$$\n\n任意时刻对 $W_{s}$ 求偏导的公式同上。\n\n如果加上激活函数， $S_{j}=tanh(W_{x}X_{j}+W_{s}S_{j-1}+b_{1})$ ，\n\n则 \n\n$$\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}} = \\prod_{j=k+1}^{t}{tanh^{'}}W_{s}$$\n\n激活函数tanh和它的导数图像如下。\n\n![tanh](/posts_res/2018-07-30-rnngradientdisappear/2.jpg)\n\n\n由上图可以看出 $tanh' \\leq 1$ ，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现 $W_{x}X_{j}+W_{s}S_{j-1}+b_{1}=0$ ，如果 $W_{s}$ 也是一个大于0小于1的值，\n则当t很大时 $\\prod_{j=k+1}^{t} {tanh'} W_{s}$ ，就会趋近于$0$，和 $0.01^{50}$ 趋近与$0$是一个道理。同理当 $W_{s}$ 很大时 $\\prod_{j=k+1}^{t}{tanh^{'}}W_{s}$ 就会趋近于无穷，\n这就是**RNN中梯度消失和爆炸的原因**。\n\n至于怎么避免这种现象，看看 $\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}$ 梯度消失和爆炸的根本原因就是 $\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}$ 这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉。至于怎么去掉，一种办法就是使 ${\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx1$，另一种办法就是使 ${\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx0$ 。其实这就是LSTM做的事情。\n\n\n摘自：\n\n>\n[RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)\n\n\n-----------\n\n### LSTM解决梯度消失问题\n\n先上一张LSTM的经典图：\n\n![LSTM](/posts_res/2018-07-30-rnngradientdisappear/3.jpg)\n\n传统RNN可以抽象成下面这幅图：\n\n![rnn](/posts_res/2018-07-30-rnngradientdisappear/4.jpg)\n\n而LSTM可以抽象成这样：\n\n![LSTM](/posts_res/2018-07-30-rnngradientdisappear/5.jpg)\n\n三个 × 分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。\n这三个gate是如何控制流入流出的呢，其实就是通过下面 $f_{t},i_{t},o_{t}$ 三个函数来控制，因为 $\\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的，\n刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。\n\n$$f_{t}=\\sigma({W_{f}X_{t}}+b_{f})$$\n\n$$i_{t}=\\sigma({W_{i}X_{t}}+b_{i})$$\n\n$$o_{i}=\\sigma({W_{o}X_{t}}+b_{o})$$\n\n当前的状态 $S_{t}=f_{t}S_{t-1}+i_{t}X_{t}$类似与传统RNN $S_{t}=W_{s}S_{t-1}+W_{x}X_{t}+b_{1}$。将LSTM的状态表达式展开后得：\n\n$$S_{t}=\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}$$\n\n如果加上激活函数， \n\n$$S_{t}=tanh\\left[\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}\\right]$$\n\nRNN梯度消失和爆炸这部分中传统RNN求偏导的过程包含 \n\n$$\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{'}W_{s}}$$\n\n对于LSTM同样也包含这样的一项，但是在LSTM中 \n\n$$\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}$$\n\n假设 $Z=tanh{'}(x)\\sigma({y})$ ，则 $Z$ 的函数图像如下图所示：\n\n![gradient_tanh](/posts_res/2018-07-30-rnngradientdisappear/6.jpg)\n\n可以看到该函数值基本上不是 0 就是 1 。\n\n再看看RNN梯度消失和爆炸原因这部分中传统RNN的求偏导过程：\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n如果在LSTM中上式可能就会变成：\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n因为 $\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}\\approx 0 \\| 1 $ ，这样就解决了传统RNN中梯度消失的问题。\n\n\n摘自：\n\n>\n[LSTM如何解决梯度消失问题](https://zhuanlan.zhihu.com/p/28749444)\n\n\n","source":"_posts/2018-07-30-rnngradientdisappear.md","raw":"---\nlayout: post\ntitle: RNN梯度消失、梯度爆炸&LSTM解决梯度消失的办法\ndate: 2018-07-30 12:10 +0800\ncategories: 深度学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: false\n---\n\n\n目录\n\n* RNN梯度消失和梯度爆炸的原因\n* LSTM解决梯度消失问题\n\n\n-----------\n\n### RNN梯度消失和梯度爆炸的原因\n\n经典的RNN结构如下图所示：\n\n![traditional_rnn](/posts_res/2018-07-30-rnngradientdisappear/1.jpg)\n\n假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：\n\n$$S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2}$$\n\n$$S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}$$\n\n$$S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}$$\n\n假设在$t=3$时刻，损失函数为 \n\n$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}$$\n\n则对于一次训练任务的损失函数为 $L=\\sum_{t=0}^{T}{L_{t}}$ ，即每一时刻损失值的累加。\n\n使用随机梯度下降法训练RNN其实就是对 $W_{x}  、 W_{s} 、 W_{o}$ 以及 $b_{1}、 b_{2}$ 求偏导，并不断调整它们以使$L$尽可能达到最小的过程。\n\n现在假设我们我们的时间序列只有三段，$t1，t2，t3$。\n\n我们只对t3时刻的 $W_{x}、W_{s}、W_{0}$ 求偏导（其他时刻类似）：\n\n$$\\frac{\\partial{L_{3}}}{\\partial{W_{0}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{W_{o}}}$$\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{x}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{x}}}\n$$\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n可以看出对于 $W_{0}$ 求偏导并没有长期依赖，但是对于 $W_{x}、W_{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S_{t}$ 随着时间序列向前传播，而 $S_{t}$ 又是 $W_{x}、W_{s}$ 的函数。\n\n根据上述求偏导的过程，我们可以得出任意时刻对 $W_{x}、W_{s}$ 求偏导的公式：\n\n$$\n\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}\n$$\n\n任意时刻对 $W_{s}$ 求偏导的公式同上。\n\n如果加上激活函数， $S_{j}=tanh(W_{x}X_{j}+W_{s}S_{j-1}+b_{1})$ ，\n\n则 \n\n$$\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}} = \\prod_{j=k+1}^{t}{tanh^{'}}W_{s}$$\n\n激活函数tanh和它的导数图像如下。\n\n![tanh](/posts_res/2018-07-30-rnngradientdisappear/2.jpg)\n\n\n由上图可以看出 $tanh' \\leq 1$ ，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现 $W_{x}X_{j}+W_{s}S_{j-1}+b_{1}=0$ ，如果 $W_{s}$ 也是一个大于0小于1的值，\n则当t很大时 $\\prod_{j=k+1}^{t} {tanh'} W_{s}$ ，就会趋近于$0$，和 $0.01^{50}$ 趋近与$0$是一个道理。同理当 $W_{s}$ 很大时 $\\prod_{j=k+1}^{t}{tanh^{'}}W_{s}$ 就会趋近于无穷，\n这就是**RNN中梯度消失和爆炸的原因**。\n\n至于怎么避免这种现象，看看 $\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}$ 梯度消失和爆炸的根本原因就是 $\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}$ 这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉。至于怎么去掉，一种办法就是使 ${\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx1$，另一种办法就是使 ${\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx0$ 。其实这就是LSTM做的事情。\n\n\n摘自：\n\n>\n[RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)\n\n\n-----------\n\n### LSTM解决梯度消失问题\n\n先上一张LSTM的经典图：\n\n![LSTM](/posts_res/2018-07-30-rnngradientdisappear/3.jpg)\n\n传统RNN可以抽象成下面这幅图：\n\n![rnn](/posts_res/2018-07-30-rnngradientdisappear/4.jpg)\n\n而LSTM可以抽象成这样：\n\n![LSTM](/posts_res/2018-07-30-rnngradientdisappear/5.jpg)\n\n三个 × 分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。\n这三个gate是如何控制流入流出的呢，其实就是通过下面 $f_{t},i_{t},o_{t}$ 三个函数来控制，因为 $\\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的，\n刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。\n\n$$f_{t}=\\sigma({W_{f}X_{t}}+b_{f})$$\n\n$$i_{t}=\\sigma({W_{i}X_{t}}+b_{i})$$\n\n$$o_{i}=\\sigma({W_{o}X_{t}}+b_{o})$$\n\n当前的状态 $S_{t}=f_{t}S_{t-1}+i_{t}X_{t}$类似与传统RNN $S_{t}=W_{s}S_{t-1}+W_{x}X_{t}+b_{1}$。将LSTM的状态表达式展开后得：\n\n$$S_{t}=\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}$$\n\n如果加上激活函数， \n\n$$S_{t}=tanh\\left[\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}\\right]$$\n\nRNN梯度消失和爆炸这部分中传统RNN求偏导的过程包含 \n\n$$\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{'}W_{s}}$$\n\n对于LSTM同样也包含这样的一项，但是在LSTM中 \n\n$$\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}$$\n\n假设 $Z=tanh{'}(x)\\sigma({y})$ ，则 $Z$ 的函数图像如下图所示：\n\n![gradient_tanh](/posts_res/2018-07-30-rnngradientdisappear/6.jpg)\n\n可以看到该函数值基本上不是 0 就是 1 。\n\n再看看RNN梯度消失和爆炸原因这部分中传统RNN的求偏导过程：\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n如果在LSTM中上式可能就会变成：\n\n$$\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}\n$$\n\n因为 $\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}\\approx 0 \\| 1 $ ，这样就解决了传统RNN中梯度消失的问题。\n\n\n摘自：\n\n>\n[LSTM如何解决梯度消失问题](https://zhuanlan.zhihu.com/p/28749444)\n\n\n","slug":"rnngradientdisappear","published":1,"updated":"2019-08-17T09:39:04.973Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwm003j2qwp8mfqgtuy","content":"<p>目录</p><ul>\n<li>RNN梯度消失和梯度爆炸的原因</li>\n<li>LSTM解决梯度消失问题</li>\n</ul><hr><h3 id=\"RNN梯度消失和梯度爆炸的原因\"><a href=\"#RNN梯度消失和梯度爆炸的原因\" class=\"headerlink\" title=\"RNN梯度消失和梯度爆炸的原因\"></a>RNN梯度消失和梯度爆炸的原因</h3><p>经典的RNN结构如下图所示：</p><p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/1.jpg\" alt=\"traditional_rnn\"></p><p>假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：</p><script type=\"math/tex; mode=display\">S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2}</script><script type=\"math/tex; mode=display\">S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}</script><script type=\"math/tex; mode=display\">S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}</script><p>假设在$t=3$时刻，损失函数为 </p><script type=\"math/tex; mode=display\">L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}</script><a id=\"more\"></a>\n\n\n\n\n\n\n<p>则对于一次训练任务的损失函数为 $L=\\sum<em>{t=0}^{T}{L</em>{t}}$ ，即每一时刻损失值的累加。</p>\n<p>使用随机梯度下降法训练RNN其实就是对 $W<em>{x}  、 W</em>{s} 、 W<em>{o}$ 以及 $b</em>{1}、 b_{2}$ 求偏导，并不断调整它们以使$L$尽可能达到最小的过程。</p>\n<p>现在假设我们我们的时间序列只有三段，$t1，t2，t3$。</p>\n<p>我们只对t3时刻的 $W<em>{x}、W</em>{s}、W_{0}$ 求偏导（其他时刻类似）：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial{L_{3}}}{\\partial{W_{0}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{W_{o}}}</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{x}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{x}}}</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>可以看出对于 $W<em>{0}$ 求偏导并没有长期依赖，但是对于 $W</em>{x}、W<em>{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S</em>{t}$ 随着时间序列向前传播，而 $S<em>{t}$ 又是 $W</em>{x}、W_{s}$ 的函数。</p>\n<p>根据上述求偏导的过程，我们可以得出任意时刻对 $W<em>{x}、W</em>{s}$ 求偏导的公式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}</script><p>任意时刻对 $W_{s}$ 求偏导的公式同上。</p>\n<p>如果加上激活函数， $S<em>{j}=tanh(W</em>{x}X<em>{j}+W</em>{s}S<em>{j-1}+b</em>{1})$ ，</p>\n<p>则 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}} = \\prod_{j=k+1}^{t}{tanh^{'}}W_{s}</script><p>激活函数tanh和它的导数图像如下。</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/2.jpg\" alt=\"tanh\"></p>\n<p>由上图可以看出 $tanh’ \\leq 1$ ，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现 $W<em>{x}X</em>{j}+W<em>{s}S</em>{j-1}+b<em>{1}=0$ ，如果 $W</em>{s}$ 也是一个大于0小于1的值，\n则当t很大时 $\\prod<em>{j=k+1}^{t} {tanh’} W</em>{s}$ ，就会趋近于$0$，和 $0.01^{50}$ 趋近与$0$是一个道理。同理当 $W<em>{s}$ 很大时 $\\prod</em>{j=k+1}^{t}{tanh^{‘}}W_{s}$ 就会趋近于无穷，\n这就是<strong>RNN中梯度消失和爆炸的原因</strong>。</p>\n<p>至于怎么避免这种现象，看看 $\\frac{\\partial{L<em>{t}}}{\\partial{W</em>{x}}}=\\sum<em>{k=0}^{t}{\\frac{\\partial{L</em>{t}}}{\\partial{O<em>{t}}}\\frac{\\partial{O</em>{t}}}{\\partial{S<em>{t}}}}(\\prod</em>{j=k+1}^{t}{\\frac{\\partial{S<em>{j}}}{\\partial{S</em>{j-1}}}})\\frac{\\partial{S<em>{k}}}{\\partial{W</em>{x}}}$ 梯度消失和爆炸的根本原因就是 $\\prod<em>{j=k+1}^{t}{\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}}$ 这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉。至于怎么去掉，一种办法就是使 ${\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}}\\approx1$，另一种办法就是使 ${\\frac{\\partial{S</em>{j}}}{\\partial{S_{j-1}}}}\\approx0$ 。其实这就是LSTM做的事情。</p>\n<p>摘自：</p>\n<p>&gt;\n<a href=\"https://zhuanlan.zhihu.com/p/28687529\" target=\"_blank\" rel=\"noopener\">RNN梯度消失和爆炸的原因</a></p>\n<hr>\n<h3 id=\"LSTM解决梯度消失问题\"><a href=\"#LSTM解决梯度消失问题\" class=\"headerlink\" title=\"LSTM解决梯度消失问题\"></a>LSTM解决梯度消失问题</h3><p>先上一张LSTM的经典图：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/3.jpg\" alt=\"LSTM\"></p>\n<p>传统RNN可以抽象成下面这幅图：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/4.jpg\" alt=\"rnn\"></p>\n<p>而LSTM可以抽象成这样：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/5.jpg\" alt=\"LSTM\"></p>\n<p>三个 × 分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。\n这三个gate是如何控制流入流出的呢，其实就是通过下面 $f<em>{t},i</em>{t},o_{t}$ 三个函数来控制，因为 $\\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的，\n刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。</p>\n<script type=\"math/tex; mode=display\">f_{t}=\\sigma({W_{f}X_{t}}+b_{f})</script><script type=\"math/tex; mode=display\">i_{t}=\\sigma({W_{i}X_{t}}+b_{i})</script><script type=\"math/tex; mode=display\">o_{i}=\\sigma({W_{o}X_{t}}+b_{o})</script><p>当前的状态 $S<em>{t}=f</em>{t}S<em>{t-1}+i</em>{t}X<em>{t}$类似与传统RNN $S</em>{t}=W<em>{s}S</em>{t-1}+W<em>{x}X</em>{t}+b_{1}$。将LSTM的状态表达式展开后得：</p>\n<script type=\"math/tex; mode=display\">S_{t}=\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}</script><p>如果加上激活函数， </p>\n<script type=\"math/tex; mode=display\">S_{t}=tanh\\left[\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}\\right]</script><p>RNN梯度消失和爆炸这部分中传统RNN求偏导的过程包含 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{'}W_{s}}</script><p>对于LSTM同样也包含这样的一项，但是在LSTM中 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}</script><p>假设 $Z=tanh{‘}(x)\\sigma({y})$ ，则 $Z$ 的函数图像如下图所示：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/6.jpg\" alt=\"gradient_tanh\"></p>\n<p>可以看到该函数值基本上不是 0 就是 1 。</p>\n<p>再看看RNN梯度消失和爆炸原因这部分中传统RNN的求偏导过程：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>如果在LSTM中上式可能就会变成：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>因为 $\\prod<em>{j=k+1}^{t}\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}=\\prod</em>{j=k+1}^{t}{tanh{’}\\sigma({W<em>{f}X</em>{t}+b_{f}})}\\approx 0 | 1 $ ，这样就解决了传统RNN中梯度消失的问题。</p>\n<p>摘自：</p>\n<p>&gt;\n<a href=\"https://zhuanlan.zhihu.com/p/28749444\" target=\"_blank\" rel=\"noopener\">LSTM如何解决梯度消失问题</a></p>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>RNN梯度消失和梯度爆炸的原因</li>\n<li>LSTM解决梯度消失问题</li>\n</ul><hr><h3 id=\"RNN梯度消失和梯度爆炸的原因\"><a href=\"#RNN梯度消失和梯度爆炸的原因\" class=\"headerlink\" title=\"RNN梯度消失和梯度爆炸的原因\"></a>RNN梯度消失和梯度爆炸的原因</h3><p>经典的RNN结构如下图所示：</p><p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/1.jpg\" alt=\"traditional_rnn\"></p><p>假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：</p><script type=\"math/tex; mode=display\">S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2}</script><script type=\"math/tex; mode=display\">S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}</script><script type=\"math/tex; mode=display\">S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}</script><p>假设在$t=3$时刻，损失函数为 </p><script type=\"math/tex; mode=display\">L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}</script>","more":"\n\n\n\n\n\n\n<p>则对于一次训练任务的损失函数为 $L=\\sum<em>{t=0}^{T}{L</em>{t}}$ ，即每一时刻损失值的累加。</p>\n<p>使用随机梯度下降法训练RNN其实就是对 $W<em>{x}  、 W</em>{s} 、 W<em>{o}$ 以及 $b</em>{1}、 b_{2}$ 求偏导，并不断调整它们以使$L$尽可能达到最小的过程。</p>\n<p>现在假设我们我们的时间序列只有三段，$t1，t2，t3$。</p>\n<p>我们只对t3时刻的 $W<em>{x}、W</em>{s}、W_{0}$ 求偏导（其他时刻类似）：</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial{L_{3}}}{\\partial{W_{0}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{W_{o}}}</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{x}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{x}}}</script><script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>可以看出对于 $W<em>{0}$ 求偏导并没有长期依赖，但是对于 $W</em>{x}、W<em>{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S</em>{t}$ 随着时间序列向前传播，而 $S<em>{t}$ 又是 $W</em>{x}、W_{s}$ 的函数。</p>\n<p>根据上述求偏导的过程，我们可以得出任意时刻对 $W<em>{x}、W</em>{s}$ 求偏导的公式：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}</script><p>任意时刻对 $W_{s}$ 求偏导的公式同上。</p>\n<p>如果加上激活函数， $S<em>{j}=tanh(W</em>{x}X<em>{j}+W</em>{s}S<em>{j-1}+b</em>{1})$ ，</p>\n<p>则 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}} = \\prod_{j=k+1}^{t}{tanh^{'}}W_{s}</script><p>激活函数tanh和它的导数图像如下。</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/2.jpg\" alt=\"tanh\"></p>\n<p>由上图可以看出 $tanh’ \\leq 1$ ，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现 $W<em>{x}X</em>{j}+W<em>{s}S</em>{j-1}+b<em>{1}=0$ ，如果 $W</em>{s}$ 也是一个大于0小于1的值，\n则当t很大时 $\\prod<em>{j=k+1}^{t} {tanh’} W</em>{s}$ ，就会趋近于$0$，和 $0.01^{50}$ 趋近与$0$是一个道理。同理当 $W<em>{s}$ 很大时 $\\prod</em>{j=k+1}^{t}{tanh^{‘}}W_{s}$ 就会趋近于无穷，\n这就是<strong>RNN中梯度消失和爆炸的原因</strong>。</p>\n<p>至于怎么避免这种现象，看看 $\\frac{\\partial{L<em>{t}}}{\\partial{W</em>{x}}}=\\sum<em>{k=0}^{t}{\\frac{\\partial{L</em>{t}}}{\\partial{O<em>{t}}}\\frac{\\partial{O</em>{t}}}{\\partial{S<em>{t}}}}(\\prod</em>{j=k+1}^{t}{\\frac{\\partial{S<em>{j}}}{\\partial{S</em>{j-1}}}})\\frac{\\partial{S<em>{k}}}{\\partial{W</em>{x}}}$ 梯度消失和爆炸的根本原因就是 $\\prod<em>{j=k+1}^{t}{\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}}$ 这一部分，要消除这种情况就需要把这一部分在求偏导的过程中去掉。至于怎么去掉，一种办法就是使 ${\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}}\\approx1$，另一种办法就是使 ${\\frac{\\partial{S</em>{j}}}{\\partial{S_{j-1}}}}\\approx0$ 。其实这就是LSTM做的事情。</p>\n<p>摘自：</p>\n<p>&gt;\n<a href=\"https://zhuanlan.zhihu.com/p/28687529\" target=\"_blank\" rel=\"noopener\">RNN梯度消失和爆炸的原因</a></p>\n<hr>\n<h3 id=\"LSTM解决梯度消失问题\"><a href=\"#LSTM解决梯度消失问题\" class=\"headerlink\" title=\"LSTM解决梯度消失问题\"></a>LSTM解决梯度消失问题</h3><p>先上一张LSTM的经典图：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/3.jpg\" alt=\"LSTM\"></p>\n<p>传统RNN可以抽象成下面这幅图：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/4.jpg\" alt=\"rnn\"></p>\n<p>而LSTM可以抽象成这样：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/5.jpg\" alt=\"LSTM\"></p>\n<p>三个 × 分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。\n这三个gate是如何控制流入流出的呢，其实就是通过下面 $f<em>{t},i</em>{t},o_{t}$ 三个函数来控制，因为 $\\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的，\n刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。</p>\n<script type=\"math/tex; mode=display\">f_{t}=\\sigma({W_{f}X_{t}}+b_{f})</script><script type=\"math/tex; mode=display\">i_{t}=\\sigma({W_{i}X_{t}}+b_{i})</script><script type=\"math/tex; mode=display\">o_{i}=\\sigma({W_{o}X_{t}}+b_{o})</script><p>当前的状态 $S<em>{t}=f</em>{t}S<em>{t-1}+i</em>{t}X<em>{t}$类似与传统RNN $S</em>{t}=W<em>{s}S</em>{t-1}+W<em>{x}X</em>{t}+b_{1}$。将LSTM的状态表达式展开后得：</p>\n<script type=\"math/tex; mode=display\">S_{t}=\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}</script><p>如果加上激活函数， </p>\n<script type=\"math/tex; mode=display\">S_{t}=tanh\\left[\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}\\right]</script><p>RNN梯度消失和爆炸这部分中传统RNN求偏导的过程包含 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{'}W_{s}}</script><p>对于LSTM同样也包含这样的一项，但是在LSTM中 </p>\n<script type=\"math/tex; mode=display\">\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}</script><p>假设 $Z=tanh{‘}(x)\\sigma({y})$ ，则 $Z$ 的函数图像如下图所示：</p>\n<p><img src=\"/posts_res/2018-07-30-rnngradientdisappear/6.jpg\" alt=\"gradient_tanh\"></p>\n<p>可以看到该函数值基本上不是 0 就是 1 。</p>\n<p>再看看RNN梯度消失和爆炸原因这部分中传统RNN的求偏导过程：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>如果在LSTM中上式可能就会变成：</p>\n<script type=\"math/tex; mode=display\">\n\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}</script><p>因为 $\\prod<em>{j=k+1}^{t}\\frac{\\partial{S</em>{j}}}{\\partial{S<em>{j-1}}}=\\prod</em>{j=k+1}^{t}{tanh{’}\\sigma({W<em>{f}X</em>{t}+b_{f}})}\\approx 0 | 1 $ ，这样就解决了传统RNN中梯度消失的问题。</p>\n<p>摘自：</p>\n<p>&gt;\n<a href=\"https://zhuanlan.zhihu.com/p/28749444\" target=\"_blank\" rel=\"noopener\">LSTM如何解决梯度消失问题</a></p>\n"},{"layout":"post","title":"My Resume (2018)","date":"2018-08-08T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n目录\n\n* 自我介绍\n* 项目1 - 基于神经网络的安卓恶意代码检测\n* 项目2 - 2017开放学术精准画像大赛\n* 项目3 - 微博用户画像\n* 面试遇到的算法问题\n* 面试遇到的数据结构问题\n\n\n[我的简历2018-08-08](/posts_res/2018-08-08-resume/my_resume.pdf)\n\n[模型适用范围及特点在这里](/2018/07/machinelearningcompare/)\n\n\n---------\n\n### 自我介绍\n\n你好，我是赵卫国，目前在哈尔滨工业大学就读，计算机科学与技术专业研二，专业排名前20%，\n2017年7月到10月参加了由微软、清华联合举办的《2017开放学术精准画像大赛》，我们的队伍在400多个队伍中获得了第3名的成绩；\n其中我担任队长的职责，主要负责任务一和任务三的解决。\n\n本科就读于青岛大学，网络工程专业，专业排名前10%，参加过一次蓝桥杯软件大赛，获得了山东赛区二等奖；\n\n生活上，有自己的爱好兴趣，喜欢运动，摄影。\n\n\n---------\n\n### 项目1 - 基于神经网络的安卓恶意代码检测\n\n任务描述：\n\n对提供的安卓应用程序，分析检测识别其中的恶意应用程序。 \n\n方法：\n1. 静态分析安卓应用程序，利用反编译工具获取函数调用关系图和操作码序列，使用污点分析技术获取API序列;\n2. 将函数调用关系图和API序列利用LSTM学习特征向量，将操作码构造成灰度图使用CNN学习特征向量； \n3. 使用SVM + Stacking技术进行分类。对安卓应用程序的最终分类准确率为96%。\n\n**涉及知识点**\n\n[LSTM](/2018/06/rnn1/)\n\n[CNN](/2018/06/cnn1/)\n\n[SVM](/2018/03/svm-1/)\n\n**面试遇到的问题**\n\n- 1.LSTM与RNN的不同\n- 2.为什么出现梯度消失和梯度爆炸 \n\n\n----------\n\n### 项目2 - 2017开放学术精准画像大赛\n\n任务描述：\n1. Task 1：基于学者名+组织名的搜索结果，确定学者的个人主页和个人信息。\n2. Task 2：基于学者的论文信息，挖掘学者的研究兴趣。\n3. Task 3：基于学者的论文信息，预测学者截止2017年6月的论文被引用量。\n\n方法：\n1. Task 1：使用搜索结果标题、URL等构造特征，样本正负比1:9，尝试过采样和负采样方法，使用XGBoost确定个人主页，CNN识别照片，朴素贝叶斯确定性别，正则表达式确定邮件等信息；\n2. Task 2： 基于论文题目，合著作者、论文引用关系等构造特征，使用PCA降维后训练三个分类器并将其输出结果后处理，选出学者研究兴趣；\n3. Task 3： 基于业务需要建立特征(h-index、论文数量等)，使用XGBoost二分类后，对正例数据使用XGBoost回归预测学者论文被引用量。\n\n[赛后解题报告](/2018/03/2017aca/)\n\n**涉及知识点**\n\n[XGBoost](/2018/04/gbdtxgboost/)\n\n[Naive Bayes](/2018/03/naive-bayes/)\n\n**面试遇到的问题**\n\n- 1.SVM与LR的区别\n- 2.SVM、LR、XGBoost各适用于什么场景？（数据规模、特征数量、特征特点）\n- 3.手写朴素贝叶斯\n\n\n---------\n\n### 项目3 - 微博用户画像\n\n任务描述：\n\n给定用户微博文本数据以及社交网络图，预测用户的性别(二分类)、年龄(三分类)、地域(八分类)，最终采用加权准确率作为评价指标。\n\n方法：\n1. 使用卡方分析提取关键词，构造BOW特征；\n2. 利用微博文本，使用Doc2Vec技术训练出用户的Document Vector；\n3. 利用社交关系，使用Graph Embedding训练用户的Node Vector作为特征；\n4. 综合上述特征，使用PCA降维、SVM基分类器，并采用Stacking技术进行模型融合。\n5. 对地域分类，额外构建了地域常识词典；对性别分类，额外建立性别倾向性词典。\n6. 最终准确率为性别88.3%，年龄64.8%，地域72.7%。\n\n**涉及知识点**\n\n[卡方检验](/2018/03/chisquaredtest/)\n\n[Word2Vec](/2018/05/word2vec/)\n\n[Doc2Vec](/2018/07/doc2vectutorial/)\n\n[PCA降维](/2018/05/dimreduction/)\n\n[LINE - Node2Vec](/2018/07/LINE/)\n\n**面试遇到的问题**\n\n- 1.Word2Vec和Doc2Vec的区别联系\n- 2.LINE的具体思想和目标函数\n\n\n-----------\n\n### 面试遇到的算法问题\n\n- **1.生成模型和判别模型辨析**\n    - 常见判别模型：KNN，SVM，LR\n    - 常见生成模型：朴素贝叶斯，隐马尔可夫模型\n\n判别模型会生成一个表示$P(Y \\| X)$的判别函数(或预测模型)，而生成模型先计算联合概率$P(Y,X)$，然后通过贝叶斯公式转化为条件概率。简单来说，判别模型不会计算联合概率分布，生成模型需要先计算联合概率分布。\n或者理解：生成算法尝试去找到底这个数据是怎么生成的，然后再对一个信号进行分类；基于生成假设，那么哪个类别最有可能产生这个信号，这个信号就属于那个类别。\n判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。\n\n<br>\n\n- **2.LR与SVM对比**\n    - 相同点\n        - 都是分类算法、监督学习算法、判别模型\n        - 不考虑核函数，都是线性分类算法，即它们的分类决策面都是线性的\n    - 不同点\n        - 损失函数不同。\n        - SVM只考虑局部的边界线附近的点(支持向量)，而LR考虑全局(远离的点对分离平面起作用)\n        - 解决非线性问题时，SVM使用核函数机制，LR通常不采用核函数(自组合特征/Sigmoid)\n        - 线性SVM依赖数据表达的距离方式，需要先做normalization，LR不受其影响\n        - SVM自带正则，LR需要额外增加正则项\n        - LR可以给出每个点属于每一类的置信度(注非概率)，而SVM只能分类\n\nLR损失函数：\n$$ \nmin_{\\theta} \\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} (-log h_{\\theta} (x^{(i)})) + (1-y^{(i)})( -log (1- h_{\\theta}(x^{(i)})) ) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 \n$$\n\nSVM损失函数：\n$$\n\\sum_{i=1}^n \\lbrace 1-y^{(i)} ( w\\cdot x^{(i)} + b ) \\rbrace _+ + \\lambda || w ||^2 \\\\\n\\lbrace z \\rbrace _+ = \n\\begin{cases}\nz, \\quad z>0 \\\\\n0, \\quad z\\leq 0\n\\end{cases}\n$$\n\n*如何选择LR和SVM*\n\n1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\n2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM + Gaussian Kernel\n3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况\n\n>\n1. [LR和SVM的异同点](https://zhuanlan.zhihu.com/p/28036014)\n\n<br>\n\n- **3.计算CNN中的感受野**\n\n公式：$ \\frac{in - size}{stride} + 1 = out $\n\n例子：经过下列卷积操作后，3×3 conv -> 3×3 conv -> 2×2 maxpool -> 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？\n\n反向推导计算：\n\n1. conv 3x3， stride=1， out=1. $\\frac{in - 3}{1} + 1 = 1 \\quad \\Rightarrow \\quad in = 3$\n2. maxpool 2x2， stride=1， out=3. $ \\frac{in - 2}{1} + 1 = 3 \\quad \\Rightarrow \\quad in = 4 $\n3. conv 3x3， stride=1， out=4. $ \\frac{in - 3}{1} + 1 = 4 \\quad \\Rightarrow \\quad in = 6 $\n4. conv 3x3， stride=1， out=6. $ \\frac{in - 3}{1} + 1 = 6 \\quad \\Rightarrow \\quad in = 8 $\n\n所以输出神经元的感受野为 8x8。\n\n<br>\n\n- **4.Bias和Variance的tradeoff**\n    - A：让Error(train)尽可能小\n    - B：让Error(train)尽可能等于Error(test)\n    - 即，因为A小 and A=B，所以B小。\n    - 让Error(train)小 –> 模型复杂化，增加参数数量 –> low bias\n    - 让Error(train) == Error(test) –> 模型简单化，减少参数数量 –> low variance\n\n![4-1.png](/posts_res/2018-08-08-resume/4-1.png)\n\nbias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲就是在样本上拟合的好不好。\n要想在bias上表现好(low bias)，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。\nlow bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。\n\nvariance描述的是样本上训练出来的模型在测试集上的表现， 要想在variance上表现好(low variance)，就要简化模型，减少模型的参数，但这样容易欠拟合(underfitting)，\n欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。\n\n这个靶子上的点(hits)可以理解成一个一个的拟合模型，如果许多个拟合模型都聚集在一堆，位置比较偏，如图中(high bias, low variance)这种情景，意味着无论什么样子的数据灌进来，\n拟合的模型都差不多，这个模型过于简陋了，参数太少了，复杂度太低了，这就是欠拟合； 但如果是图中(low bias, high variance)这种情景，所有拟合模型都围绕中间那个 correct target 均匀分布，\n但又不够集中、很散，这就意味着，灌进来的数据一有风吹草动，拟合模型就跟着剧烈变化，这说明这个拟合模型过于复杂了、不具有普适性、泛化能力差，就是过拟合(overfitting)。\n\n所以 bias 和 variance 的选择是一个 tradeoff ， 过高的 variance 对应的概念，有点“剑走偏锋”、“矫枉过正”的意思，\n如果说一个人variance比较高，可以理解为，这个人性格比较极端偏执，一条道走到黑哪怕是错的。 \n而过高的 bias 对应的概念，有点像“面面俱到”、“大巧若拙”的意思，如果说一个人bias比较高，可以理解为，这个人是个圆滑世故，可以照顾到每一个人，所以就比较离散。\n\n训练一个模型的最终目的，是为了让这个模型在测试数据上拟合效果好，也就是 Error(test) 比较小，但在实际问题中 testdata 是拿不到的，\n也根本不知道 testdata 的内在规律，所以我们通过什么策略来减小Error(test)呢？\n\n>\n1. [机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？](https://www.zhihu.com/question/27068705)\n2. [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n<br>\n\n- **5.神经网络中最后两个全连接层的区别**\n    - 最后一层为logits代表分类的后验概率(对于分类问题)，神经元数目为类别数目，通常连接sigmoid softmax来得到最后的后验概率；\n    - 普通的全连接层，神经元数目没有固定的要求，相当于用不同的模式将前层信息进行融合。\n\n<br>\n\n- **6.核函数与VC维之间的关系**\n\n参见：[结构化风险最小、VC维到SVM的理解](https://blog.csdn.net/huruzun/article/details/41795325)\n\n<br>\n\n- **7.归一化和标准化的区别**\n    - 归一化(normalization)：为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性。\n    - 标准化(standardization)：方便数据的下一步处理而进行的数据缩放等变换，并不是为了方便与其他数据异同处理或比较，比如数据经过零均值标准化之后，更利于使用标准正态分布的性质。\n\nRescaling: $ x' = \\frac{x - min(x)}{max(x) - min(x)} $\n\nMean Normalization: $ x' = \\frac{x - mean(x)}{max(x) - min(x)} $\n\nStandardization: $ x' = \\frac{x - \\bar{x}}{\\sigma} $\n\nScaling to unit length: $ x' = \\frac{x}{\\|\\| x \\|\\|} $\n\n<br>\n\n- **8.Word2Vec**\n    - 介绍Word2Vec\n        - Word2Vec的两个模型分别是CBOW和Skip-gram，两个加快训练的方法是Hierarchical Softmax和负采样；\n        - 假设一个训练样本是由中心词$w$和上下文$context(w)$组成，那么CBOW就是用$context(w)$去预测$w$；而Skip-gram则相反，用$w$去预测$context(w)$里的所有词。\n        - HS是试图用词频建立一颗哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，和词典中词的数量一致，而非叶节点是模型的参数。要预测的词，转化成预测从根节点到该词所在的叶子节点的路径，是多个二分类问题。\n        - 对于负采样，则是把原来的Softmax多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测1，负例预测0，这样更新局部的参数。\n    - 对比CBOW和Skip-gram\n        - 训练速度上CBOW会更快一点，因为每次会更新$context(w)$的词向量，而Skip-gram只更新中心词的词向量；\n        - Skip-gram对低频词效果比CBOW好，因为是尝试用当前词去预测上下文，当前词是低频高频没有区别，但CBOW类似完形填空，会倾向于选择常见或概率大的词来补全，不太会选择低频词。\n        - Skip-gram在大数据集上可以提取更多的信息，总体比CBOW要好一些；\n    - 对比HS和负采样\n        - 负采样更快，特别是词表比较大的时候；\n    - 负采样为什么要用词频来做采样频率？\n        - 这样可以让频率高的词先学习，然后带动其他词学习；\n    - 训练完有两套词向量，为什么只用其中一套？\n        - 对于HS来说，哈夫曼树中的参数是不能拿来做词向量的，因为没办法和词典中的词对应。\n        - 负采样中的参数其实可以考虑做词向量，因为中间是和前一套词向量做内积，应该是有意义的，但是考虑到负采样是基于词频的，有些词可能采不到，也就学的不好；\n\n<br>\n\n- **9.training set, validation set, test set的区别**\n    - training set: 用来训练模型\n    - validation set：用来模型选择\n    - test set：用来评估所选模型的实际性能\n\n<br>\n\n- **10.LSTM的公式**\n    - 遗忘门：$ f_t = \\sigma ( W_f x_t + U_f h_{t-1} + b_f ) $\n    - 输入门：$ i_t = \\sigma ( W_i x_t + U_i h_{t-1} + b_i ) $\n    - 输出门：$ o_t = \\sigma ( W_o x_t + U_o h_{t-1} + b_o ) $\n    - 神经元状态：$ c_t = f_t \\circ c_{t-1} + i_t \\circ \\tanh ( W_c x_t + U_c h_{t-1} + b_c ) $\n    - 输出值：$ h_t = o_t \\circ \\sigma ( c_t ) $\n    - [LSTM详细内容](/2018/06/rnn1/)\n\n<br>\n\n- **11.GBDT、随机森林、XGBoost的区别**\n    - 随机森林\n        - 容易理解和解释，树可以被可视化\n        - 不需要进行数据归一化\n        - 不容易过拟合\n        - 自带out-of-bag (oob)错误评估功能\n        - 易于并行化\n        - 不适合小样本，只适合大样本\n        - 树之间的相关性越大，整体错误率越高\n        - 单棵树的错误率越高，整体错误率越高\n        - 基树可以是分类树，也可以是回归树\n        - 能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性\n    - GBDT\n        - 基树只能是回归树，因为要拟合残差\n        - 灵活的处理各种类型的数据\n        - 基树的关系，只能串行\n        - 缺失值敏感\n    - [XGBoost](http://xgboost.apachecn.org/cn/latest/model.html#tree-ensembles)\n        - 不仅支持回归树，还支持线性分类器\n        - GBDT用到一阶导数，XGB用到二阶导数\n        - 代价函数中有正则项（叶结点数量+叶结点输出值的平方）\n        - 列抽样\n        - 缺失值不敏感，可以自学习分裂方向\n        - 部分并行\n\n<br>\n\n- **12.数据之间如果不是独立同分布的会怎样**\n    - 独立同分布是为了合理用于测试集，可解释\n    - 统计机器学习的假设就是数据独立同分布\n\n<br>\n\n- **13.神经网络里面的损失函数**\n    - Softmax Cross Entropy Loss\n        - $ E(y, \\hat{y} ) = - \\sum_j y_j log \\hat{y}_j $\n        - 其中$y$表示神经网络的目标标签，$\\hat{y}$表示神经网络的输出值。\n        - $\\hat{y}$表示softmax损失函数\n        - $ \\hat{y}_j = softmax(z_j) = e^{z_j} / \\sum_j e^{z_j} $\n    - Mean Square Loss\n        - $ mse = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 $\n\n<br>\n\n- **14.常见的激活函数及导数**\n    - 指数\n        - $ y = exp(h) $\n        - $ y' = exp(h) $\n    - sigmoid\n        - $ y = \\frac{1}{1+e^{-h}} $\n        - $ y' = y \\times (1-y) $\n    - tanh\n        - $ y = tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} $\n        - $ y' =  1- h^2 $\n    - 零均值的解释\n        - Sigmoid 的输出不是0均值的，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正(e.g. x>0 elementwise in ),那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。\n\n<br>\n\n- **15.bagging减小方差,boosting减小偏差** [来源](https://www.zhihu.com/question/26760839/answer/40337791)\n    - Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均\n    - Boostring是增量学习算法，每次学习前向的偏差(梯度)，将多个模型的结果相加\n    - bagging由于$ E[\\sum X_i / n] = E[X_i] $，所以bagging后的bias和单个子模型的bias接近，不能减小bias\n    - bagging由于$ Var[\\sum X_i / n] = Var(X_i) / n $，所以可以显著降低variance\n    - boostring是在迭代最小化损失函数，其bias自然逐步下降。但是各子模型之间是强相关的，故子模型之和不能降低variance\n\n<br>\n\n- **16.深度学习中,L2和dropout的区别**\n    - L2通过控制参数的值，而不修改模型\n    - dropout直接减少训练参数的数量，将大模型分割成多个小模型\n\n<br>\n\n\n<br>\n\n- **机器学习算法使用场景**\n    - CRF分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被 RNN 以及它的变种们所替代，LSTM+CRF的解决方案取得了state of art的效果\n    - LR做CTR预估，商品推荐转换为点击率预估也可以用该模型。着重了解推导，正则化及并行，但现在越来越多的依赖于深度学习，包括DNN，DRL\n    - SVM可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用SMO求解，还有了解下核函数\n    - adaboost 本身是exp loss在boosting方法下的模型\n    - EM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的还是pLSA，k-means背后也有em的思想\n    - 决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。\n    - HMM在基础的一阶马尔可夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错\n\n\n---------\n\n### 面试遇到的数据结构问题\n\n- 1.链表判断是否有环及环的入口节点\n    - [判断链表有环没环及环的入口结点问题 和 判断两个链表是否相交](https://blog.csdn.net/helloworldding/article/details/62040341)\n\n先用一快一慢指针，当慢指针追上快指针时-有环；当快指针指向空时-没有环；之后，统计环的长度，快指针和慢指针相等时，慢指针停下，快指针接着走，再次相等时可以得到环的长度K；\n重新从起始点开始，快指针比慢指针多走K步，二者相等时所指的节点为环的入口节点；\n\n- 2.快速排序、二分查找\n    - [快速排序](/2017/12/qsort/)\n    - [二分查找](https://blog.csdn.net/lovesummerforever/article/details/24588989)\n\n- 3.TopK问题\n    - [应用C++ STL以最小堆方法解决Top K 问题](https://blog.csdn.net/sptoor/article/details/7791649)\n\n\n------------\n\n### 面试遇到的实践题\n\n- 1.往模型中加入一个特征，如何判定这个特征是否有效\n    - [如何判断分类特征值选取是否有效-知乎](https://www.zhihu.com/question/23471490)\n\n- 2.逻辑回归的所有样本的都是正样本，那么学出来的超平面是怎样的？\n    - 所有数据点分布在超平面的一侧\n\n","source":"_posts/2018-08-08-resume.md","raw":"---\nlayout: post\ntitle: My Resume (2018)\ndate: 2018-08-08 12:10 +0800\ncategories: 个人小结\nmathjax: true\ncopyright: true\n---\n\n目录\n\n* 自我介绍\n* 项目1 - 基于神经网络的安卓恶意代码检测\n* 项目2 - 2017开放学术精准画像大赛\n* 项目3 - 微博用户画像\n* 面试遇到的算法问题\n* 面试遇到的数据结构问题\n\n\n[我的简历2018-08-08](/posts_res/2018-08-08-resume/my_resume.pdf)\n\n[模型适用范围及特点在这里](/2018/07/machinelearningcompare/)\n\n\n---------\n\n### 自我介绍\n\n你好，我是赵卫国，目前在哈尔滨工业大学就读，计算机科学与技术专业研二，专业排名前20%，\n2017年7月到10月参加了由微软、清华联合举办的《2017开放学术精准画像大赛》，我们的队伍在400多个队伍中获得了第3名的成绩；\n其中我担任队长的职责，主要负责任务一和任务三的解决。\n\n本科就读于青岛大学，网络工程专业，专业排名前10%，参加过一次蓝桥杯软件大赛，获得了山东赛区二等奖；\n\n生活上，有自己的爱好兴趣，喜欢运动，摄影。\n\n\n---------\n\n### 项目1 - 基于神经网络的安卓恶意代码检测\n\n任务描述：\n\n对提供的安卓应用程序，分析检测识别其中的恶意应用程序。 \n\n方法：\n1. 静态分析安卓应用程序，利用反编译工具获取函数调用关系图和操作码序列，使用污点分析技术获取API序列;\n2. 将函数调用关系图和API序列利用LSTM学习特征向量，将操作码构造成灰度图使用CNN学习特征向量； \n3. 使用SVM + Stacking技术进行分类。对安卓应用程序的最终分类准确率为96%。\n\n**涉及知识点**\n\n[LSTM](/2018/06/rnn1/)\n\n[CNN](/2018/06/cnn1/)\n\n[SVM](/2018/03/svm-1/)\n\n**面试遇到的问题**\n\n- 1.LSTM与RNN的不同\n- 2.为什么出现梯度消失和梯度爆炸 \n\n\n----------\n\n### 项目2 - 2017开放学术精准画像大赛\n\n任务描述：\n1. Task 1：基于学者名+组织名的搜索结果，确定学者的个人主页和个人信息。\n2. Task 2：基于学者的论文信息，挖掘学者的研究兴趣。\n3. Task 3：基于学者的论文信息，预测学者截止2017年6月的论文被引用量。\n\n方法：\n1. Task 1：使用搜索结果标题、URL等构造特征，样本正负比1:9，尝试过采样和负采样方法，使用XGBoost确定个人主页，CNN识别照片，朴素贝叶斯确定性别，正则表达式确定邮件等信息；\n2. Task 2： 基于论文题目，合著作者、论文引用关系等构造特征，使用PCA降维后训练三个分类器并将其输出结果后处理，选出学者研究兴趣；\n3. Task 3： 基于业务需要建立特征(h-index、论文数量等)，使用XGBoost二分类后，对正例数据使用XGBoost回归预测学者论文被引用量。\n\n[赛后解题报告](/2018/03/2017aca/)\n\n**涉及知识点**\n\n[XGBoost](/2018/04/gbdtxgboost/)\n\n[Naive Bayes](/2018/03/naive-bayes/)\n\n**面试遇到的问题**\n\n- 1.SVM与LR的区别\n- 2.SVM、LR、XGBoost各适用于什么场景？（数据规模、特征数量、特征特点）\n- 3.手写朴素贝叶斯\n\n\n---------\n\n### 项目3 - 微博用户画像\n\n任务描述：\n\n给定用户微博文本数据以及社交网络图，预测用户的性别(二分类)、年龄(三分类)、地域(八分类)，最终采用加权准确率作为评价指标。\n\n方法：\n1. 使用卡方分析提取关键词，构造BOW特征；\n2. 利用微博文本，使用Doc2Vec技术训练出用户的Document Vector；\n3. 利用社交关系，使用Graph Embedding训练用户的Node Vector作为特征；\n4. 综合上述特征，使用PCA降维、SVM基分类器，并采用Stacking技术进行模型融合。\n5. 对地域分类，额外构建了地域常识词典；对性别分类，额外建立性别倾向性词典。\n6. 最终准确率为性别88.3%，年龄64.8%，地域72.7%。\n\n**涉及知识点**\n\n[卡方检验](/2018/03/chisquaredtest/)\n\n[Word2Vec](/2018/05/word2vec/)\n\n[Doc2Vec](/2018/07/doc2vectutorial/)\n\n[PCA降维](/2018/05/dimreduction/)\n\n[LINE - Node2Vec](/2018/07/LINE/)\n\n**面试遇到的问题**\n\n- 1.Word2Vec和Doc2Vec的区别联系\n- 2.LINE的具体思想和目标函数\n\n\n-----------\n\n### 面试遇到的算法问题\n\n- **1.生成模型和判别模型辨析**\n    - 常见判别模型：KNN，SVM，LR\n    - 常见生成模型：朴素贝叶斯，隐马尔可夫模型\n\n判别模型会生成一个表示$P(Y \\| X)$的判别函数(或预测模型)，而生成模型先计算联合概率$P(Y,X)$，然后通过贝叶斯公式转化为条件概率。简单来说，判别模型不会计算联合概率分布，生成模型需要先计算联合概率分布。\n或者理解：生成算法尝试去找到底这个数据是怎么生成的，然后再对一个信号进行分类；基于生成假设，那么哪个类别最有可能产生这个信号，这个信号就属于那个类别。\n判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。\n\n<br>\n\n- **2.LR与SVM对比**\n    - 相同点\n        - 都是分类算法、监督学习算法、判别模型\n        - 不考虑核函数，都是线性分类算法，即它们的分类决策面都是线性的\n    - 不同点\n        - 损失函数不同。\n        - SVM只考虑局部的边界线附近的点(支持向量)，而LR考虑全局(远离的点对分离平面起作用)\n        - 解决非线性问题时，SVM使用核函数机制，LR通常不采用核函数(自组合特征/Sigmoid)\n        - 线性SVM依赖数据表达的距离方式，需要先做normalization，LR不受其影响\n        - SVM自带正则，LR需要额外增加正则项\n        - LR可以给出每个点属于每一类的置信度(注非概率)，而SVM只能分类\n\nLR损失函数：\n$$ \nmin_{\\theta} \\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} (-log h_{\\theta} (x^{(i)})) + (1-y^{(i)})( -log (1- h_{\\theta}(x^{(i)})) ) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2 \n$$\n\nSVM损失函数：\n$$\n\\sum_{i=1}^n \\lbrace 1-y^{(i)} ( w\\cdot x^{(i)} + b ) \\rbrace _+ + \\lambda || w ||^2 \\\\\n\\lbrace z \\rbrace _+ = \n\\begin{cases}\nz, \\quad z>0 \\\\\n0, \\quad z\\leq 0\n\\end{cases}\n$$\n\n*如何选择LR和SVM*\n\n1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM\n2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM + Gaussian Kernel\n3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况\n\n>\n1. [LR和SVM的异同点](https://zhuanlan.zhihu.com/p/28036014)\n\n<br>\n\n- **3.计算CNN中的感受野**\n\n公式：$ \\frac{in - size}{stride} + 1 = out $\n\n例子：经过下列卷积操作后，3×3 conv -> 3×3 conv -> 2×2 maxpool -> 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？\n\n反向推导计算：\n\n1. conv 3x3， stride=1， out=1. $\\frac{in - 3}{1} + 1 = 1 \\quad \\Rightarrow \\quad in = 3$\n2. maxpool 2x2， stride=1， out=3. $ \\frac{in - 2}{1} + 1 = 3 \\quad \\Rightarrow \\quad in = 4 $\n3. conv 3x3， stride=1， out=4. $ \\frac{in - 3}{1} + 1 = 4 \\quad \\Rightarrow \\quad in = 6 $\n4. conv 3x3， stride=1， out=6. $ \\frac{in - 3}{1} + 1 = 6 \\quad \\Rightarrow \\quad in = 8 $\n\n所以输出神经元的感受野为 8x8。\n\n<br>\n\n- **4.Bias和Variance的tradeoff**\n    - A：让Error(train)尽可能小\n    - B：让Error(train)尽可能等于Error(test)\n    - 即，因为A小 and A=B，所以B小。\n    - 让Error(train)小 –> 模型复杂化，增加参数数量 –> low bias\n    - 让Error(train) == Error(test) –> 模型简单化，减少参数数量 –> low variance\n\n![4-1.png](/posts_res/2018-08-08-resume/4-1.png)\n\nbias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲就是在样本上拟合的好不好。\n要想在bias上表现好(low bias)，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。\nlow bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。\n\nvariance描述的是样本上训练出来的模型在测试集上的表现， 要想在variance上表现好(low variance)，就要简化模型，减少模型的参数，但这样容易欠拟合(underfitting)，\n欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。\n\n这个靶子上的点(hits)可以理解成一个一个的拟合模型，如果许多个拟合模型都聚集在一堆，位置比较偏，如图中(high bias, low variance)这种情景，意味着无论什么样子的数据灌进来，\n拟合的模型都差不多，这个模型过于简陋了，参数太少了，复杂度太低了，这就是欠拟合； 但如果是图中(low bias, high variance)这种情景，所有拟合模型都围绕中间那个 correct target 均匀分布，\n但又不够集中、很散，这就意味着，灌进来的数据一有风吹草动，拟合模型就跟着剧烈变化，这说明这个拟合模型过于复杂了、不具有普适性、泛化能力差，就是过拟合(overfitting)。\n\n所以 bias 和 variance 的选择是一个 tradeoff ， 过高的 variance 对应的概念，有点“剑走偏锋”、“矫枉过正”的意思，\n如果说一个人variance比较高，可以理解为，这个人性格比较极端偏执，一条道走到黑哪怕是错的。 \n而过高的 bias 对应的概念，有点像“面面俱到”、“大巧若拙”的意思，如果说一个人bias比较高，可以理解为，这个人是个圆滑世故，可以照顾到每一个人，所以就比较离散。\n\n训练一个模型的最终目的，是为了让这个模型在测试数据上拟合效果好，也就是 Error(test) 比较小，但在实际问题中 testdata 是拿不到的，\n也根本不知道 testdata 的内在规律，所以我们通过什么策略来减小Error(test)呢？\n\n>\n1. [机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？](https://www.zhihu.com/question/27068705)\n2. [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n<br>\n\n- **5.神经网络中最后两个全连接层的区别**\n    - 最后一层为logits代表分类的后验概率(对于分类问题)，神经元数目为类别数目，通常连接sigmoid softmax来得到最后的后验概率；\n    - 普通的全连接层，神经元数目没有固定的要求，相当于用不同的模式将前层信息进行融合。\n\n<br>\n\n- **6.核函数与VC维之间的关系**\n\n参见：[结构化风险最小、VC维到SVM的理解](https://blog.csdn.net/huruzun/article/details/41795325)\n\n<br>\n\n- **7.归一化和标准化的区别**\n    - 归一化(normalization)：为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性。\n    - 标准化(standardization)：方便数据的下一步处理而进行的数据缩放等变换，并不是为了方便与其他数据异同处理或比较，比如数据经过零均值标准化之后，更利于使用标准正态分布的性质。\n\nRescaling: $ x' = \\frac{x - min(x)}{max(x) - min(x)} $\n\nMean Normalization: $ x' = \\frac{x - mean(x)}{max(x) - min(x)} $\n\nStandardization: $ x' = \\frac{x - \\bar{x}}{\\sigma} $\n\nScaling to unit length: $ x' = \\frac{x}{\\|\\| x \\|\\|} $\n\n<br>\n\n- **8.Word2Vec**\n    - 介绍Word2Vec\n        - Word2Vec的两个模型分别是CBOW和Skip-gram，两个加快训练的方法是Hierarchical Softmax和负采样；\n        - 假设一个训练样本是由中心词$w$和上下文$context(w)$组成，那么CBOW就是用$context(w)$去预测$w$；而Skip-gram则相反，用$w$去预测$context(w)$里的所有词。\n        - HS是试图用词频建立一颗哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，和词典中词的数量一致，而非叶节点是模型的参数。要预测的词，转化成预测从根节点到该词所在的叶子节点的路径，是多个二分类问题。\n        - 对于负采样，则是把原来的Softmax多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测1，负例预测0，这样更新局部的参数。\n    - 对比CBOW和Skip-gram\n        - 训练速度上CBOW会更快一点，因为每次会更新$context(w)$的词向量，而Skip-gram只更新中心词的词向量；\n        - Skip-gram对低频词效果比CBOW好，因为是尝试用当前词去预测上下文，当前词是低频高频没有区别，但CBOW类似完形填空，会倾向于选择常见或概率大的词来补全，不太会选择低频词。\n        - Skip-gram在大数据集上可以提取更多的信息，总体比CBOW要好一些；\n    - 对比HS和负采样\n        - 负采样更快，特别是词表比较大的时候；\n    - 负采样为什么要用词频来做采样频率？\n        - 这样可以让频率高的词先学习，然后带动其他词学习；\n    - 训练完有两套词向量，为什么只用其中一套？\n        - 对于HS来说，哈夫曼树中的参数是不能拿来做词向量的，因为没办法和词典中的词对应。\n        - 负采样中的参数其实可以考虑做词向量，因为中间是和前一套词向量做内积，应该是有意义的，但是考虑到负采样是基于词频的，有些词可能采不到，也就学的不好；\n\n<br>\n\n- **9.training set, validation set, test set的区别**\n    - training set: 用来训练模型\n    - validation set：用来模型选择\n    - test set：用来评估所选模型的实际性能\n\n<br>\n\n- **10.LSTM的公式**\n    - 遗忘门：$ f_t = \\sigma ( W_f x_t + U_f h_{t-1} + b_f ) $\n    - 输入门：$ i_t = \\sigma ( W_i x_t + U_i h_{t-1} + b_i ) $\n    - 输出门：$ o_t = \\sigma ( W_o x_t + U_o h_{t-1} + b_o ) $\n    - 神经元状态：$ c_t = f_t \\circ c_{t-1} + i_t \\circ \\tanh ( W_c x_t + U_c h_{t-1} + b_c ) $\n    - 输出值：$ h_t = o_t \\circ \\sigma ( c_t ) $\n    - [LSTM详细内容](/2018/06/rnn1/)\n\n<br>\n\n- **11.GBDT、随机森林、XGBoost的区别**\n    - 随机森林\n        - 容易理解和解释，树可以被可视化\n        - 不需要进行数据归一化\n        - 不容易过拟合\n        - 自带out-of-bag (oob)错误评估功能\n        - 易于并行化\n        - 不适合小样本，只适合大样本\n        - 树之间的相关性越大，整体错误率越高\n        - 单棵树的错误率越高，整体错误率越高\n        - 基树可以是分类树，也可以是回归树\n        - 能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性\n    - GBDT\n        - 基树只能是回归树，因为要拟合残差\n        - 灵活的处理各种类型的数据\n        - 基树的关系，只能串行\n        - 缺失值敏感\n    - [XGBoost](http://xgboost.apachecn.org/cn/latest/model.html#tree-ensembles)\n        - 不仅支持回归树，还支持线性分类器\n        - GBDT用到一阶导数，XGB用到二阶导数\n        - 代价函数中有正则项（叶结点数量+叶结点输出值的平方）\n        - 列抽样\n        - 缺失值不敏感，可以自学习分裂方向\n        - 部分并行\n\n<br>\n\n- **12.数据之间如果不是独立同分布的会怎样**\n    - 独立同分布是为了合理用于测试集，可解释\n    - 统计机器学习的假设就是数据独立同分布\n\n<br>\n\n- **13.神经网络里面的损失函数**\n    - Softmax Cross Entropy Loss\n        - $ E(y, \\hat{y} ) = - \\sum_j y_j log \\hat{y}_j $\n        - 其中$y$表示神经网络的目标标签，$\\hat{y}$表示神经网络的输出值。\n        - $\\hat{y}$表示softmax损失函数\n        - $ \\hat{y}_j = softmax(z_j) = e^{z_j} / \\sum_j e^{z_j} $\n    - Mean Square Loss\n        - $ mse = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 $\n\n<br>\n\n- **14.常见的激活函数及导数**\n    - 指数\n        - $ y = exp(h) $\n        - $ y' = exp(h) $\n    - sigmoid\n        - $ y = \\frac{1}{1+e^{-h}} $\n        - $ y' = y \\times (1-y) $\n    - tanh\n        - $ y = tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} $\n        - $ y' =  1- h^2 $\n    - 零均值的解释\n        - Sigmoid 的输出不是0均值的，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正(e.g. x>0 elementwise in ),那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。\n\n<br>\n\n- **15.bagging减小方差,boosting减小偏差** [来源](https://www.zhihu.com/question/26760839/answer/40337791)\n    - Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均\n    - Boostring是增量学习算法，每次学习前向的偏差(梯度)，将多个模型的结果相加\n    - bagging由于$ E[\\sum X_i / n] = E[X_i] $，所以bagging后的bias和单个子模型的bias接近，不能减小bias\n    - bagging由于$ Var[\\sum X_i / n] = Var(X_i) / n $，所以可以显著降低variance\n    - boostring是在迭代最小化损失函数，其bias自然逐步下降。但是各子模型之间是强相关的，故子模型之和不能降低variance\n\n<br>\n\n- **16.深度学习中,L2和dropout的区别**\n    - L2通过控制参数的值，而不修改模型\n    - dropout直接减少训练参数的数量，将大模型分割成多个小模型\n\n<br>\n\n\n<br>\n\n- **机器学习算法使用场景**\n    - CRF分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被 RNN 以及它的变种们所替代，LSTM+CRF的解决方案取得了state of art的效果\n    - LR做CTR预估，商品推荐转换为点击率预估也可以用该模型。着重了解推导，正则化及并行，但现在越来越多的依赖于深度学习，包括DNN，DRL\n    - SVM可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用SMO求解，还有了解下核函数\n    - adaboost 本身是exp loss在boosting方法下的模型\n    - EM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的还是pLSA，k-means背后也有em的思想\n    - 决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。\n    - HMM在基础的一阶马尔可夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错\n\n\n---------\n\n### 面试遇到的数据结构问题\n\n- 1.链表判断是否有环及环的入口节点\n    - [判断链表有环没环及环的入口结点问题 和 判断两个链表是否相交](https://blog.csdn.net/helloworldding/article/details/62040341)\n\n先用一快一慢指针，当慢指针追上快指针时-有环；当快指针指向空时-没有环；之后，统计环的长度，快指针和慢指针相等时，慢指针停下，快指针接着走，再次相等时可以得到环的长度K；\n重新从起始点开始，快指针比慢指针多走K步，二者相等时所指的节点为环的入口节点；\n\n- 2.快速排序、二分查找\n    - [快速排序](/2017/12/qsort/)\n    - [二分查找](https://blog.csdn.net/lovesummerforever/article/details/24588989)\n\n- 3.TopK问题\n    - [应用C++ STL以最小堆方法解决Top K 问题](https://blog.csdn.net/sptoor/article/details/7791649)\n\n\n------------\n\n### 面试遇到的实践题\n\n- 1.往模型中加入一个特征，如何判定这个特征是否有效\n    - [如何判断分类特征值选取是否有效-知乎](https://www.zhihu.com/question/23471490)\n\n- 2.逻辑回归的所有样本的都是正样本，那么学出来的超平面是怎样的？\n    - 所有数据点分布在超平面的一侧\n\n","slug":"resume","published":1,"updated":"2019-08-17T09:39:16.973Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwo003n2qwp235hjd83","content":"<p>目录</p><ul>\n<li>自我介绍</li>\n<li>项目1 - 基于神经网络的安卓恶意代码检测</li>\n<li>项目2 - 2017开放学术精准画像大赛</li>\n<li>项目3 - 微博用户画像</li>\n<li>面试遇到的算法问题</li>\n<li>面试遇到的数据结构问题</li>\n</ul><p><a href=\"/posts_res/2018-08-08-resume/my_resume.pdf\">我的简历2018-08-08</a></p><p><a href=\"/2018/07/machinelearningcompare/\">模型适用范围及特点在这里</a></p><hr><h3 id=\"自我介绍\"><a href=\"#自我介绍\" class=\"headerlink\" title=\"自我介绍\"></a>自我介绍</h3><p>你好，我是赵卫国，目前在哈尔滨工业大学就读，计算机科学与技术专业研二，专业排名前20%，\n2017年7月到10月参加了由微软、清华联合举办的《2017开放学术精准画像大赛》，我们的队伍在400多个队伍中获得了第3名的成绩；\n其中我担任队长的职责，主要负责任务一和任务三的解决。</p><p>本科就读于青岛大学，网络工程专业，专业排名前10%，参加过一次蓝桥杯软件大赛，获得了山东赛区二等奖；</p><a id=\"more\"></a>\n\n\n\n\n\n\n<p>生活上，有自己的爱好兴趣，喜欢运动，摄影。</p>\n<hr>\n<h3 id=\"项目1-基于神经网络的安卓恶意代码检测\"><a href=\"#项目1-基于神经网络的安卓恶意代码检测\" class=\"headerlink\" title=\"项目1 - 基于神经网络的安卓恶意代码检测\"></a>项目1 - 基于神经网络的安卓恶意代码检测</h3><p>任务描述：</p>\n<p>对提供的安卓应用程序，分析检测识别其中的恶意应用程序。 </p>\n<p>方法：</p>\n<ol>\n<li>静态分析安卓应用程序，利用反编译工具获取函数调用关系图和操作码序列，使用污点分析技术获取API序列;</li>\n<li>将函数调用关系图和API序列利用LSTM学习特征向量，将操作码构造成灰度图使用CNN学习特征向量； </li>\n<li>使用SVM + Stacking技术进行分类。对安卓应用程序的最终分类准确率为96%。</li>\n</ol>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/06/rnn1/\">LSTM</a></p>\n<p><a href=\"/2018/06/cnn1/\">CNN</a></p>\n<p><a href=\"/2018/03/svm-1/\">SVM</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.LSTM与RNN的不同</li>\n<li>2.为什么出现梯度消失和梯度爆炸 </li>\n</ul>\n<hr>\n<h3 id=\"项目2-2017开放学术精准画像大赛\"><a href=\"#项目2-2017开放学术精准画像大赛\" class=\"headerlink\" title=\"项目2 - 2017开放学术精准画像大赛\"></a>项目2 - 2017开放学术精准画像大赛</h3><p>任务描述：</p>\n<ol>\n<li>Task 1：基于学者名+组织名的搜索结果，确定学者的个人主页和个人信息。</li>\n<li>Task 2：基于学者的论文信息，挖掘学者的研究兴趣。</li>\n<li>Task 3：基于学者的论文信息，预测学者截止2017年6月的论文被引用量。</li>\n</ol>\n<p>方法：</p>\n<ol>\n<li>Task 1：使用搜索结果标题、URL等构造特征，样本正负比1:9，尝试过采样和负采样方法，使用XGBoost确定个人主页，CNN识别照片，朴素贝叶斯确定性别，正则表达式确定邮件等信息；</li>\n<li>Task 2： 基于论文题目，合著作者、论文引用关系等构造特征，使用PCA降维后训练三个分类器并将其输出结果后处理，选出学者研究兴趣；</li>\n<li>Task 3： 基于业务需要建立特征(h-index、论文数量等)，使用XGBoost二分类后，对正例数据使用XGBoost回归预测学者论文被引用量。</li>\n</ol>\n<p><a href=\"/2018/03/2017aca/\">赛后解题报告</a></p>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/04/gbdtxgboost/\">XGBoost</a></p>\n<p><a href=\"/2018/03/naive-bayes/\">Naive Bayes</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.SVM与LR的区别</li>\n<li>2.SVM、LR、XGBoost各适用于什么场景？（数据规模、特征数量、特征特点）</li>\n<li>3.手写朴素贝叶斯</li>\n</ul>\n<hr>\n<h3 id=\"项目3-微博用户画像\"><a href=\"#项目3-微博用户画像\" class=\"headerlink\" title=\"项目3 - 微博用户画像\"></a>项目3 - 微博用户画像</h3><p>任务描述：</p>\n<p>给定用户微博文本数据以及社交网络图，预测用户的性别(二分类)、年龄(三分类)、地域(八分类)，最终采用加权准确率作为评价指标。</p>\n<p>方法：</p>\n<ol>\n<li>使用卡方分析提取关键词，构造BOW特征；</li>\n<li>利用微博文本，使用Doc2Vec技术训练出用户的Document Vector；</li>\n<li>利用社交关系，使用Graph Embedding训练用户的Node Vector作为特征；</li>\n<li>综合上述特征，使用PCA降维、SVM基分类器，并采用Stacking技术进行模型融合。</li>\n<li>对地域分类，额外构建了地域常识词典；对性别分类，额外建立性别倾向性词典。</li>\n<li>最终准确率为性别88.3%，年龄64.8%，地域72.7%。</li>\n</ol>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/03/chisquaredtest/\">卡方检验</a></p>\n<p><a href=\"/2018/05/word2vec/\">Word2Vec</a></p>\n<p><a href=\"/2018/07/doc2vectutorial/\">Doc2Vec</a></p>\n<p><a href=\"/2018/05/dimreduction/\">PCA降维</a></p>\n<p><a href=\"/2018/07/LINE/\">LINE - Node2Vec</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.Word2Vec和Doc2Vec的区别联系</li>\n<li>2.LINE的具体思想和目标函数</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的算法问题\"><a href=\"#面试遇到的算法问题\" class=\"headerlink\" title=\"面试遇到的算法问题\"></a>面试遇到的算法问题</h3><ul>\n<li><strong>1.生成模型和判别模型辨析</strong><ul>\n<li>常见判别模型：KNN，SVM，LR</li>\n<li>常见生成模型：朴素贝叶斯，隐马尔可夫模型</li>\n</ul>\n</li>\n</ul>\n<p>判别模型会生成一个表示$P(Y | X)$的判别函数(或预测模型)，而生成模型先计算联合概率$P(Y,X)$，然后通过贝叶斯公式转化为条件概率。简单来说，判别模型不会计算联合概率分布，生成模型需要先计算联合概率分布。\n或者理解：生成算法尝试去找到底这个数据是怎么生成的，然后再对一个信号进行分类；基于生成假设，那么哪个类别最有可能产生这个信号，这个信号就属于那个类别。\n判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。</p>\n<p><br></p>\n<ul>\n<li><strong>2.LR与SVM对比</strong><ul>\n<li>相同点<ul>\n<li>都是分类算法、监督学习算法、判别模型</li>\n<li>不考虑核函数，都是线性分类算法，即它们的分类决策面都是线性的</li>\n</ul>\n</li>\n<li>不同点<ul>\n<li>损失函数不同。</li>\n<li>SVM只考虑局部的边界线附近的点(支持向量)，而LR考虑全局(远离的点对分离平面起作用)</li>\n<li>解决非线性问题时，SVM使用核函数机制，LR通常不采用核函数(自组合特征/Sigmoid)</li>\n<li>线性SVM依赖数据表达的距离方式，需要先做normalization，LR不受其影响</li>\n<li>SVM自带正则，LR需要额外增加正则项</li>\n<li>LR可以给出每个点属于每一类的置信度(注非概率)，而SVM只能分类</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>LR损失函数：</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta} \\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} (-log h_{\\theta} (x^{(i)})) + (1-y^{(i)})( -log (1- h_{\\theta}(x^{(i)})) ) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2</script><p>SVM损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^n \\lbrace 1-y^{(i)} ( w\\cdot x^{(i)} + b ) \\rbrace _+ + \\lambda || w ||^2 \\\\\n\\lbrace z \\rbrace _+ = \n\\begin{cases}\nz, \\quad z>0 \\\\\n0, \\quad z\\leq 0\n\\end{cases}</script><p><em>如何选择LR和SVM</em></p>\n<ol>\n<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>\n<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM + Gaussian Kernel</li>\n<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>\n</ol>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/28036014\" target=\"_blank\" rel=\"noopener\">LR和SVM的异同点</a></li>\n</ol>\n<p><br></p>\n<ul>\n<li><strong>3.计算CNN中的感受野</strong></li>\n</ul>\n<p>公式：$ \\frac{in - size}{stride} + 1 = out $</p>\n<p>例子：经过下列卷积操作后，3×3 conv -&gt; 3×3 conv -&gt; 2×2 maxpool -&gt; 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？</p>\n<p>反向推导计算：</p>\n<ol>\n<li>conv 3x3， stride=1， out=1. $\\frac{in - 3}{1} + 1 = 1 \\quad \\Rightarrow \\quad in = 3$</li>\n<li>maxpool 2x2， stride=1， out=3. $ \\frac{in - 2}{1} + 1 = 3 \\quad \\Rightarrow \\quad in = 4 $</li>\n<li>conv 3x3， stride=1， out=4. $ \\frac{in - 3}{1} + 1 = 4 \\quad \\Rightarrow \\quad in = 6 $</li>\n<li>conv 3x3， stride=1， out=6. $ \\frac{in - 3}{1} + 1 = 6 \\quad \\Rightarrow \\quad in = 8 $</li>\n</ol>\n<p>所以输出神经元的感受野为 8x8。</p>\n<p><br></p>\n<ul>\n<li><strong>4.Bias和Variance的tradeoff</strong><ul>\n<li>A：让Error(train)尽可能小</li>\n<li>B：让Error(train)尽可能等于Error(test)</li>\n<li>即，因为A小 and A=B，所以B小。</li>\n<li>让Error(train)小 –&gt; 模型复杂化，增加参数数量 –&gt; low bias</li>\n<li>让Error(train) == Error(test) –&gt; 模型简单化，减少参数数量 –&gt; low variance</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-08-08-resume/4-1.png\" alt=\"4-1.png\"></p>\n<p>bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲就是在样本上拟合的好不好。\n要想在bias上表现好(low bias)，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。\nlow bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。</p>\n<p>variance描述的是样本上训练出来的模型在测试集上的表现， 要想在variance上表现好(low variance)，就要简化模型，减少模型的参数，但这样容易欠拟合(underfitting)，\n欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。</p>\n<p>这个靶子上的点(hits)可以理解成一个一个的拟合模型，如果许多个拟合模型都聚集在一堆，位置比较偏，如图中(high bias, low variance)这种情景，意味着无论什么样子的数据灌进来，\n拟合的模型都差不多，这个模型过于简陋了，参数太少了，复杂度太低了，这就是欠拟合； 但如果是图中(low bias, high variance)这种情景，所有拟合模型都围绕中间那个 correct target 均匀分布，\n但又不够集中、很散，这就意味着，灌进来的数据一有风吹草动，拟合模型就跟着剧烈变化，这说明这个拟合模型过于复杂了、不具有普适性、泛化能力差，就是过拟合(overfitting)。</p>\n<p>所以 bias 和 variance 的选择是一个 tradeoff ， 过高的 variance 对应的概念，有点“剑走偏锋”、“矫枉过正”的意思，\n如果说一个人variance比较高，可以理解为，这个人性格比较极端偏执，一条道走到黑哪怕是错的。 \n而过高的 bias 对应的概念，有点像“面面俱到”、“大巧若拙”的意思，如果说一个人bias比较高，可以理解为，这个人是个圆滑世故，可以照顾到每一个人，所以就比较离散。</p>\n<p>训练一个模型的最终目的，是为了让这个模型在测试数据上拟合效果好，也就是 Error(test) 比较小，但在实际问题中 testdata 是拿不到的，\n也根本不知道 testdata 的内在规律，所以我们通过什么策略来减小Error(test)呢？</p>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://www.zhihu.com/question/27068705\" target=\"_blank\" rel=\"noopener\">机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？</a></li>\n<li><a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" target=\"_blank\" rel=\"noopener\">Understanding the Bias-Variance Tradeoff</a></li>\n</ol>\n<p><br></p>\n<ul>\n<li><strong>5.神经网络中最后两个全连接层的区别</strong><ul>\n<li>最后一层为logits代表分类的后验概率(对于分类问题)，神经元数目为类别数目，通常连接sigmoid softmax来得到最后的后验概率；</li>\n<li>普通的全连接层，神经元数目没有固定的要求，相当于用不同的模式将前层信息进行融合。</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>6.核函数与VC维之间的关系</strong></li>\n</ul>\n<p>参见：<a href=\"https://blog.csdn.net/huruzun/article/details/41795325\" target=\"_blank\" rel=\"noopener\">结构化风险最小、VC维到SVM的理解</a></p>\n<p><br></p>\n<ul>\n<li><strong>7.归一化和标准化的区别</strong><ul>\n<li>归一化(normalization)：为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性。</li>\n<li>标准化(standardization)：方便数据的下一步处理而进行的数据缩放等变换，并不是为了方便与其他数据异同处理或比较，比如数据经过零均值标准化之后，更利于使用标准正态分布的性质。</li>\n</ul>\n</li>\n</ul>\n<p>Rescaling: $ x’ = \\frac{x - min(x)}{max(x) - min(x)} $</p>\n<p>Mean Normalization: $ x’ = \\frac{x - mean(x)}{max(x) - min(x)} $</p>\n<p>Standardization: $ x’ = \\frac{x - \\bar{x}}{\\sigma} $</p>\n<p>Scaling to unit length: $ x’ = \\frac{x}{|| x ||} $</p>\n<p><br></p>\n<ul>\n<li><strong>8.Word2Vec</strong><ul>\n<li>介绍Word2Vec<ul>\n<li>Word2Vec的两个模型分别是CBOW和Skip-gram，两个加快训练的方法是Hierarchical Softmax和负采样；</li>\n<li>假设一个训练样本是由中心词$w$和上下文$context(w)$组成，那么CBOW就是用$context(w)$去预测$w$；而Skip-gram则相反，用$w$去预测$context(w)$里的所有词。</li>\n<li>HS是试图用词频建立一颗哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，和词典中词的数量一致，而非叶节点是模型的参数。要预测的词，转化成预测从根节点到该词所在的叶子节点的路径，是多个二分类问题。</li>\n<li>对于负采样，则是把原来的Softmax多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测1，负例预测0，这样更新局部的参数。</li>\n</ul>\n</li>\n<li>对比CBOW和Skip-gram<ul>\n<li>训练速度上CBOW会更快一点，因为每次会更新$context(w)$的词向量，而Skip-gram只更新中心词的词向量；</li>\n<li>Skip-gram对低频词效果比CBOW好，因为是尝试用当前词去预测上下文，当前词是低频高频没有区别，但CBOW类似完形填空，会倾向于选择常见或概率大的词来补全，不太会选择低频词。</li>\n<li>Skip-gram在大数据集上可以提取更多的信息，总体比CBOW要好一些；</li>\n</ul>\n</li>\n<li>对比HS和负采样<ul>\n<li>负采样更快，特别是词表比较大的时候；</li>\n</ul>\n</li>\n<li>负采样为什么要用词频来做采样频率？<ul>\n<li>这样可以让频率高的词先学习，然后带动其他词学习；</li>\n</ul>\n</li>\n<li>训练完有两套词向量，为什么只用其中一套？<ul>\n<li>对于HS来说，哈夫曼树中的参数是不能拿来做词向量的，因为没办法和词典中的词对应。</li>\n<li>负采样中的参数其实可以考虑做词向量，因为中间是和前一套词向量做内积，应该是有意义的，但是考虑到负采样是基于词频的，有些词可能采不到，也就学的不好；</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>9.training set, validation set, test set的区别</strong><ul>\n<li>training set: 用来训练模型</li>\n<li>validation set：用来模型选择</li>\n<li>test set：用来评估所选模型的实际性能</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>10.LSTM的公式</strong><ul>\n<li>遗忘门：$ f<em>t = \\sigma ( W_f x_t + U_f h</em>{t-1} + b_f ) $</li>\n<li>输入门：$ i<em>t = \\sigma ( W_i x_t + U_i h</em>{t-1} + b_i ) $</li>\n<li>输出门：$ o<em>t = \\sigma ( W_o x_t + U_o h</em>{t-1} + b_o ) $</li>\n<li>神经元状态：$ c<em>t = f_t \\circ c</em>{t-1} + i<em>t \\circ \\tanh ( W_c x_t + U_c h</em>{t-1} + b_c ) $</li>\n<li>输出值：$ h_t = o_t \\circ \\sigma ( c_t ) $</li>\n<li><a href=\"/2018/06/rnn1/\">LSTM详细内容</a></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>11.GBDT、随机森林、XGBoost的区别</strong><ul>\n<li>随机森林<ul>\n<li>容易理解和解释，树可以被可视化</li>\n<li>不需要进行数据归一化</li>\n<li>不容易过拟合</li>\n<li>自带out-of-bag (oob)错误评估功能</li>\n<li>易于并行化</li>\n<li>不适合小样本，只适合大样本</li>\n<li>树之间的相关性越大，整体错误率越高</li>\n<li>单棵树的错误率越高，整体错误率越高</li>\n<li>基树可以是分类树，也可以是回归树</li>\n<li>能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性</li>\n</ul>\n</li>\n<li>GBDT<ul>\n<li>基树只能是回归树，因为要拟合残差</li>\n<li>灵活的处理各种类型的数据</li>\n<li>基树的关系，只能串行</li>\n<li>缺失值敏感</li>\n</ul>\n</li>\n<li><a href=\"http://xgboost.apachecn.org/cn/latest/model.html#tree-ensembles\" target=\"_blank\" rel=\"noopener\">XGBoost</a><ul>\n<li>不仅支持回归树，还支持线性分类器</li>\n<li>GBDT用到一阶导数，XGB用到二阶导数</li>\n<li>代价函数中有正则项（叶结点数量+叶结点输出值的平方）</li>\n<li>列抽样</li>\n<li>缺失值不敏感，可以自学习分裂方向</li>\n<li>部分并行</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>12.数据之间如果不是独立同分布的会怎样</strong><ul>\n<li>独立同分布是为了合理用于测试集，可解释</li>\n<li>统计机器学习的假设就是数据独立同分布</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>13.神经网络里面的损失函数</strong><ul>\n<li>Softmax Cross Entropy Loss<ul>\n<li>$ E(y, \\hat{y} ) = - \\sum_j y_j log \\hat{y}_j $</li>\n<li>其中$y$表示神经网络的目标标签，$\\hat{y}$表示神经网络的输出值。</li>\n<li>$\\hat{y}$表示softmax损失函数</li>\n<li>$ \\hat{y}_j = softmax(z_j) = e^{z_j} / \\sum_j e^{z_j} $</li>\n</ul>\n</li>\n<li>Mean Square Loss<ul>\n<li>$ mse = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>14.常见的激活函数及导数</strong><ul>\n<li>指数<ul>\n<li>$ y = exp(h) $</li>\n<li>$ y’ = exp(h) $</li>\n</ul>\n</li>\n<li>sigmoid<ul>\n<li>$ y = \\frac{1}{1+e^{-h}} $</li>\n<li>$ y’ = y \\times (1-y) $</li>\n</ul>\n</li>\n<li>tanh<ul>\n<li>$ y = tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} $</li>\n<li>$ y’ =  1- h^2 $</li>\n</ul>\n</li>\n<li>零均值的解释<ul>\n<li>Sigmoid 的输出不是0均值的，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正(e.g. x&gt;0 elementwise in ),那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>15.bagging减小方差,boosting减小偏差</strong> <a href=\"https://www.zhihu.com/question/26760839/answer/40337791\" target=\"_blank\" rel=\"noopener\">来源</a><ul>\n<li>Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均</li>\n<li>Boostring是增量学习算法，每次学习前向的偏差(梯度)，将多个模型的结果相加</li>\n<li>bagging由于$ E[\\sum X_i / n] = E[X_i] $，所以bagging后的bias和单个子模型的bias接近，不能减小bias</li>\n<li>bagging由于$ Var[\\sum X_i / n] = Var(X_i) / n $，所以可以显著降低variance</li>\n<li>boostring是在迭代最小化损失函数，其bias自然逐步下降。但是各子模型之间是强相关的，故子模型之和不能降低variance</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>16.深度学习中,L2和dropout的区别</strong><ul>\n<li>L2通过控制参数的值，而不修改模型</li>\n<li>dropout直接减少训练参数的数量，将大模型分割成多个小模型</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<p><br></p>\n<ul>\n<li><strong>机器学习算法使用场景</strong><ul>\n<li>CRF分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被 RNN 以及它的变种们所替代，LSTM+CRF的解决方案取得了state of art的效果</li>\n<li>LR做CTR预估，商品推荐转换为点击率预估也可以用该模型。着重了解推导，正则化及并行，但现在越来越多的依赖于深度学习，包括DNN，DRL</li>\n<li>SVM可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用SMO求解，还有了解下核函数</li>\n<li>adaboost 本身是exp loss在boosting方法下的模型</li>\n<li>EM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的还是pLSA，k-means背后也有em的思想</li>\n<li>决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。</li>\n<li>HMM在基础的一阶马尔可夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的数据结构问题\"><a href=\"#面试遇到的数据结构问题\" class=\"headerlink\" title=\"面试遇到的数据结构问题\"></a>面试遇到的数据结构问题</h3><ul>\n<li>1.链表判断是否有环及环的入口节点<ul>\n<li><a href=\"https://blog.csdn.net/helloworldding/article/details/62040341\" target=\"_blank\" rel=\"noopener\">判断链表有环没环及环的入口结点问题 和 判断两个链表是否相交</a></li>\n</ul>\n</li>\n</ul>\n<p>先用一快一慢指针，当慢指针追上快指针时-有环；当快指针指向空时-没有环；之后，统计环的长度，快指针和慢指针相等时，慢指针停下，快指针接着走，再次相等时可以得到环的长度K；\n重新从起始点开始，快指针比慢指针多走K步，二者相等时所指的节点为环的入口节点；</p>\n<ul>\n<li><p>2.快速排序、二分查找</p>\n<ul>\n<li><a href=\"/2017/12/qsort/\">快速排序</a></li>\n<li><a href=\"https://blog.csdn.net/lovesummerforever/article/details/24588989\" target=\"_blank\" rel=\"noopener\">二分查找</a></li>\n</ul>\n</li>\n<li><p>3.TopK问题</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/sptoor/article/details/7791649\" target=\"_blank\" rel=\"noopener\">应用C++ STL以最小堆方法解决Top K 问题</a></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的实践题\"><a href=\"#面试遇到的实践题\" class=\"headerlink\" title=\"面试遇到的实践题\"></a>面试遇到的实践题</h3><ul>\n<li><p>1.往模型中加入一个特征，如何判定这个特征是否有效</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/23471490\" target=\"_blank\" rel=\"noopener\">如何判断分类特征值选取是否有效-知乎</a></li>\n</ul>\n</li>\n<li><p>2.逻辑回归的所有样本的都是正样本，那么学出来的超平面是怎样的？</p>\n<ul>\n<li>所有数据点分布在超平面的一侧</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>目录</p><ul>\n<li>自我介绍</li>\n<li>项目1 - 基于神经网络的安卓恶意代码检测</li>\n<li>项目2 - 2017开放学术精准画像大赛</li>\n<li>项目3 - 微博用户画像</li>\n<li>面试遇到的算法问题</li>\n<li>面试遇到的数据结构问题</li>\n</ul><p><a href=\"/posts_res/2018-08-08-resume/my_resume.pdf\">我的简历2018-08-08</a></p><p><a href=\"/2018/07/machinelearningcompare/\">模型适用范围及特点在这里</a></p><hr><h3 id=\"自我介绍\"><a href=\"#自我介绍\" class=\"headerlink\" title=\"自我介绍\"></a>自我介绍</h3><p>你好，我是赵卫国，目前在哈尔滨工业大学就读，计算机科学与技术专业研二，专业排名前20%，\n2017年7月到10月参加了由微软、清华联合举办的《2017开放学术精准画像大赛》，我们的队伍在400多个队伍中获得了第3名的成绩；\n其中我担任队长的职责，主要负责任务一和任务三的解决。</p><p>本科就读于青岛大学，网络工程专业，专业排名前10%，参加过一次蓝桥杯软件大赛，获得了山东赛区二等奖；</p>","more":"\n\n\n\n\n\n\n<p>生活上，有自己的爱好兴趣，喜欢运动，摄影。</p>\n<hr>\n<h3 id=\"项目1-基于神经网络的安卓恶意代码检测\"><a href=\"#项目1-基于神经网络的安卓恶意代码检测\" class=\"headerlink\" title=\"项目1 - 基于神经网络的安卓恶意代码检测\"></a>项目1 - 基于神经网络的安卓恶意代码检测</h3><p>任务描述：</p>\n<p>对提供的安卓应用程序，分析检测识别其中的恶意应用程序。 </p>\n<p>方法：</p>\n<ol>\n<li>静态分析安卓应用程序，利用反编译工具获取函数调用关系图和操作码序列，使用污点分析技术获取API序列;</li>\n<li>将函数调用关系图和API序列利用LSTM学习特征向量，将操作码构造成灰度图使用CNN学习特征向量； </li>\n<li>使用SVM + Stacking技术进行分类。对安卓应用程序的最终分类准确率为96%。</li>\n</ol>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/06/rnn1/\">LSTM</a></p>\n<p><a href=\"/2018/06/cnn1/\">CNN</a></p>\n<p><a href=\"/2018/03/svm-1/\">SVM</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.LSTM与RNN的不同</li>\n<li>2.为什么出现梯度消失和梯度爆炸 </li>\n</ul>\n<hr>\n<h3 id=\"项目2-2017开放学术精准画像大赛\"><a href=\"#项目2-2017开放学术精准画像大赛\" class=\"headerlink\" title=\"项目2 - 2017开放学术精准画像大赛\"></a>项目2 - 2017开放学术精准画像大赛</h3><p>任务描述：</p>\n<ol>\n<li>Task 1：基于学者名+组织名的搜索结果，确定学者的个人主页和个人信息。</li>\n<li>Task 2：基于学者的论文信息，挖掘学者的研究兴趣。</li>\n<li>Task 3：基于学者的论文信息，预测学者截止2017年6月的论文被引用量。</li>\n</ol>\n<p>方法：</p>\n<ol>\n<li>Task 1：使用搜索结果标题、URL等构造特征，样本正负比1:9，尝试过采样和负采样方法，使用XGBoost确定个人主页，CNN识别照片，朴素贝叶斯确定性别，正则表达式确定邮件等信息；</li>\n<li>Task 2： 基于论文题目，合著作者、论文引用关系等构造特征，使用PCA降维后训练三个分类器并将其输出结果后处理，选出学者研究兴趣；</li>\n<li>Task 3： 基于业务需要建立特征(h-index、论文数量等)，使用XGBoost二分类后，对正例数据使用XGBoost回归预测学者论文被引用量。</li>\n</ol>\n<p><a href=\"/2018/03/2017aca/\">赛后解题报告</a></p>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/04/gbdtxgboost/\">XGBoost</a></p>\n<p><a href=\"/2018/03/naive-bayes/\">Naive Bayes</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.SVM与LR的区别</li>\n<li>2.SVM、LR、XGBoost各适用于什么场景？（数据规模、特征数量、特征特点）</li>\n<li>3.手写朴素贝叶斯</li>\n</ul>\n<hr>\n<h3 id=\"项目3-微博用户画像\"><a href=\"#项目3-微博用户画像\" class=\"headerlink\" title=\"项目3 - 微博用户画像\"></a>项目3 - 微博用户画像</h3><p>任务描述：</p>\n<p>给定用户微博文本数据以及社交网络图，预测用户的性别(二分类)、年龄(三分类)、地域(八分类)，最终采用加权准确率作为评价指标。</p>\n<p>方法：</p>\n<ol>\n<li>使用卡方分析提取关键词，构造BOW特征；</li>\n<li>利用微博文本，使用Doc2Vec技术训练出用户的Document Vector；</li>\n<li>利用社交关系，使用Graph Embedding训练用户的Node Vector作为特征；</li>\n<li>综合上述特征，使用PCA降维、SVM基分类器，并采用Stacking技术进行模型融合。</li>\n<li>对地域分类，额外构建了地域常识词典；对性别分类，额外建立性别倾向性词典。</li>\n<li>最终准确率为性别88.3%，年龄64.8%，地域72.7%。</li>\n</ol>\n<p><strong>涉及知识点</strong></p>\n<p><a href=\"/2018/03/chisquaredtest/\">卡方检验</a></p>\n<p><a href=\"/2018/05/word2vec/\">Word2Vec</a></p>\n<p><a href=\"/2018/07/doc2vectutorial/\">Doc2Vec</a></p>\n<p><a href=\"/2018/05/dimreduction/\">PCA降维</a></p>\n<p><a href=\"/2018/07/LINE/\">LINE - Node2Vec</a></p>\n<p><strong>面试遇到的问题</strong></p>\n<ul>\n<li>1.Word2Vec和Doc2Vec的区别联系</li>\n<li>2.LINE的具体思想和目标函数</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的算法问题\"><a href=\"#面试遇到的算法问题\" class=\"headerlink\" title=\"面试遇到的算法问题\"></a>面试遇到的算法问题</h3><ul>\n<li><strong>1.生成模型和判别模型辨析</strong><ul>\n<li>常见判别模型：KNN，SVM，LR</li>\n<li>常见生成模型：朴素贝叶斯，隐马尔可夫模型</li>\n</ul>\n</li>\n</ul>\n<p>判别模型会生成一个表示$P(Y | X)$的判别函数(或预测模型)，而生成模型先计算联合概率$P(Y,X)$，然后通过贝叶斯公式转化为条件概率。简单来说，判别模型不会计算联合概率分布，生成模型需要先计算联合概率分布。\n或者理解：生成算法尝试去找到底这个数据是怎么生成的，然后再对一个信号进行分类；基于生成假设，那么哪个类别最有可能产生这个信号，这个信号就属于那个类别。\n判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。</p>\n<p><br></p>\n<ul>\n<li><strong>2.LR与SVM对比</strong><ul>\n<li>相同点<ul>\n<li>都是分类算法、监督学习算法、判别模型</li>\n<li>不考虑核函数，都是线性分类算法，即它们的分类决策面都是线性的</li>\n</ul>\n</li>\n<li>不同点<ul>\n<li>损失函数不同。</li>\n<li>SVM只考虑局部的边界线附近的点(支持向量)，而LR考虑全局(远离的点对分离平面起作用)</li>\n<li>解决非线性问题时，SVM使用核函数机制，LR通常不采用核函数(自组合特征/Sigmoid)</li>\n<li>线性SVM依赖数据表达的距离方式，需要先做normalization，LR不受其影响</li>\n<li>SVM自带正则，LR需要额外增加正则项</li>\n<li>LR可以给出每个点属于每一类的置信度(注非概率)，而SVM只能分类</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>LR损失函数：</p>\n<script type=\"math/tex; mode=display\">\nmin_{\\theta} \\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} (-log h_{\\theta} (x^{(i)})) + (1-y^{(i)})( -log (1- h_{\\theta}(x^{(i)})) ) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2</script><p>SVM损失函数：</p>\n<script type=\"math/tex; mode=display\">\n\\sum_{i=1}^n \\lbrace 1-y^{(i)} ( w\\cdot x^{(i)} + b ) \\rbrace _+ + \\lambda || w ||^2 \\\\\n\\lbrace z \\rbrace _+ = \n\\begin{cases}\nz, \\quad z>0 \\\\\n0, \\quad z\\leq 0\n\\end{cases}</script><p><em>如何选择LR和SVM</em></p>\n<ol>\n<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>\n<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM + Gaussian Kernel</li>\n<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>\n</ol>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/28036014\" target=\"_blank\" rel=\"noopener\">LR和SVM的异同点</a></li>\n</ol>\n<p><br></p>\n<ul>\n<li><strong>3.计算CNN中的感受野</strong></li>\n</ul>\n<p>公式：$ \\frac{in - size}{stride} + 1 = out $</p>\n<p>例子：经过下列卷积操作后，3×3 conv -&gt; 3×3 conv -&gt; 2×2 maxpool -&gt; 3×3 conv，卷积步长为 1，没有填充，输出神经元的感受野是多大？</p>\n<p>反向推导计算：</p>\n<ol>\n<li>conv 3x3， stride=1， out=1. $\\frac{in - 3}{1} + 1 = 1 \\quad \\Rightarrow \\quad in = 3$</li>\n<li>maxpool 2x2， stride=1， out=3. $ \\frac{in - 2}{1} + 1 = 3 \\quad \\Rightarrow \\quad in = 4 $</li>\n<li>conv 3x3， stride=1， out=4. $ \\frac{in - 3}{1} + 1 = 4 \\quad \\Rightarrow \\quad in = 6 $</li>\n<li>conv 3x3， stride=1， out=6. $ \\frac{in - 3}{1} + 1 = 6 \\quad \\Rightarrow \\quad in = 8 $</li>\n</ol>\n<p>所以输出神经元的感受野为 8x8。</p>\n<p><br></p>\n<ul>\n<li><strong>4.Bias和Variance的tradeoff</strong><ul>\n<li>A：让Error(train)尽可能小</li>\n<li>B：让Error(train)尽可能等于Error(test)</li>\n<li>即，因为A小 and A=B，所以B小。</li>\n<li>让Error(train)小 –&gt; 模型复杂化，增加参数数量 –&gt; low bias</li>\n<li>让Error(train) == Error(test) –&gt; 模型简单化，减少参数数量 –&gt; low variance</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/posts_res/2018-08-08-resume/4-1.png\" alt=\"4-1.png\"></p>\n<p>bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲就是在样本上拟合的好不好。\n要想在bias上表现好(low bias)，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。\nlow bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。</p>\n<p>variance描述的是样本上训练出来的模型在测试集上的表现， 要想在variance上表现好(low variance)，就要简化模型，减少模型的参数，但这样容易欠拟合(underfitting)，\n欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。</p>\n<p>这个靶子上的点(hits)可以理解成一个一个的拟合模型，如果许多个拟合模型都聚集在一堆，位置比较偏，如图中(high bias, low variance)这种情景，意味着无论什么样子的数据灌进来，\n拟合的模型都差不多，这个模型过于简陋了，参数太少了，复杂度太低了，这就是欠拟合； 但如果是图中(low bias, high variance)这种情景，所有拟合模型都围绕中间那个 correct target 均匀分布，\n但又不够集中、很散，这就意味着，灌进来的数据一有风吹草动，拟合模型就跟着剧烈变化，这说明这个拟合模型过于复杂了、不具有普适性、泛化能力差，就是过拟合(overfitting)。</p>\n<p>所以 bias 和 variance 的选择是一个 tradeoff ， 过高的 variance 对应的概念，有点“剑走偏锋”、“矫枉过正”的意思，\n如果说一个人variance比较高，可以理解为，这个人性格比较极端偏执，一条道走到黑哪怕是错的。 \n而过高的 bias 对应的概念，有点像“面面俱到”、“大巧若拙”的意思，如果说一个人bias比较高，可以理解为，这个人是个圆滑世故，可以照顾到每一个人，所以就比较离散。</p>\n<p>训练一个模型的最终目的，是为了让这个模型在测试数据上拟合效果好，也就是 Error(test) 比较小，但在实际问题中 testdata 是拿不到的，\n也根本不知道 testdata 的内在规律，所以我们通过什么策略来减小Error(test)呢？</p>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://www.zhihu.com/question/27068705\" target=\"_blank\" rel=\"noopener\">机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？</a></li>\n<li><a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" target=\"_blank\" rel=\"noopener\">Understanding the Bias-Variance Tradeoff</a></li>\n</ol>\n<p><br></p>\n<ul>\n<li><strong>5.神经网络中最后两个全连接层的区别</strong><ul>\n<li>最后一层为logits代表分类的后验概率(对于分类问题)，神经元数目为类别数目，通常连接sigmoid softmax来得到最后的后验概率；</li>\n<li>普通的全连接层，神经元数目没有固定的要求，相当于用不同的模式将前层信息进行融合。</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>6.核函数与VC维之间的关系</strong></li>\n</ul>\n<p>参见：<a href=\"https://blog.csdn.net/huruzun/article/details/41795325\" target=\"_blank\" rel=\"noopener\">结构化风险最小、VC维到SVM的理解</a></p>\n<p><br></p>\n<ul>\n<li><strong>7.归一化和标准化的区别</strong><ul>\n<li>归一化(normalization)：为了消除不同数据之间的量纲，方便数据比较和共同处理，比如在神经网络中，归一化可以加快训练网络的收敛性。</li>\n<li>标准化(standardization)：方便数据的下一步处理而进行的数据缩放等变换，并不是为了方便与其他数据异同处理或比较，比如数据经过零均值标准化之后，更利于使用标准正态分布的性质。</li>\n</ul>\n</li>\n</ul>\n<p>Rescaling: $ x’ = \\frac{x - min(x)}{max(x) - min(x)} $</p>\n<p>Mean Normalization: $ x’ = \\frac{x - mean(x)}{max(x) - min(x)} $</p>\n<p>Standardization: $ x’ = \\frac{x - \\bar{x}}{\\sigma} $</p>\n<p>Scaling to unit length: $ x’ = \\frac{x}{|| x ||} $</p>\n<p><br></p>\n<ul>\n<li><strong>8.Word2Vec</strong><ul>\n<li>介绍Word2Vec<ul>\n<li>Word2Vec的两个模型分别是CBOW和Skip-gram，两个加快训练的方法是Hierarchical Softmax和负采样；</li>\n<li>假设一个训练样本是由中心词$w$和上下文$context(w)$组成，那么CBOW就是用$context(w)$去预测$w$；而Skip-gram则相反，用$w$去预测$context(w)$里的所有词。</li>\n<li>HS是试图用词频建立一颗哈夫曼树，那么经常出现的词路径会比较短。树的叶子节点表示词，和词典中词的数量一致，而非叶节点是模型的参数。要预测的词，转化成预测从根节点到该词所在的叶子节点的路径，是多个二分类问题。</li>\n<li>对于负采样，则是把原来的Softmax多分类问题，直接转化成一个正例和多个负例的二分类问题。让正例预测1，负例预测0，这样更新局部的参数。</li>\n</ul>\n</li>\n<li>对比CBOW和Skip-gram<ul>\n<li>训练速度上CBOW会更快一点，因为每次会更新$context(w)$的词向量，而Skip-gram只更新中心词的词向量；</li>\n<li>Skip-gram对低频词效果比CBOW好，因为是尝试用当前词去预测上下文，当前词是低频高频没有区别，但CBOW类似完形填空，会倾向于选择常见或概率大的词来补全，不太会选择低频词。</li>\n<li>Skip-gram在大数据集上可以提取更多的信息，总体比CBOW要好一些；</li>\n</ul>\n</li>\n<li>对比HS和负采样<ul>\n<li>负采样更快，特别是词表比较大的时候；</li>\n</ul>\n</li>\n<li>负采样为什么要用词频来做采样频率？<ul>\n<li>这样可以让频率高的词先学习，然后带动其他词学习；</li>\n</ul>\n</li>\n<li>训练完有两套词向量，为什么只用其中一套？<ul>\n<li>对于HS来说，哈夫曼树中的参数是不能拿来做词向量的，因为没办法和词典中的词对应。</li>\n<li>负采样中的参数其实可以考虑做词向量，因为中间是和前一套词向量做内积，应该是有意义的，但是考虑到负采样是基于词频的，有些词可能采不到，也就学的不好；</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>9.training set, validation set, test set的区别</strong><ul>\n<li>training set: 用来训练模型</li>\n<li>validation set：用来模型选择</li>\n<li>test set：用来评估所选模型的实际性能</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>10.LSTM的公式</strong><ul>\n<li>遗忘门：$ f<em>t = \\sigma ( W_f x_t + U_f h</em>{t-1} + b_f ) $</li>\n<li>输入门：$ i<em>t = \\sigma ( W_i x_t + U_i h</em>{t-1} + b_i ) $</li>\n<li>输出门：$ o<em>t = \\sigma ( W_o x_t + U_o h</em>{t-1} + b_o ) $</li>\n<li>神经元状态：$ c<em>t = f_t \\circ c</em>{t-1} + i<em>t \\circ \\tanh ( W_c x_t + U_c h</em>{t-1} + b_c ) $</li>\n<li>输出值：$ h_t = o_t \\circ \\sigma ( c_t ) $</li>\n<li><a href=\"/2018/06/rnn1/\">LSTM详细内容</a></li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>11.GBDT、随机森林、XGBoost的区别</strong><ul>\n<li>随机森林<ul>\n<li>容易理解和解释，树可以被可视化</li>\n<li>不需要进行数据归一化</li>\n<li>不容易过拟合</li>\n<li>自带out-of-bag (oob)错误评估功能</li>\n<li>易于并行化</li>\n<li>不适合小样本，只适合大样本</li>\n<li>树之间的相关性越大，整体错误率越高</li>\n<li>单棵树的错误率越高，整体错误率越高</li>\n<li>基树可以是分类树，也可以是回归树</li>\n<li>能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性</li>\n</ul>\n</li>\n<li>GBDT<ul>\n<li>基树只能是回归树，因为要拟合残差</li>\n<li>灵活的处理各种类型的数据</li>\n<li>基树的关系，只能串行</li>\n<li>缺失值敏感</li>\n</ul>\n</li>\n<li><a href=\"http://xgboost.apachecn.org/cn/latest/model.html#tree-ensembles\" target=\"_blank\" rel=\"noopener\">XGBoost</a><ul>\n<li>不仅支持回归树，还支持线性分类器</li>\n<li>GBDT用到一阶导数，XGB用到二阶导数</li>\n<li>代价函数中有正则项（叶结点数量+叶结点输出值的平方）</li>\n<li>列抽样</li>\n<li>缺失值不敏感，可以自学习分裂方向</li>\n<li>部分并行</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>12.数据之间如果不是独立同分布的会怎样</strong><ul>\n<li>独立同分布是为了合理用于测试集，可解释</li>\n<li>统计机器学习的假设就是数据独立同分布</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>13.神经网络里面的损失函数</strong><ul>\n<li>Softmax Cross Entropy Loss<ul>\n<li>$ E(y, \\hat{y} ) = - \\sum_j y_j log \\hat{y}_j $</li>\n<li>其中$y$表示神经网络的目标标签，$\\hat{y}$表示神经网络的输出值。</li>\n<li>$\\hat{y}$表示softmax损失函数</li>\n<li>$ \\hat{y}_j = softmax(z_j) = e^{z_j} / \\sum_j e^{z_j} $</li>\n</ul>\n</li>\n<li>Mean Square Loss<ul>\n<li>$ mse = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 $</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>14.常见的激活函数及导数</strong><ul>\n<li>指数<ul>\n<li>$ y = exp(h) $</li>\n<li>$ y’ = exp(h) $</li>\n</ul>\n</li>\n<li>sigmoid<ul>\n<li>$ y = \\frac{1}{1+e^{-h}} $</li>\n<li>$ y’ = y \\times (1-y) $</li>\n</ul>\n</li>\n<li>tanh<ul>\n<li>$ y = tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} $</li>\n<li>$ y’ =  1- h^2 $</li>\n</ul>\n</li>\n<li>零均值的解释<ul>\n<li>Sigmoid 的输出不是0均值的，这是我们不希望的，因为这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响：假设后层神经元的输入都为正(e.g. x&gt;0 elementwise in ),那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>15.bagging减小方差,boosting减小偏差</strong> <a href=\"https://www.zhihu.com/question/26760839/answer/40337791\" target=\"_blank\" rel=\"noopener\">来源</a><ul>\n<li>Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均</li>\n<li>Boostring是增量学习算法，每次学习前向的偏差(梯度)，将多个模型的结果相加</li>\n<li>bagging由于$ E[\\sum X_i / n] = E[X_i] $，所以bagging后的bias和单个子模型的bias接近，不能减小bias</li>\n<li>bagging由于$ Var[\\sum X_i / n] = Var(X_i) / n $，所以可以显著降低variance</li>\n<li>boostring是在迭代最小化损失函数，其bias自然逐步下降。但是各子模型之间是强相关的，故子模型之和不能降低variance</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<ul>\n<li><strong>16.深度学习中,L2和dropout的区别</strong><ul>\n<li>L2通过控制参数的值，而不修改模型</li>\n<li>dropout直接减少训练参数的数量，将大模型分割成多个小模型</li>\n</ul>\n</li>\n</ul>\n<p><br></p>\n<p><br></p>\n<ul>\n<li><strong>机器学习算法使用场景</strong><ul>\n<li>CRF分词、语法依存分析、命名实体识别，但是现在正在越来越多的应用场景里被 RNN 以及它的变种们所替代，LSTM+CRF的解决方案取得了state of art的效果</li>\n<li>LR做CTR预估，商品推荐转换为点击率预估也可以用该模型。着重了解推导，正则化及并行，但现在越来越多的依赖于深度学习，包括DNN，DRL</li>\n<li>SVM可以做文本分类，人脸识别等，了解下原始问题如何转换为对偶问题，然后使用SMO求解，还有了解下核函数</li>\n<li>adaboost 本身是exp loss在boosting方法下的模型</li>\n<li>EM 是一种优化算法，本质个人认为有点类似于离散空间的梯度上升算法，一般是结合具体的算法来用，比如混合高斯模型，最大熵，无监督HMM等，但比较经典的还是pLSA，k-means背后也有em的思想</li>\n<li>决策树可以认为是将空间进行划分，ID3和C4.5算是比较经典的决策树算法，可以用来分类，也可以用来回归，但业界很少直接使用一棵树，一般使用多棵树，组成committee，较为经典有GBDT 和RF，两者都是ensemble learning的典范，只不过前者使用boosting降低bias，后者使用bagging降低variance从而提升模型的performance。在ESL中有个对比，使树形模型几乎完爆其他算法，泛化能力和学习能力都很牛逼。业界的话一般用来做搜索排序和相关性。</li>\n<li>HMM在基础的一阶马尔可夫的基础之上，加入隐含状态，有两者假设，解决三种问题，一般时间序列预测都可以用该模型，当然了，NLP中的分词，语音识别等效果也还不错</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的数据结构问题\"><a href=\"#面试遇到的数据结构问题\" class=\"headerlink\" title=\"面试遇到的数据结构问题\"></a>面试遇到的数据结构问题</h3><ul>\n<li>1.链表判断是否有环及环的入口节点<ul>\n<li><a href=\"https://blog.csdn.net/helloworldding/article/details/62040341\" target=\"_blank\" rel=\"noopener\">判断链表有环没环及环的入口结点问题 和 判断两个链表是否相交</a></li>\n</ul>\n</li>\n</ul>\n<p>先用一快一慢指针，当慢指针追上快指针时-有环；当快指针指向空时-没有环；之后，统计环的长度，快指针和慢指针相等时，慢指针停下，快指针接着走，再次相等时可以得到环的长度K；\n重新从起始点开始，快指针比慢指针多走K步，二者相等时所指的节点为环的入口节点；</p>\n<ul>\n<li><p>2.快速排序、二分查找</p>\n<ul>\n<li><a href=\"/2017/12/qsort/\">快速排序</a></li>\n<li><a href=\"https://blog.csdn.net/lovesummerforever/article/details/24588989\" target=\"_blank\" rel=\"noopener\">二分查找</a></li>\n</ul>\n</li>\n<li><p>3.TopK问题</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/sptoor/article/details/7791649\" target=\"_blank\" rel=\"noopener\">应用C++ STL以最小堆方法解决Top K 问题</a></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"面试遇到的实践题\"><a href=\"#面试遇到的实践题\" class=\"headerlink\" title=\"面试遇到的实践题\"></a>面试遇到的实践题</h3><ul>\n<li><p>1.往模型中加入一个特征，如何判定这个特征是否有效</p>\n<ul>\n<li><a href=\"https://www.zhihu.com/question/23471490\" target=\"_blank\" rel=\"noopener\">如何判断分类特征值选取是否有效-知乎</a></li>\n</ul>\n</li>\n<li><p>2.逻辑回归的所有样本的都是正样本，那么学出来的超平面是怎样的？</p>\n<ul>\n<li>所有数据点分布在超平面的一侧</li>\n</ul>\n</li>\n</ul>\n"},{"layout":"post","title":"t-SNE降维","date":"2018-11-12T02:10:00.000Z","mathjax":true,"copyright":false,"_content":"\n转载自：[t-SNE完整笔记](http://www.datakit.cn/blog/2017/02/05/t_sne_full.html)\n\n\nt-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，是由Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。\n此外，t-SNE是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。\nt-SNE是由SNE([Stochastic Neighbor Embedding](https://cs.nyu.edu/~roweis/papers/sne_final.pdf), SNE; Hinton and Roweis, 2002)发展而来。\n我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。\n\n\n-------------\n\n### 1.SNE\n\nSNE是通过[仿射(affinitie)变换](https://baike.baidu.com/item/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2)将数据点映射到概率分布上，主要包括两个步骤：\n\n- SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。\n- SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。\n\n我们看到t-SNE模型是非监督的降维，它跟k-means等不同，它不能通过训练得到一些东西之后再用于其它数据\n(比如k-means可以通过训练得到 k 个点，再用于其它数据集，而 t-SNE 只能单独的对数据做操作，也就是说他只有 fit_transform，而没有fit操作）\n\n<br>\n\nSNE是先将**欧氏距离转换为条件概率来表达点与点之间的相似度**。具体来说，给定 N 个高维的数据 $x_1, ..., x_N$ (注意 N 不是维度),\nt-SNE首先是计算概率$p_{ij}$，正比于$x_i$和$x_j$之间的相似度(这种概率是我们自主构建的)，即：\n\n$$ p_{j∣i} = \\frac{exp(− ∣∣ x_i − x_j ∣∣ ^2 / (2 \\sigma ^2_i))}{ \\sum_{k \\not= i} exp(− ∣∣ x_i − x_k ∣∣ ^2 / (2 \\sigma ^2_i))} $$\n\n这里的有一个参数是$ \\sigma_i $，对于不同的点 $x_i$ 取值不一样，后续会讨论如何设置。此外设置 $p_{x∣x} = 0$, 因为我们关注的是两两之间的相似度。\n\n<br>\n\n那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\\frac{1}{\\sqrt{2}}$，因此它们之间的相似度如下:\n\n$$ q_{j∣i} = \\frac{ exp(− ∣∣ x_i − x_j ∣∣ ^2)}{ \\sum_{k\\not=i} exp(−∣∣ x_i − x_k ∣∣ ^2)} $$\n\n同样，设定$q_{i∣i} = 0$。\n\n<br>\n\n如果降维的效果比较好，局部特征保留完整，那么 $p_{i∣j} = q_{i∣j}$, 因此我们优化两个分布之间的距离 - KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:\n\n$$ C = \\sum_{i} KL( P_i ∣∣ Q_i ) = \\sum_i \\sum_j p_{j∣i} log \\frac{p_{j∣i}}{q_{j∣i}} $$\n\n这里的 $P_i$ 表示了给定点 $x_i$ 下，其他所有数据点的条件概率分布。需要注意的是KL散度具有不对称性，在低维映射中不同的距离对应的惩罚权重是不同的，\n具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小\n(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q_{j∣i} = 0.2$ 来建模较大的 $p_{j∣i}=0.8$, $cost = p log(\\frac{p}{q}) = 1.11$, \n同样用较大的 $q_{j∣i}=0.8$ 来建模较大的 $p_{j∣i}=0.2$, $cost = -0.277$, 因此，**SNE会倾向于保留数据中的局部特征**。\n\n<br>\n\n下面我们开始正式的推导SNE。首先不同的点具有不同的 $\\sigma_i$，$P_i$ 的熵(entropy)会随着 $\\sigma_i$ 的增加而增加。\nSNE使用困惑度([perplexity](https://zh.wikipedia.org/wiki/%E5%9B%B0%E6%83%91%E5%BA%A6))的概念，用二分搜索的方式来寻找一个最佳的 $\\sigma$。其中困惑度指:\n\n$$ Perp(P_i) = 2^{H(P_i)} $$\n\n这里的 $H(P_i)$是 $P_i$ 的熵，即:\n\n$$ H(P_i) = − \\sum_j p_{j∣i} log_2 p_{j∣i} $$\n\n困惑度可以解释为一个点附近的有效近邻点个数。**SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间**，给定之后，使用二分搜索的方式寻找合适的$\\sigma$。\n\n那么核心问题是如何求解梯度了，目标函数等价于 $\\sum \\sum −p log(q) $这个式子与softmax非常的类似，我们知道softmax的目标函数是 $ \\sum −y log p$ ，对应的梯度是$y−p$\n(注：这里的softmax中 $y$ 表示label，$p$ 表示预估值)。同样我们可以推导SNE的目标函数中的$i$在$j$下的条件概率情况的梯度是$2 (p_{i∣j} − q_{i∣j})(y_i − y_j) $，\n同样$j$在$i$下的条件概率的梯度是 $2 ( p_{j∣i} − q_{j∣i})(y_i − y_j)$，最后得到完整的梯度公式如下:\n\n$$ \\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j∣i} − q_{j∣i} + p_{i∣j} − q_{i∣j})(y_i − y_j) $$\n\n在初始化中，可以用较小的 $\\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。\n即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:\n\n$$ Y^{(t)} = Y^{(t−1)} + \\eta \\frac{\\partial C}{ \\partial Y} + \\alpha (t)( Y^{(t−1)} − Y^{(t−2)}) $$\n\n这里的$Y^{(t)}$表示迭代$t$次的解，$\\eta$表示学习速率 ，$\\alpha (t)$ 表示迭代$t$次的动量。\n\n此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。\n因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。\n\n\n----------\n\n### 2.t-SNE\n\n尽管SNE提供了很好的可视化方法，但是它很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:\n\n- 使用对称版的SNE，简化梯度公式\n- 低维空间下，使用t分布替代高斯分布表达两点之间的相似度\n\nt-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。\n\n<br>\n\n#### 2.1 对称版SNE\n\n优化$p_{i∣j}$和$q_{i∣j}$的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即$P$是高维空间里各个点的联合概率分布，$Q$是低维空间下的，目标函数为:\n\n$$ C = KL( P ∣∣ Q) = \\sum_i \\sum_j p_{i,j} log p_{ij} q_{ij} $$\n\n这里的$p_{ii},q_{ii}$为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意$i, p_{ij} = p_{ji}, q_{ij} = q_{ji} $，因此概率分布可以改写为:\n\n$$ p_{ij} = \\frac{ exp(−∣∣ x_i − x_j ∣∣ ^2 / 2 \\sigma^2)}{\\sum_{k\\not=l} exp(−∣∣ x_k − x_l ∣∣ ^2 / 2 \\sigma^2)} \\quad \\quad \nq_{ij} = \\frac{exp(−∣∣ y_i − y_j ∣∣^2)}{ \\sum_{k≠l} exp(−∣∣ y_k − y_l ∣∣^2)} $$\n\n这种表达方式，使得整体简洁了很多。但是会引入**异常值**的问题。\n比如$x_i$是异常值，那么$∣∣ x_i − x_j ∣∣^2$会很大，对应的所有的$j, p_{ij}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对cost影响很小。\n\n\n为了解决这个问题，我们将联合概率分布定义修正为: $p_{ij} = (p_{i∣j} + p_{j∣i}) / 2 $, 这保证了 $\\sum_j p_{ij} > \\frac{1}{2n}$, 使得每个点对于cost都会有一定的贡献。\n对称SNE的最大优点是梯度计算变得简单了，如下:\n\n$$ \\frac{\\partial C}{ \\partial y_i} = 4 \\sum_j (p_{ij} − q_{ij})(y_i − y_j) $$\n\n实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。\n\n<br>\n\n#### 2.2 Crowding问题\n\n拥挤问题就是说各个簇聚集在一起，无法区分。\n比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，\n比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。\n进一步的说明，假设一个以数据点$x_i$为中心，半径为$r$的$m$维球(三维空间就是球)，其体积是按$r^m$增长的，\n假设数据点是在$m$维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。\n\n![crowding](/posts_res/2018-11-12-tSNE/2-2.png)\n\n从上图可以看到，随着维度的增大，大部分数据点都聚集在$m$维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。\n\n<br>\n\n> 怎么解决crowding问题呢？\n\n<br>\n\nCook et al.(2007) 提出一种slight repulsion的方式，在基线概率分布(uniform background)中引入一个较小的混合因子 $\\rho$，\n这样$q_{ij}$就永远不会小于 $ \\frac{2 \\rho}{n(n-1)}$ (因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q_{ij}$总是会比$p_{ij}$大一点。\n这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\\rho$为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\\rho$。\n直接优化UNI-SNE是不行的(即一开始$\\rho$不为0)，因为距离较远的两个点基本是一样的$q_{ij}$(等于基线分布), 即使$p_{ij}$很大，一些距离变化很难在$q_{ij}$中产生作用。\n也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。\n\n<br>\n\n#### 2.3 t-SNE\n\n对称SNE实际上在高维度下另外一种减轻”拥挤问题”的方法：\n\n在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。\n\n![outlier](/posts_res/2018-11-12-tSNE/2-3.png)\n\n我们对比一下高斯分布和t分布(如上图)，t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。\n\n使用了t分布之后的q变化，如下:\n\n$$ q_{ij} = \\frac{(1+∣∣ y_i − y_j ∣∣ ^2)^{−1}}{\\sum_{k\\not=l} (1 + ∣∣ y_i − y_j ∣∣ ^2)^{−1}} $$\n\n此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:\n\n$$ \\frac{\\partial C}{\\partial y_i} = 4 \\sum_j ( p_{ij} − q_{ij} ) (y_i − y_j) ( 1 + ∣∣ y_i − y_j ∣∣^2)^{−1} $$\n\nt-SNE的有效性，也可以从上图中看到：\n\n横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；\n而对于低相似度的点，t分布在低维空间中的距离需要更远。\n这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。\n\n总结一下，t-SNE的梯度更新有两大优势：\n\n- 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。\n- 这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。\n\n<br>\n\n#### 2.4 算法过程\n\n算法详细过程如下：\n\n- Data: $X = x_1, ..., x_n$\n- 计算cost function的参数：困惑度Perp\n- 优化参数: 设置迭代次数T，学习速率$\\eta$, 动量$\\alpha(t)$\n- 目标结果是低维数据表示 $Y^T = y_1, ..., y_n$\n- 开始优化\n    - 计算在给定Perp下的条件概率$p_{j∣i}$ (参见上面公式)\n    - 令 $p_{ij} = (p_{j∣i} + p_{i∣j}) / 2n $\n    - 用 $N(0, 10^{−4}I)$ 随机初始化 Y\n    - 迭代，从 t = 1 到 T， 做如下操作:\n        - 计算低维度下的 $q_{ij}$ (参见上面的公式)\n        - 计算梯度（参见上面的公式）\n        - 更新 $Y^t = Y^{t−1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)(Y^{t−1} − Y^{t−2}) $\n    - 结束\n- 结束\n\n<br>\n\n优化过程中可以尝试的两个trick:\n\n- 提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。\n- 提前夸大(early exaggeration)：在开始优化阶段，$p_{ij}$乘以一个大于1的数进行扩大，来避免因为$q_{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4。\n\n<br>\n\n#### 2.5 不足\n\n主要不足有四个:\n\n- 主要用于可视化，很难用于其他目的。\n    - 比如测试集合降维，因为它没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。\n- t-SNE倾向于保存局部特征。\n    - 对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间。\n- t-SNE没有唯一最优解，且没有预估部分。\n    - 如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。\n- 训练太慢。有很多基于树的算法在t-SNE上做一些改进\n\n<br>\n\n#### 2.6 sklearn中TSNE参数\n\n**函数参数表：**\n\n![tsne-params](/posts_res/2018-11-12-tSNE/2-6.jpg)\n\n**返回对象的属性表：**\n\n![tsne-params](/posts_res/2018-11-12-tSNE/2-6-2.jpg)\n\n","source":"_posts/2018-11-12-tSNE.md","raw":"---\nlayout: post\ntitle: t-SNE降维\ndate: 2018-11-12 10:10 +0800\ncategories: 机器学习\ntags:\n- 优化算法\nmathjax: true\ncopyright: false\n---\n\n转载自：[t-SNE完整笔记](http://www.datakit.cn/blog/2017/02/05/t_sne_full.html)\n\n\nt-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，是由Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。\n此外，t-SNE是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。\nt-SNE是由SNE([Stochastic Neighbor Embedding](https://cs.nyu.edu/~roweis/papers/sne_final.pdf), SNE; Hinton and Roweis, 2002)发展而来。\n我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。\n\n\n-------------\n\n### 1.SNE\n\nSNE是通过[仿射(affinitie)变换](https://baike.baidu.com/item/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2)将数据点映射到概率分布上，主要包括两个步骤：\n\n- SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。\n- SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。\n\n我们看到t-SNE模型是非监督的降维，它跟k-means等不同，它不能通过训练得到一些东西之后再用于其它数据\n(比如k-means可以通过训练得到 k 个点，再用于其它数据集，而 t-SNE 只能单独的对数据做操作，也就是说他只有 fit_transform，而没有fit操作）\n\n<br>\n\nSNE是先将**欧氏距离转换为条件概率来表达点与点之间的相似度**。具体来说，给定 N 个高维的数据 $x_1, ..., x_N$ (注意 N 不是维度),\nt-SNE首先是计算概率$p_{ij}$，正比于$x_i$和$x_j$之间的相似度(这种概率是我们自主构建的)，即：\n\n$$ p_{j∣i} = \\frac{exp(− ∣∣ x_i − x_j ∣∣ ^2 / (2 \\sigma ^2_i))}{ \\sum_{k \\not= i} exp(− ∣∣ x_i − x_k ∣∣ ^2 / (2 \\sigma ^2_i))} $$\n\n这里的有一个参数是$ \\sigma_i $，对于不同的点 $x_i$ 取值不一样，后续会讨论如何设置。此外设置 $p_{x∣x} = 0$, 因为我们关注的是两两之间的相似度。\n\n<br>\n\n那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\\frac{1}{\\sqrt{2}}$，因此它们之间的相似度如下:\n\n$$ q_{j∣i} = \\frac{ exp(− ∣∣ x_i − x_j ∣∣ ^2)}{ \\sum_{k\\not=i} exp(−∣∣ x_i − x_k ∣∣ ^2)} $$\n\n同样，设定$q_{i∣i} = 0$。\n\n<br>\n\n如果降维的效果比较好，局部特征保留完整，那么 $p_{i∣j} = q_{i∣j}$, 因此我们优化两个分布之间的距离 - KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:\n\n$$ C = \\sum_{i} KL( P_i ∣∣ Q_i ) = \\sum_i \\sum_j p_{j∣i} log \\frac{p_{j∣i}}{q_{j∣i}} $$\n\n这里的 $P_i$ 表示了给定点 $x_i$ 下，其他所有数据点的条件概率分布。需要注意的是KL散度具有不对称性，在低维映射中不同的距离对应的惩罚权重是不同的，\n具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小\n(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q_{j∣i} = 0.2$ 来建模较大的 $p_{j∣i}=0.8$, $cost = p log(\\frac{p}{q}) = 1.11$, \n同样用较大的 $q_{j∣i}=0.8$ 来建模较大的 $p_{j∣i}=0.2$, $cost = -0.277$, 因此，**SNE会倾向于保留数据中的局部特征**。\n\n<br>\n\n下面我们开始正式的推导SNE。首先不同的点具有不同的 $\\sigma_i$，$P_i$ 的熵(entropy)会随着 $\\sigma_i$ 的增加而增加。\nSNE使用困惑度([perplexity](https://zh.wikipedia.org/wiki/%E5%9B%B0%E6%83%91%E5%BA%A6))的概念，用二分搜索的方式来寻找一个最佳的 $\\sigma$。其中困惑度指:\n\n$$ Perp(P_i) = 2^{H(P_i)} $$\n\n这里的 $H(P_i)$是 $P_i$ 的熵，即:\n\n$$ H(P_i) = − \\sum_j p_{j∣i} log_2 p_{j∣i} $$\n\n困惑度可以解释为一个点附近的有效近邻点个数。**SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间**，给定之后，使用二分搜索的方式寻找合适的$\\sigma$。\n\n那么核心问题是如何求解梯度了，目标函数等价于 $\\sum \\sum −p log(q) $这个式子与softmax非常的类似，我们知道softmax的目标函数是 $ \\sum −y log p$ ，对应的梯度是$y−p$\n(注：这里的softmax中 $y$ 表示label，$p$ 表示预估值)。同样我们可以推导SNE的目标函数中的$i$在$j$下的条件概率情况的梯度是$2 (p_{i∣j} − q_{i∣j})(y_i − y_j) $，\n同样$j$在$i$下的条件概率的梯度是 $2 ( p_{j∣i} − q_{j∣i})(y_i − y_j)$，最后得到完整的梯度公式如下:\n\n$$ \\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j∣i} − q_{j∣i} + p_{i∣j} − q_{i∣j})(y_i − y_j) $$\n\n在初始化中，可以用较小的 $\\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。\n即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:\n\n$$ Y^{(t)} = Y^{(t−1)} + \\eta \\frac{\\partial C}{ \\partial Y} + \\alpha (t)( Y^{(t−1)} − Y^{(t−2)}) $$\n\n这里的$Y^{(t)}$表示迭代$t$次的解，$\\eta$表示学习速率 ，$\\alpha (t)$ 表示迭代$t$次的动量。\n\n此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。\n因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。\n\n\n----------\n\n### 2.t-SNE\n\n尽管SNE提供了很好的可视化方法，但是它很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:\n\n- 使用对称版的SNE，简化梯度公式\n- 低维空间下，使用t分布替代高斯分布表达两点之间的相似度\n\nt-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。\n\n<br>\n\n#### 2.1 对称版SNE\n\n优化$p_{i∣j}$和$q_{i∣j}$的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即$P$是高维空间里各个点的联合概率分布，$Q$是低维空间下的，目标函数为:\n\n$$ C = KL( P ∣∣ Q) = \\sum_i \\sum_j p_{i,j} log p_{ij} q_{ij} $$\n\n这里的$p_{ii},q_{ii}$为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意$i, p_{ij} = p_{ji}, q_{ij} = q_{ji} $，因此概率分布可以改写为:\n\n$$ p_{ij} = \\frac{ exp(−∣∣ x_i − x_j ∣∣ ^2 / 2 \\sigma^2)}{\\sum_{k\\not=l} exp(−∣∣ x_k − x_l ∣∣ ^2 / 2 \\sigma^2)} \\quad \\quad \nq_{ij} = \\frac{exp(−∣∣ y_i − y_j ∣∣^2)}{ \\sum_{k≠l} exp(−∣∣ y_k − y_l ∣∣^2)} $$\n\n这种表达方式，使得整体简洁了很多。但是会引入**异常值**的问题。\n比如$x_i$是异常值，那么$∣∣ x_i − x_j ∣∣^2$会很大，对应的所有的$j, p_{ij}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对cost影响很小。\n\n\n为了解决这个问题，我们将联合概率分布定义修正为: $p_{ij} = (p_{i∣j} + p_{j∣i}) / 2 $, 这保证了 $\\sum_j p_{ij} > \\frac{1}{2n}$, 使得每个点对于cost都会有一定的贡献。\n对称SNE的最大优点是梯度计算变得简单了，如下:\n\n$$ \\frac{\\partial C}{ \\partial y_i} = 4 \\sum_j (p_{ij} − q_{ij})(y_i − y_j) $$\n\n实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。\n\n<br>\n\n#### 2.2 Crowding问题\n\n拥挤问题就是说各个簇聚集在一起，无法区分。\n比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，\n比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。\n进一步的说明，假设一个以数据点$x_i$为中心，半径为$r$的$m$维球(三维空间就是球)，其体积是按$r^m$增长的，\n假设数据点是在$m$维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。\n\n![crowding](/posts_res/2018-11-12-tSNE/2-2.png)\n\n从上图可以看到，随着维度的增大，大部分数据点都聚集在$m$维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。\n\n<br>\n\n> 怎么解决crowding问题呢？\n\n<br>\n\nCook et al.(2007) 提出一种slight repulsion的方式，在基线概率分布(uniform background)中引入一个较小的混合因子 $\\rho$，\n这样$q_{ij}$就永远不会小于 $ \\frac{2 \\rho}{n(n-1)}$ (因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q_{ij}$总是会比$p_{ij}$大一点。\n这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\\rho$为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\\rho$。\n直接优化UNI-SNE是不行的(即一开始$\\rho$不为0)，因为距离较远的两个点基本是一样的$q_{ij}$(等于基线分布), 即使$p_{ij}$很大，一些距离变化很难在$q_{ij}$中产生作用。\n也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。\n\n<br>\n\n#### 2.3 t-SNE\n\n对称SNE实际上在高维度下另外一种减轻”拥挤问题”的方法：\n\n在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。\n\n![outlier](/posts_res/2018-11-12-tSNE/2-3.png)\n\n我们对比一下高斯分布和t分布(如上图)，t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。\n\n使用了t分布之后的q变化，如下:\n\n$$ q_{ij} = \\frac{(1+∣∣ y_i − y_j ∣∣ ^2)^{−1}}{\\sum_{k\\not=l} (1 + ∣∣ y_i − y_j ∣∣ ^2)^{−1}} $$\n\n此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:\n\n$$ \\frac{\\partial C}{\\partial y_i} = 4 \\sum_j ( p_{ij} − q_{ij} ) (y_i − y_j) ( 1 + ∣∣ y_i − y_j ∣∣^2)^{−1} $$\n\nt-SNE的有效性，也可以从上图中看到：\n\n横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；\n而对于低相似度的点，t分布在低维空间中的距离需要更远。\n这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。\n\n总结一下，t-SNE的梯度更新有两大优势：\n\n- 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。\n- 这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。\n\n<br>\n\n#### 2.4 算法过程\n\n算法详细过程如下：\n\n- Data: $X = x_1, ..., x_n$\n- 计算cost function的参数：困惑度Perp\n- 优化参数: 设置迭代次数T，学习速率$\\eta$, 动量$\\alpha(t)$\n- 目标结果是低维数据表示 $Y^T = y_1, ..., y_n$\n- 开始优化\n    - 计算在给定Perp下的条件概率$p_{j∣i}$ (参见上面公式)\n    - 令 $p_{ij} = (p_{j∣i} + p_{i∣j}) / 2n $\n    - 用 $N(0, 10^{−4}I)$ 随机初始化 Y\n    - 迭代，从 t = 1 到 T， 做如下操作:\n        - 计算低维度下的 $q_{ij}$ (参见上面的公式)\n        - 计算梯度（参见上面的公式）\n        - 更新 $Y^t = Y^{t−1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)(Y^{t−1} − Y^{t−2}) $\n    - 结束\n- 结束\n\n<br>\n\n优化过程中可以尝试的两个trick:\n\n- 提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。\n- 提前夸大(early exaggeration)：在开始优化阶段，$p_{ij}$乘以一个大于1的数进行扩大，来避免因为$q_{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4。\n\n<br>\n\n#### 2.5 不足\n\n主要不足有四个:\n\n- 主要用于可视化，很难用于其他目的。\n    - 比如测试集合降维，因为它没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。\n- t-SNE倾向于保存局部特征。\n    - 对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间。\n- t-SNE没有唯一最优解，且没有预估部分。\n    - 如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。\n- 训练太慢。有很多基于树的算法在t-SNE上做一些改进\n\n<br>\n\n#### 2.6 sklearn中TSNE参数\n\n**函数参数表：**\n\n![tsne-params](/posts_res/2018-11-12-tSNE/2-6.jpg)\n\n**返回对象的属性表：**\n\n![tsne-params](/posts_res/2018-11-12-tSNE/2-6-2.jpg)\n\n","slug":"tSNE","published":1,"updated":"2019-08-17T09:40:37.459Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnwq003q2qwp8wt6fia6","content":"<p>转载自：<a href=\"http://www.datakit.cn/blog/2017/02/05/t_sne_full.html\" target=\"_blank\" rel=\"noopener\">t-SNE完整笔记</a></p><p>t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，是由Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。\n此外，t-SNE是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。\nt-SNE是由SNE(<a href=\"https://cs.nyu.edu/~roweis/papers/sne_final.pdf\" target=\"_blank\" rel=\"noopener\">Stochastic Neighbor Embedding</a>, SNE; Hinton and Roweis, 2002)发展而来。\n我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。</p><hr><h3 id=\"1-SNE\"><a href=\"#1-SNE\" class=\"headerlink\" title=\"1.SNE\"></a>1.SNE</h3><a id=\"more\"></a>\n\n\n<p>SNE是通过<a href=\"https://baike.baidu.com/item/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2\" target=\"_blank\" rel=\"noopener\">仿射(affinitie)变换</a>将数据点映射到概率分布上，主要包括两个步骤：</p>\n<ul>\n<li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li>\n<li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li>\n</ul>\n<p>我们看到t-SNE模型是非监督的降维，它跟k-means等不同，它不能通过训练得到一些东西之后再用于其它数据\n(比如k-means可以通过训练得到 k 个点，再用于其它数据集，而 t-SNE 只能单独的对数据做操作，也就是说他只有 fit_transform，而没有fit操作）</p>\n<p><br></p>\n<p>SNE是先将<strong>欧氏距离转换为条件概率来表达点与点之间的相似度</strong>。具体来说，给定 N 个高维的数据 $x<em>1, …, x_N$ (注意 N 不是维度),\nt-SNE首先是计算概率$p</em>{ij}$，正比于$x_i$和$x_j$之间的相似度(这种概率是我们自主构建的)，即：</p>\n<script type=\"math/tex; mode=display\">p_{j∣i} = \\frac{exp(− ∣∣ x_i − x_j ∣∣ ^2 / (2 \\sigma ^2_i))}{ \\sum_{k \\not= i} exp(− ∣∣ x_i − x_k ∣∣ ^2 / (2 \\sigma ^2_i))}</script><p>这里的有一个参数是$ \\sigma<em>i $，对于不同的点 $x_i$ 取值不一样，后续会讨论如何设置。此外设置 $p</em>{x∣x} = 0$, 因为我们关注的是两两之间的相似度。</p>\n<p><br></p>\n<p>那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\\frac{1}{\\sqrt{2}}$，因此它们之间的相似度如下:</p>\n<script type=\"math/tex; mode=display\">q_{j∣i} = \\frac{ exp(− ∣∣ x_i − x_j ∣∣ ^2)}{ \\sum_{k\\not=i} exp(−∣∣ x_i − x_k ∣∣ ^2)}</script><p>同样，设定$q_{i∣i} = 0$。</p>\n<p><br></p>\n<p>如果降维的效果比较好，局部特征保留完整，那么 $p<em>{i∣j} = q</em>{i∣j}$, 因此我们优化两个分布之间的距离 - KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:</p>\n<script type=\"math/tex; mode=display\">C = \\sum_{i} KL( P_i ∣∣ Q_i ) = \\sum_i \\sum_j p_{j∣i} log \\frac{p_{j∣i}}{q_{j∣i}}</script><p>这里的 $P<em>i$ 表示了给定点 $x_i$ 下，其他所有数据点的条件概率分布。需要注意的是KL散度具有不对称性，在低维映射中不同的距离对应的惩罚权重是不同的，\n具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小\n(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q</em>{j∣i} = 0.2$ 来建模较大的 $p<em>{j∣i}=0.8$, $cost = p log(\\frac{p}{q}) = 1.11$, \n同样用较大的 $q</em>{j∣i}=0.8$ 来建模较大的 $p_{j∣i}=0.2$, $cost = -0.277$, 因此，<strong>SNE会倾向于保留数据中的局部特征</strong>。</p>\n<p><br></p>\n<p>下面我们开始正式的推导SNE。首先不同的点具有不同的 $\\sigma_i$，$P_i$ 的熵(entropy)会随着 $\\sigma_i$ 的增加而增加。\nSNE使用困惑度(<a href=\"https://zh.wikipedia.org/wiki/%E5%9B%B0%E6%83%91%E5%BA%A6\" target=\"_blank\" rel=\"noopener\">perplexity</a>)的概念，用二分搜索的方式来寻找一个最佳的 $\\sigma$。其中困惑度指:</p>\n<script type=\"math/tex; mode=display\">Perp(P_i) = 2^{H(P_i)}</script><p>这里的 $H(P_i)$是 $P_i$ 的熵，即:</p>\n<script type=\"math/tex; mode=display\">H(P_i) = − \\sum_j p_{j∣i} log_2 p_{j∣i}</script><p>困惑度可以解释为一个点附近的有效近邻点个数。<strong>SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间</strong>，给定之后，使用二分搜索的方式寻找合适的$\\sigma$。</p>\n<p>那么核心问题是如何求解梯度了，目标函数等价于 $\\sum \\sum −p log(q) $这个式子与softmax非常的类似，我们知道softmax的目标函数是 $ \\sum −y log p$ ，对应的梯度是$y−p$\n(注：这里的softmax中 $y$ 表示label，$p$ 表示预估值)。同样我们可以推导SNE的目标函数中的$i$在$j$下的条件概率情况的梯度是$2 (p<em>{i∣j} − q</em>{i∣j})(y<em>i − y_j) $，\n同样$j$在$i$下的条件概率的梯度是 $2 ( p</em>{j∣i} − q_{j∣i})(y_i − y_j)$，最后得到完整的梯度公式如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j∣i} − q_{j∣i} + p_{i∣j} − q_{i∣j})(y_i − y_j)</script><p>在初始化中，可以用较小的 $\\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。\n即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:</p>\n<script type=\"math/tex; mode=display\">Y^{(t)} = Y^{(t−1)} + \\eta \\frac{\\partial C}{ \\partial Y} + \\alpha (t)( Y^{(t−1)} − Y^{(t−2)})</script><p>这里的$Y^{(t)}$表示迭代$t$次的解，$\\eta$表示学习速率 ，$\\alpha (t)$ 表示迭代$t$次的动量。</p>\n<p>此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。\n因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。</p>\n<hr>\n<h3 id=\"2-t-SNE\"><a href=\"#2-t-SNE\" class=\"headerlink\" title=\"2.t-SNE\"></a>2.t-SNE</h3><p>尽管SNE提供了很好的可视化方法，但是它很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:</p>\n<ul>\n<li>使用对称版的SNE，简化梯度公式</li>\n<li>低维空间下，使用t分布替代高斯分布表达两点之间的相似度</li>\n</ul>\n<p>t-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。</p>\n<p><br></p>\n<h4 id=\"2-1-对称版SNE\"><a href=\"#2-1-对称版SNE\" class=\"headerlink\" title=\"2.1 对称版SNE\"></a>2.1 对称版SNE</h4><p>优化$p<em>{i∣j}$和$q</em>{i∣j}$的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即$P$是高维空间里各个点的联合概率分布，$Q$是低维空间下的，目标函数为:</p>\n<script type=\"math/tex; mode=display\">C = KL( P ∣∣ Q) = \\sum_i \\sum_j p_{i,j} log p_{ij} q_{ij}</script><p>这里的$p<em>{ii},q</em>{ii}$为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意$i, p<em>{ij} = p</em>{ji}, q<em>{ij} = q</em>{ji} $，因此概率分布可以改写为:</p>\n<script type=\"math/tex; mode=display\">p_{ij} = \\frac{ exp(−∣∣ x_i − x_j ∣∣ ^2 / 2 \\sigma^2)}{\\sum_{k\\not=l} exp(−∣∣ x_k − x_l ∣∣ ^2 / 2 \\sigma^2)} \\quad \\quad \nq_{ij} = \\frac{exp(−∣∣ y_i − y_j ∣∣^2)}{ \\sum_{k≠l} exp(−∣∣ y_k − y_l ∣∣^2)}</script><p>这种表达方式，使得整体简洁了很多。但是会引入<strong>异常值</strong>的问题。\n比如$x<em>i$是异常值，那么$∣∣ x_i − x_j ∣∣^2$会很大，对应的所有的$j, p</em>{ij}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对cost影响很小。</p>\n<p>为了解决这个问题，我们将联合概率分布定义修正为: $p<em>{ij} = (p</em>{i∣j} + p<em>{j∣i}) / 2 $, 这保证了 $\\sum_j p</em>{ij} &gt; \\frac{1}{2n}$, 使得每个点对于cost都会有一定的贡献。\n对称SNE的最大优点是梯度计算变得简单了，如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{ \\partial y_i} = 4 \\sum_j (p_{ij} − q_{ij})(y_i − y_j)</script><p>实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。</p>\n<p><br></p>\n<h4 id=\"2-2-Crowding问题\"><a href=\"#2-2-Crowding问题\" class=\"headerlink\" title=\"2.2 Crowding问题\"></a>2.2 Crowding问题</h4><p>拥挤问题就是说各个簇聚集在一起，无法区分。\n比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，\n比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。\n进一步的说明，假设一个以数据点$x_i$为中心，半径为$r$的$m$维球(三维空间就是球)，其体积是按$r^m$增长的，\n假设数据点是在$m$维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。</p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-2.png\" alt=\"crowding\"></p>\n<p>从上图可以看到，随着维度的增大，大部分数据点都聚集在$m$维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。</p>\n<p><br></p>\n<blockquote>\n<p>怎么解决crowding问题呢？</p>\n</blockquote>\n<p><br></p>\n<p>Cook et al.(2007) 提出一种slight repulsion的方式，在基线概率分布(uniform background)中引入一个较小的混合因子 $\\rho$，\n这样$q<em>{ij}$就永远不会小于 $ \\frac{2 \\rho}{n(n-1)}$ (因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q</em>{ij}$总是会比$p<em>{ij}$大一点。\n这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\\rho$为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\\rho$。\n直接优化UNI-SNE是不行的(即一开始$\\rho$不为0)，因为距离较远的两个点基本是一样的$q</em>{ij}$(等于基线分布), 即使$p<em>{ij}$很大，一些距离变化很难在$q</em>{ij}$中产生作用。\n也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。</p>\n<p><br></p>\n<h4 id=\"2-3-t-SNE\"><a href=\"#2-3-t-SNE\" class=\"headerlink\" title=\"2.3 t-SNE\"></a>2.3 t-SNE</h4><p>对称SNE实际上在高维度下另外一种减轻”拥挤问题”的方法：</p>\n<p>在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。</p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-3.png\" alt=\"outlier\"></p>\n<p>我们对比一下高斯分布和t分布(如上图)，t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。</p>\n<p>使用了t分布之后的q变化，如下:</p>\n<script type=\"math/tex; mode=display\">q_{ij} = \\frac{(1+∣∣ y_i − y_j ∣∣ ^2)^{−1}}{\\sum_{k\\not=l} (1 + ∣∣ y_i − y_j ∣∣ ^2)^{−1}}</script><p>此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j ( p_{ij} − q_{ij} ) (y_i − y_j) ( 1 + ∣∣ y_i − y_j ∣∣^2)^{−1}</script><p>t-SNE的有效性，也可以从上图中看到：</p>\n<p>横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；\n而对于低相似度的点，t分布在低维空间中的距离需要更远。\n这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。</p>\n<p>总结一下，t-SNE的梯度更新有两大优势：</p>\n<ul>\n<li>对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。</li>\n<li>这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-4-算法过程\"><a href=\"#2-4-算法过程\" class=\"headerlink\" title=\"2.4 算法过程\"></a>2.4 算法过程</h4><p>算法详细过程如下：</p>\n<ul>\n<li>Data: $X = x_1, …, x_n$</li>\n<li>计算cost function的参数：困惑度Perp</li>\n<li>优化参数: 设置迭代次数T，学习速率$\\eta$, 动量$\\alpha(t)$</li>\n<li>目标结果是低维数据表示 $Y^T = y_1, …, y_n$</li>\n<li>开始优化<ul>\n<li>计算在给定Perp下的条件概率$p_{j∣i}$ (参见上面公式)</li>\n<li>令 $p<em>{ij} = (p</em>{j∣i} + p_{i∣j}) / 2n $</li>\n<li>用 $N(0, 10^{−4}I)$ 随机初始化 Y</li>\n<li>迭代，从 t = 1 到 T， 做如下操作:<ul>\n<li>计算低维度下的 $q_{ij}$ (参见上面的公式)</li>\n<li>计算梯度（参见上面的公式）</li>\n<li>更新 $Y^t = Y^{t−1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)(Y^{t−1} − Y^{t−2}) $</li>\n</ul>\n</li>\n<li>结束</li>\n</ul>\n</li>\n<li>结束</li>\n</ul>\n<p><br></p>\n<p>优化过程中可以尝试的两个trick:</p>\n<ul>\n<li>提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。</li>\n<li>提前夸大(early exaggeration)：在开始优化阶段，$p<em>{ij}$乘以一个大于1的数进行扩大，来避免因为$q</em>{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4。</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-5-不足\"><a href=\"#2-5-不足\" class=\"headerlink\" title=\"2.5 不足\"></a>2.5 不足</h4><p>主要不足有四个:</p>\n<ul>\n<li>主要用于可视化，很难用于其他目的。<ul>\n<li>比如测试集合降维，因为它没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。</li>\n</ul>\n</li>\n<li>t-SNE倾向于保存局部特征。<ul>\n<li>对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间。</li>\n</ul>\n</li>\n<li>t-SNE没有唯一最优解，且没有预估部分。<ul>\n<li>如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。</li>\n</ul>\n</li>\n<li>训练太慢。有很多基于树的算法在t-SNE上做一些改进</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-6-sklearn中TSNE参数\"><a href=\"#2-6-sklearn中TSNE参数\" class=\"headerlink\" title=\"2.6 sklearn中TSNE参数\"></a>2.6 sklearn中TSNE参数</h4><p><strong>函数参数表：</strong></p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-6.jpg\" alt=\"tsne-params\"></p>\n<p><strong>返回对象的属性表：</strong></p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-6-2.jpg\" alt=\"tsne-params\"></p>\n","site":{"data":{}},"excerpt":"<p>转载自：<a href=\"http://www.datakit.cn/blog/2017/02/05/t_sne_full.html\" target=\"_blank\" rel=\"noopener\">t-SNE完整笔记</a></p><p>t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，是由Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。\n此外，t-SNE是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。\nt-SNE是由SNE(<a href=\"https://cs.nyu.edu/~roweis/papers/sne_final.pdf\" target=\"_blank\" rel=\"noopener\">Stochastic Neighbor Embedding</a>, SNE; Hinton and Roweis, 2002)发展而来。\n我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。</p><hr><h3 id=\"1-SNE\"><a href=\"#1-SNE\" class=\"headerlink\" title=\"1.SNE\"></a>1.SNE</h3>","more":"\n\n\n<p>SNE是通过<a href=\"https://baike.baidu.com/item/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2\" target=\"_blank\" rel=\"noopener\">仿射(affinitie)变换</a>将数据点映射到概率分布上，主要包括两个步骤：</p>\n<ul>\n<li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li>\n<li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li>\n</ul>\n<p>我们看到t-SNE模型是非监督的降维，它跟k-means等不同，它不能通过训练得到一些东西之后再用于其它数据\n(比如k-means可以通过训练得到 k 个点，再用于其它数据集，而 t-SNE 只能单独的对数据做操作，也就是说他只有 fit_transform，而没有fit操作）</p>\n<p><br></p>\n<p>SNE是先将<strong>欧氏距离转换为条件概率来表达点与点之间的相似度</strong>。具体来说，给定 N 个高维的数据 $x<em>1, …, x_N$ (注意 N 不是维度),\nt-SNE首先是计算概率$p</em>{ij}$，正比于$x_i$和$x_j$之间的相似度(这种概率是我们自主构建的)，即：</p>\n<script type=\"math/tex; mode=display\">p_{j∣i} = \\frac{exp(− ∣∣ x_i − x_j ∣∣ ^2 / (2 \\sigma ^2_i))}{ \\sum_{k \\not= i} exp(− ∣∣ x_i − x_k ∣∣ ^2 / (2 \\sigma ^2_i))}</script><p>这里的有一个参数是$ \\sigma<em>i $，对于不同的点 $x_i$ 取值不一样，后续会讨论如何设置。此外设置 $p</em>{x∣x} = 0$, 因为我们关注的是两两之间的相似度。</p>\n<p><br></p>\n<p>那对于低维度下的$y_i$，我们可以指定高斯分布为方差为$\\frac{1}{\\sqrt{2}}$，因此它们之间的相似度如下:</p>\n<script type=\"math/tex; mode=display\">q_{j∣i} = \\frac{ exp(− ∣∣ x_i − x_j ∣∣ ^2)}{ \\sum_{k\\not=i} exp(−∣∣ x_i − x_k ∣∣ ^2)}</script><p>同样，设定$q_{i∣i} = 0$。</p>\n<p><br></p>\n<p>如果降维的效果比较好，局部特征保留完整，那么 $p<em>{i∣j} = q</em>{i∣j}$, 因此我们优化两个分布之间的距离 - KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:</p>\n<script type=\"math/tex; mode=display\">C = \\sum_{i} KL( P_i ∣∣ Q_i ) = \\sum_i \\sum_j p_{j∣i} log \\frac{p_{j∣i}}{q_{j∣i}}</script><p>这里的 $P<em>i$ 表示了给定点 $x_i$ 下，其他所有数据点的条件概率分布。需要注意的是KL散度具有不对称性，在低维映射中不同的距离对应的惩罚权重是不同的，\n具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小\n(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $q</em>{j∣i} = 0.2$ 来建模较大的 $p<em>{j∣i}=0.8$, $cost = p log(\\frac{p}{q}) = 1.11$, \n同样用较大的 $q</em>{j∣i}=0.8$ 来建模较大的 $p_{j∣i}=0.2$, $cost = -0.277$, 因此，<strong>SNE会倾向于保留数据中的局部特征</strong>。</p>\n<p><br></p>\n<p>下面我们开始正式的推导SNE。首先不同的点具有不同的 $\\sigma_i$，$P_i$ 的熵(entropy)会随着 $\\sigma_i$ 的增加而增加。\nSNE使用困惑度(<a href=\"https://zh.wikipedia.org/wiki/%E5%9B%B0%E6%83%91%E5%BA%A6\" target=\"_blank\" rel=\"noopener\">perplexity</a>)的概念，用二分搜索的方式来寻找一个最佳的 $\\sigma$。其中困惑度指:</p>\n<script type=\"math/tex; mode=display\">Perp(P_i) = 2^{H(P_i)}</script><p>这里的 $H(P_i)$是 $P_i$ 的熵，即:</p>\n<script type=\"math/tex; mode=display\">H(P_i) = − \\sum_j p_{j∣i} log_2 p_{j∣i}</script><p>困惑度可以解释为一个点附近的有效近邻点个数。<strong>SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间</strong>，给定之后，使用二分搜索的方式寻找合适的$\\sigma$。</p>\n<p>那么核心问题是如何求解梯度了，目标函数等价于 $\\sum \\sum −p log(q) $这个式子与softmax非常的类似，我们知道softmax的目标函数是 $ \\sum −y log p$ ，对应的梯度是$y−p$\n(注：这里的softmax中 $y$ 表示label，$p$ 表示预估值)。同样我们可以推导SNE的目标函数中的$i$在$j$下的条件概率情况的梯度是$2 (p<em>{i∣j} − q</em>{i∣j})(y<em>i − y_j) $，\n同样$j$在$i$下的条件概率的梯度是 $2 ( p</em>{j∣i} − q_{j∣i})(y_i − y_j)$，最后得到完整的梯度公式如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{\\partial y_i} = 2 \\sum_j (p_{j∣i} − q_{j∣i} + p_{i∣j} − q_{i∣j})(y_i − y_j)</script><p>在初始化中，可以用较小的 $\\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。\n即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:</p>\n<script type=\"math/tex; mode=display\">Y^{(t)} = Y^{(t−1)} + \\eta \\frac{\\partial C}{ \\partial Y} + \\alpha (t)( Y^{(t−1)} − Y^{(t−2)})</script><p>这里的$Y^{(t)}$表示迭代$t$次的解，$\\eta$表示学习速率 ，$\\alpha (t)$ 表示迭代$t$次的动量。</p>\n<p>此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。\n因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。</p>\n<hr>\n<h3 id=\"2-t-SNE\"><a href=\"#2-t-SNE\" class=\"headerlink\" title=\"2.t-SNE\"></a>2.t-SNE</h3><p>尽管SNE提供了很好的可视化方法，但是它很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:</p>\n<ul>\n<li>使用对称版的SNE，简化梯度公式</li>\n<li>低维空间下，使用t分布替代高斯分布表达两点之间的相似度</li>\n</ul>\n<p>t-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。</p>\n<p><br></p>\n<h4 id=\"2-1-对称版SNE\"><a href=\"#2-1-对称版SNE\" class=\"headerlink\" title=\"2.1 对称版SNE\"></a>2.1 对称版SNE</h4><p>优化$p<em>{i∣j}$和$q</em>{i∣j}$的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即$P$是高维空间里各个点的联合概率分布，$Q$是低维空间下的，目标函数为:</p>\n<script type=\"math/tex; mode=display\">C = KL( P ∣∣ Q) = \\sum_i \\sum_j p_{i,j} log p_{ij} q_{ij}</script><p>这里的$p<em>{ii},q</em>{ii}$为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意$i, p<em>{ij} = p</em>{ji}, q<em>{ij} = q</em>{ji} $，因此概率分布可以改写为:</p>\n<script type=\"math/tex; mode=display\">p_{ij} = \\frac{ exp(−∣∣ x_i − x_j ∣∣ ^2 / 2 \\sigma^2)}{\\sum_{k\\not=l} exp(−∣∣ x_k − x_l ∣∣ ^2 / 2 \\sigma^2)} \\quad \\quad \nq_{ij} = \\frac{exp(−∣∣ y_i − y_j ∣∣^2)}{ \\sum_{k≠l} exp(−∣∣ y_k − y_l ∣∣^2)}</script><p>这种表达方式，使得整体简洁了很多。但是会引入<strong>异常值</strong>的问题。\n比如$x<em>i$是异常值，那么$∣∣ x_i − x_j ∣∣^2$会很大，对应的所有的$j, p</em>{ij}$都会很小(之前是仅在$x_i$下很小)，导致低维映射下的$y_i$对cost影响很小。</p>\n<p>为了解决这个问题，我们将联合概率分布定义修正为: $p<em>{ij} = (p</em>{i∣j} + p<em>{j∣i}) / 2 $, 这保证了 $\\sum_j p</em>{ij} &gt; \\frac{1}{2n}$, 使得每个点对于cost都会有一定的贡献。\n对称SNE的最大优点是梯度计算变得简单了，如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{ \\partial y_i} = 4 \\sum_j (p_{ij} − q_{ij})(y_i − y_j)</script><p>实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。</p>\n<p><br></p>\n<h4 id=\"2-2-Crowding问题\"><a href=\"#2-2-Crowding问题\" class=\"headerlink\" title=\"2.2 Crowding问题\"></a>2.2 Crowding问题</h4><p>拥挤问题就是说各个簇聚集在一起，无法区分。\n比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，\n比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。\n进一步的说明，假设一个以数据点$x_i$为中心，半径为$r$的$m$维球(三维空间就是球)，其体积是按$r^m$增长的，\n假设数据点是在$m$维球中均匀分布的，我们来看看其他数据点与$x_i$的距离随维度增大而产生的变化。</p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-2.png\" alt=\"crowding\"></p>\n<p>从上图可以看到，随着维度的增大，大部分数据点都聚集在$m$维球的表面附近，与点$x_i$的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。</p>\n<p><br></p>\n<blockquote>\n<p>怎么解决crowding问题呢？</p>\n</blockquote>\n<p><br></p>\n<p>Cook et al.(2007) 提出一种slight repulsion的方式，在基线概率分布(uniform background)中引入一个较小的混合因子 $\\rho$，\n这样$q<em>{ij}$就永远不会小于 $ \\frac{2 \\rho}{n(n-1)}$ (因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q</em>{ij}$总是会比$p<em>{ij}$大一点。\n这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\\rho$为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\\rho$。\n直接优化UNI-SNE是不行的(即一开始$\\rho$不为0)，因为距离较远的两个点基本是一样的$q</em>{ij}$(等于基线分布), 即使$p<em>{ij}$很大，一些距离变化很难在$q</em>{ij}$中产生作用。\n也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。</p>\n<p><br></p>\n<h4 id=\"2-3-t-SNE\"><a href=\"#2-3-t-SNE\" class=\"headerlink\" title=\"2.3 t-SNE\"></a>2.3 t-SNE</h4><p>对称SNE实际上在高维度下另外一种减轻”拥挤问题”的方法：</p>\n<p>在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。</p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-3.png\" alt=\"outlier\"></p>\n<p>我们对比一下高斯分布和t分布(如上图)，t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。</p>\n<p>使用了t分布之后的q变化，如下:</p>\n<script type=\"math/tex; mode=display\">q_{ij} = \\frac{(1+∣∣ y_i − y_j ∣∣ ^2)^{−1}}{\\sum_{k\\not=l} (1 + ∣∣ y_i − y_j ∣∣ ^2)^{−1}}</script><p>此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:</p>\n<script type=\"math/tex; mode=display\">\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j ( p_{ij} − q_{ij} ) (y_i − y_j) ( 1 + ∣∣ y_i − y_j ∣∣^2)^{−1}</script><p>t-SNE的有效性，也可以从上图中看到：</p>\n<p>横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；\n而对于低相似度的点，t分布在低维空间中的距离需要更远。\n这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。</p>\n<p>总结一下，t-SNE的梯度更新有两大优势：</p>\n<ul>\n<li>对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。</li>\n<li>这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-4-算法过程\"><a href=\"#2-4-算法过程\" class=\"headerlink\" title=\"2.4 算法过程\"></a>2.4 算法过程</h4><p>算法详细过程如下：</p>\n<ul>\n<li>Data: $X = x_1, …, x_n$</li>\n<li>计算cost function的参数：困惑度Perp</li>\n<li>优化参数: 设置迭代次数T，学习速率$\\eta$, 动量$\\alpha(t)$</li>\n<li>目标结果是低维数据表示 $Y^T = y_1, …, y_n$</li>\n<li>开始优化<ul>\n<li>计算在给定Perp下的条件概率$p_{j∣i}$ (参见上面公式)</li>\n<li>令 $p<em>{ij} = (p</em>{j∣i} + p_{i∣j}) / 2n $</li>\n<li>用 $N(0, 10^{−4}I)$ 随机初始化 Y</li>\n<li>迭代，从 t = 1 到 T， 做如下操作:<ul>\n<li>计算低维度下的 $q_{ij}$ (参见上面的公式)</li>\n<li>计算梯度（参见上面的公式）</li>\n<li>更新 $Y^t = Y^{t−1} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha (t)(Y^{t−1} − Y^{t−2}) $</li>\n</ul>\n</li>\n<li>结束</li>\n</ul>\n</li>\n<li>结束</li>\n</ul>\n<p><br></p>\n<p>优化过程中可以尝试的两个trick:</p>\n<ul>\n<li>提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。</li>\n<li>提前夸大(early exaggeration)：在开始优化阶段，$p<em>{ij}$乘以一个大于1的数进行扩大，来避免因为$q</em>{ij}$太小导致优化太慢的问题。比如前50次迭代，$p_{ij}$乘以4。</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-5-不足\"><a href=\"#2-5-不足\" class=\"headerlink\" title=\"2.5 不足\"></a>2.5 不足</h4><p>主要不足有四个:</p>\n<ul>\n<li>主要用于可视化，很难用于其他目的。<ul>\n<li>比如测试集合降维，因为它没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。</li>\n</ul>\n</li>\n<li>t-SNE倾向于保存局部特征。<ul>\n<li>对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间。</li>\n</ul>\n</li>\n<li>t-SNE没有唯一最优解，且没有预估部分。<ul>\n<li>如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。</li>\n</ul>\n</li>\n<li>训练太慢。有很多基于树的算法在t-SNE上做一些改进</li>\n</ul>\n<p><br></p>\n<h4 id=\"2-6-sklearn中TSNE参数\"><a href=\"#2-6-sklearn中TSNE参数\" class=\"headerlink\" title=\"2.6 sklearn中TSNE参数\"></a>2.6 sklearn中TSNE参数</h4><p><strong>函数参数表：</strong></p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-6.jpg\" alt=\"tsne-params\"></p>\n<p><strong>返回对象的属性表：</strong></p>\n<p><img src=\"/posts_res/2018-11-12-tSNE/2-6-2.jpg\" alt=\"tsne-params\"></p>\n"},{"title":"hexo博客同步管理及迁移","mathjax":true,"copyright":false,"date":"2018-11-28T11:23:53.000Z","_content":"\n转自：[使用hexo，如果换了电脑怎么更新博客？ - 容与的回答 - 知乎](https://www.zhihu.com/question/21193762/answer/369050999)\n\n其他hexo自定义教程: [打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化](https://io-oi.me/tech/hexo-next-optimization.html)\n\n\n背景：一台电脑上已有一个在用的博客，又新用了一台电脑，实现原电脑和新电脑都可以提交更新博客，实现同步或者说博客的版本管理。\n\n\n## 原电脑操作\n\n- 在原电脑上操作，给 username.github.io 博客仓库创建 hexo 分支，并设为默认分支。\n\n在Github的username.github.io仓库上新建一个hexo分支，并切换到该分支；\n\n![1](/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg)\n\n并在该仓库->Settings->Branches->Default branch中将默认分支设为hexo，update更新保存；\n\n![2](/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg)\n\n- 如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。\n\n（如何创建和添加参考 [github help](https://link.zhihu.com/?target=https%3A//help.github.com/articles/connecting-to-github-with-ssh/) ）\n\n- 随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。\n\n在username.github.io目录使用Git Bash执行``git branch``命令查看当前所在分支，应为新建的分支hexo\n\n![3](/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg)\n\n- 显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。\n\n- 命令行 cd 到 clone 的仓库（username.github.io）执行下列命令\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\n```\n\n把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。\n\n- 将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地username.github.io目录删掉了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。\n\n- 将博客目录下 themes 文件夹下每个主题文件夹里面的 .git目录和 .gitignore文件删掉。 \n\n- 命令行 cd 到博客目录，执行下列命令\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\n```\n\n将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git目录和 .gitignore文件，主题文件夹下内容将传不上去。\n\n- 至此原电脑上的操作结束。\n\n\n## 新电脑操作\n\n- 先把环境安装好\n\n1. [node.js](https://nodejs.org/zh-cn/)\n2. [git](https://git-scm.com/downloads)\n3. [hexo](https://hexo.io/zh-cn/docs/setup)\n4. [ssh key]() 也[创建看这里](https://www.jianshu.com/p/fceaf373d797)和[添加看这里](https://github.com/settings/developers)好。\n\n- 创建添加ssh\n\n    - 命令行进入用户主目录,比如在我的电脑下：c:\\users\\administrator。\n    - 命令行运行`ssh-keygen -t rsa -C \"username@example.com\"`。它会提示你输入邮箱，输入完成以后一路回车就行。完成这一步，找到该目录下的.ssh文件夹，里面多了两个文件：id_rsa和id_rsa.pub分别是你这台电脑的ssh key的私钥和公钥。用文本打开公钥文件，复制里面的所有内容备用。\n    - 登录github后，进入 settings -> SSH and GPG keys。点击 New SSH key ,然后title随便填，比如可以是你的电脑名称，把刚才复制的公钥内容黏贴到key中，最后点击 Add SHH key 就完成了\n\n- 选好博客安装的目录，clone仓库。\n\n```\ngit clone git@github.com:username/username.github.io.git\n```\n\n- cd 到博客目录，执行下列命令\n\n```\nnpm install\nhexo g && hexo s\n```\n\n安装依赖，生成和启动博客服务。\n\n- 正常的话，浏览器打开 localhost:4000 可以看到博客了。\n- 至此新电脑操作完毕。\n\n\n## 更新源文件、部署静态文件\n\n以后无论在哪台电脑上，更新以及提交博客，依次执行。\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\nhexo clean && hexo g -d\n```\n\n即可备份源文件到hexo分支，部署静态文件到master分支。\n\n\n","source":"_posts/2018-11-28-hexo博客同步管理及迁移.md","raw":"---\ntitle: hexo博客同步管理及迁移\nmathjax: true\ncopyright: false\ndate: 2018-11-28 19:23:53\ncategories: 小工具\n---\n\n转自：[使用hexo，如果换了电脑怎么更新博客？ - 容与的回答 - 知乎](https://www.zhihu.com/question/21193762/answer/369050999)\n\n其他hexo自定义教程: [打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化](https://io-oi.me/tech/hexo-next-optimization.html)\n\n\n背景：一台电脑上已有一个在用的博客，又新用了一台电脑，实现原电脑和新电脑都可以提交更新博客，实现同步或者说博客的版本管理。\n\n\n## 原电脑操作\n\n- 在原电脑上操作，给 username.github.io 博客仓库创建 hexo 分支，并设为默认分支。\n\n在Github的username.github.io仓库上新建一个hexo分支，并切换到该分支；\n\n![1](/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg)\n\n并在该仓库->Settings->Branches->Default branch中将默认分支设为hexo，update更新保存；\n\n![2](/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg)\n\n- 如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。\n\n（如何创建和添加参考 [github help](https://link.zhihu.com/?target=https%3A//help.github.com/articles/connecting-to-github-with-ssh/) ）\n\n- 随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。\n\n在username.github.io目录使用Git Bash执行``git branch``命令查看当前所在分支，应为新建的分支hexo\n\n![3](/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg)\n\n- 显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。\n\n- 命令行 cd 到 clone 的仓库（username.github.io）执行下列命令\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\n```\n\n把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。\n\n- 将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地username.github.io目录删掉了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。\n\n- 将博客目录下 themes 文件夹下每个主题文件夹里面的 .git目录和 .gitignore文件删掉。 \n\n- 命令行 cd 到博客目录，执行下列命令\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\n```\n\n将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git目录和 .gitignore文件，主题文件夹下内容将传不上去。\n\n- 至此原电脑上的操作结束。\n\n\n## 新电脑操作\n\n- 先把环境安装好\n\n1. [node.js](https://nodejs.org/zh-cn/)\n2. [git](https://git-scm.com/downloads)\n3. [hexo](https://hexo.io/zh-cn/docs/setup)\n4. [ssh key]() 也[创建看这里](https://www.jianshu.com/p/fceaf373d797)和[添加看这里](https://github.com/settings/developers)好。\n\n- 创建添加ssh\n\n    - 命令行进入用户主目录,比如在我的电脑下：c:\\users\\administrator。\n    - 命令行运行`ssh-keygen -t rsa -C \"username@example.com\"`。它会提示你输入邮箱，输入完成以后一路回车就行。完成这一步，找到该目录下的.ssh文件夹，里面多了两个文件：id_rsa和id_rsa.pub分别是你这台电脑的ssh key的私钥和公钥。用文本打开公钥文件，复制里面的所有内容备用。\n    - 登录github后，进入 settings -> SSH and GPG keys。点击 New SSH key ,然后title随便填，比如可以是你的电脑名称，把刚才复制的公钥内容黏贴到key中，最后点击 Add SHH key 就完成了\n\n- 选好博客安装的目录，clone仓库。\n\n```\ngit clone git@github.com:username/username.github.io.git\n```\n\n- cd 到博客目录，执行下列命令\n\n```\nnpm install\nhexo g && hexo s\n```\n\n安装依赖，生成和启动博客服务。\n\n- 正常的话，浏览器打开 localhost:4000 可以看到博客了。\n- 至此新电脑操作完毕。\n\n\n## 更新源文件、部署静态文件\n\n以后无论在哪台电脑上，更新以及提交博客，依次执行。\n\n```\ngit add -A\ngit commit -m \"--\"\ngit push origin hexo\nhexo clean && hexo g -d\n```\n\n即可备份源文件到hexo分支，部署静态文件到master分支。\n\n\n","slug":"hexo博客同步管理及迁移","published":1,"updated":"2019-08-18T03:48:19.364Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnws003u2qwpd047udpn","content":"<p>转自：<a href=\"https://www.zhihu.com/question/21193762/answer/369050999\" target=\"_blank\" rel=\"noopener\">使用hexo，如果换了电脑怎么更新博客？ - 容与的回答 - 知乎</a></p><p>其他hexo自定义教程: <a href=\"https://io-oi.me/tech/hexo-next-optimization.html\" target=\"_blank\" rel=\"noopener\">打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化</a></p><p>背景：一台电脑上已有一个在用的博客，又新用了一台电脑，实现原电脑和新电脑都可以提交更新博客，实现同步或者说博客的版本管理。</p><h2 id=\"原电脑操作\"><a href=\"#原电脑操作\" class=\"headerlink\" title=\"原电脑操作\"></a>原电脑操作</h2><ul>\n<li>在原电脑上操作，给 username.github.io 博客仓库创建 hexo 分支，并设为默认分支。</li>\n</ul><p>在Github的username.github.io仓库上新建一个hexo分支，并切换到该分支；</p><p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg\" alt=\"1\"></p><p>并在该仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为hexo，update更新保存；</p><a id=\"more\"></a>\n\n\n\n\n\n\n<p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg\" alt=\"2\"></p>\n<ul>\n<li>如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。</li>\n</ul>\n<p>（如何创建和添加参考 <a href=\"https://link.zhihu.com/?target=https%3A//help.github.com/articles/connecting-to-github-with-ssh/\" target=\"_blank\" rel=\"noopener\">github help</a> ）</p>\n<ul>\n<li>随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。</li>\n</ul>\n<p>在username.github.io目录使用Git Bash执行<code>git branch</code>命令查看当前所在分支，应为新建的分支hexo</p>\n<p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg\" alt=\"3\"></p>\n<ul>\n<li><p>显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。</p>\n</li>\n<li><p>命令行 cd 到 clone 的仓库（username.github.io）执行下列命令</p>\n</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br></pre></td></tr></table></figure>\n<p>把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。</p>\n<ul>\n<li><p>将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地username.github.io目录删掉了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。</p>\n</li>\n<li><p>将博客目录下 themes 文件夹下每个主题文件夹里面的 .git目录和 .gitignore文件删掉。 </p>\n</li>\n<li><p>命令行 cd 到博客目录，执行下列命令</p>\n</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br></pre></td></tr></table></figure>\n<p>将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git目录和 .gitignore文件，主题文件夹下内容将传不上去。</p>\n<ul>\n<li>至此原电脑上的操作结束。</li>\n</ul>\n<h2 id=\"新电脑操作\"><a href=\"#新电脑操作\" class=\"headerlink\" title=\"新电脑操作\"></a>新电脑操作</h2><ul>\n<li>先把环境安装好</li>\n</ul>\n<ol>\n<li><a href=\"https://nodejs.org/zh-cn/\" target=\"_blank\" rel=\"noopener\">node.js</a></li>\n<li><a href=\"https://git-scm.com/downloads\" target=\"_blank\" rel=\"noopener\">git</a></li>\n<li><a href=\"https://hexo.io/zh-cn/docs/setup\" target=\"_blank\" rel=\"noopener\">hexo</a></li>\n<li><a href>ssh key</a> 也<a href=\"https://www.jianshu.com/p/fceaf373d797\" target=\"_blank\" rel=\"noopener\">创建看这里</a>和<a href=\"https://github.com/settings/developers\" target=\"_blank\" rel=\"noopener\">添加看这里</a>好。</li>\n</ol>\n<ul>\n<li><p>创建添加ssh</p>\n<ul>\n<li>命令行进入用户主目录,比如在我的电脑下：c:\\users\\administrator。</li>\n<li>命令行运行<code>ssh-keygen -t rsa -C &quot;username@example.com&quot;</code>。它会提示你输入邮箱，输入完成以后一路回车就行。完成这一步，找到该目录下的.ssh文件夹，里面多了两个文件：id_rsa和id_rsa.pub分别是你这台电脑的ssh key的私钥和公钥。用文本打开公钥文件，复制里面的所有内容备用。</li>\n<li>登录github后，进入 settings -&gt; SSH and GPG keys。点击 New SSH key ,然后title随便填，比如可以是你的电脑名称，把刚才复制的公钥内容黏贴到key中，最后点击 Add SHH key 就完成了</li>\n</ul>\n</li>\n<li><p>选好博客安装的目录，clone仓库。</p>\n</li>\n</ul>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone git@github<span class=\"selector-class\">.com</span>:username/username<span class=\"selector-class\">.github</span><span class=\"selector-class\">.io</span><span class=\"selector-class\">.git</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>cd 到博客目录，执行下列命令</li>\n</ul>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install</span><br><span class=\"line\">hexo g <span class=\"meta\">&amp;&amp; hexo s</span></span><br></pre></td></tr></table></figure>\n<p>安装依赖，生成和启动博客服务。</p>\n<ul>\n<li>正常的话，浏览器打开 localhost:4000 可以看到博客了。</li>\n<li>至此新电脑操作完毕。</li>\n</ul>\n<h2 id=\"更新源文件、部署静态文件\"><a href=\"#更新源文件、部署静态文件\" class=\"headerlink\" title=\"更新源文件、部署静态文件\"></a>更新源文件、部署静态文件</h2><p>以后无论在哪台电脑上，更新以及提交博客，依次执行。</p>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br><span class=\"line\"><span class=\"symbol\">hexo</span> clean &amp;&amp; hexo g -d</span><br></pre></td></tr></table></figure>\n<p>即可备份源文件到hexo分支，部署静态文件到master分支。</p>\n","site":{"data":{}},"excerpt":"<p>转自：<a href=\"https://www.zhihu.com/question/21193762/answer/369050999\" target=\"_blank\" rel=\"noopener\">使用hexo，如果换了电脑怎么更新博客？ - 容与的回答 - 知乎</a></p><p>其他hexo自定义教程: <a href=\"https://io-oi.me/tech/hexo-next-optimization.html\" target=\"_blank\" rel=\"noopener\">打造个性超赞博客 Hexo + NexT + GitHub Pages 的超深度优化</a></p><p>背景：一台电脑上已有一个在用的博客，又新用了一台电脑，实现原电脑和新电脑都可以提交更新博客，实现同步或者说博客的版本管理。</p><h2 id=\"原电脑操作\"><a href=\"#原电脑操作\" class=\"headerlink\" title=\"原电脑操作\"></a>原电脑操作</h2><ul>\n<li>在原电脑上操作，给 username.github.io 博客仓库创建 hexo 分支，并设为默认分支。</li>\n</ul><p>在Github的username.github.io仓库上新建一个hexo分支，并切换到该分支；</p><p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/1.jpg\" alt=\"1\"></p><p>并在该仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为hexo，update更新保存；</p>","more":"\n\n\n\n\n\n\n<p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/2.jpg\" alt=\"2\"></p>\n<ul>\n<li>如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。</li>\n</ul>\n<p>（如何创建和添加参考 <a href=\"https://link.zhihu.com/?target=https%3A//help.github.com/articles/connecting-to-github-with-ssh/\" target=\"_blank\" rel=\"noopener\">github help</a> ）</p>\n<ul>\n<li>随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。</li>\n</ul>\n<p>在username.github.io目录使用Git Bash执行<code>git branch</code>命令查看当前所在分支，应为新建的分支hexo</p>\n<p><img src=\"/posts_res/2018-11-28-hexo博客同步管理及迁移/3.jpg\" alt=\"3\"></p>\n<ul>\n<li><p>显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。</p>\n</li>\n<li><p>命令行 cd 到 clone 的仓库（username.github.io）执行下列命令</p>\n</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br></pre></td></tr></table></figure>\n<p>把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。</p>\n<ul>\n<li><p>将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地username.github.io目录删掉了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。</p>\n</li>\n<li><p>将博客目录下 themes 文件夹下每个主题文件夹里面的 .git目录和 .gitignore文件删掉。 </p>\n</li>\n<li><p>命令行 cd 到博客目录，执行下列命令</p>\n</li>\n</ul>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br></pre></td></tr></table></figure>\n<p>将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git目录和 .gitignore文件，主题文件夹下内容将传不上去。</p>\n<ul>\n<li>至此原电脑上的操作结束。</li>\n</ul>\n<h2 id=\"新电脑操作\"><a href=\"#新电脑操作\" class=\"headerlink\" title=\"新电脑操作\"></a>新电脑操作</h2><ul>\n<li>先把环境安装好</li>\n</ul>\n<ol>\n<li><a href=\"https://nodejs.org/zh-cn/\" target=\"_blank\" rel=\"noopener\">node.js</a></li>\n<li><a href=\"https://git-scm.com/downloads\" target=\"_blank\" rel=\"noopener\">git</a></li>\n<li><a href=\"https://hexo.io/zh-cn/docs/setup\" target=\"_blank\" rel=\"noopener\">hexo</a></li>\n<li><a href>ssh key</a> 也<a href=\"https://www.jianshu.com/p/fceaf373d797\" target=\"_blank\" rel=\"noopener\">创建看这里</a>和<a href=\"https://github.com/settings/developers\" target=\"_blank\" rel=\"noopener\">添加看这里</a>好。</li>\n</ol>\n<ul>\n<li><p>创建添加ssh</p>\n<ul>\n<li>命令行进入用户主目录,比如在我的电脑下：c:\\users\\administrator。</li>\n<li>命令行运行<code>ssh-keygen -t rsa -C &quot;username@example.com&quot;</code>。它会提示你输入邮箱，输入完成以后一路回车就行。完成这一步，找到该目录下的.ssh文件夹，里面多了两个文件：id_rsa和id_rsa.pub分别是你这台电脑的ssh key的私钥和公钥。用文本打开公钥文件，复制里面的所有内容备用。</li>\n<li>登录github后，进入 settings -&gt; SSH and GPG keys。点击 New SSH key ,然后title随便填，比如可以是你的电脑名称，把刚才复制的公钥内容黏贴到key中，最后点击 Add SHH key 就完成了</li>\n</ul>\n</li>\n<li><p>选好博客安装的目录，clone仓库。</p>\n</li>\n</ul>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone git@github<span class=\"selector-class\">.com</span>:username/username<span class=\"selector-class\">.github</span><span class=\"selector-class\">.io</span><span class=\"selector-class\">.git</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>cd 到博客目录，执行下列命令</li>\n</ul>\n<figure class=\"highlight 1c\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm install</span><br><span class=\"line\">hexo g <span class=\"meta\">&amp;&amp; hexo s</span></span><br></pre></td></tr></table></figure>\n<p>安装依赖，生成和启动博客服务。</p>\n<ul>\n<li>正常的话，浏览器打开 localhost:4000 可以看到博客了。</li>\n<li>至此新电脑操作完毕。</li>\n</ul>\n<h2 id=\"更新源文件、部署静态文件\"><a href=\"#更新源文件、部署静态文件\" class=\"headerlink\" title=\"更新源文件、部署静态文件\"></a>更新源文件、部署静态文件</h2><p>以后无论在哪台电脑上，更新以及提交博客，依次执行。</p>\n<figure class=\"highlight armasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">add </span>-A</span><br><span class=\"line\"><span class=\"symbol\">git</span> commit -m <span class=\"string\">\"--\"</span></span><br><span class=\"line\"><span class=\"symbol\">git</span> <span class=\"keyword\">push </span>origin hexo</span><br><span class=\"line\"><span class=\"symbol\">hexo</span> clean &amp;&amp; hexo g -d</span><br></pre></td></tr></table></figure>\n<p>即可备份源文件到hexo分支，部署静态文件到master分支。</p>\n"},{"title":"推荐系统HandBook Chapter3 基于内容的推荐系统-前沿和趋势","mathjax":true,"copyright":true,"date":"2018-11-28T07:20:33.000Z","_content":"\n\n物品推荐的问题已经有广泛的研究，现有两类主要方式。**基于内容**的推荐系统试图推荐给定用户过去喜欢的相似物品，\n而**协同过滤**推荐方式的系统识别出拥有相同爱好的用户，并推荐他们喜欢过的物品。\n\n\n## 基于内容的推荐系统的基础\n\n一个基于内容的推荐系统的高层次结构下图所示。推荐的过程包含有三个阶段，每一个阶段都由独立的部件控制。\n\n![基于内容的推荐系统的高层次结构](/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg)\n\n- 内容分析器\n    - 该部件的主要功能是将来自信息源的对象的内容表示成恰当的格式，以便于下一阶段的处理。\n- 信息学习器\n    - 该部件收集用户偏好的数据特征，并试图通过机器学习技术泛化这些数据，从而构建用户特征信息。\n- 过滤组件\n    - 该部件将用户与物品在表示空间中进行匹配，之后生成一个潜在感兴趣物品的排名清单。\n\n### 基于内容的推荐系统的推荐流程\n\n1. 根据用户u对物品i的评分r，信息学习器通过监督学习算法，为该用户生成偏好特征；\n2. 用户偏好特征存储在信息资源库里，并被接下来的过滤组件使用；\n3. 比较用户偏好特征和新物品的特征，过滤组件预测用户对新物品的感兴趣程度；\n4. 过滤组件生成一个推荐列表给用户。\n5. 用户偏好会不断变化，上述步骤不断迭代。\n\n\n### 基于内容过滤的优缺点\n\n基于内容的推荐与基于协同过滤的推荐相比有一下优点：\n\n- 用户独立性\n    - 只使用当前用户提供的评分构建个人信息\n    - 协同过滤使用近邻用户，只有用户近邻喜欢的物品才有可能被推荐\n- 透明度\n    - 通过显式列出推荐列表中的物品特征，可以对推荐进行解释\n    - 协同过滤是黑盒子，解释仅来自相似的未知用户\n- 新物品\n    - 新物品没有用户评分也能被推荐\n    - 协同过滤只有被一系列用户评分后，才有可能被推荐\n\n缺点：\n\n- 可分析的内容有限\n    - 受特征数量、类型以及领域知识的限制\n- 过度特化\n    - 无法获得出人意料的推荐。（不能发现用户的潜在兴趣）\n- 新用户\n    - 必须收集到足够的用户评分后，才能理解用户偏好并给予推荐\n\n\n## 基于内容的推荐系统的现状\n\n\n### 基于关键词系统概述\n\n- Web推荐系统领域\n    - [Letizia]()\n    - [Personal Web Watcher]()\n    - [Syskill&Webert]()\n    - [ifWeb]()\n    - [Amalthea]()\n    - [WebMate]()\n- 新闻过滤领域\n    - [NewT]()\n    - [PSUN]()\n    - [INFOrmer]()\n    - [NewsDude]()\n    - [Daily Learner]()\n    - [YourNews]()\n- 音乐领域\n    - [Last.fm]()使用协同过滤\n    - [MyStrands]()使用协同过滤\n    - [Pandora]()使用人工基于内容的描述\n- 融合协同过滤和基于内容的方法的混合推荐系统\n    - [Fab]()\n    - [WebWacther]()\n    - [ProfBuilder]()\n    - [PTV]()\n    - [Content-boosted Collaborative Filtering]()\n    - [CinemaScreen]()\n\n\n### 基于本体的语义分析\n\n- [SiteIF]()\n    - 多语言新闻网站的个性化工具\n    - 第一个采用基于感知的文档表示来对用户兴趣建模\n    - 词领域消歧\n- [ITR, Item Recommender]()\n    - 词感知消歧\n- [SEWeP]()\n    - 利用日志和Web站点内容的语义实现个性化\n- [Quickstep]()\n    - 在线学术研究论文的推荐系统\n- [informed Recommender]()\n    - 使用消费者产品评价给出推荐建议\n    - 系统通过作为知识表示和共享的翻译本体，把用户观点转换成一个结构化形式\n- [News@hand]()\n    - 采用基于本体表示物品特征和用户偏好来推荐新闻的系统\n\n\n## 趋势和未来研究\n\n略\n\n","source":"_posts/2018-11-28-推荐系统HandBook-Chapter3.md","raw":"---\ntitle: 推荐系统HandBook Chapter3 基于内容的推荐系统-前沿和趋势\nmathjax: true\ncopyright: true\ndate: 2018-11-28 15:20:33\ncategories: 推荐系统HandBook读书笔记\ntags:\n- 读书笔记\n---\n\n\n物品推荐的问题已经有广泛的研究，现有两类主要方式。**基于内容**的推荐系统试图推荐给定用户过去喜欢的相似物品，\n而**协同过滤**推荐方式的系统识别出拥有相同爱好的用户，并推荐他们喜欢过的物品。\n\n\n## 基于内容的推荐系统的基础\n\n一个基于内容的推荐系统的高层次结构下图所示。推荐的过程包含有三个阶段，每一个阶段都由独立的部件控制。\n\n![基于内容的推荐系统的高层次结构](/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg)\n\n- 内容分析器\n    - 该部件的主要功能是将来自信息源的对象的内容表示成恰当的格式，以便于下一阶段的处理。\n- 信息学习器\n    - 该部件收集用户偏好的数据特征，并试图通过机器学习技术泛化这些数据，从而构建用户特征信息。\n- 过滤组件\n    - 该部件将用户与物品在表示空间中进行匹配，之后生成一个潜在感兴趣物品的排名清单。\n\n### 基于内容的推荐系统的推荐流程\n\n1. 根据用户u对物品i的评分r，信息学习器通过监督学习算法，为该用户生成偏好特征；\n2. 用户偏好特征存储在信息资源库里，并被接下来的过滤组件使用；\n3. 比较用户偏好特征和新物品的特征，过滤组件预测用户对新物品的感兴趣程度；\n4. 过滤组件生成一个推荐列表给用户。\n5. 用户偏好会不断变化，上述步骤不断迭代。\n\n\n### 基于内容过滤的优缺点\n\n基于内容的推荐与基于协同过滤的推荐相比有一下优点：\n\n- 用户独立性\n    - 只使用当前用户提供的评分构建个人信息\n    - 协同过滤使用近邻用户，只有用户近邻喜欢的物品才有可能被推荐\n- 透明度\n    - 通过显式列出推荐列表中的物品特征，可以对推荐进行解释\n    - 协同过滤是黑盒子，解释仅来自相似的未知用户\n- 新物品\n    - 新物品没有用户评分也能被推荐\n    - 协同过滤只有被一系列用户评分后，才有可能被推荐\n\n缺点：\n\n- 可分析的内容有限\n    - 受特征数量、类型以及领域知识的限制\n- 过度特化\n    - 无法获得出人意料的推荐。（不能发现用户的潜在兴趣）\n- 新用户\n    - 必须收集到足够的用户评分后，才能理解用户偏好并给予推荐\n\n\n## 基于内容的推荐系统的现状\n\n\n### 基于关键词系统概述\n\n- Web推荐系统领域\n    - [Letizia]()\n    - [Personal Web Watcher]()\n    - [Syskill&Webert]()\n    - [ifWeb]()\n    - [Amalthea]()\n    - [WebMate]()\n- 新闻过滤领域\n    - [NewT]()\n    - [PSUN]()\n    - [INFOrmer]()\n    - [NewsDude]()\n    - [Daily Learner]()\n    - [YourNews]()\n- 音乐领域\n    - [Last.fm]()使用协同过滤\n    - [MyStrands]()使用协同过滤\n    - [Pandora]()使用人工基于内容的描述\n- 融合协同过滤和基于内容的方法的混合推荐系统\n    - [Fab]()\n    - [WebWacther]()\n    - [ProfBuilder]()\n    - [PTV]()\n    - [Content-boosted Collaborative Filtering]()\n    - [CinemaScreen]()\n\n\n### 基于本体的语义分析\n\n- [SiteIF]()\n    - 多语言新闻网站的个性化工具\n    - 第一个采用基于感知的文档表示来对用户兴趣建模\n    - 词领域消歧\n- [ITR, Item Recommender]()\n    - 词感知消歧\n- [SEWeP]()\n    - 利用日志和Web站点内容的语义实现个性化\n- [Quickstep]()\n    - 在线学术研究论文的推荐系统\n- [informed Recommender]()\n    - 使用消费者产品评价给出推荐建议\n    - 系统通过作为知识表示和共享的翻译本体，把用户观点转换成一个结构化形式\n- [News@hand]()\n    - 采用基于本体表示物品特征和用户偏好来推荐新闻的系统\n\n\n## 趋势和未来研究\n\n略\n\n","slug":"推荐系统HandBook-Chapter3","published":1,"updated":"2019-08-17T09:41:13.271Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnwt003x2qwpimqu5mdk","content":"<p>物品推荐的问题已经有广泛的研究，现有两类主要方式。<strong>基于内容</strong>的推荐系统试图推荐给定用户过去喜欢的相似物品，\n而<strong>协同过滤</strong>推荐方式的系统识别出拥有相同爱好的用户，并推荐他们喜欢过的物品。</p><h2 id=\"基于内容的推荐系统的基础\"><a href=\"#基于内容的推荐系统的基础\" class=\"headerlink\" title=\"基于内容的推荐系统的基础\"></a>基于内容的推荐系统的基础</h2><p>一个基于内容的推荐系统的高层次结构下图所示。推荐的过程包含有三个阶段，每一个阶段都由独立的部件控制。</p><p><img src=\"/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg\" alt=\"基于内容的推荐系统的高层次结构\"></p><ul>\n<li>内容分析器<ul>\n<li>该部件的主要功能是将来自信息源的对象的内容表示成恰当的格式，以便于下一阶段的处理。</li>\n</ul>\n</li>\n<li>信息学习器<ul>\n<li>该部件收集用户偏好的数据特征，并试图通过机器学习技术泛化这些数据，从而构建用户特征信息。</li>\n</ul>\n</li>\n<li>过滤组件<ul>\n<li>该部件将用户与物品在表示空间中进行匹配，之后生成一个潜在感兴趣物品的排名清单。</li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n\n\n\n<h3 id=\"基于内容的推荐系统的推荐流程\"><a href=\"#基于内容的推荐系统的推荐流程\" class=\"headerlink\" title=\"基于内容的推荐系统的推荐流程\"></a>基于内容的推荐系统的推荐流程</h3><ol>\n<li>根据用户u对物品i的评分r，信息学习器通过监督学习算法，为该用户生成偏好特征；</li>\n<li>用户偏好特征存储在信息资源库里，并被接下来的过滤组件使用；</li>\n<li>比较用户偏好特征和新物品的特征，过滤组件预测用户对新物品的感兴趣程度；</li>\n<li>过滤组件生成一个推荐列表给用户。</li>\n<li>用户偏好会不断变化，上述步骤不断迭代。</li>\n</ol>\n<h3 id=\"基于内容过滤的优缺点\"><a href=\"#基于内容过滤的优缺点\" class=\"headerlink\" title=\"基于内容过滤的优缺点\"></a>基于内容过滤的优缺点</h3><p>基于内容的推荐与基于协同过滤的推荐相比有一下优点：</p>\n<ul>\n<li>用户独立性<ul>\n<li>只使用当前用户提供的评分构建个人信息</li>\n<li>协同过滤使用近邻用户，只有用户近邻喜欢的物品才有可能被推荐</li>\n</ul>\n</li>\n<li>透明度<ul>\n<li>通过显式列出推荐列表中的物品特征，可以对推荐进行解释</li>\n<li>协同过滤是黑盒子，解释仅来自相似的未知用户</li>\n</ul>\n</li>\n<li>新物品<ul>\n<li>新物品没有用户评分也能被推荐</li>\n<li>协同过滤只有被一系列用户评分后，才有可能被推荐</li>\n</ul>\n</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>可分析的内容有限<ul>\n<li>受特征数量、类型以及领域知识的限制</li>\n</ul>\n</li>\n<li>过度特化<ul>\n<li>无法获得出人意料的推荐。（不能发现用户的潜在兴趣）</li>\n</ul>\n</li>\n<li>新用户<ul>\n<li>必须收集到足够的用户评分后，才能理解用户偏好并给予推荐</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"基于内容的推荐系统的现状\"><a href=\"#基于内容的推荐系统的现状\" class=\"headerlink\" title=\"基于内容的推荐系统的现状\"></a>基于内容的推荐系统的现状</h2><h3 id=\"基于关键词系统概述\"><a href=\"#基于关键词系统概述\" class=\"headerlink\" title=\"基于关键词系统概述\"></a>基于关键词系统概述</h3><ul>\n<li>Web推荐系统领域<ul>\n<li><a href>Letizia</a></li>\n<li><a href>Personal Web Watcher</a></li>\n<li><a href>Syskill&amp;Webert</a></li>\n<li><a href>ifWeb</a></li>\n<li><a href>Amalthea</a></li>\n<li><a href>WebMate</a></li>\n</ul>\n</li>\n<li>新闻过滤领域<ul>\n<li><a href>NewT</a></li>\n<li><a href>PSUN</a></li>\n<li><a href>INFOrmer</a></li>\n<li><a href>NewsDude</a></li>\n<li><a href>Daily Learner</a></li>\n<li><a href>YourNews</a></li>\n</ul>\n</li>\n<li>音乐领域<ul>\n<li><a href>Last.fm</a>使用协同过滤</li>\n<li><a href>MyStrands</a>使用协同过滤</li>\n<li><a href>Pandora</a>使用人工基于内容的描述</li>\n</ul>\n</li>\n<li>融合协同过滤和基于内容的方法的混合推荐系统<ul>\n<li><a href>Fab</a></li>\n<li><a href>WebWacther</a></li>\n<li><a href>ProfBuilder</a></li>\n<li><a href>PTV</a></li>\n<li><a href>Content-boosted Collaborative Filtering</a></li>\n<li><a href>CinemaScreen</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"基于本体的语义分析\"><a href=\"#基于本体的语义分析\" class=\"headerlink\" title=\"基于本体的语义分析\"></a>基于本体的语义分析</h3><ul>\n<li><a href>SiteIF</a><ul>\n<li>多语言新闻网站的个性化工具</li>\n<li>第一个采用基于感知的文档表示来对用户兴趣建模</li>\n<li>词领域消歧</li>\n</ul>\n</li>\n<li><a href>ITR, Item Recommender</a><ul>\n<li>词感知消歧</li>\n</ul>\n</li>\n<li><a href>SEWeP</a><ul>\n<li>利用日志和Web站点内容的语义实现个性化</li>\n</ul>\n</li>\n<li><a href>Quickstep</a><ul>\n<li>在线学术研究论文的推荐系统</li>\n</ul>\n</li>\n<li><a href>informed Recommender</a><ul>\n<li>使用消费者产品评价给出推荐建议</li>\n<li>系统通过作为知识表示和共享的翻译本体，把用户观点转换成一个结构化形式</li>\n</ul>\n</li>\n<li><a href>News@hand</a><ul>\n<li>采用基于本体表示物品特征和用户偏好来推荐新闻的系统</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"趋势和未来研究\"><a href=\"#趋势和未来研究\" class=\"headerlink\" title=\"趋势和未来研究\"></a>趋势和未来研究</h2><p>略</p>\n","site":{"data":{}},"excerpt":"<p>物品推荐的问题已经有广泛的研究，现有两类主要方式。<strong>基于内容</strong>的推荐系统试图推荐给定用户过去喜欢的相似物品，\n而<strong>协同过滤</strong>推荐方式的系统识别出拥有相同爱好的用户，并推荐他们喜欢过的物品。</p><h2 id=\"基于内容的推荐系统的基础\"><a href=\"#基于内容的推荐系统的基础\" class=\"headerlink\" title=\"基于内容的推荐系统的基础\"></a>基于内容的推荐系统的基础</h2><p>一个基于内容的推荐系统的高层次结构下图所示。推荐的过程包含有三个阶段，每一个阶段都由独立的部件控制。</p><p><img src=\"/posts_res/2018-11-26-推荐系统HandBook/3-1.jpg\" alt=\"基于内容的推荐系统的高层次结构\"></p><ul>\n<li>内容分析器<ul>\n<li>该部件的主要功能是将来自信息源的对象的内容表示成恰当的格式，以便于下一阶段的处理。</li>\n</ul>\n</li>\n<li>信息学习器<ul>\n<li>该部件收集用户偏好的数据特征，并试图通过机器学习技术泛化这些数据，从而构建用户特征信息。</li>\n</ul>\n</li>\n<li>过滤组件<ul>\n<li>该部件将用户与物品在表示空间中进行匹配，之后生成一个潜在感兴趣物品的排名清单。</li>\n</ul>\n</li>\n</ul>","more":"\n\n\n\n<h3 id=\"基于内容的推荐系统的推荐流程\"><a href=\"#基于内容的推荐系统的推荐流程\" class=\"headerlink\" title=\"基于内容的推荐系统的推荐流程\"></a>基于内容的推荐系统的推荐流程</h3><ol>\n<li>根据用户u对物品i的评分r，信息学习器通过监督学习算法，为该用户生成偏好特征；</li>\n<li>用户偏好特征存储在信息资源库里，并被接下来的过滤组件使用；</li>\n<li>比较用户偏好特征和新物品的特征，过滤组件预测用户对新物品的感兴趣程度；</li>\n<li>过滤组件生成一个推荐列表给用户。</li>\n<li>用户偏好会不断变化，上述步骤不断迭代。</li>\n</ol>\n<h3 id=\"基于内容过滤的优缺点\"><a href=\"#基于内容过滤的优缺点\" class=\"headerlink\" title=\"基于内容过滤的优缺点\"></a>基于内容过滤的优缺点</h3><p>基于内容的推荐与基于协同过滤的推荐相比有一下优点：</p>\n<ul>\n<li>用户独立性<ul>\n<li>只使用当前用户提供的评分构建个人信息</li>\n<li>协同过滤使用近邻用户，只有用户近邻喜欢的物品才有可能被推荐</li>\n</ul>\n</li>\n<li>透明度<ul>\n<li>通过显式列出推荐列表中的物品特征，可以对推荐进行解释</li>\n<li>协同过滤是黑盒子，解释仅来自相似的未知用户</li>\n</ul>\n</li>\n<li>新物品<ul>\n<li>新物品没有用户评分也能被推荐</li>\n<li>协同过滤只有被一系列用户评分后，才有可能被推荐</li>\n</ul>\n</li>\n</ul>\n<p>缺点：</p>\n<ul>\n<li>可分析的内容有限<ul>\n<li>受特征数量、类型以及领域知识的限制</li>\n</ul>\n</li>\n<li>过度特化<ul>\n<li>无法获得出人意料的推荐。（不能发现用户的潜在兴趣）</li>\n</ul>\n</li>\n<li>新用户<ul>\n<li>必须收集到足够的用户评分后，才能理解用户偏好并给予推荐</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"基于内容的推荐系统的现状\"><a href=\"#基于内容的推荐系统的现状\" class=\"headerlink\" title=\"基于内容的推荐系统的现状\"></a>基于内容的推荐系统的现状</h2><h3 id=\"基于关键词系统概述\"><a href=\"#基于关键词系统概述\" class=\"headerlink\" title=\"基于关键词系统概述\"></a>基于关键词系统概述</h3><ul>\n<li>Web推荐系统领域<ul>\n<li><a href>Letizia</a></li>\n<li><a href>Personal Web Watcher</a></li>\n<li><a href>Syskill&amp;Webert</a></li>\n<li><a href>ifWeb</a></li>\n<li><a href>Amalthea</a></li>\n<li><a href>WebMate</a></li>\n</ul>\n</li>\n<li>新闻过滤领域<ul>\n<li><a href>NewT</a></li>\n<li><a href>PSUN</a></li>\n<li><a href>INFOrmer</a></li>\n<li><a href>NewsDude</a></li>\n<li><a href>Daily Learner</a></li>\n<li><a href>YourNews</a></li>\n</ul>\n</li>\n<li>音乐领域<ul>\n<li><a href>Last.fm</a>使用协同过滤</li>\n<li><a href>MyStrands</a>使用协同过滤</li>\n<li><a href>Pandora</a>使用人工基于内容的描述</li>\n</ul>\n</li>\n<li>融合协同过滤和基于内容的方法的混合推荐系统<ul>\n<li><a href>Fab</a></li>\n<li><a href>WebWacther</a></li>\n<li><a href>ProfBuilder</a></li>\n<li><a href>PTV</a></li>\n<li><a href>Content-boosted Collaborative Filtering</a></li>\n<li><a href>CinemaScreen</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"基于本体的语义分析\"><a href=\"#基于本体的语义分析\" class=\"headerlink\" title=\"基于本体的语义分析\"></a>基于本体的语义分析</h3><ul>\n<li><a href>SiteIF</a><ul>\n<li>多语言新闻网站的个性化工具</li>\n<li>第一个采用基于感知的文档表示来对用户兴趣建模</li>\n<li>词领域消歧</li>\n</ul>\n</li>\n<li><a href>ITR, Item Recommender</a><ul>\n<li>词感知消歧</li>\n</ul>\n</li>\n<li><a href>SEWeP</a><ul>\n<li>利用日志和Web站点内容的语义实现个性化</li>\n</ul>\n</li>\n<li><a href>Quickstep</a><ul>\n<li>在线学术研究论文的推荐系统</li>\n</ul>\n</li>\n<li><a href>informed Recommender</a><ul>\n<li>使用消费者产品评价给出推荐建议</li>\n<li>系统通过作为知识表示和共享的翻译本体，把用户观点转换成一个结构化形式</li>\n</ul>\n</li>\n<li><a href>News@hand</a><ul>\n<li>采用基于本体表示物品特征和用户偏好来推荐新闻的系统</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"趋势和未来研究\"><a href=\"#趋势和未来研究\" class=\"headerlink\" title=\"趋势和未来研究\"></a>趋势和未来研究</h2><p>略</p>\n"},{"title":"推荐系统HandBook Chapter1 概述","mathjax":true,"copyright":true,"date":"2018-11-26T01:51:32.000Z","_content":"\n推荐系统是一种软件工具和技术方法，它可以向用户建议有用的物品，其是一种解决**信息过载**的有效工具。\n\n### 推荐系统的功能\n\n- 服务提供商采用这种技术的原因：\n    - 提高转化率，即接受推荐并消费物品的用户数量比上仅浏览了这些信息的普通访客数量；\n    - 出售更多种类的物品；\n    - 增加用户满意度；\n    - 更好地了解用户需求；\n\n\n### 推荐系统分类\n\n- 基于内容(content-based)\n    - 系统为用户推荐与他们过去的兴趣类似的物品。\n    - 物品间的相似性基于被比较的物品的特征来计算。\n- 协同过滤(collaborative filtering)\n    - 找到与用户有相同品味的用户，然后将相似用户过去喜欢的物品推荐给用户。\n    - 两用户间的相似偏好通过计算用户历史评分记录相似度得到。\n    - 最流行&最广泛实现的技术。\n- 基于人口统计学(demographic)\n    - 基于用户的语言、国籍等人口统计学信息进行个性化推荐。\n    - 推荐系统对这方面的研究较少。\n- 基于知识(knowledge-based)\n    - 根据特定领域知识推荐物品。\n    - 这些知识是关于如何确定物品的哪些特征能够满足用户需要和偏好，以及最终如何确定物品对用户有用。\n    - 这些系统中，相似函数用来估算用户需求(问题描述)与推荐(解决问题)的匹配度。\n- 基于社区(community-based，社会化推荐)\n    - 依赖用户朋友的偏好。\n    - 关注度越来越高。\n- 混合推荐系统(hybird recommender system)\n    - 综合上述的技术进行推荐。\n\n\n### 应用与评价\n\n系统设计阶段首先要考虑的是推荐系统应用的领域。\n\n- 娱乐： 电影、音乐和IPTV的推荐。\n- 内容： 个性化新闻报纸、文件推荐、网页推荐、电子学习程序和电子邮件过滤等。\n- 电子商务： 为消费者提供需购买的产品，如书籍、电脑等。\n- 服务： 旅游服务推荐、专家咨询推荐、租房推荐或者中介服务。\n\n\n另外一个重要的问题是推荐系统的评测。出于各种各样的目的，推荐系统生命周期的不同阶段都要进行评测。\n\n推荐解释在推荐系统中扮演的七大作用：\n\n1. 透明度：说明系统是如何工作的；\n2. 可反馈性：允许用户告诉系统有错误；\n3. 信任：增加用户对系统的信心；\n4. 有效性：帮助用户做出好的决定；\n5. 说服力：说服用户去尝试或购买；\n6. 高效性：帮助用户快速抉择；\n7. 满意度：增加用户舒适度或乐趣。\n\n\n### 推荐系统的挑战\n\n本书没有涵盖，但是对推荐系统的研究发展比较重要的挑战。\n\n- 大的真实数据集背景下的算法扩展性\n    - 较小的离线测试后，该方法在非常大的数据集上可能失效或完全不适应，需要重新评估研究\n- 主动推荐系统\n    - 虽然没有明确的请求，推荐系统也产生推荐\n    - 需要预测推荐什么，什么时候，如何推荐\n- 推荐系统中的用户隐私保护\n    - 简约明智地使用用户信息，同时避免被恶意用户获得\n- 推荐给用户的物品多样性\n    - 推荐列表中包含的物品具有多样性，则用户更可能找到合适的物品\n- 在建立推荐列表的过程中整合用户的长期和短期偏好\n    - 通常要使用混合模型来正确地整合用户的长期和短期偏好\n- 通用的用户模型和交叉领域推荐系统能够在不同的系统和应用领域调配用户数据\n- 在开放式网络中运行的分布式推荐系统\n- 最优化推荐序列的推荐系统\n- 移动上下文的推荐系统\n\n","source":"_posts/2018-11-26-推荐系统HandBook-Chapter1.md","raw":"---\ntitle: 推荐系统HandBook Chapter1 概述\nmathjax: true\ncopyright: true\ndate: 2018-11-26 09:51:32\ncategories: 推荐系统HandBook读书笔记\ntags:\n- 读书笔记\n---\n\n推荐系统是一种软件工具和技术方法，它可以向用户建议有用的物品，其是一种解决**信息过载**的有效工具。\n\n### 推荐系统的功能\n\n- 服务提供商采用这种技术的原因：\n    - 提高转化率，即接受推荐并消费物品的用户数量比上仅浏览了这些信息的普通访客数量；\n    - 出售更多种类的物品；\n    - 增加用户满意度；\n    - 更好地了解用户需求；\n\n\n### 推荐系统分类\n\n- 基于内容(content-based)\n    - 系统为用户推荐与他们过去的兴趣类似的物品。\n    - 物品间的相似性基于被比较的物品的特征来计算。\n- 协同过滤(collaborative filtering)\n    - 找到与用户有相同品味的用户，然后将相似用户过去喜欢的物品推荐给用户。\n    - 两用户间的相似偏好通过计算用户历史评分记录相似度得到。\n    - 最流行&最广泛实现的技术。\n- 基于人口统计学(demographic)\n    - 基于用户的语言、国籍等人口统计学信息进行个性化推荐。\n    - 推荐系统对这方面的研究较少。\n- 基于知识(knowledge-based)\n    - 根据特定领域知识推荐物品。\n    - 这些知识是关于如何确定物品的哪些特征能够满足用户需要和偏好，以及最终如何确定物品对用户有用。\n    - 这些系统中，相似函数用来估算用户需求(问题描述)与推荐(解决问题)的匹配度。\n- 基于社区(community-based，社会化推荐)\n    - 依赖用户朋友的偏好。\n    - 关注度越来越高。\n- 混合推荐系统(hybird recommender system)\n    - 综合上述的技术进行推荐。\n\n\n### 应用与评价\n\n系统设计阶段首先要考虑的是推荐系统应用的领域。\n\n- 娱乐： 电影、音乐和IPTV的推荐。\n- 内容： 个性化新闻报纸、文件推荐、网页推荐、电子学习程序和电子邮件过滤等。\n- 电子商务： 为消费者提供需购买的产品，如书籍、电脑等。\n- 服务： 旅游服务推荐、专家咨询推荐、租房推荐或者中介服务。\n\n\n另外一个重要的问题是推荐系统的评测。出于各种各样的目的，推荐系统生命周期的不同阶段都要进行评测。\n\n推荐解释在推荐系统中扮演的七大作用：\n\n1. 透明度：说明系统是如何工作的；\n2. 可反馈性：允许用户告诉系统有错误；\n3. 信任：增加用户对系统的信心；\n4. 有效性：帮助用户做出好的决定；\n5. 说服力：说服用户去尝试或购买；\n6. 高效性：帮助用户快速抉择；\n7. 满意度：增加用户舒适度或乐趣。\n\n\n### 推荐系统的挑战\n\n本书没有涵盖，但是对推荐系统的研究发展比较重要的挑战。\n\n- 大的真实数据集背景下的算法扩展性\n    - 较小的离线测试后，该方法在非常大的数据集上可能失效或完全不适应，需要重新评估研究\n- 主动推荐系统\n    - 虽然没有明确的请求，推荐系统也产生推荐\n    - 需要预测推荐什么，什么时候，如何推荐\n- 推荐系统中的用户隐私保护\n    - 简约明智地使用用户信息，同时避免被恶意用户获得\n- 推荐给用户的物品多样性\n    - 推荐列表中包含的物品具有多样性，则用户更可能找到合适的物品\n- 在建立推荐列表的过程中整合用户的长期和短期偏好\n    - 通常要使用混合模型来正确地整合用户的长期和短期偏好\n- 通用的用户模型和交叉领域推荐系统能够在不同的系统和应用领域调配用户数据\n- 在开放式网络中运行的分布式推荐系统\n- 最优化推荐序列的推荐系统\n- 移动上下文的推荐系统\n\n","slug":"推荐系统HandBook-Chapter1","published":1,"updated":"2019-08-17T09:40:43.429Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnwv00412qwp105ttxo6","content":"<p>推荐系统是一种软件工具和技术方法，它可以向用户建议有用的物品，其是一种解决<strong>信息过载</strong>的有效工具。</p><h3 id=\"推荐系统的功能\"><a href=\"#推荐系统的功能\" class=\"headerlink\" title=\"推荐系统的功能\"></a>推荐系统的功能</h3><ul>\n<li>服务提供商采用这种技术的原因：<ul>\n<li>提高转化率，即接受推荐并消费物品的用户数量比上仅浏览了这些信息的普通访客数量；</li>\n<li>出售更多种类的物品；</li>\n<li>增加用户满意度；</li>\n<li>更好地了解用户需求；</li>\n</ul>\n</li>\n</ul><h3 id=\"推荐系统分类\"><a href=\"#推荐系统分类\" class=\"headerlink\" title=\"推荐系统分类\"></a>推荐系统分类</h3><ul>\n<li>基于内容(content-based)<ul>\n<li>系统为用户推荐与他们过去的兴趣类似的物品。</li>\n<li>物品间的相似性基于被比较的物品的特征来计算。</li>\n</ul>\n</li>\n<li>协同过滤(collaborative filtering)<ul>\n<li>找到与用户有相同品味的用户，然后将相似用户过去喜欢的物品推荐给用户。</li>\n<li>两用户间的相似偏好通过计算用户历史评分记录相似度得到。</li>\n<li>最流行&amp;最广泛实现的技术。</li>\n</ul>\n</li>\n<li>基于人口统计学(demographic)<ul>\n<li>基于用户的语言、国籍等人口统计学信息进行个性化推荐。</li>\n<li>推荐系统对这方面的研究较少。</li>\n</ul>\n</li>\n<li>基于知识(knowledge-based)<ul>\n<li>根据特定领域知识推荐物品。</li>\n<li>这些知识是关于如何确定物品的哪些特征能够满足用户需要和偏好，以及最终如何确定物品对用户有用。</li>\n<li>这些系统中，相似函数用来估算用户需求(问题描述)与推荐(解决问题)的匹配度。</li>\n</ul>\n</li>\n<li>基于社区(community-based，社会化推荐)<ul>\n<li>依赖用户朋友的偏好。</li>\n<li>关注度越来越高。</li>\n</ul>\n</li>\n<li>混合推荐系统(hybird recommender system)<ul>\n<li>综合上述的技术进行推荐。</li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n\n\n<h3 id=\"应用与评价\"><a href=\"#应用与评价\" class=\"headerlink\" title=\"应用与评价\"></a>应用与评价</h3><p>系统设计阶段首先要考虑的是推荐系统应用的领域。</p>\n<ul>\n<li>娱乐： 电影、音乐和IPTV的推荐。</li>\n<li>内容： 个性化新闻报纸、文件推荐、网页推荐、电子学习程序和电子邮件过滤等。</li>\n<li>电子商务： 为消费者提供需购买的产品，如书籍、电脑等。</li>\n<li>服务： 旅游服务推荐、专家咨询推荐、租房推荐或者中介服务。</li>\n</ul>\n<p>另外一个重要的问题是推荐系统的评测。出于各种各样的目的，推荐系统生命周期的不同阶段都要进行评测。</p>\n<p>推荐解释在推荐系统中扮演的七大作用：</p>\n<ol>\n<li>透明度：说明系统是如何工作的；</li>\n<li>可反馈性：允许用户告诉系统有错误；</li>\n<li>信任：增加用户对系统的信心；</li>\n<li>有效性：帮助用户做出好的决定；</li>\n<li>说服力：说服用户去尝试或购买；</li>\n<li>高效性：帮助用户快速抉择；</li>\n<li>满意度：增加用户舒适度或乐趣。</li>\n</ol>\n<h3 id=\"推荐系统的挑战\"><a href=\"#推荐系统的挑战\" class=\"headerlink\" title=\"推荐系统的挑战\"></a>推荐系统的挑战</h3><p>本书没有涵盖，但是对推荐系统的研究发展比较重要的挑战。</p>\n<ul>\n<li>大的真实数据集背景下的算法扩展性<ul>\n<li>较小的离线测试后，该方法在非常大的数据集上可能失效或完全不适应，需要重新评估研究</li>\n</ul>\n</li>\n<li>主动推荐系统<ul>\n<li>虽然没有明确的请求，推荐系统也产生推荐</li>\n<li>需要预测推荐什么，什么时候，如何推荐</li>\n</ul>\n</li>\n<li>推荐系统中的用户隐私保护<ul>\n<li>简约明智地使用用户信息，同时避免被恶意用户获得</li>\n</ul>\n</li>\n<li>推荐给用户的物品多样性<ul>\n<li>推荐列表中包含的物品具有多样性，则用户更可能找到合适的物品</li>\n</ul>\n</li>\n<li>在建立推荐列表的过程中整合用户的长期和短期偏好<ul>\n<li>通常要使用混合模型来正确地整合用户的长期和短期偏好</li>\n</ul>\n</li>\n<li>通用的用户模型和交叉领域推荐系统能够在不同的系统和应用领域调配用户数据</li>\n<li>在开放式网络中运行的分布式推荐系统</li>\n<li>最优化推荐序列的推荐系统</li>\n<li>移动上下文的推荐系统</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>推荐系统是一种软件工具和技术方法，它可以向用户建议有用的物品，其是一种解决<strong>信息过载</strong>的有效工具。</p><h3 id=\"推荐系统的功能\"><a href=\"#推荐系统的功能\" class=\"headerlink\" title=\"推荐系统的功能\"></a>推荐系统的功能</h3><ul>\n<li>服务提供商采用这种技术的原因：<ul>\n<li>提高转化率，即接受推荐并消费物品的用户数量比上仅浏览了这些信息的普通访客数量；</li>\n<li>出售更多种类的物品；</li>\n<li>增加用户满意度；</li>\n<li>更好地了解用户需求；</li>\n</ul>\n</li>\n</ul><h3 id=\"推荐系统分类\"><a href=\"#推荐系统分类\" class=\"headerlink\" title=\"推荐系统分类\"></a>推荐系统分类</h3><ul>\n<li>基于内容(content-based)<ul>\n<li>系统为用户推荐与他们过去的兴趣类似的物品。</li>\n<li>物品间的相似性基于被比较的物品的特征来计算。</li>\n</ul>\n</li>\n<li>协同过滤(collaborative filtering)<ul>\n<li>找到与用户有相同品味的用户，然后将相似用户过去喜欢的物品推荐给用户。</li>\n<li>两用户间的相似偏好通过计算用户历史评分记录相似度得到。</li>\n<li>最流行&amp;最广泛实现的技术。</li>\n</ul>\n</li>\n<li>基于人口统计学(demographic)<ul>\n<li>基于用户的语言、国籍等人口统计学信息进行个性化推荐。</li>\n<li>推荐系统对这方面的研究较少。</li>\n</ul>\n</li>\n<li>基于知识(knowledge-based)<ul>\n<li>根据特定领域知识推荐物品。</li>\n<li>这些知识是关于如何确定物品的哪些特征能够满足用户需要和偏好，以及最终如何确定物品对用户有用。</li>\n<li>这些系统中，相似函数用来估算用户需求(问题描述)与推荐(解决问题)的匹配度。</li>\n</ul>\n</li>\n<li>基于社区(community-based，社会化推荐)<ul>\n<li>依赖用户朋友的偏好。</li>\n<li>关注度越来越高。</li>\n</ul>\n</li>\n<li>混合推荐系统(hybird recommender system)<ul>\n<li>综合上述的技术进行推荐。</li>\n</ul>\n</li>\n</ul>","more":"\n\n\n<h3 id=\"应用与评价\"><a href=\"#应用与评价\" class=\"headerlink\" title=\"应用与评价\"></a>应用与评价</h3><p>系统设计阶段首先要考虑的是推荐系统应用的领域。</p>\n<ul>\n<li>娱乐： 电影、音乐和IPTV的推荐。</li>\n<li>内容： 个性化新闻报纸、文件推荐、网页推荐、电子学习程序和电子邮件过滤等。</li>\n<li>电子商务： 为消费者提供需购买的产品，如书籍、电脑等。</li>\n<li>服务： 旅游服务推荐、专家咨询推荐、租房推荐或者中介服务。</li>\n</ul>\n<p>另外一个重要的问题是推荐系统的评测。出于各种各样的目的，推荐系统生命周期的不同阶段都要进行评测。</p>\n<p>推荐解释在推荐系统中扮演的七大作用：</p>\n<ol>\n<li>透明度：说明系统是如何工作的；</li>\n<li>可反馈性：允许用户告诉系统有错误；</li>\n<li>信任：增加用户对系统的信心；</li>\n<li>有效性：帮助用户做出好的决定；</li>\n<li>说服力：说服用户去尝试或购买；</li>\n<li>高效性：帮助用户快速抉择；</li>\n<li>满意度：增加用户舒适度或乐趣。</li>\n</ol>\n<h3 id=\"推荐系统的挑战\"><a href=\"#推荐系统的挑战\" class=\"headerlink\" title=\"推荐系统的挑战\"></a>推荐系统的挑战</h3><p>本书没有涵盖，但是对推荐系统的研究发展比较重要的挑战。</p>\n<ul>\n<li>大的真实数据集背景下的算法扩展性<ul>\n<li>较小的离线测试后，该方法在非常大的数据集上可能失效或完全不适应，需要重新评估研究</li>\n</ul>\n</li>\n<li>主动推荐系统<ul>\n<li>虽然没有明确的请求，推荐系统也产生推荐</li>\n<li>需要预测推荐什么，什么时候，如何推荐</li>\n</ul>\n</li>\n<li>推荐系统中的用户隐私保护<ul>\n<li>简约明智地使用用户信息，同时避免被恶意用户获得</li>\n</ul>\n</li>\n<li>推荐给用户的物品多样性<ul>\n<li>推荐列表中包含的物品具有多样性，则用户更可能找到合适的物品</li>\n</ul>\n</li>\n<li>在建立推荐列表的过程中整合用户的长期和短期偏好<ul>\n<li>通常要使用混合模型来正确地整合用户的长期和短期偏好</li>\n</ul>\n</li>\n<li>通用的用户模型和交叉领域推荐系统能够在不同的系统和应用领域调配用户数据</li>\n<li>在开放式网络中运行的分布式推荐系统</li>\n<li>最优化推荐序列的推荐系统</li>\n<li>移动上下文的推荐系统</li>\n</ul>\n"},{"title":"推荐系统HandBook Chapter2 推荐系统中的数据挖掘方法","mathjax":true,"copyright":true,"date":"2018-11-27T03:45:16.000Z","_content":"\n数据挖掘的过程一般由三个连续执行的步骤组成：数据预处理、数据分析和结果解释。\n\n![数据挖掘中主要的步骤和方法](/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg)\n\n\n### 数据预处理\n\n#### 相似度度量方法\n\n- 欧几里德距离\n\n$$ d(x,y) = \\sqrt{\\sum_{k=1}^n (x_k - y_k)^2} $$\n\n- 马氏距离\n\n$$ d(x,y) = \\sqrt{(x-y) \\sigma^{-1} (x-y)^T} $$\n\n其中$ \\sigma $是数据的协方差矩阵。\n\n- 余弦度量\n\n$$ cos(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||} $$\n\n- 皮卡逊相关性系数\n\n$$ Person(x,y) = \\frac{\\sum (x, y)}{\\sigma_x \\times \\sigma_y} $$\n\n\n#### 抽样\n\n根据抽样是否放回，可以分为：\n\n- 有放回抽样\n- 无放回抽样\n\n根据抽样方法，可以分为：\n\n- 简单随机抽样\n    - 每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。\n- 系统(等距)抽样\n    - 将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。\n- 分层抽样\n    - 将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。\n- 整群抽样\n    - 将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。\n\n> [抽样-维基百科](https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3)\n\n**除非数据集足够大，否则交叉验证可能不可信**\n\n\n#### 降维\n\n推荐系统反复出现的问题：\n\n- 稀疏：只有几个有限的特征有值，其他大部分为零；\n- [维度灾难](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE)：高维空间并没有给结果带来正相关；\n\n推荐系统中最相关的降维算法，这些技术可以单独作为推荐方法使用，或者作为其他技术预处理的步骤。\n\n- 主成份分析(PCA)\n    - 获得一组有序的成分列表，其根据最小平方误差计算出变化最大的值；\n    - 列表中第一个成分代表的变化量要比第二个成分所代表的变化量大，以此类推；\n    - 通过忽略变化贡献较小的成分来降低维度；\n- 奇异值分解(SVD)\n    - 一大优势是有增量算法来计算近似的分解；\n\n具体降维算法详细内容见-->[这里的学习小结](https://weiguozhao.github.io/categories/%E9%99%8D%E7%BB%B4/)\n\n\n#### 去噪\n\n去噪是预处理步骤中非常重要的一步，其目的是在最大化信息量时去除掉不必要的影响。\n\n噪声的分类：\n\n- 自然的噪声\n    - 用户在选择偏好反馈时无意产生的\n- 恶意的噪声\n    - 用户为了偏离结果在系统中故意引入的\n\n**通过预处理步骤来提高精确度能够比复杂的算法优化效果要好得多**\n\n\n### 分类\n\n#### 最近邻\n\n给出一个要分类的点，kNN分类器能够从训练记录中发现k个最近的点，然后按照它最近邻的类标签类确定所属类标签。\n\n选择k值的问题：\n- k值太小，分类可能对噪声点太敏感；\n- k值太大，近邻范围可能会包含其他类中太多的点；\n\n\n#### 决策树\n\n**决策树的优点**：构建代价比较小并且在分类未知的对象方面速度比较快；在维持精度的同时，能够容易解释。\n\n推荐系统中使用决策树：\n- 用在基于模型的方法中；\n- 作为物品排序的工具。\n\n\n#### 基于规则的分类\n\n规则的产生途径：\n\n- 从数据中直接抽取规则，例子为RIPPER或CN2\n- 从其他分类模型中间接抽取，例如决策树模型或神经模型\n\n**不流行、很难建立完整的规则**\n\n\n#### 贝叶斯分类器\n\n朴素贝叶斯收孤立噪声点和不相关属性的影响小，且在概率估算期间可以通过忽略实例来处理缺失值。\n但是独立性假设对一些相互关联的属性来说可能不成立，通常的解决办法是使用贝叶斯信念网络。\n\n具体算法详细内容见-->[这里](https://weiguozhao.github.io/2018-03-08-naive-bayes.html)\n\n\n#### 分类器的集成\n\n从训练数据构造一系列的分类器，并通过聚集预测值(或中间特征权重)来预测类标签。\n\n- Bagging\n- Boosting\n- Stacking\n\n\n#### 评估分类器\n\n一些常用的评估指标见-->[这里](https://weiguozhao.github.io/categories/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/)\n\n\n### 聚类分析\n\n#### K-Means聚类\n\n算法(分块方法)一开始会随机选择k个中心点，所有物品都会被分配到它们最靠近的中心节点的类中。\n由于聚类新添加或移出物品，新聚类的中心节点需要更新，聚类的成员关系也需要更新。\n这个操作持续下去，知道再没有物品改变它们的聚类成员关系。\n算法第一次迭代时，大部分的聚类的最终位置就会发生，因此，跳出迭代的条件一般改变成“知道相对少的点改变聚类”来提高效率。\n\n- 优点\n    - 简单\n    - 有效\n- 缺点\n    - k值确定需要先验的数据知识\n    - 最终聚类对初始中心点敏感\n    - 可能产生空聚类\n\n具体算法详细见-->[这里](https://weiguozhao.github.io/2018-10-09-kmeans.html)\n\n\n#### 改进的K-Means聚类\n\nDBSCAN(基于密度)通过建立密度定义作为在一定范围内的点的数量。\n\nDBSCAN定义了三种点：\n\n- 核心点：在给定距离内拥有超过一定数量邻居的点；\n- 边界点：没有超过指定数量的邻居但属于核心点邻居；\n- 噪声点：既不是核心点也不是边界点；\n\n其他改进的聚类算法：\n\n- 消息传递的聚类算法（基于图聚类）\n- 分层聚类按照层级树的结构产生一系列嵌套聚类\n\n**注意**\n\n改进的k-means算法并没有应用于推荐算法中。k-means算法的简单和效率要优于它的替代算法。\n\n\n### 关联规则挖掘\n\n给定物品的频繁度称为支持量（比如（牛奶，啤酒，尿布）=131）*类比频次*\n\n物品集的支持度是包含它的事务的比例（比如（牛奶，啤酒，尿布）=0.12）*类比频率*\n\n关联规则挖掘的两步方法：\n\n1. 产生了所有支持度大于等于最小支持度的物品集（**频繁项集生成**）\n2. 从每一频繁物品集中产生高置信规则（**规则生成**）\n\n具体可以参考Apriori算法。\n\n","source":"_posts/2018-11-27-推荐系统HandBook-Chapter2.md","raw":"---\ntitle: 推荐系统HandBook Chapter2 推荐系统中的数据挖掘方法\nmathjax: true\ncopyright: true\ndate: 2018-11-27 11:45:16\ncategories: 推荐系统HandBook读书笔记\ntags:\n- 读书笔记\n---\n\n数据挖掘的过程一般由三个连续执行的步骤组成：数据预处理、数据分析和结果解释。\n\n![数据挖掘中主要的步骤和方法](/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg)\n\n\n### 数据预处理\n\n#### 相似度度量方法\n\n- 欧几里德距离\n\n$$ d(x,y) = \\sqrt{\\sum_{k=1}^n (x_k - y_k)^2} $$\n\n- 马氏距离\n\n$$ d(x,y) = \\sqrt{(x-y) \\sigma^{-1} (x-y)^T} $$\n\n其中$ \\sigma $是数据的协方差矩阵。\n\n- 余弦度量\n\n$$ cos(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||} $$\n\n- 皮卡逊相关性系数\n\n$$ Person(x,y) = \\frac{\\sum (x, y)}{\\sigma_x \\times \\sigma_y} $$\n\n\n#### 抽样\n\n根据抽样是否放回，可以分为：\n\n- 有放回抽样\n- 无放回抽样\n\n根据抽样方法，可以分为：\n\n- 简单随机抽样\n    - 每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。\n- 系统(等距)抽样\n    - 将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。\n- 分层抽样\n    - 将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。\n- 整群抽样\n    - 将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。\n\n> [抽样-维基百科](https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3)\n\n**除非数据集足够大，否则交叉验证可能不可信**\n\n\n#### 降维\n\n推荐系统反复出现的问题：\n\n- 稀疏：只有几个有限的特征有值，其他大部分为零；\n- [维度灾难](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE)：高维空间并没有给结果带来正相关；\n\n推荐系统中最相关的降维算法，这些技术可以单独作为推荐方法使用，或者作为其他技术预处理的步骤。\n\n- 主成份分析(PCA)\n    - 获得一组有序的成分列表，其根据最小平方误差计算出变化最大的值；\n    - 列表中第一个成分代表的变化量要比第二个成分所代表的变化量大，以此类推；\n    - 通过忽略变化贡献较小的成分来降低维度；\n- 奇异值分解(SVD)\n    - 一大优势是有增量算法来计算近似的分解；\n\n具体降维算法详细内容见-->[这里的学习小结](https://weiguozhao.github.io/categories/%E9%99%8D%E7%BB%B4/)\n\n\n#### 去噪\n\n去噪是预处理步骤中非常重要的一步，其目的是在最大化信息量时去除掉不必要的影响。\n\n噪声的分类：\n\n- 自然的噪声\n    - 用户在选择偏好反馈时无意产生的\n- 恶意的噪声\n    - 用户为了偏离结果在系统中故意引入的\n\n**通过预处理步骤来提高精确度能够比复杂的算法优化效果要好得多**\n\n\n### 分类\n\n#### 最近邻\n\n给出一个要分类的点，kNN分类器能够从训练记录中发现k个最近的点，然后按照它最近邻的类标签类确定所属类标签。\n\n选择k值的问题：\n- k值太小，分类可能对噪声点太敏感；\n- k值太大，近邻范围可能会包含其他类中太多的点；\n\n\n#### 决策树\n\n**决策树的优点**：构建代价比较小并且在分类未知的对象方面速度比较快；在维持精度的同时，能够容易解释。\n\n推荐系统中使用决策树：\n- 用在基于模型的方法中；\n- 作为物品排序的工具。\n\n\n#### 基于规则的分类\n\n规则的产生途径：\n\n- 从数据中直接抽取规则，例子为RIPPER或CN2\n- 从其他分类模型中间接抽取，例如决策树模型或神经模型\n\n**不流行、很难建立完整的规则**\n\n\n#### 贝叶斯分类器\n\n朴素贝叶斯收孤立噪声点和不相关属性的影响小，且在概率估算期间可以通过忽略实例来处理缺失值。\n但是独立性假设对一些相互关联的属性来说可能不成立，通常的解决办法是使用贝叶斯信念网络。\n\n具体算法详细内容见-->[这里](https://weiguozhao.github.io/2018-03-08-naive-bayes.html)\n\n\n#### 分类器的集成\n\n从训练数据构造一系列的分类器，并通过聚集预测值(或中间特征权重)来预测类标签。\n\n- Bagging\n- Boosting\n- Stacking\n\n\n#### 评估分类器\n\n一些常用的评估指标见-->[这里](https://weiguozhao.github.io/categories/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/)\n\n\n### 聚类分析\n\n#### K-Means聚类\n\n算法(分块方法)一开始会随机选择k个中心点，所有物品都会被分配到它们最靠近的中心节点的类中。\n由于聚类新添加或移出物品，新聚类的中心节点需要更新，聚类的成员关系也需要更新。\n这个操作持续下去，知道再没有物品改变它们的聚类成员关系。\n算法第一次迭代时，大部分的聚类的最终位置就会发生，因此，跳出迭代的条件一般改变成“知道相对少的点改变聚类”来提高效率。\n\n- 优点\n    - 简单\n    - 有效\n- 缺点\n    - k值确定需要先验的数据知识\n    - 最终聚类对初始中心点敏感\n    - 可能产生空聚类\n\n具体算法详细见-->[这里](https://weiguozhao.github.io/2018-10-09-kmeans.html)\n\n\n#### 改进的K-Means聚类\n\nDBSCAN(基于密度)通过建立密度定义作为在一定范围内的点的数量。\n\nDBSCAN定义了三种点：\n\n- 核心点：在给定距离内拥有超过一定数量邻居的点；\n- 边界点：没有超过指定数量的邻居但属于核心点邻居；\n- 噪声点：既不是核心点也不是边界点；\n\n其他改进的聚类算法：\n\n- 消息传递的聚类算法（基于图聚类）\n- 分层聚类按照层级树的结构产生一系列嵌套聚类\n\n**注意**\n\n改进的k-means算法并没有应用于推荐算法中。k-means算法的简单和效率要优于它的替代算法。\n\n\n### 关联规则挖掘\n\n给定物品的频繁度称为支持量（比如（牛奶，啤酒，尿布）=131）*类比频次*\n\n物品集的支持度是包含它的事务的比例（比如（牛奶，啤酒，尿布）=0.12）*类比频率*\n\n关联规则挖掘的两步方法：\n\n1. 产生了所有支持度大于等于最小支持度的物品集（**频繁项集生成**）\n2. 从每一频繁物品集中产生高置信规则（**规则生成**）\n\n具体可以参考Apriori算法。\n\n","slug":"推荐系统HandBook-Chapter2","published":1,"updated":"2019-08-17T09:41:08.506Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnww00452qwpzin1aodg","content":"<p>数据挖掘的过程一般由三个连续执行的步骤组成：数据预处理、数据分析和结果解释。</p><p><img src=\"/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg\" alt=\"数据挖掘中主要的步骤和方法\"></p><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"相似度度量方法\"><a href=\"#相似度度量方法\" class=\"headerlink\" title=\"相似度度量方法\"></a>相似度度量方法</h4><ul>\n<li>欧几里德距离</li>\n</ul><script type=\"math/tex; mode=display\">d(x,y) = \\sqrt{\\sum_{k=1}^n (x_k - y_k)^2}</script><ul>\n<li>马氏距离</li>\n</ul><script type=\"math/tex; mode=display\">d(x,y) = \\sqrt{(x-y) \\sigma^{-1} (x-y)^T}</script><p>其中$ \\sigma $是数据的协方差矩阵。</p><ul>\n<li>余弦度量</li>\n</ul><script type=\"math/tex; mode=display\">cos(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}</script><ul>\n<li>皮卡逊相关性系数</li>\n</ul><script type=\"math/tex; mode=display\">Person(x,y) = \\frac{\\sum (x, y)}{\\sigma_x \\times \\sigma_y}</script><h4 id=\"抽样\"><a href=\"#抽样\" class=\"headerlink\" title=\"抽样\"></a>抽样</h4><p>根据抽样是否放回，可以分为：</p><a id=\"more\"></a>\n\n\n\n\n\n\n\n<ul>\n<li>有放回抽样</li>\n<li>无放回抽样</li>\n</ul>\n<p>根据抽样方法，可以分为：</p>\n<ul>\n<li>简单随机抽样<ul>\n<li>每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。</li>\n</ul>\n</li>\n<li>系统(等距)抽样<ul>\n<li>将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。</li>\n</ul>\n</li>\n<li>分层抽样<ul>\n<li>将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。</li>\n</ul>\n</li>\n<li>整群抽样<ul>\n<li>将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3\" target=\"_blank\" rel=\"noopener\">抽样-维基百科</a></p>\n</blockquote>\n<p><strong>除非数据集足够大，否则交叉验证可能不可信</strong></p>\n<h4 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h4><p>推荐系统反复出现的问题：</p>\n<ul>\n<li>稀疏：只有几个有限的特征有值，其他大部分为零；</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE\" target=\"_blank\" rel=\"noopener\">维度灾难</a>：高维空间并没有给结果带来正相关；</li>\n</ul>\n<p>推荐系统中最相关的降维算法，这些技术可以单独作为推荐方法使用，或者作为其他技术预处理的步骤。</p>\n<ul>\n<li>主成份分析(PCA)<ul>\n<li>获得一组有序的成分列表，其根据最小平方误差计算出变化最大的值；</li>\n<li>列表中第一个成分代表的变化量要比第二个成分所代表的变化量大，以此类推；</li>\n<li>通过忽略变化贡献较小的成分来降低维度；</li>\n</ul>\n</li>\n<li>奇异值分解(SVD)<ul>\n<li>一大优势是有增量算法来计算近似的分解；</li>\n</ul>\n</li>\n</ul>\n<p>具体降维算法详细内容见—&gt;<a href=\"https://weiguozhao.github.io/categories/%E9%99%8D%E7%BB%B4/\">这里的学习小结</a></p>\n<h4 id=\"去噪\"><a href=\"#去噪\" class=\"headerlink\" title=\"去噪\"></a>去噪</h4><p>去噪是预处理步骤中非常重要的一步，其目的是在最大化信息量时去除掉不必要的影响。</p>\n<p>噪声的分类：</p>\n<ul>\n<li>自然的噪声<ul>\n<li>用户在选择偏好反馈时无意产生的</li>\n</ul>\n</li>\n<li>恶意的噪声<ul>\n<li>用户为了偏离结果在系统中故意引入的</li>\n</ul>\n</li>\n</ul>\n<p><strong>通过预处理步骤来提高精确度能够比复杂的算法优化效果要好得多</strong></p>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><h4 id=\"最近邻\"><a href=\"#最近邻\" class=\"headerlink\" title=\"最近邻\"></a>最近邻</h4><p>给出一个要分类的点，kNN分类器能够从训练记录中发现k个最近的点，然后按照它最近邻的类标签类确定所属类标签。</p>\n<p>选择k值的问题：</p>\n<ul>\n<li>k值太小，分类可能对噪声点太敏感；</li>\n<li>k值太大，近邻范围可能会包含其他类中太多的点；</li>\n</ul>\n<h4 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h4><p><strong>决策树的优点</strong>：构建代价比较小并且在分类未知的对象方面速度比较快；在维持精度的同时，能够容易解释。</p>\n<p>推荐系统中使用决策树：</p>\n<ul>\n<li>用在基于模型的方法中；</li>\n<li>作为物品排序的工具。</li>\n</ul>\n<h4 id=\"基于规则的分类\"><a href=\"#基于规则的分类\" class=\"headerlink\" title=\"基于规则的分类\"></a>基于规则的分类</h4><p>规则的产生途径：</p>\n<ul>\n<li>从数据中直接抽取规则，例子为RIPPER或CN2</li>\n<li>从其他分类模型中间接抽取，例如决策树模型或神经模型</li>\n</ul>\n<p><strong>不流行、很难建立完整的规则</strong></p>\n<h4 id=\"贝叶斯分类器\"><a href=\"#贝叶斯分类器\" class=\"headerlink\" title=\"贝叶斯分类器\"></a>贝叶斯分类器</h4><p>朴素贝叶斯收孤立噪声点和不相关属性的影响小，且在概率估算期间可以通过忽略实例来处理缺失值。\n但是独立性假设对一些相互关联的属性来说可能不成立，通常的解决办法是使用贝叶斯信念网络。</p>\n<p>具体算法详细内容见—&gt;<a href=\"https://weiguozhao.github.io/2018-03-08-naive-bayes.html\">这里</a></p>\n<h4 id=\"分类器的集成\"><a href=\"#分类器的集成\" class=\"headerlink\" title=\"分类器的集成\"></a>分类器的集成</h4><p>从训练数据构造一系列的分类器，并通过聚集预测值(或中间特征权重)来预测类标签。</p>\n<ul>\n<li>Bagging</li>\n<li>Boosting</li>\n<li>Stacking</li>\n</ul>\n<h4 id=\"评估分类器\"><a href=\"#评估分类器\" class=\"headerlink\" title=\"评估分类器\"></a>评估分类器</h4><p>一些常用的评估指标见—&gt;<a href=\"https://weiguozhao.github.io/categories/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/\">这里</a></p>\n<h3 id=\"聚类分析\"><a href=\"#聚类分析\" class=\"headerlink\" title=\"聚类分析\"></a>聚类分析</h3><h4 id=\"K-Means聚类\"><a href=\"#K-Means聚类\" class=\"headerlink\" title=\"K-Means聚类\"></a>K-Means聚类</h4><p>算法(分块方法)一开始会随机选择k个中心点，所有物品都会被分配到它们最靠近的中心节点的类中。\n由于聚类新添加或移出物品，新聚类的中心节点需要更新，聚类的成员关系也需要更新。\n这个操作持续下去，知道再没有物品改变它们的聚类成员关系。\n算法第一次迭代时，大部分的聚类的最终位置就会发生，因此，跳出迭代的条件一般改变成“知道相对少的点改变聚类”来提高效率。</p>\n<ul>\n<li>优点<ul>\n<li>简单</li>\n<li>有效</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>k值确定需要先验的数据知识</li>\n<li>最终聚类对初始中心点敏感</li>\n<li>可能产生空聚类</li>\n</ul>\n</li>\n</ul>\n<p>具体算法详细见—&gt;<a href=\"https://weiguozhao.github.io/2018-10-09-kmeans.html\">这里</a></p>\n<h4 id=\"改进的K-Means聚类\"><a href=\"#改进的K-Means聚类\" class=\"headerlink\" title=\"改进的K-Means聚类\"></a>改进的K-Means聚类</h4><p>DBSCAN(基于密度)通过建立密度定义作为在一定范围内的点的数量。</p>\n<p>DBSCAN定义了三种点：</p>\n<ul>\n<li>核心点：在给定距离内拥有超过一定数量邻居的点；</li>\n<li>边界点：没有超过指定数量的邻居但属于核心点邻居；</li>\n<li>噪声点：既不是核心点也不是边界点；</li>\n</ul>\n<p>其他改进的聚类算法：</p>\n<ul>\n<li>消息传递的聚类算法（基于图聚类）</li>\n<li>分层聚类按照层级树的结构产生一系列嵌套聚类</li>\n</ul>\n<p><strong>注意</strong></p>\n<p>改进的k-means算法并没有应用于推荐算法中。k-means算法的简单和效率要优于它的替代算法。</p>\n<h3 id=\"关联规则挖掘\"><a href=\"#关联规则挖掘\" class=\"headerlink\" title=\"关联规则挖掘\"></a>关联规则挖掘</h3><p>给定物品的频繁度称为支持量（比如（牛奶，啤酒，尿布）=131）<em>类比频次</em></p>\n<p>物品集的支持度是包含它的事务的比例（比如（牛奶，啤酒，尿布）=0.12）<em>类比频率</em></p>\n<p>关联规则挖掘的两步方法：</p>\n<ol>\n<li>产生了所有支持度大于等于最小支持度的物品集（<strong>频繁项集生成</strong>）</li>\n<li>从每一频繁物品集中产生高置信规则（<strong>规则生成</strong>）</li>\n</ol>\n<p>具体可以参考Apriori算法。</p>\n","site":{"data":{}},"excerpt":"<p>数据挖掘的过程一般由三个连续执行的步骤组成：数据预处理、数据分析和结果解释。</p><p><img src=\"/posts_res/2018-11-26-推荐系统HandBook/2-1.jpg\" alt=\"数据挖掘中主要的步骤和方法\"></p><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"相似度度量方法\"><a href=\"#相似度度量方法\" class=\"headerlink\" title=\"相似度度量方法\"></a>相似度度量方法</h4><ul>\n<li>欧几里德距离</li>\n</ul><script type=\"math/tex; mode=display\">d(x,y) = \\sqrt{\\sum_{k=1}^n (x_k - y_k)^2}</script><ul>\n<li>马氏距离</li>\n</ul><script type=\"math/tex; mode=display\">d(x,y) = \\sqrt{(x-y) \\sigma^{-1} (x-y)^T}</script><p>其中$ \\sigma $是数据的协方差矩阵。</p><ul>\n<li>余弦度量</li>\n</ul><script type=\"math/tex; mode=display\">cos(x,y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}</script><ul>\n<li>皮卡逊相关性系数</li>\n</ul><script type=\"math/tex; mode=display\">Person(x,y) = \\frac{\\sum (x, y)}{\\sigma_x \\times \\sigma_y}</script><h4 id=\"抽样\"><a href=\"#抽样\" class=\"headerlink\" title=\"抽样\"></a>抽样</h4><p>根据抽样是否放回，可以分为：</p>","more":"\n\n\n\n\n\n\n\n<ul>\n<li>有放回抽样</li>\n<li>无放回抽样</li>\n</ul>\n<p>根据抽样方法，可以分为：</p>\n<ul>\n<li>简单随机抽样<ul>\n<li>每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。</li>\n</ul>\n</li>\n<li>系统(等距)抽样<ul>\n<li>将总体中的所有单位按一定顺序排列，在规定的范围内随机地抽取一个单位作为初始单位，然后按事先规定好的规则确定其他样本单位。</li>\n</ul>\n</li>\n<li>分层抽样<ul>\n<li>将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。</li>\n</ul>\n</li>\n<li>整群抽样<ul>\n<li>将总体中若干个单位合并为组，抽样时直接抽取群，然后对中选群中的所有单位全部实施调查。</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%8A%BD%E6%A8%A3\" target=\"_blank\" rel=\"noopener\">抽样-维基百科</a></p>\n</blockquote>\n<p><strong>除非数据集足够大，否则交叉验证可能不可信</strong></p>\n<h4 id=\"降维\"><a href=\"#降维\" class=\"headerlink\" title=\"降维\"></a>降维</h4><p>推荐系统反复出现的问题：</p>\n<ul>\n<li>稀疏：只有几个有限的特征有值，其他大部分为零；</li>\n<li><a href=\"https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE\" target=\"_blank\" rel=\"noopener\">维度灾难</a>：高维空间并没有给结果带来正相关；</li>\n</ul>\n<p>推荐系统中最相关的降维算法，这些技术可以单独作为推荐方法使用，或者作为其他技术预处理的步骤。</p>\n<ul>\n<li>主成份分析(PCA)<ul>\n<li>获得一组有序的成分列表，其根据最小平方误差计算出变化最大的值；</li>\n<li>列表中第一个成分代表的变化量要比第二个成分所代表的变化量大，以此类推；</li>\n<li>通过忽略变化贡献较小的成分来降低维度；</li>\n</ul>\n</li>\n<li>奇异值分解(SVD)<ul>\n<li>一大优势是有增量算法来计算近似的分解；</li>\n</ul>\n</li>\n</ul>\n<p>具体降维算法详细内容见—&gt;<a href=\"https://weiguozhao.github.io/categories/%E9%99%8D%E7%BB%B4/\">这里的学习小结</a></p>\n<h4 id=\"去噪\"><a href=\"#去噪\" class=\"headerlink\" title=\"去噪\"></a>去噪</h4><p>去噪是预处理步骤中非常重要的一步，其目的是在最大化信息量时去除掉不必要的影响。</p>\n<p>噪声的分类：</p>\n<ul>\n<li>自然的噪声<ul>\n<li>用户在选择偏好反馈时无意产生的</li>\n</ul>\n</li>\n<li>恶意的噪声<ul>\n<li>用户为了偏离结果在系统中故意引入的</li>\n</ul>\n</li>\n</ul>\n<p><strong>通过预处理步骤来提高精确度能够比复杂的算法优化效果要好得多</strong></p>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><h4 id=\"最近邻\"><a href=\"#最近邻\" class=\"headerlink\" title=\"最近邻\"></a>最近邻</h4><p>给出一个要分类的点，kNN分类器能够从训练记录中发现k个最近的点，然后按照它最近邻的类标签类确定所属类标签。</p>\n<p>选择k值的问题：</p>\n<ul>\n<li>k值太小，分类可能对噪声点太敏感；</li>\n<li>k值太大，近邻范围可能会包含其他类中太多的点；</li>\n</ul>\n<h4 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h4><p><strong>决策树的优点</strong>：构建代价比较小并且在分类未知的对象方面速度比较快；在维持精度的同时，能够容易解释。</p>\n<p>推荐系统中使用决策树：</p>\n<ul>\n<li>用在基于模型的方法中；</li>\n<li>作为物品排序的工具。</li>\n</ul>\n<h4 id=\"基于规则的分类\"><a href=\"#基于规则的分类\" class=\"headerlink\" title=\"基于规则的分类\"></a>基于规则的分类</h4><p>规则的产生途径：</p>\n<ul>\n<li>从数据中直接抽取规则，例子为RIPPER或CN2</li>\n<li>从其他分类模型中间接抽取，例如决策树模型或神经模型</li>\n</ul>\n<p><strong>不流行、很难建立完整的规则</strong></p>\n<h4 id=\"贝叶斯分类器\"><a href=\"#贝叶斯分类器\" class=\"headerlink\" title=\"贝叶斯分类器\"></a>贝叶斯分类器</h4><p>朴素贝叶斯收孤立噪声点和不相关属性的影响小，且在概率估算期间可以通过忽略实例来处理缺失值。\n但是独立性假设对一些相互关联的属性来说可能不成立，通常的解决办法是使用贝叶斯信念网络。</p>\n<p>具体算法详细内容见—&gt;<a href=\"https://weiguozhao.github.io/2018-03-08-naive-bayes.html\">这里</a></p>\n<h4 id=\"分类器的集成\"><a href=\"#分类器的集成\" class=\"headerlink\" title=\"分类器的集成\"></a>分类器的集成</h4><p>从训练数据构造一系列的分类器，并通过聚集预测值(或中间特征权重)来预测类标签。</p>\n<ul>\n<li>Bagging</li>\n<li>Boosting</li>\n<li>Stacking</li>\n</ul>\n<h4 id=\"评估分类器\"><a href=\"#评估分类器\" class=\"headerlink\" title=\"评估分类器\"></a>评估分类器</h4><p>一些常用的评估指标见—&gt;<a href=\"https://weiguozhao.github.io/categories/%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/\">这里</a></p>\n<h3 id=\"聚类分析\"><a href=\"#聚类分析\" class=\"headerlink\" title=\"聚类分析\"></a>聚类分析</h3><h4 id=\"K-Means聚类\"><a href=\"#K-Means聚类\" class=\"headerlink\" title=\"K-Means聚类\"></a>K-Means聚类</h4><p>算法(分块方法)一开始会随机选择k个中心点，所有物品都会被分配到它们最靠近的中心节点的类中。\n由于聚类新添加或移出物品，新聚类的中心节点需要更新，聚类的成员关系也需要更新。\n这个操作持续下去，知道再没有物品改变它们的聚类成员关系。\n算法第一次迭代时，大部分的聚类的最终位置就会发生，因此，跳出迭代的条件一般改变成“知道相对少的点改变聚类”来提高效率。</p>\n<ul>\n<li>优点<ul>\n<li>简单</li>\n<li>有效</li>\n</ul>\n</li>\n<li>缺点<ul>\n<li>k值确定需要先验的数据知识</li>\n<li>最终聚类对初始中心点敏感</li>\n<li>可能产生空聚类</li>\n</ul>\n</li>\n</ul>\n<p>具体算法详细见—&gt;<a href=\"https://weiguozhao.github.io/2018-10-09-kmeans.html\">这里</a></p>\n<h4 id=\"改进的K-Means聚类\"><a href=\"#改进的K-Means聚类\" class=\"headerlink\" title=\"改进的K-Means聚类\"></a>改进的K-Means聚类</h4><p>DBSCAN(基于密度)通过建立密度定义作为在一定范围内的点的数量。</p>\n<p>DBSCAN定义了三种点：</p>\n<ul>\n<li>核心点：在给定距离内拥有超过一定数量邻居的点；</li>\n<li>边界点：没有超过指定数量的邻居但属于核心点邻居；</li>\n<li>噪声点：既不是核心点也不是边界点；</li>\n</ul>\n<p>其他改进的聚类算法：</p>\n<ul>\n<li>消息传递的聚类算法（基于图聚类）</li>\n<li>分层聚类按照层级树的结构产生一系列嵌套聚类</li>\n</ul>\n<p><strong>注意</strong></p>\n<p>改进的k-means算法并没有应用于推荐算法中。k-means算法的简单和效率要优于它的替代算法。</p>\n<h3 id=\"关联规则挖掘\"><a href=\"#关联规则挖掘\" class=\"headerlink\" title=\"关联规则挖掘\"></a>关联规则挖掘</h3><p>给定物品的频繁度称为支持量（比如（牛奶，啤酒，尿布）=131）<em>类比频次</em></p>\n<p>物品集的支持度是包含它的事务的比例（比如（牛奶，啤酒，尿布）=0.12）<em>类比频率</em></p>\n<p>关联规则挖掘的两步方法：</p>\n<ol>\n<li>产生了所有支持度大于等于最小支持度的物品集（<strong>频繁项集生成</strong>）</li>\n<li>从每一频繁物品集中产生高置信规则（<strong>规则生成</strong>）</li>\n</ol>\n<p>具体可以参考Apriori算法。</p>\n"},{"title":"pig基础","mathjax":true,"copyright":true,"date":"2019-02-14T12:17:07.000Z","_content":"\n### Pig基础知识\n\n- 数据模型\n    - 包（bag）是元组（tuple）的集合\n    - 元组（tuple）是有序的字段（field）集\n    - 字段（field）是一段数据\n\n- 语句\n    - 每个语句以分号（；）结尾\n    - 只有illustrate、dump、store等操作的时候，才会执行MapReduce\n\n- 注释\n    - 单行注释 `--`\n    - 多行注释 `/* code */`\n\n- 数据类型\n    - int\n    - long\n    - float\n    - double\n    - chararray\n    - Bytearray\n    - Boolean\n    - Datetime\n    - Biginteger\n    - Bigdecimal\n    - Tuple\n    - Bag\n    - Map\n    - NULL值（上述类型都可以为NULL值，等同与Python中的None）\n\n### Pig运算符\n\n#### 加载数据\n\n```\nrelation = load 'data_path' using PigStorage() as (name:chararray, gpa:double);\n```\n\n其中的`PigStorage()`为加载或存储结构化数据是使用，还有一些其他的函数[参考这里](https://www.w3cschool.cn/apache_pig/apache_pig_load_store_functions.html)，as 后面跟的是数据的模式，样例中假设了数据模式为`name,gpa`，并需要指定每个字段的类型。\n\n读取hive表时使用`org.apache.hive.hcatalog.pig.HCatLoader()`，不需要指定模式。\n\n#### 存储数据\n\n```\nstore relation into 'data_path' [ using PigStorage() ];\n```\n\n同加载数据时类似，这里不需要再指定数据的模式了。\n\n#### Dump运算符\n\n```\ndump relation;\n```\n\n用于将关系数据输出到终端中，通常用于调试。调用该命令可以执行MapReduce。\n\n#### Describe运算符\n\n```\ndescribe relation;\n```\n\n输出relation的模型，调用该命令不会执行MapReduce。\n\n#### Explain运算符\n\n```\nexplain relation;\n```\n\n显示relation的逻辑，物理结构以及MapReduce执行计划。调用该命令不会执行MapReduce。\n\n#### Illustrate运算符\n\n```\nillustrate relation;\n```\n\n提供了一系列语句的逐步执行。调用该命令会执行MapReduce。\n\n\n#### 分组和连接\n\n```\nGroup_data = GROUP Relation_name BY age;\n```\n通常是一个关系自我进行分组，可以指定一个、多个或者ALL来分组（当指定为ALL的时候，不需要使用BY）\n\n```\nCogroup_data = COGROUP Relation1 by age, Relation2 by age;\n```\n和group具有相同的功能，区别是cogroup通常用于多个关系进行分组。\n\n```\nJoin_data = Join customers by id, orders by customer_id;\n```\nJoin操作和SQL的Join基本一致，也分self-join, left-join, right-join, full-outer-join\n\n\n#### 合并和拆分\n\n```\nUnion_data = UNION relation1, relation2;\n```\n将多个关系上下罗列，注意关系只见的模式（schema）要相同。\n\n```\nSPLIT relation1 into relation2 if (condition), relation3 (condition2);\n```\n\n#### 过滤\n\n```\nRelation2_name = FILTER Relation1_name BY (condition);\n```\n\n```\nRelation_name2 = DISTINCT Relatin_name1;\n```\n去重，通常会花费比较多的时间\n\n```\nRelation_name2 = FOREACH Relatin_name1 GENERATE (required data);\n```\n通常用完filter、group、join等操作后，紧接着使用foreach\n\n\n#### 排序\n\n```\nRelation_name2 = ORDER Relatin_name1 BY (ASC|DESC);\n````\n\n```\nResult = LIMIT Relation_name （required number of tuples）;\n```\n限制记录的数量\n\n\n#### 其他\n\n##### 内置函数\n\n- eval函数\n    - AVG()         计算包内数值的平均值\n    - BagToString() 将包的元素连接成字符串\n    - CONCAT()      连接两个或多个相同类型的表达式\n    - COUNT()       获取包中元素的数量，同时计算包中元组的数量\n    - COUNT_STAR()  计算包中的元素数量\n    - DIFF()        比较元组中的两个包(字段)\n    - isEmpty()     检查包或映射是否为空\n    - MAX()         计算单列包中的列(数值或字符)的最大值\n    - MIN()         计算单列包中的列(数值或字符)的最小值\n    - PluckTuple()  可定义Prefix，并过滤以给定Prefix开头的关系中的列\n    - SIZE()        基于任何Pig数据类型计算元素的数量\n    - SUBtTRACT(A，B) 两个包的差，结果为在A不在B种的元组的包\n    - SUM()         计算单列包中的某列的数值总和\n    - TOKENIZE()    要在单个元组中拆分字符串(其中包含一组字)，病返回包含拆分操作的输出的包\n\n- load函数 / store函数\n    - PigStorage()      加载和存储结构化文件\n    - TextLoader()      将非结构化数据加载到Pig中\n    - BinStorage()      使用机器可读格式将数据加载并存储到Pig中\n    - Handling Compression 加载和存储压缩数据\n\n- bag函数 / tuple函数\n    - TOBAG()           将两个或多个表达式转换为包\n    - TOP()             获取关系的顶部N个元组\n    - TOTUPLE()         将一个或多个表达式转换为元祖\n    - TOMAP()           将key-value对转换为Map\n\n- stirng函数\n- date函数\n- math函数\n\n-  注释 \n    - /* multi-lines code */\n    - \\-\\- single line code\n\n具体使用方法及其他的一些内置函数，常用函数，可以[参考w3cschool](https://www.w3cschool.cn/apache_pig/apache_pig_eval_functions.html)\n\n写Pig脚本，**重点是UDF的编写**，这里我写的jython_UDF比较多（其实就是按照Python写的），\n需要注意的是要定义好outputSchema，并在UDF函数头部加上修饰符@\n\n```python\n#!/use/bin/python\n# -*- coding:utf-8 -*-\n\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\ndef outputSchema(schema_def):\n\tdef decorate(func):\n\t\tfunc.outputSchema = schema_def\n\t\treturn func\n\treturn decorate\n\n@outputSchema('newDate:chararray')\ndef dateChangeFormat(thisDate):\n\ttry:\n\t\treturn thisDate.strip().split()[0];\n\texcept:\n\t\treturn '2000-01-01'\n```\n\n题外话：现在Spark是主流，建议转Spark\n\n\n### Pig的一些实例\n\n- 差集的计算\n\n```\nA = load 'input1' as (x, y);\nB = load 'input2' as (u, v);\nC = cogroup A by x, B by u;\nD = filter C by IsEmpty(B);\nE = foreach D generate flatten(A);\n```\n解释：A和B分组后，如果B中为空，那么说明与之对应的A元素仅在A中出现，即在A不在B，也即为A-B的差集\n\n- 传递参数\n\n```\npig -param date='2014-05-17' example.pig\n```\npig脚本中用`$date`调用该值\n\n- FLATTEN关键字\n\n可以消除嵌套\n\n- 项目相关\n\n```\n%default PDAY '2019-02-02'\n```\n在脚本前面使用该命令设定脚本传递参数的默认值\n\n```\nregister xxx.jar as xxx\n```\n在脚本的最前面注册使用jar文件，其中as重命名不是必要的\n\n- 统计行数\n\n```\nA = LOAD '1.txt' USING PigStorage (' ‘) AS(col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double); \nB = GROUP A all;\nC = FOREACH B GENERATE COUNT(col2);\nDUMP C;\n```\n\n- 加载多个文件\n\n```\nLOAD '/data/201{8,9}'\nload/data/2018 /data/2019两个目录下的数据\n```\n\n- 其他常用的实例\n\n参考[pig实战 pig常用语法总结，教你快速入门——算法篇](https://www.cnblogs.com/uttu/archive/2013/02/19/2917438.html)\n\n","source":"_posts/2019-02-14-pig基础.md","raw":"---\ntitle: pig基础\nmathjax: true\ncopyright: true\ndate: 2019-02-14 20:17:07\ncategories: 大数据\ntags:\n- 语言基础\n---\n\n### Pig基础知识\n\n- 数据模型\n    - 包（bag）是元组（tuple）的集合\n    - 元组（tuple）是有序的字段（field）集\n    - 字段（field）是一段数据\n\n- 语句\n    - 每个语句以分号（；）结尾\n    - 只有illustrate、dump、store等操作的时候，才会执行MapReduce\n\n- 注释\n    - 单行注释 `--`\n    - 多行注释 `/* code */`\n\n- 数据类型\n    - int\n    - long\n    - float\n    - double\n    - chararray\n    - Bytearray\n    - Boolean\n    - Datetime\n    - Biginteger\n    - Bigdecimal\n    - Tuple\n    - Bag\n    - Map\n    - NULL值（上述类型都可以为NULL值，等同与Python中的None）\n\n### Pig运算符\n\n#### 加载数据\n\n```\nrelation = load 'data_path' using PigStorage() as (name:chararray, gpa:double);\n```\n\n其中的`PigStorage()`为加载或存储结构化数据是使用，还有一些其他的函数[参考这里](https://www.w3cschool.cn/apache_pig/apache_pig_load_store_functions.html)，as 后面跟的是数据的模式，样例中假设了数据模式为`name,gpa`，并需要指定每个字段的类型。\n\n读取hive表时使用`org.apache.hive.hcatalog.pig.HCatLoader()`，不需要指定模式。\n\n#### 存储数据\n\n```\nstore relation into 'data_path' [ using PigStorage() ];\n```\n\n同加载数据时类似，这里不需要再指定数据的模式了。\n\n#### Dump运算符\n\n```\ndump relation;\n```\n\n用于将关系数据输出到终端中，通常用于调试。调用该命令可以执行MapReduce。\n\n#### Describe运算符\n\n```\ndescribe relation;\n```\n\n输出relation的模型，调用该命令不会执行MapReduce。\n\n#### Explain运算符\n\n```\nexplain relation;\n```\n\n显示relation的逻辑，物理结构以及MapReduce执行计划。调用该命令不会执行MapReduce。\n\n#### Illustrate运算符\n\n```\nillustrate relation;\n```\n\n提供了一系列语句的逐步执行。调用该命令会执行MapReduce。\n\n\n#### 分组和连接\n\n```\nGroup_data = GROUP Relation_name BY age;\n```\n通常是一个关系自我进行分组，可以指定一个、多个或者ALL来分组（当指定为ALL的时候，不需要使用BY）\n\n```\nCogroup_data = COGROUP Relation1 by age, Relation2 by age;\n```\n和group具有相同的功能，区别是cogroup通常用于多个关系进行分组。\n\n```\nJoin_data = Join customers by id, orders by customer_id;\n```\nJoin操作和SQL的Join基本一致，也分self-join, left-join, right-join, full-outer-join\n\n\n#### 合并和拆分\n\n```\nUnion_data = UNION relation1, relation2;\n```\n将多个关系上下罗列，注意关系只见的模式（schema）要相同。\n\n```\nSPLIT relation1 into relation2 if (condition), relation3 (condition2);\n```\n\n#### 过滤\n\n```\nRelation2_name = FILTER Relation1_name BY (condition);\n```\n\n```\nRelation_name2 = DISTINCT Relatin_name1;\n```\n去重，通常会花费比较多的时间\n\n```\nRelation_name2 = FOREACH Relatin_name1 GENERATE (required data);\n```\n通常用完filter、group、join等操作后，紧接着使用foreach\n\n\n#### 排序\n\n```\nRelation_name2 = ORDER Relatin_name1 BY (ASC|DESC);\n````\n\n```\nResult = LIMIT Relation_name （required number of tuples）;\n```\n限制记录的数量\n\n\n#### 其他\n\n##### 内置函数\n\n- eval函数\n    - AVG()         计算包内数值的平均值\n    - BagToString() 将包的元素连接成字符串\n    - CONCAT()      连接两个或多个相同类型的表达式\n    - COUNT()       获取包中元素的数量，同时计算包中元组的数量\n    - COUNT_STAR()  计算包中的元素数量\n    - DIFF()        比较元组中的两个包(字段)\n    - isEmpty()     检查包或映射是否为空\n    - MAX()         计算单列包中的列(数值或字符)的最大值\n    - MIN()         计算单列包中的列(数值或字符)的最小值\n    - PluckTuple()  可定义Prefix，并过滤以给定Prefix开头的关系中的列\n    - SIZE()        基于任何Pig数据类型计算元素的数量\n    - SUBtTRACT(A，B) 两个包的差，结果为在A不在B种的元组的包\n    - SUM()         计算单列包中的某列的数值总和\n    - TOKENIZE()    要在单个元组中拆分字符串(其中包含一组字)，病返回包含拆分操作的输出的包\n\n- load函数 / store函数\n    - PigStorage()      加载和存储结构化文件\n    - TextLoader()      将非结构化数据加载到Pig中\n    - BinStorage()      使用机器可读格式将数据加载并存储到Pig中\n    - Handling Compression 加载和存储压缩数据\n\n- bag函数 / tuple函数\n    - TOBAG()           将两个或多个表达式转换为包\n    - TOP()             获取关系的顶部N个元组\n    - TOTUPLE()         将一个或多个表达式转换为元祖\n    - TOMAP()           将key-value对转换为Map\n\n- stirng函数\n- date函数\n- math函数\n\n-  注释 \n    - /* multi-lines code */\n    - \\-\\- single line code\n\n具体使用方法及其他的一些内置函数，常用函数，可以[参考w3cschool](https://www.w3cschool.cn/apache_pig/apache_pig_eval_functions.html)\n\n写Pig脚本，**重点是UDF的编写**，这里我写的jython_UDF比较多（其实就是按照Python写的），\n需要注意的是要定义好outputSchema，并在UDF函数头部加上修饰符@\n\n```python\n#!/use/bin/python\n# -*- coding:utf-8 -*-\n\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\ndef outputSchema(schema_def):\n\tdef decorate(func):\n\t\tfunc.outputSchema = schema_def\n\t\treturn func\n\treturn decorate\n\n@outputSchema('newDate:chararray')\ndef dateChangeFormat(thisDate):\n\ttry:\n\t\treturn thisDate.strip().split()[0];\n\texcept:\n\t\treturn '2000-01-01'\n```\n\n题外话：现在Spark是主流，建议转Spark\n\n\n### Pig的一些实例\n\n- 差集的计算\n\n```\nA = load 'input1' as (x, y);\nB = load 'input2' as (u, v);\nC = cogroup A by x, B by u;\nD = filter C by IsEmpty(B);\nE = foreach D generate flatten(A);\n```\n解释：A和B分组后，如果B中为空，那么说明与之对应的A元素仅在A中出现，即在A不在B，也即为A-B的差集\n\n- 传递参数\n\n```\npig -param date='2014-05-17' example.pig\n```\npig脚本中用`$date`调用该值\n\n- FLATTEN关键字\n\n可以消除嵌套\n\n- 项目相关\n\n```\n%default PDAY '2019-02-02'\n```\n在脚本前面使用该命令设定脚本传递参数的默认值\n\n```\nregister xxx.jar as xxx\n```\n在脚本的最前面注册使用jar文件，其中as重命名不是必要的\n\n- 统计行数\n\n```\nA = LOAD '1.txt' USING PigStorage (' ‘) AS(col1:chararray, col2:int, col3:int, col4:int, col5:double, col6:double); \nB = GROUP A all;\nC = FOREACH B GENERATE COUNT(col2);\nDUMP C;\n```\n\n- 加载多个文件\n\n```\nLOAD '/data/201{8,9}'\nload/data/2018 /data/2019两个目录下的数据\n```\n\n- 其他常用的实例\n\n参考[pig实战 pig常用语法总结，教你快速入门——算法篇](https://www.cnblogs.com/uttu/archive/2013/02/19/2917438.html)\n\n","slug":"pig基础","published":1,"updated":"2019-08-17T09:41:46.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnwy00492qwphyg3jp6d","content":"<h3 id=\"Pig基础知识\"><a href=\"#Pig基础知识\" class=\"headerlink\" title=\"Pig基础知识\"></a>Pig基础知识</h3><ul>\n<li><p>数据模型</p>\n<ul>\n<li>包（bag）是元组（tuple）的集合</li>\n<li>元组（tuple）是有序的字段（field）集</li>\n<li>字段（field）是一段数据</li>\n</ul>\n</li>\n<li><p>语句</p>\n<ul>\n<li>每个语句以分号（；）结尾</li>\n<li>只有illustrate、dump、store等操作的时候，才会执行MapReduce</li>\n</ul>\n</li>\n<li><p>注释</p>\n<ul>\n<li>单行注释 <code>--</code></li>\n<li>多行注释 <code>/* code */</code></li>\n</ul>\n</li>\n<li><p>数据类型</p>\n<ul>\n<li>int</li>\n<li>long</li>\n<li>float</li>\n<li>double</li>\n<li>chararray</li>\n<li>Bytearray</li>\n<li>Boolean</li>\n<li>Datetime</li>\n<li>Biginteger</li>\n<li>Bigdecimal</li>\n<li>Tuple</li>\n<li>Bag</li>\n<li>Map</li>\n<li>NULL值（上述类型都可以为NULL值，等同与Python中的None）</li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n<h3 id=\"Pig运算符\"><a href=\"#Pig运算符\" class=\"headerlink\" title=\"Pig运算符\"></a>Pig运算符</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">relation = <span class=\"keyword\">load</span> <span class=\"string\">'data_path'</span> <span class=\"keyword\">using</span> PigStorage() <span class=\"keyword\">as</span> (<span class=\"keyword\">name</span>:chararray, gpa:<span class=\"keyword\">double</span>);</span><br></pre></td></tr></table></figure>\n<p>其中的<code>PigStorage()</code>为加载或存储结构化数据是使用，还有一些其他的函数<a href=\"https://www.w3cschool.cn/apache_pig/apache_pig_load_store_functions.html\" target=\"_blank\" rel=\"noopener\">参考这里</a>，as 后面跟的是数据的模式，样例中假设了数据模式为<code>name,gpa</code>，并需要指定每个字段的类型。</p>\n<p>读取hive表时使用<code>org.apache.hive.hcatalog.pig.HCatLoader()</code>，不需要指定模式。</p>\n<h4 id=\"存储数据\"><a href=\"#存储数据\" class=\"headerlink\" title=\"存储数据\"></a>存储数据</h4><figure class=\"highlight cs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">store relation <span class=\"keyword\">into</span> <span class=\"string\">'data_path'</span> [ <span class=\"function\"><span class=\"keyword\">using</span> <span class=\"title\">PigStorage</span>(<span class=\"params\"></span>) ]</span>;</span><br></pre></td></tr></table></figure>\n<p>同加载数据时类似，这里不需要再指定数据的模式了。</p>\n<h4 id=\"Dump运算符\"><a href=\"#Dump运算符\" class=\"headerlink\" title=\"Dump运算符\"></a>Dump运算符</h4><figure class=\"highlight abnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dump relation<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>用于将关系数据输出到终端中，通常用于调试。调用该命令可以执行MapReduce。</p>\n<h4 id=\"Describe运算符\"><a href=\"#Describe运算符\" class=\"headerlink\" title=\"Describe运算符\"></a>Describe运算符</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">describe</span> relation;</span><br></pre></td></tr></table></figure>\n<p>输出relation的模型，调用该命令不会执行MapReduce。</p>\n<h4 id=\"Explain运算符\"><a href=\"#Explain运算符\" class=\"headerlink\" title=\"Explain运算符\"></a>Explain运算符</h4><figure class=\"highlight n1ql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">explain</span> relation;</span><br></pre></td></tr></table></figure>\n<p>显示relation的逻辑，物理结构以及MapReduce执行计划。调用该命令不会执行MapReduce。</p>\n<h4 id=\"Illustrate运算符\"><a href=\"#Illustrate运算符\" class=\"headerlink\" title=\"Illustrate运算符\"></a>Illustrate运算符</h4><figure class=\"highlight abnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">illustrate relation<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>提供了一系列语句的逐步执行。调用该命令会执行MapReduce。</p>\n<h4 id=\"分组和连接\"><a href=\"#分组和连接\" class=\"headerlink\" title=\"分组和连接\"></a>分组和连接</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Group_data =<span class=\"built_in\"> GROUP </span>Relation_name BY age;</span><br></pre></td></tr></table></figure>\n<p>通常是一个关系自我进行分组，可以指定一个、多个或者ALL来分组（当指定为ALL的时候，不需要使用BY）</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Cogroup_data</span> = COGROUP Relation1 by age, Relation2 by age<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>和group具有相同的功能，区别是cogroup通常用于多个关系进行分组。</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">Join_data </span>= <span class=\"keyword\">Join </span>customers <span class=\"keyword\">by </span>id, <span class=\"keyword\">orders </span><span class=\"keyword\">by </span>customer_id<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>Join操作和SQL的Join基本一致，也分self-join, left-join, right-join, full-outer-join</p>\n<h4 id=\"合并和拆分\"><a href=\"#合并和拆分\" class=\"headerlink\" title=\"合并和拆分\"></a>合并和拆分</h4><figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Union_data</span> = UNION relation1, relation2<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>将多个关系上下罗列，注意关系只见的模式（schema）要相同。</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPLIT relation1 <span class=\"keyword\">into</span> relation2 <span class=\"keyword\">if</span> (condition), relation3 (condition2);</span><br></pre></td></tr></table></figure>\n<h4 id=\"过滤\"><a href=\"#过滤\" class=\"headerlink\" title=\"过滤\"></a>过滤</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relation2_name =<span class=\"built_in\"> FILTER </span>Relation1_name BY (condition);</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Relation_name2</span> = DISTINCT Relatin_name1<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>去重，通常会花费比较多的时间</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Relation_name2</span> = FOREACH Relatin_name1 GENERATE (required data)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>通常用完filter、group、join等操作后，紧接着使用foreach</p>\n<h4 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h4><figure class=\"highlight oxygene\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relation_name2 = <span class=\"keyword\">ORDER</span> Relatin_name1 <span class=\"keyword\">BY</span> (<span class=\"keyword\">ASC</span>|<span class=\"keyword\">DESC</span>);</span><br><span class=\"line\">`</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Result</span> = LIMIT Relation_name （required number of tuples）<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>限制记录的数量</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><h5 id=\"内置函数\"><a href=\"#内置函数\" class=\"headerlink\" title=\"内置函数\"></a>内置函数</h5><ul>\n<li><p>eval函数</p>\n<ul>\n<li>AVG()         计算包内数值的平均值</li>\n<li>BagToString() 将包的元素连接成字符串</li>\n<li>CONCAT()      连接两个或多个相同类型的表达式</li>\n<li>COUNT()       获取包中元素的数量，同时计算包中元组的数量</li>\n<li>COUNT_STAR()  计算包中的元素数量</li>\n<li>DIFF()        比较元组中的两个包(字段)</li>\n<li>isEmpty()     检查包或映射是否为空</li>\n<li>MAX()         计算单列包中的列(数值或字符)的最大值</li>\n<li>MIN()         计算单列包中的列(数值或字符)的最小值</li>\n<li>PluckTuple()  可定义Prefix，并过滤以给定Prefix开头的关系中的列</li>\n<li>SIZE()        基于任何Pig数据类型计算元素的数量</li>\n<li>SUBtTRACT(A，B) 两个包的差，结果为在A不在B种的元组的包</li>\n<li>SUM()         计算单列包中的某列的数值总和</li>\n<li>TOKENIZE()    要在单个元组中拆分字符串(其中包含一组字)，病返回包含拆分操作的输出的包</li>\n</ul>\n</li>\n<li><p>load函数 / store函数</p>\n<ul>\n<li>PigStorage()      加载和存储结构化文件</li>\n<li>TextLoader()      将非结构化数据加载到Pig中</li>\n<li>BinStorage()      使用机器可读格式将数据加载并存储到Pig中</li>\n<li>Handling Compression 加载和存储压缩数据</li>\n</ul>\n</li>\n<li><p>bag函数 / tuple函数</p>\n<ul>\n<li>TOBAG()           将两个或多个表达式转换为包</li>\n<li>TOP()             获取关系的顶部N个元组</li>\n<li>TOTUPLE()         将一个或多个表达式转换为元祖</li>\n<li>TOMAP()           将key-value对转换为Map</li>\n</ul>\n</li>\n<li><p>stirng函数</p>\n</li>\n<li>date函数</li>\n<li><p>math函数</p>\n</li>\n<li><p>注释 </p>\n<ul>\n<li>/<em> multi-lines code </em>/</li>\n<li>-- single line code</li>\n</ul>\n</li>\n</ul>\n<p>具体使用方法及其他的一些内置函数，常用函数，可以<a href=\"https://www.w3cschool.cn/apache_pig/apache_pig_eval_functions.html\" target=\"_blank\" rel=\"noopener\">参考w3cschool</a></p>\n<p>写Pig脚本，<strong>重点是UDF的编写</strong>，这里我写的jython_UDF比较多（其实就是按照Python写的），\n需要注意的是要定义好outputSchema，并在UDF函数头部加上修饰符@</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/use/bin/python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">reload(sys)</span><br><span class=\"line\">sys.setdefaultencoding(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">outputSchema</span><span class=\"params\">(schema_def)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decorate</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\">\t\tfunc.outputSchema = schema_def</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> func</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> decorate</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@outputSchema('newDate:chararray')</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dateChangeFormat</span><span class=\"params\">(thisDate)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> thisDate.strip().split()[<span class=\"number\">0</span>];</span><br><span class=\"line\">\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">'2000-01-01'</span></span><br></pre></td></tr></table></figure>\n<p>题外话：现在Spark是主流，建议转Spark</p>\n<h3 id=\"Pig的一些实例\"><a href=\"#Pig的一些实例\" class=\"headerlink\" title=\"Pig的一些实例\"></a>Pig的一些实例</h3><ul>\n<li>差集的计算</li>\n</ul>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">A</span> = load <span class=\"string\">'input1'</span> as (x, y)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">B</span> = load <span class=\"string\">'input2'</span> as (u, v)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">C</span> = cogroup A by x, B by u<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">D</span> = filter C by IsEmpty(B)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">E</span> = foreach D generate flatten(A)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>解释：A和B分组后，如果B中为空，那么说明与之对应的A元素仅在A中出现，即在A不在B，也即为A-B的差集</p>\n<ul>\n<li>传递参数</li>\n</ul>\n<figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pig -<span class=\"built_in\">param</span> <span class=\"built_in\">date</span>=<span class=\"string\">'2014-05-17'</span> example.pig</span><br></pre></td></tr></table></figure>\n<p>pig脚本中用<code>$date</code>调用该值</p>\n<ul>\n<li>FLATTEN关键字</li>\n</ul>\n<p>可以消除嵌套</p>\n<ul>\n<li>项目相关</li>\n</ul>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%<span class=\"section\">default</span> PDAY '<span class=\"number\">2019</span><span class=\"number\">-02</span><span class=\"number\">-02</span>'</span><br></pre></td></tr></table></figure>\n<p>在脚本前面使用该命令设定脚本传递参数的默认值</p>\n<figure class=\"highlight delphi\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">register</span> xxx.jar <span class=\"keyword\">as</span> xxx</span><br></pre></td></tr></table></figure>\n<p>在脚本的最前面注册使用jar文件，其中as重命名不是必要的</p>\n<ul>\n<li>统计行数</li>\n</ul>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = LOAD '<span class=\"number\">1.</span>txt' USING PigStorage (' ‘) AS(col1:chararray, col2:<span class=\"type\">int</span>, col3:<span class=\"type\">int</span>, col4:<span class=\"type\">int</span>, col5:<span class=\"type\">double</span>, col6:<span class=\"type\">double</span>); </span><br><span class=\"line\">B = GROUP A <span class=\"built_in\">all</span>;</span><br><span class=\"line\">C = FOREACH B GENERATE COUNT(col2);</span><br><span class=\"line\">DUMP C;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>加载多个文件</li>\n</ul>\n<figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">LOAD</span> '/<span class=\"class\"><span class=\"keyword\">data</span>/201&#123;8,9&#125;'</span></span><br><span class=\"line\"><span class=\"title\">load</span>/<span class=\"class\"><span class=\"keyword\">data</span>/2018 /<span class=\"keyword\">data</span>/2019两个目录下的数据</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>其他常用的实例</li>\n</ul>\n<p>参考<a href=\"https://www.cnblogs.com/uttu/archive/2013/02/19/2917438.html\" target=\"_blank\" rel=\"noopener\">pig实战 pig常用语法总结，教你快速入门——算法篇</a></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"Pig基础知识\"><a href=\"#Pig基础知识\" class=\"headerlink\" title=\"Pig基础知识\"></a>Pig基础知识</h3><ul>\n<li><p>数据模型</p>\n<ul>\n<li>包（bag）是元组（tuple）的集合</li>\n<li>元组（tuple）是有序的字段（field）集</li>\n<li>字段（field）是一段数据</li>\n</ul>\n</li>\n<li><p>语句</p>\n<ul>\n<li>每个语句以分号（；）结尾</li>\n<li>只有illustrate、dump、store等操作的时候，才会执行MapReduce</li>\n</ul>\n</li>\n<li><p>注释</p>\n<ul>\n<li>单行注释 <code>--</code></li>\n<li>多行注释 <code>/* code */</code></li>\n</ul>\n</li>\n<li><p>数据类型</p>\n<ul>\n<li>int</li>\n<li>long</li>\n<li>float</li>\n<li>double</li>\n<li>chararray</li>\n<li>Bytearray</li>\n<li>Boolean</li>\n<li>Datetime</li>\n<li>Biginteger</li>\n<li>Bigdecimal</li>\n<li>Tuple</li>\n<li>Bag</li>\n<li>Map</li>\n<li>NULL值（上述类型都可以为NULL值，等同与Python中的None）</li>\n</ul>\n</li>\n</ul>","more":"\n<h3 id=\"Pig运算符\"><a href=\"#Pig运算符\" class=\"headerlink\" title=\"Pig运算符\"></a>Pig运算符</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">relation = <span class=\"keyword\">load</span> <span class=\"string\">'data_path'</span> <span class=\"keyword\">using</span> PigStorage() <span class=\"keyword\">as</span> (<span class=\"keyword\">name</span>:chararray, gpa:<span class=\"keyword\">double</span>);</span><br></pre></td></tr></table></figure>\n<p>其中的<code>PigStorage()</code>为加载或存储结构化数据是使用，还有一些其他的函数<a href=\"https://www.w3cschool.cn/apache_pig/apache_pig_load_store_functions.html\" target=\"_blank\" rel=\"noopener\">参考这里</a>，as 后面跟的是数据的模式，样例中假设了数据模式为<code>name,gpa</code>，并需要指定每个字段的类型。</p>\n<p>读取hive表时使用<code>org.apache.hive.hcatalog.pig.HCatLoader()</code>，不需要指定模式。</p>\n<h4 id=\"存储数据\"><a href=\"#存储数据\" class=\"headerlink\" title=\"存储数据\"></a>存储数据</h4><figure class=\"highlight cs\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">store relation <span class=\"keyword\">into</span> <span class=\"string\">'data_path'</span> [ <span class=\"function\"><span class=\"keyword\">using</span> <span class=\"title\">PigStorage</span>(<span class=\"params\"></span>) ]</span>;</span><br></pre></td></tr></table></figure>\n<p>同加载数据时类似，这里不需要再指定数据的模式了。</p>\n<h4 id=\"Dump运算符\"><a href=\"#Dump运算符\" class=\"headerlink\" title=\"Dump运算符\"></a>Dump运算符</h4><figure class=\"highlight abnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dump relation<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>用于将关系数据输出到终端中，通常用于调试。调用该命令可以执行MapReduce。</p>\n<h4 id=\"Describe运算符\"><a href=\"#Describe运算符\" class=\"headerlink\" title=\"Describe运算符\"></a>Describe运算符</h4><figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">describe</span> relation;</span><br></pre></td></tr></table></figure>\n<p>输出relation的模型，调用该命令不会执行MapReduce。</p>\n<h4 id=\"Explain运算符\"><a href=\"#Explain运算符\" class=\"headerlink\" title=\"Explain运算符\"></a>Explain运算符</h4><figure class=\"highlight n1ql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">explain</span> relation;</span><br></pre></td></tr></table></figure>\n<p>显示relation的逻辑，物理结构以及MapReduce执行计划。调用该命令不会执行MapReduce。</p>\n<h4 id=\"Illustrate运算符\"><a href=\"#Illustrate运算符\" class=\"headerlink\" title=\"Illustrate运算符\"></a>Illustrate运算符</h4><figure class=\"highlight abnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">illustrate relation<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>提供了一系列语句的逐步执行。调用该命令会执行MapReduce。</p>\n<h4 id=\"分组和连接\"><a href=\"#分组和连接\" class=\"headerlink\" title=\"分组和连接\"></a>分组和连接</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Group_data =<span class=\"built_in\"> GROUP </span>Relation_name BY age;</span><br></pre></td></tr></table></figure>\n<p>通常是一个关系自我进行分组，可以指定一个、多个或者ALL来分组（当指定为ALL的时候，不需要使用BY）</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Cogroup_data</span> = COGROUP Relation1 by age, Relation2 by age<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>和group具有相同的功能，区别是cogroup通常用于多个关系进行分组。</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">Join_data </span>= <span class=\"keyword\">Join </span>customers <span class=\"keyword\">by </span>id, <span class=\"keyword\">orders </span><span class=\"keyword\">by </span>customer_id<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>Join操作和SQL的Join基本一致，也分self-join, left-join, right-join, full-outer-join</p>\n<h4 id=\"合并和拆分\"><a href=\"#合并和拆分\" class=\"headerlink\" title=\"合并和拆分\"></a>合并和拆分</h4><figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Union_data</span> = UNION relation1, relation2<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>将多个关系上下罗列，注意关系只见的模式（schema）要相同。</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SPLIT relation1 <span class=\"keyword\">into</span> relation2 <span class=\"keyword\">if</span> (condition), relation3 (condition2);</span><br></pre></td></tr></table></figure>\n<h4 id=\"过滤\"><a href=\"#过滤\" class=\"headerlink\" title=\"过滤\"></a>过滤</h4><figure class=\"highlight routeros\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relation2_name =<span class=\"built_in\"> FILTER </span>Relation1_name BY (condition);</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Relation_name2</span> = DISTINCT Relatin_name1<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>去重，通常会花费比较多的时间</p>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Relation_name2</span> = FOREACH Relatin_name1 GENERATE (required data)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>通常用完filter、group、join等操作后，紧接着使用foreach</p>\n<h4 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h4><figure class=\"highlight oxygene\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Relation_name2 = <span class=\"keyword\">ORDER</span> Relatin_name1 <span class=\"keyword\">BY</span> (<span class=\"keyword\">ASC</span>|<span class=\"keyword\">DESC</span>);</span><br><span class=\"line\">`</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">Result</span> = LIMIT Relation_name （required number of tuples）<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>限制记录的数量</p>\n<h4 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h4><h5 id=\"内置函数\"><a href=\"#内置函数\" class=\"headerlink\" title=\"内置函数\"></a>内置函数</h5><ul>\n<li><p>eval函数</p>\n<ul>\n<li>AVG()         计算包内数值的平均值</li>\n<li>BagToString() 将包的元素连接成字符串</li>\n<li>CONCAT()      连接两个或多个相同类型的表达式</li>\n<li>COUNT()       获取包中元素的数量，同时计算包中元组的数量</li>\n<li>COUNT_STAR()  计算包中的元素数量</li>\n<li>DIFF()        比较元组中的两个包(字段)</li>\n<li>isEmpty()     检查包或映射是否为空</li>\n<li>MAX()         计算单列包中的列(数值或字符)的最大值</li>\n<li>MIN()         计算单列包中的列(数值或字符)的最小值</li>\n<li>PluckTuple()  可定义Prefix，并过滤以给定Prefix开头的关系中的列</li>\n<li>SIZE()        基于任何Pig数据类型计算元素的数量</li>\n<li>SUBtTRACT(A，B) 两个包的差，结果为在A不在B种的元组的包</li>\n<li>SUM()         计算单列包中的某列的数值总和</li>\n<li>TOKENIZE()    要在单个元组中拆分字符串(其中包含一组字)，病返回包含拆分操作的输出的包</li>\n</ul>\n</li>\n<li><p>load函数 / store函数</p>\n<ul>\n<li>PigStorage()      加载和存储结构化文件</li>\n<li>TextLoader()      将非结构化数据加载到Pig中</li>\n<li>BinStorage()      使用机器可读格式将数据加载并存储到Pig中</li>\n<li>Handling Compression 加载和存储压缩数据</li>\n</ul>\n</li>\n<li><p>bag函数 / tuple函数</p>\n<ul>\n<li>TOBAG()           将两个或多个表达式转换为包</li>\n<li>TOP()             获取关系的顶部N个元组</li>\n<li>TOTUPLE()         将一个或多个表达式转换为元祖</li>\n<li>TOMAP()           将key-value对转换为Map</li>\n</ul>\n</li>\n<li><p>stirng函数</p>\n</li>\n<li>date函数</li>\n<li><p>math函数</p>\n</li>\n<li><p>注释 </p>\n<ul>\n<li>/<em> multi-lines code </em>/</li>\n<li>-- single line code</li>\n</ul>\n</li>\n</ul>\n<p>具体使用方法及其他的一些内置函数，常用函数，可以<a href=\"https://www.w3cschool.cn/apache_pig/apache_pig_eval_functions.html\" target=\"_blank\" rel=\"noopener\">参考w3cschool</a></p>\n<p>写Pig脚本，<strong>重点是UDF的编写</strong>，这里我写的jython_UDF比较多（其实就是按照Python写的），\n需要注意的是要定义好outputSchema，并在UDF函数头部加上修饰符@</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/use/bin/python</span></span><br><span class=\"line\"><span class=\"comment\"># -*- coding:utf-8 -*-</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">reload(sys)</span><br><span class=\"line\">sys.setdefaultencoding(<span class=\"string\">'utf-8'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">outputSchema</span><span class=\"params\">(schema_def)</span>:</span></span><br><span class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">decorate</span><span class=\"params\">(func)</span>:</span></span><br><span class=\"line\">\t\tfunc.outputSchema = schema_def</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> func</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> decorate</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@outputSchema('newDate:chararray')</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dateChangeFormat</span><span class=\"params\">(thisDate)</span>:</span></span><br><span class=\"line\">\t<span class=\"keyword\">try</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> thisDate.strip().split()[<span class=\"number\">0</span>];</span><br><span class=\"line\">\t<span class=\"keyword\">except</span>:</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"string\">'2000-01-01'</span></span><br></pre></td></tr></table></figure>\n<p>题外话：现在Spark是主流，建议转Spark</p>\n<h3 id=\"Pig的一些实例\"><a href=\"#Pig的一些实例\" class=\"headerlink\" title=\"Pig的一些实例\"></a>Pig的一些实例</h3><ul>\n<li>差集的计算</li>\n</ul>\n<figure class=\"highlight ini\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">A</span> = load <span class=\"string\">'input1'</span> as (x, y)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">B</span> = load <span class=\"string\">'input2'</span> as (u, v)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">C</span> = cogroup A by x, B by u<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">D</span> = filter C by IsEmpty(B)<span class=\"comment\">;</span></span><br><span class=\"line\"><span class=\"attr\">E</span> = foreach D generate flatten(A)<span class=\"comment\">;</span></span><br></pre></td></tr></table></figure>\n<p>解释：A和B分组后，如果B中为空，那么说明与之对应的A元素仅在A中出现，即在A不在B，也即为A-B的差集</p>\n<ul>\n<li>传递参数</li>\n</ul>\n<figure class=\"highlight livecodeserver\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pig -<span class=\"built_in\">param</span> <span class=\"built_in\">date</span>=<span class=\"string\">'2014-05-17'</span> example.pig</span><br></pre></td></tr></table></figure>\n<p>pig脚本中用<code>$date</code>调用该值</p>\n<ul>\n<li>FLATTEN关键字</li>\n</ul>\n<p>可以消除嵌套</p>\n<ul>\n<li>项目相关</li>\n</ul>\n<figure class=\"highlight lsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%<span class=\"section\">default</span> PDAY '<span class=\"number\">2019</span><span class=\"number\">-02</span><span class=\"number\">-02</span>'</span><br></pre></td></tr></table></figure>\n<p>在脚本前面使用该命令设定脚本传递参数的默认值</p>\n<figure class=\"highlight delphi\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">register</span> xxx.jar <span class=\"keyword\">as</span> xxx</span><br></pre></td></tr></table></figure>\n<p>在脚本的最前面注册使用jar文件，其中as重命名不是必要的</p>\n<ul>\n<li>统计行数</li>\n</ul>\n<figure class=\"highlight glsl\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = LOAD '<span class=\"number\">1.</span>txt' USING PigStorage (' ‘) AS(col1:chararray, col2:<span class=\"type\">int</span>, col3:<span class=\"type\">int</span>, col4:<span class=\"type\">int</span>, col5:<span class=\"type\">double</span>, col6:<span class=\"type\">double</span>); </span><br><span class=\"line\">B = GROUP A <span class=\"built_in\">all</span>;</span><br><span class=\"line\">C = FOREACH B GENERATE COUNT(col2);</span><br><span class=\"line\">DUMP C;</span><br></pre></td></tr></table></figure>\n<ul>\n<li>加载多个文件</li>\n</ul>\n<figure class=\"highlight haskell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">LOAD</span> '/<span class=\"class\"><span class=\"keyword\">data</span>/201&#123;8,9&#125;'</span></span><br><span class=\"line\"><span class=\"title\">load</span>/<span class=\"class\"><span class=\"keyword\">data</span>/2018 /<span class=\"keyword\">data</span>/2019两个目录下的数据</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>其他常用的实例</li>\n</ul>\n<p>参考<a href=\"https://www.cnblogs.com/uttu/archive/2013/02/19/2917438.html\" target=\"_blank\" rel=\"noopener\">pig实战 pig常用语法总结，教你快速入门——算法篇</a></p>\n"},{"layout":"post","title":"正负样本不平衡的处理方法","date":"2018-06-21T04:10:00.000Z","mathjax":true,"copyright":true,"_content":"\n\n### 1 通过过抽样和欠抽样解决样本不均衡\n\n抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。\n\n**过抽样**\n\n过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见[原始论文](/posts_res/2018-04-03-interview/SMOTE_Synthetic Minority Over-sampling Technique.pdf)。\n\n这里有SMOTE算法的多个不同语言的实现版本： \n* Python: [UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。\n* R: [DMwR package](https://blog.csdn.net/heyongluoyao8/article/details/DMwR%20packagehttps://cran.r-project.org/web/packages/DMwR/index.html)。\n* Weka: [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html)\n\n**欠抽样**\n\n欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。\n\n总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。\n\n\n------\n\n### 2 通过正负样本的惩罚权重解决样本不均衡\n\n通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。\n\n使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight : {dict, 'balanced'}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。\n\n如果算法本身支持，这种思路是更加简单且高效的方法。\n\n\n----------\n\n### 3 通过组合/集成方法解决样本不均衡\n\n组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。\n\n例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。\n\n这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。\n\n如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。\n\n\n-----------\n\n### 4 通过特征选择解决样本不均衡\n\n上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。\n\n一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。\n\n\n----------\n\n### 5 尝试其它评价指标\n\n* 混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。\n* 精确度(Precision)\n* 召回率(Recall)\n* F1值(F1 Score)：精确度与找召回率的加权平均。\n* Kappa ([Cohen kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))\n* ROC曲线(ROC Curves)：见[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)\n\n\n---------\n\n### 6 尝试不同的分类算法\n\n应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据，具体请见[Why you should be Spot-Checking Algorithms on your Machine Learning Problems](https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/)\n\n### 7 尝试一个新的角度理解问题 \n\n我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。\n \n* 异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。\n* 变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 \n\n将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。\n\n推荐看论文[Learning from Imbalanced Data](/posts_res/2018-04-03-interview/Learning from Imbalanced Data.pdf)\n\n主要包括四大类方法:\n>\n1. Sampling\n2. Cost Sensitive Methods\n3. Kernal-Based Methods and Active Learning Methods\n4. One-Class Learning or Novelty Detection Methods\n\n\n**极端情况下，只有正例(负例)如何做分类？**\n\n*当作异常点检测问题或变化趋势检测问题*\n\n>\n识别异常点可以用四分位数(Tukey's text)方法，详见[这里](https://www.zhihu.com/question/38066650/answer/202282227)\n[One-Class SVM介绍](https://zhuanlan.zhihu.com/p/32784067)\n\n\n-------------\n\n>\n1. [CTR 预估正负样本 不平衡，请问怎么解决?](https://www.zhihu.com/question/27535832/answer/223882022)\n2. [在分类中如何处理训练集中不平衡问题](https://blog.csdn.net/heyongluoyao8/article/details/49408131)\n","source":"_posts/2018-6-21-unbalancedata.md","raw":"---\nlayout: post\ntitle: 正负样本不平衡的处理方法\ndate: 2018-06-21 12:10 +0800\ncategories: 机器学习\ntags:\n- 特征选择\nmathjax: true\ncopyright: true\n---\n\n\n### 1 通过过抽样和欠抽样解决样本不均衡\n\n抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。\n\n**过抽样**\n\n过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见[原始论文](/posts_res/2018-04-03-interview/SMOTE_Synthetic Minority Over-sampling Technique.pdf)。\n\n这里有SMOTE算法的多个不同语言的实现版本： \n* Python: [UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。\n* R: [DMwR package](https://blog.csdn.net/heyongluoyao8/article/details/DMwR%20packagehttps://cran.r-project.org/web/packages/DMwR/index.html)。\n* Weka: [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html)\n\n**欠抽样**\n\n欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。\n\n总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。\n\n\n------\n\n### 2 通过正负样本的惩罚权重解决样本不均衡\n\n通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。\n\n使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight : {dict, 'balanced'}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。\n\n如果算法本身支持，这种思路是更加简单且高效的方法。\n\n\n----------\n\n### 3 通过组合/集成方法解决样本不均衡\n\n组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。\n\n例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。\n\n这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。\n\n如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。\n\n\n-----------\n\n### 4 通过特征选择解决样本不均衡\n\n上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。\n\n一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。\n\n\n----------\n\n### 5 尝试其它评价指标\n\n* 混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。\n* 精确度(Precision)\n* 召回率(Recall)\n* F1值(F1 Score)：精确度与找召回率的加权平均。\n* Kappa ([Cohen kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))\n* ROC曲线(ROC Curves)：见[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)\n\n\n---------\n\n### 6 尝试不同的分类算法\n\n应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据，具体请见[Why you should be Spot-Checking Algorithms on your Machine Learning Problems](https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/)\n\n### 7 尝试一个新的角度理解问题 \n\n我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。\n \n* 异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。\n* 变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 \n\n将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。\n\n推荐看论文[Learning from Imbalanced Data](/posts_res/2018-04-03-interview/Learning from Imbalanced Data.pdf)\n\n主要包括四大类方法:\n>\n1. Sampling\n2. Cost Sensitive Methods\n3. Kernal-Based Methods and Active Learning Methods\n4. One-Class Learning or Novelty Detection Methods\n\n\n**极端情况下，只有正例(负例)如何做分类？**\n\n*当作异常点检测问题或变化趋势检测问题*\n\n>\n识别异常点可以用四分位数(Tukey's text)方法，详见[这里](https://www.zhihu.com/question/38066650/answer/202282227)\n[One-Class SVM介绍](https://zhuanlan.zhihu.com/p/32784067)\n\n\n-------------\n\n>\n1. [CTR 预估正负样本 不平衡，请问怎么解决?](https://www.zhihu.com/question/27535832/answer/223882022)\n2. [在分类中如何处理训练集中不平衡问题](https://blog.csdn.net/heyongluoyao8/article/details/49408131)\n","slug":"2018-6-21-unbalancedata","published":1,"updated":"2019-08-17T09:37:58.851Z","comments":1,"photos":[],"link":"","_id":"cjzl0wnx9004c2qwpjfie59xv","content":"<h3 id=\"1-通过过抽样和欠抽样解决样本不均衡\"><a href=\"#1-通过过抽样和欠抽样解决样本不均衡\" class=\"headerlink\" title=\"1 通过过抽样和欠抽样解决样本不均衡\"></a>1 通过过抽样和欠抽样解决样本不均衡</h3><p>抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。</p><p><strong>过抽样</strong></p><p>过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见<a href=\"/posts_res/2018-04-03-interview/SMOTE_Synthetic Minority Over-sampling Technique.pdf\">原始论文</a>。</p><a id=\"more\"></a>\n\n\n<p>这里有SMOTE算法的多个不同语言的实现版本： </p>\n<ul>\n<li>Python: <a href=\"https://github.com/fmfn/UnbalancedDataset\" target=\"_blank\" rel=\"noopener\">UnbalancedDataset</a>模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。</li>\n<li>R: <a href=\"https://blog.csdn.net/heyongluoyao8/article/details/DMwR%20packagehttps://cran.r-project.org/web/packages/DMwR/index.html\" target=\"_blank\" rel=\"noopener\">DMwR package</a>。</li>\n<li>Weka: <a href=\"http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html\" target=\"_blank\" rel=\"noopener\">SMOTE supervised filter</a></li>\n</ul>\n<p><strong>欠抽样</strong></p>\n<p>欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。</p>\n<p>总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。</p>\n<hr>\n<h3 id=\"2-通过正负样本的惩罚权重解决样本不均衡\"><a href=\"#2-通过正负样本的惩罚权重解决样本不均衡\" class=\"headerlink\" title=\"2 通过正负样本的惩罚权重解决样本不均衡\"></a>2 通过正负样本的惩罚权重解决样本不均衡</h3><p>通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。</p>\n<p>使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight : {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。</p>\n<p>如果算法本身支持，这种思路是更加简单且高效的方法。</p>\n<hr>\n<h3 id=\"3-通过组合-集成方法解决样本不均衡\"><a href=\"#3-通过组合-集成方法解决样本不均衡\" class=\"headerlink\" title=\"3 通过组合/集成方法解决样本不均衡\"></a>3 通过组合/集成方法解决样本不均衡</h3><p>组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。</p>\n<p>例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。</p>\n<p>这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。</p>\n<p>如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。</p>\n<hr>\n<h3 id=\"4-通过特征选择解决样本不均衡\"><a href=\"#4-通过特征选择解决样本不均衡\" class=\"headerlink\" title=\"4 通过特征选择解决样本不均衡\"></a>4 通过特征选择解决样本不均衡</h3><p>上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。</p>\n<p>一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。</p>\n<hr>\n<h3 id=\"5-尝试其它评价指标\"><a href=\"#5-尝试其它评价指标\" class=\"headerlink\" title=\"5 尝试其它评价指标\"></a>5 尝试其它评价指标</h3><ul>\n<li>混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。</li>\n<li>精确度(Precision)</li>\n<li>召回率(Recall)</li>\n<li>F1值(F1 Score)：精确度与找召回率的加权平均。</li>\n<li>Kappa (<a href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\" target=\"_blank\" rel=\"noopener\">Cohen kappa</a>)</li>\n<li>ROC曲线(ROC Curves)：见<a href=\"http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/\" target=\"_blank\" rel=\"noopener\">Assessing and Comparing Classifier Performance with ROC Curves</a></li>\n</ul>\n<hr>\n<h3 id=\"6-尝试不同的分类算法\"><a href=\"#6-尝试不同的分类算法\" class=\"headerlink\" title=\"6 尝试不同的分类算法\"></a>6 尝试不同的分类算法</h3><p>应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据，具体请见<a href=\"https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/\" target=\"_blank\" rel=\"noopener\">Why you should be Spot-Checking Algorithms on your Machine Learning Problems</a></p>\n<h3 id=\"7-尝试一个新的角度理解问题\"><a href=\"#7-尝试一个新的角度理解问题\" class=\"headerlink\" title=\"7 尝试一个新的角度理解问题\"></a>7 尝试一个新的角度理解问题</h3><p>我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。</p>\n<ul>\n<li>异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。</li>\n<li>变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 </li>\n</ul>\n<p>将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。</p>\n<p>推荐看论文<a href=\"/posts_res/2018-04-03-interview/Learning from Imbalanced Data.pdf\">Learning from Imbalanced Data</a></p>\n<p>主要包括四大类方法:\n&gt;</p>\n<ol>\n<li>Sampling</li>\n<li>Cost Sensitive Methods</li>\n<li>Kernal-Based Methods and Active Learning Methods</li>\n<li>One-Class Learning or Novelty Detection Methods</li>\n</ol>\n<p><strong>极端情况下，只有正例(负例)如何做分类？</strong></p>\n<p><em>当作异常点检测问题或变化趋势检测问题</em></p>\n<p>&gt;\n识别异常点可以用四分位数(Tukey’s text)方法，详见<a href=\"https://www.zhihu.com/question/38066650/answer/202282227\" target=\"_blank\" rel=\"noopener\">这里</a>\n<a href=\"https://zhuanlan.zhihu.com/p/32784067\" target=\"_blank\" rel=\"noopener\">One-Class SVM介绍</a></p>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://www.zhihu.com/question/27535832/answer/223882022\" target=\"_blank\" rel=\"noopener\">CTR 预估正负样本 不平衡，请问怎么解决?</a></li>\n<li><a href=\"https://blog.csdn.net/heyongluoyao8/article/details/49408131\" target=\"_blank\" rel=\"noopener\">在分类中如何处理训练集中不平衡问题</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"1-通过过抽样和欠抽样解决样本不均衡\"><a href=\"#1-通过过抽样和欠抽样解决样本不均衡\" class=\"headerlink\" title=\"1 通过过抽样和欠抽样解决样本不均衡\"></a>1 通过过抽样和欠抽样解决样本不均衡</h3><p>抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种。</p><p><strong>过抽样</strong></p><p>过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，这种方法的缺点是如果样本特征少而可能导致过拟合的问题；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见<a href=\"/posts_res/2018-04-03-interview/SMOTE_Synthetic Minority Over-sampling Technique.pdf\">原始论文</a>。</p>","more":"\n\n\n<p>这里有SMOTE算法的多个不同语言的实现版本： </p>\n<ul>\n<li>Python: <a href=\"https://github.com/fmfn/UnbalancedDataset\" target=\"_blank\" rel=\"noopener\">UnbalancedDataset</a>模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。</li>\n<li>R: <a href=\"https://blog.csdn.net/heyongluoyao8/article/details/DMwR%20packagehttps://cran.r-project.org/web/packages/DMwR/index.html\" target=\"_blank\" rel=\"noopener\">DMwR package</a>。</li>\n<li>Weka: <a href=\"http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html\" target=\"_blank\" rel=\"noopener\">SMOTE supervised filter</a></li>\n</ul>\n<p><strong>欠抽样</strong></p>\n<p>欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。</p>\n<p>总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种（过抽样）方法应用更加广泛。</p>\n<hr>\n<h3 id=\"2-通过正负样本的惩罚权重解决样本不均衡\"><a href=\"#2-通过正负样本的惩罚权重解决样本不均衡\" class=\"headerlink\" title=\"2 通过正负样本的惩罚权重解决样本不均衡\"></a>2 通过正负样本的惩罚权重解决样本不均衡</h3><p>通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。</p>\n<p>使用这种方法时需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在class_weight : {dict, ‘balanced’}中针对不同类别针对不同的权重，来手动指定不同类别的权重。如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理，计算公式为：n_samples / (n_classes * np.bincount(y))。</p>\n<p>如果算法本身支持，这种思路是更加简单且高效的方法。</p>\n<hr>\n<h3 id=\"3-通过组合-集成方法解决样本不均衡\"><a href=\"#3-通过组合-集成方法解决样本不均衡\" class=\"headerlink\" title=\"3 通过组合/集成方法解决样本不均衡\"></a>3 通过组合/集成方法解决样本不均衡</h3><p>组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。</p>\n<p>例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。</p>\n<p>这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。</p>\n<p>如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适。</p>\n<hr>\n<h3 id=\"4-通过特征选择解决样本不均衡\"><a href=\"#4-通过特征选择解决样本不均衡\" class=\"headerlink\" title=\"4 通过特征选择解决样本不均衡\"></a>4 通过特征选择解决样本不均衡</h3><p>上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。</p>\n<p>一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。</p>\n<hr>\n<h3 id=\"5-尝试其它评价指标\"><a href=\"#5-尝试其它评价指标\" class=\"headerlink\" title=\"5 尝试其它评价指标\"></a>5 尝试其它评价指标</h3><ul>\n<li>混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。</li>\n<li>精确度(Precision)</li>\n<li>召回率(Recall)</li>\n<li>F1值(F1 Score)：精确度与找召回率的加权平均。</li>\n<li>Kappa (<a href=\"https://en.wikipedia.org/wiki/Cohen%27s_kappa\" target=\"_blank\" rel=\"noopener\">Cohen kappa</a>)</li>\n<li>ROC曲线(ROC Curves)：见<a href=\"http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/\" target=\"_blank\" rel=\"noopener\">Assessing and Comparing Classifier Performance with ROC Curves</a></li>\n</ul>\n<hr>\n<h3 id=\"6-尝试不同的分类算法\"><a href=\"#6-尝试不同的分类算法\" class=\"headerlink\" title=\"6 尝试不同的分类算法\"></a>6 尝试不同的分类算法</h3><p>应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据，具体请见<a href=\"https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/\" target=\"_blank\" rel=\"noopener\">Why you should be Spot-Checking Algorithms on your Machine Learning Problems</a></p>\n<h3 id=\"7-尝试一个新的角度理解问题\"><a href=\"#7-尝试一个新的角度理解问题\" class=\"headerlink\" title=\"7 尝试一个新的角度理解问题\"></a>7 尝试一个新的角度理解问题</h3><p>我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。</p>\n<ul>\n<li>异常点检测即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。</li>\n<li>变化趋势检测类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 </li>\n</ul>\n<p>将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。</p>\n<p>推荐看论文<a href=\"/posts_res/2018-04-03-interview/Learning from Imbalanced Data.pdf\">Learning from Imbalanced Data</a></p>\n<p>主要包括四大类方法:\n&gt;</p>\n<ol>\n<li>Sampling</li>\n<li>Cost Sensitive Methods</li>\n<li>Kernal-Based Methods and Active Learning Methods</li>\n<li>One-Class Learning or Novelty Detection Methods</li>\n</ol>\n<p><strong>极端情况下，只有正例(负例)如何做分类？</strong></p>\n<p><em>当作异常点检测问题或变化趋势检测问题</em></p>\n<p>&gt;\n识别异常点可以用四分位数(Tukey’s text)方法，详见<a href=\"https://www.zhihu.com/question/38066650/answer/202282227\" target=\"_blank\" rel=\"noopener\">这里</a>\n<a href=\"https://zhuanlan.zhihu.com/p/32784067\" target=\"_blank\" rel=\"noopener\">One-Class SVM介绍</a></p>\n<hr>\n<p>&gt;</p>\n<ol>\n<li><a href=\"https://www.zhihu.com/question/27535832/answer/223882022\" target=\"_blank\" rel=\"noopener\">CTR 预估正负样本 不平衡，请问怎么解决?</a></li>\n<li><a href=\"https://blog.csdn.net/heyongluoyao8/article/details/49408131\" target=\"_blank\" rel=\"noopener\">在分类中如何处理训练集中不平衡问题</a></li>\n</ol>\n"},{"title":"Spark基础","mathjax":true,"copyright":true,"date":"2019-03-07T12:39:32.000Z","_content":"\n## 多说几句\n\n写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。\n\nSpark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录，\n大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。\n\n现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑，\n网络上的主要学习来源是[子雨大数据之Spark入门教程](http://dblab.xmu.edu.cn/blog/spark/)，这个教程真的只是入门。\n\n以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述\n\n## RDD编程\n\n- 基本RDD“转换”运算\n    - **map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集**\n    - **filter(func)：筛选出满足函数func的元素，并返回一个新的数据集**\n    - **flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果**\n    - distinct（去重运算）\n    - randomSplit（根据指定的比例随机分为N各RDD）\n    - **reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合**\n    - **groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集**\n    - union（两个RDD取并集）\n    - intersection（两个RDD取交集）\n    - subtract（两个RDD取差集）\n    - cartesian（两个RDD进行笛卡尔积运算）\n- 基本RDD“动作”运算\n    - **count() 返回数据集中的元素个数**\n    - **collect() 以数组的形式返回数据集中的所有元素**\n    - **first() 返回数据集中的第一个元素**\n    - **take(n) 以数组的形式返回数据集中的前n个元素**\n    - **reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素**\n    - **foreach(func) 将数据集中的每个元素传递到函数func中运行**\n    - takeOrdered（排序后取前N条数据）\n- Key-Value形式 RDD“转换”运算\n    - filter（过滤符合条件的数据）\n    - mapValues（对value值进行转换）\n    - sortByKey（根据key值进行排序）\n    - reduceByKey（合并相同key值的数据）\n    - join（内连接两个KDD）\n    - leftOuterJoin（左外连接两个KDD）\n    - rightOuterJoin（右外连接两个RDD）\n    - subtractByKey（相当于key值得差集运算）\n- Key-Value形式 RDD“动作”运算\n    - first（取第一条数据）\n    - take（取前几条数据）\n    - countByKey（根据key值分组统计）\n    - lookup（根据key值查找value值）\n- RDD持久化\n    - persist用于对RDD进行持久化\n    - unpersist取消RDD的持久化，注意持久化的存储等级\n\n\n## 读text文本数据(需要自己解析)\n\nspark 1.6.1\n\n```scala\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nsparkContext = new SparkContext(sparkConf)\nsparkContext.textFile(\"text_path\").map(...)\n```\n\n读取文本数据后，使用map进行自定义的解析\n\nspark 2.1.0\n\n```scala\nval warehouseLocation = \"spark-warehouse\"\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"Spark Hive Example\")\n      .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n      .enableHiveSupport()\n      .getOrCreate()\nsparkSession.read.textFile(\"text_path\").map(...)\n```\n\n## 读Hive表数据\n\nspark 1.6.1\n\n```scala\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nsparkContext = new SparkContext(sparkConf)\nhiveContext = new HiveContext(sparkContext)\nhiveContext.sql(\"select * from table where condition\").map(...)\n```\n\n\nspark 2.1.0\n\n```scala\nval warehouseLocation = \"spark-warehouse\"\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"Spark Hive Example\")\n      .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n      .enableHiveSupport()\n      .getOrCreate()\nsparkSession.sql(\"select * from table where condition\").map(...)\n```\n\n## 提交到集群中运行\n\npyspark\n\n```bash\n#!/usr/bin/env bash\nspark_submit=${spark_bin}/spark-submit\nspark_master=yarn-client\n\ntime \\\n$spark_submit \\\n\t--queue whichQueue \\\n\t--master $spark_master \\\n\t--driver-memory 10g \\\n\t--executor-memory 40g \\\n\t--num-executors 24 \\\n\t--executor-cores 8 \\\n\t--jars SomeThirdPartyJarFile.jar \\\n\t--conf spark.yarn.executor.memoryOverhead=600 \\\n\tyour_script.py params\n```\n\nscala 2.11 \n\nspark 2.1.0\n\n```bash\n#!/usr/bin/env bash\n\nspark_submit=${spark_bin}/spark-submit\njar_file=packageAccordingMavenOrSbt.jar\n\ntime \\\n$spark_submit \\\n    --class \"package.className\" \\\n    --master yarn \\\n    --deploy-mode client \\\n    --num-executors 8 \\\n    --executor-cores 3 \\\n    --driver-memory 8g \\\n    --executor-memory 24g \\\n    --queue whichQueue \\\n    --conf spark.classpath.userClassPathFirst=true \\\n    ${jar_file} \\\n    --param ${param}\n```\n\nscala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析\n\n## Spark的一些技巧\n\n> [wwcom614/Spark](https://github.com/wwcom614/Spark)\n","source":"_posts/2019-03-07-Spark基础.md","raw":"---\ntitle: Spark基础\nmathjax: true\ncopyright: true\ndate: 2019-03-07 20:39:32\ncategories: 大数据\ntags:\n- 语言基础\n---\n\n## 多说几句\n\n写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。\n\nSpark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录，\n大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。\n\n现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑，\n网络上的主要学习来源是[子雨大数据之Spark入门教程](http://dblab.xmu.edu.cn/blog/spark/)，这个教程真的只是入门。\n\n以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述\n\n## RDD编程\n\n- 基本RDD“转换”运算\n    - **map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集**\n    - **filter(func)：筛选出满足函数func的元素，并返回一个新的数据集**\n    - **flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果**\n    - distinct（去重运算）\n    - randomSplit（根据指定的比例随机分为N各RDD）\n    - **reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合**\n    - **groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集**\n    - union（两个RDD取并集）\n    - intersection（两个RDD取交集）\n    - subtract（两个RDD取差集）\n    - cartesian（两个RDD进行笛卡尔积运算）\n- 基本RDD“动作”运算\n    - **count() 返回数据集中的元素个数**\n    - **collect() 以数组的形式返回数据集中的所有元素**\n    - **first() 返回数据集中的第一个元素**\n    - **take(n) 以数组的形式返回数据集中的前n个元素**\n    - **reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素**\n    - **foreach(func) 将数据集中的每个元素传递到函数func中运行**\n    - takeOrdered（排序后取前N条数据）\n- Key-Value形式 RDD“转换”运算\n    - filter（过滤符合条件的数据）\n    - mapValues（对value值进行转换）\n    - sortByKey（根据key值进行排序）\n    - reduceByKey（合并相同key值的数据）\n    - join（内连接两个KDD）\n    - leftOuterJoin（左外连接两个KDD）\n    - rightOuterJoin（右外连接两个RDD）\n    - subtractByKey（相当于key值得差集运算）\n- Key-Value形式 RDD“动作”运算\n    - first（取第一条数据）\n    - take（取前几条数据）\n    - countByKey（根据key值分组统计）\n    - lookup（根据key值查找value值）\n- RDD持久化\n    - persist用于对RDD进行持久化\n    - unpersist取消RDD的持久化，注意持久化的存储等级\n\n\n## 读text文本数据(需要自己解析)\n\nspark 1.6.1\n\n```scala\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nsparkContext = new SparkContext(sparkConf)\nsparkContext.textFile(\"text_path\").map(...)\n```\n\n读取文本数据后，使用map进行自定义的解析\n\nspark 2.1.0\n\n```scala\nval warehouseLocation = \"spark-warehouse\"\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"Spark Hive Example\")\n      .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n      .enableHiveSupport()\n      .getOrCreate()\nsparkSession.read.textFile(\"text_path\").map(...)\n```\n\n## 读Hive表数据\n\nspark 1.6.1\n\n```scala\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nsparkContext = new SparkContext(sparkConf)\nhiveContext = new HiveContext(sparkContext)\nhiveContext.sql(\"select * from table where condition\").map(...)\n```\n\n\nspark 2.1.0\n\n```scala\nval warehouseLocation = \"spark-warehouse\"\nval sparkConf = new SparkConf()\nsparkConf.setAppName(\"...\")\nsparkConf.set(\"key\", \"value\")\nval sparkSession = SparkSession\n      .builder()\n      .appName(\"Spark Hive Example\")\n      .config(\"spark.sql.warehouse.dir\", warehouseLocation)\n      .enableHiveSupport()\n      .getOrCreate()\nsparkSession.sql(\"select * from table where condition\").map(...)\n```\n\n## 提交到集群中运行\n\npyspark\n\n```bash\n#!/usr/bin/env bash\nspark_submit=${spark_bin}/spark-submit\nspark_master=yarn-client\n\ntime \\\n$spark_submit \\\n\t--queue whichQueue \\\n\t--master $spark_master \\\n\t--driver-memory 10g \\\n\t--executor-memory 40g \\\n\t--num-executors 24 \\\n\t--executor-cores 8 \\\n\t--jars SomeThirdPartyJarFile.jar \\\n\t--conf spark.yarn.executor.memoryOverhead=600 \\\n\tyour_script.py params\n```\n\nscala 2.11 \n\nspark 2.1.0\n\n```bash\n#!/usr/bin/env bash\n\nspark_submit=${spark_bin}/spark-submit\njar_file=packageAccordingMavenOrSbt.jar\n\ntime \\\n$spark_submit \\\n    --class \"package.className\" \\\n    --master yarn \\\n    --deploy-mode client \\\n    --num-executors 8 \\\n    --executor-cores 3 \\\n    --driver-memory 8g \\\n    --executor-memory 24g \\\n    --queue whichQueue \\\n    --conf spark.classpath.userClassPathFirst=true \\\n    ${jar_file} \\\n    --param ${param}\n```\n\nscala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析\n\n## Spark的一些技巧\n\n> [wwcom614/Spark](https://github.com/wwcom614/Spark)\n","slug":"Spark基础","published":1,"updated":"2019-08-19T09:33:45.932Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnxb004h2qwp0dscmuch","content":"<h2 id=\"多说几句\"><a href=\"#多说几句\" class=\"headerlink\" title=\"多说几句\"></a>多说几句</h2><p>写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。</p><p>Spark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录，\n大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。</p><p>现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑，\n网络上的主要学习来源是<a href=\"http://dblab.xmu.edu.cn/blog/spark/\" target=\"_blank\" rel=\"noopener\">子雨大数据之Spark入门教程</a>，这个教程真的只是入门。</p><p>以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述</p><h2 id=\"RDD编程\"><a href=\"#RDD编程\" class=\"headerlink\" title=\"RDD编程\"></a>RDD编程</h2><ul>\n<li>基本RDD“转换”运算<ul>\n<li><strong>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</strong></li>\n<li><strong>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</strong></li>\n<li><strong>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</strong></li>\n<li>distinct（去重运算）</li>\n<li>randomSplit（根据指定的比例随机分为N各RDD）</li>\n<li><strong>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合</strong></li>\n<li><strong>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</strong></li>\n<li>union（两个RDD取并集）</li>\n<li>intersection（两个RDD取交集）</li>\n<li>subtract（两个RDD取差集）</li>\n<li>cartesian（两个RDD进行笛卡尔积运算）</li>\n</ul>\n</li>\n<li>基本RDD“动作”运算<ul>\n<li><strong>count() 返回数据集中的元素个数</strong></li>\n<li><strong>collect() 以数组的形式返回数据集中的所有元素</strong></li>\n<li><strong>first() 返回数据集中的第一个元素</strong></li>\n<li><strong>take(n) 以数组的形式返回数据集中的前n个元素</strong></li>\n<li><strong>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</strong></li>\n<li><strong>foreach(func) 将数据集中的每个元素传递到函数func中运行</strong></li>\n<li>takeOrdered（排序后取前N条数据）</li>\n</ul>\n</li>\n<li>Key-Value形式 RDD“转换”运算<ul>\n<li>filter（过滤符合条件的数据）</li>\n<li>mapValues（对value值进行转换）</li>\n<li>sortByKey（根据key值进行排序）</li>\n<li>reduceByKey（合并相同key值的数据）</li>\n<li>join（内连接两个KDD）</li>\n<li>leftOuterJoin（左外连接两个KDD）</li>\n<li>rightOuterJoin（右外连接两个RDD）</li>\n<li>subtractByKey（相当于key值得差集运算）</li>\n</ul>\n</li>\n<li>Key-Value形式 RDD“动作”运算<ul>\n<li>first（取第一条数据）</li>\n<li>take（取前几条数据）</li>\n<li>countByKey（根据key值分组统计）</li>\n<li>lookup（根据key值查找value值）</li>\n</ul>\n</li>\n<li>RDD持久化<ul>\n<li>persist用于对RDD进行持久化</li>\n<li>unpersist取消RDD的持久化，注意持久化的存储等级</li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n\n\n\n\n<h2 id=\"读text文本数据-需要自己解析\"><a href=\"#读text文本数据-需要自己解析\" class=\"headerlink\" title=\"读text文本数据(需要自己解析)\"></a>读text文本数据(需要自己解析)</h2><p>spark 1.6.1</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\">sparkContext = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">sparkContext.textFile(<span class=\"string\">\"text_path\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<p>读取文本数据后，使用map进行自定义的解析</p>\n<p>spark 2.1.0</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> warehouseLocation = <span class=\"string\">\"spark-warehouse\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkSession = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .appName(<span class=\"string\">\"Spark Hive Example\"</span>)</span><br><span class=\"line\">      .config(<span class=\"string\">\"spark.sql.warehouse.dir\"</span>, warehouseLocation)</span><br><span class=\"line\">      .enableHiveSupport()</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">sparkSession.read.textFile(<span class=\"string\">\"text_path\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"读Hive表数据\"><a href=\"#读Hive表数据\" class=\"headerlink\" title=\"读Hive表数据\"></a>读Hive表数据</h2><p>spark 1.6.1</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\">sparkContext = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">hiveContext = <span class=\"keyword\">new</span> <span class=\"type\">HiveContext</span>(sparkContext)</span><br><span class=\"line\">hiveContext.sql(<span class=\"string\">\"select * from table where condition\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<p>spark 2.1.0</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> warehouseLocation = <span class=\"string\">\"spark-warehouse\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkSession = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .appName(<span class=\"string\">\"Spark Hive Example\"</span>)</span><br><span class=\"line\">      .config(<span class=\"string\">\"spark.sql.warehouse.dir\"</span>, warehouseLocation)</span><br><span class=\"line\">      .enableHiveSupport()</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">sparkSession.sql(<span class=\"string\">\"select * from table where condition\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"提交到集群中运行\"><a href=\"#提交到集群中运行\" class=\"headerlink\" title=\"提交到集群中运行\"></a>提交到集群中运行</h2><p>pyspark</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\">spark_submit=<span class=\"variable\">$&#123;spark_bin&#125;</span>/spark-submit</span><br><span class=\"line\">spark_master=yarn-client</span><br><span class=\"line\"></span><br><span class=\"line\">time \\</span><br><span class=\"line\"><span class=\"variable\">$spark_submit</span> \\</span><br><span class=\"line\">\t--queue whichQueue \\</span><br><span class=\"line\">\t--master <span class=\"variable\">$spark_master</span> \\</span><br><span class=\"line\">\t--driver-memory 10g \\</span><br><span class=\"line\">\t--executor-memory 40g \\</span><br><span class=\"line\">\t--num-executors 24 \\</span><br><span class=\"line\">\t--executor-cores 8 \\</span><br><span class=\"line\">\t--jars SomeThirdPartyJarFile.jar \\</span><br><span class=\"line\">\t--conf spark.yarn.executor.memoryOverhead=600 \\</span><br><span class=\"line\">\tyour_script.py params</span><br></pre></td></tr></table></figure>\n<p>scala 2.11 </p>\n<p>spark 2.1.0</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">spark_submit=<span class=\"variable\">$&#123;spark_bin&#125;</span>/spark-submit</span><br><span class=\"line\">jar_file=packageAccordingMavenOrSbt.jar</span><br><span class=\"line\"></span><br><span class=\"line\">time \\</span><br><span class=\"line\"><span class=\"variable\">$spark_submit</span> \\</span><br><span class=\"line\">    --class <span class=\"string\">\"package.className\"</span> \\</span><br><span class=\"line\">    --master yarn \\</span><br><span class=\"line\">    --deploy-mode client \\</span><br><span class=\"line\">    --num-executors 8 \\</span><br><span class=\"line\">    --executor-cores 3 \\</span><br><span class=\"line\">    --driver-memory 8g \\</span><br><span class=\"line\">    --executor-memory 24g \\</span><br><span class=\"line\">    --queue whichQueue \\</span><br><span class=\"line\">    --conf spark.classpath.userClassPathFirst=<span class=\"literal\">true</span> \\</span><br><span class=\"line\">    <span class=\"variable\">$&#123;jar_file&#125;</span> \\</span><br><span class=\"line\">    --param <span class=\"variable\">$&#123;param&#125;</span></span><br></pre></td></tr></table></figure>\n<p>scala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析</p>\n<h2 id=\"Spark的一些技巧\"><a href=\"#Spark的一些技巧\" class=\"headerlink\" title=\"Spark的一些技巧\"></a>Spark的一些技巧</h2><blockquote>\n<p><a href=\"https://github.com/wwcom614/Spark\" target=\"_blank\" rel=\"noopener\">wwcom614/Spark</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"多说几句\"><a href=\"#多说几句\" class=\"headerlink\" title=\"多说几句\"></a>多说几句</h2><p>写这个post只是为了自己有所遗忘的时候，方便快速回忆上手。</p><p>Spark现在提供很多的版本：Java、Python、R、Scala，本文主要针对Python和Scala版本的进行记录，\n大概先从公共的一些操作方法开始，之后记一下spark-submit是怎么用的，以及工程上的一些东西。</p><p>现在网上有很多Spark的教程，本文 = 我学习网上的资源 + 自己的理解 + 自己遇到的坑，\n网络上的主要学习来源是<a href=\"http://dblab.xmu.edu.cn/blog/spark/\" target=\"_blank\" rel=\"noopener\">子雨大数据之Spark入门教程</a>，这个教程真的只是入门。</p><p>以Spark 2.1.0，Python2.7，Scala 2.11 版本进行描述</p><h2 id=\"RDD编程\"><a href=\"#RDD编程\" class=\"headerlink\" title=\"RDD编程\"></a>RDD编程</h2><ul>\n<li>基本RDD“转换”运算<ul>\n<li><strong>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</strong></li>\n<li><strong>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</strong></li>\n<li><strong>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</strong></li>\n<li>distinct（去重运算）</li>\n<li>randomSplit（根据指定的比例随机分为N各RDD）</li>\n<li><strong>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合</strong></li>\n<li><strong>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</strong></li>\n<li>union（两个RDD取并集）</li>\n<li>intersection（两个RDD取交集）</li>\n<li>subtract（两个RDD取差集）</li>\n<li>cartesian（两个RDD进行笛卡尔积运算）</li>\n</ul>\n</li>\n<li>基本RDD“动作”运算<ul>\n<li><strong>count() 返回数据集中的元素个数</strong></li>\n<li><strong>collect() 以数组的形式返回数据集中的所有元素</strong></li>\n<li><strong>first() 返回数据集中的第一个元素</strong></li>\n<li><strong>take(n) 以数组的形式返回数据集中的前n个元素</strong></li>\n<li><strong>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</strong></li>\n<li><strong>foreach(func) 将数据集中的每个元素传递到函数func中运行</strong></li>\n<li>takeOrdered（排序后取前N条数据）</li>\n</ul>\n</li>\n<li>Key-Value形式 RDD“转换”运算<ul>\n<li>filter（过滤符合条件的数据）</li>\n<li>mapValues（对value值进行转换）</li>\n<li>sortByKey（根据key值进行排序）</li>\n<li>reduceByKey（合并相同key值的数据）</li>\n<li>join（内连接两个KDD）</li>\n<li>leftOuterJoin（左外连接两个KDD）</li>\n<li>rightOuterJoin（右外连接两个RDD）</li>\n<li>subtractByKey（相当于key值得差集运算）</li>\n</ul>\n</li>\n<li>Key-Value形式 RDD“动作”运算<ul>\n<li>first（取第一条数据）</li>\n<li>take（取前几条数据）</li>\n<li>countByKey（根据key值分组统计）</li>\n<li>lookup（根据key值查找value值）</li>\n</ul>\n</li>\n<li>RDD持久化<ul>\n<li>persist用于对RDD进行持久化</li>\n<li>unpersist取消RDD的持久化，注意持久化的存储等级</li>\n</ul>\n</li>\n</ul>","more":"\n\n\n\n\n<h2 id=\"读text文本数据-需要自己解析\"><a href=\"#读text文本数据-需要自己解析\" class=\"headerlink\" title=\"读text文本数据(需要自己解析)\"></a>读text文本数据(需要自己解析)</h2><p>spark 1.6.1</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\">sparkContext = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">sparkContext.textFile(<span class=\"string\">\"text_path\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<p>读取文本数据后，使用map进行自定义的解析</p>\n<p>spark 2.1.0</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> warehouseLocation = <span class=\"string\">\"spark-warehouse\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkSession = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .appName(<span class=\"string\">\"Spark Hive Example\"</span>)</span><br><span class=\"line\">      .config(<span class=\"string\">\"spark.sql.warehouse.dir\"</span>, warehouseLocation)</span><br><span class=\"line\">      .enableHiveSupport()</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">sparkSession.read.textFile(<span class=\"string\">\"text_path\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"读Hive表数据\"><a href=\"#读Hive表数据\" class=\"headerlink\" title=\"读Hive表数据\"></a>读Hive表数据</h2><p>spark 1.6.1</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\">sparkContext = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf)</span><br><span class=\"line\">hiveContext = <span class=\"keyword\">new</span> <span class=\"type\">HiveContext</span>(sparkContext)</span><br><span class=\"line\">hiveContext.sql(<span class=\"string\">\"select * from table where condition\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<p>spark 2.1.0</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">val</span> warehouseLocation = <span class=\"string\">\"spark-warehouse\"</span></span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>()</span><br><span class=\"line\">sparkConf.setAppName(<span class=\"string\">\"...\"</span>)</span><br><span class=\"line\">sparkConf.set(<span class=\"string\">\"key\"</span>, <span class=\"string\">\"value\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">val</span> sparkSession = <span class=\"type\">SparkSession</span></span><br><span class=\"line\">      .builder()</span><br><span class=\"line\">      .appName(<span class=\"string\">\"Spark Hive Example\"</span>)</span><br><span class=\"line\">      .config(<span class=\"string\">\"spark.sql.warehouse.dir\"</span>, warehouseLocation)</span><br><span class=\"line\">      .enableHiveSupport()</span><br><span class=\"line\">      .getOrCreate()</span><br><span class=\"line\">sparkSession.sql(<span class=\"string\">\"select * from table where condition\"</span>).map(...)</span><br></pre></td></tr></table></figure>\n<h2 id=\"提交到集群中运行\"><a href=\"#提交到集群中运行\" class=\"headerlink\" title=\"提交到集群中运行\"></a>提交到集群中运行</h2><p>pyspark</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\">spark_submit=<span class=\"variable\">$&#123;spark_bin&#125;</span>/spark-submit</span><br><span class=\"line\">spark_master=yarn-client</span><br><span class=\"line\"></span><br><span class=\"line\">time \\</span><br><span class=\"line\"><span class=\"variable\">$spark_submit</span> \\</span><br><span class=\"line\">\t--queue whichQueue \\</span><br><span class=\"line\">\t--master <span class=\"variable\">$spark_master</span> \\</span><br><span class=\"line\">\t--driver-memory 10g \\</span><br><span class=\"line\">\t--executor-memory 40g \\</span><br><span class=\"line\">\t--num-executors 24 \\</span><br><span class=\"line\">\t--executor-cores 8 \\</span><br><span class=\"line\">\t--jars SomeThirdPartyJarFile.jar \\</span><br><span class=\"line\">\t--conf spark.yarn.executor.memoryOverhead=600 \\</span><br><span class=\"line\">\tyour_script.py params</span><br></pre></td></tr></table></figure>\n<p>scala 2.11 </p>\n<p>spark 2.1.0</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#!/usr/bin/env bash</span></span><br><span class=\"line\"></span><br><span class=\"line\">spark_submit=<span class=\"variable\">$&#123;spark_bin&#125;</span>/spark-submit</span><br><span class=\"line\">jar_file=packageAccordingMavenOrSbt.jar</span><br><span class=\"line\"></span><br><span class=\"line\">time \\</span><br><span class=\"line\"><span class=\"variable\">$spark_submit</span> \\</span><br><span class=\"line\">    --class <span class=\"string\">\"package.className\"</span> \\</span><br><span class=\"line\">    --master yarn \\</span><br><span class=\"line\">    --deploy-mode client \\</span><br><span class=\"line\">    --num-executors 8 \\</span><br><span class=\"line\">    --executor-cores 3 \\</span><br><span class=\"line\">    --driver-memory 8g \\</span><br><span class=\"line\">    --executor-memory 24g \\</span><br><span class=\"line\">    --queue whichQueue \\</span><br><span class=\"line\">    --conf spark.classpath.userClassPathFirst=<span class=\"literal\">true</span> \\</span><br><span class=\"line\">    <span class=\"variable\">$&#123;jar_file&#125;</span> \\</span><br><span class=\"line\">    --param <span class=\"variable\">$&#123;param&#125;</span></span><br></pre></td></tr></table></figure>\n<p>scala建议使用：Idea开发 + Maven依赖打包 + Scopt参数解析</p>\n<h2 id=\"Spark的一些技巧\"><a href=\"#Spark的一些技巧\" class=\"headerlink\" title=\"Spark的一些技巧\"></a>Spark的一些技巧</h2><blockquote>\n<p><a href=\"https://github.com/wwcom614/Spark\" target=\"_blank\" rel=\"noopener\">wwcom614/Spark</a></p>\n</blockquote>\n"},{"title":"python常用实例","mathjax":true,"copyright":true,"date":"2019-03-13T08:19:11.000Z","_content":"\n### 求两个时间之间的差\n\n```python\nfrom datetime import datetime as dt\ninsert = dt.strptime('2018-03-22 10:00:00', \"%Y-%m-%d %H:%M:%S\")\ncurrent = dt.strptime(str(dt.now().strftime('%Y-%m-%d')), '%Y-%m-%d')\nprint (current-insert).days             # 天\nprint (current-insert).seconds          # 秒\nprint (current-insert).microseconds     # 毫秒\n```","source":"_posts/2019-03-13-python常用实例.md","raw":"---\ntitle: python常用实例\ntags:\n  - 常用实例\nmathjax: true\ncopyright: true\ndate: 2019-03-13 16:19:11\ncategories: 小工具\n---\n\n### 求两个时间之间的差\n\n```python\nfrom datetime import datetime as dt\ninsert = dt.strptime('2018-03-22 10:00:00', \"%Y-%m-%d %H:%M:%S\")\ncurrent = dt.strptime(str(dt.now().strftime('%Y-%m-%d')), '%Y-%m-%d')\nprint (current-insert).days             # 天\nprint (current-insert).seconds          # 秒\nprint (current-insert).microseconds     # 毫秒\n```","slug":"python常用实例","published":1,"updated":"2019-08-17T09:42:23.335Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnxc004k2qwpc6kpwub5","content":"<h3 id=\"求两个时间之间的差\"><a href=\"#求两个时间之间的差\" class=\"headerlink\" title=\"求两个时间之间的差\"></a>求两个时间之间的差</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime <span class=\"keyword\">as</span> dt</span><br><span class=\"line\">insert = dt.strptime(<span class=\"string\">'2018-03-22 10:00:00'</span>, <span class=\"string\">\"%Y-%m-%d %H:%M:%S\"</span>)</span><br><span class=\"line\">current = dt.strptime(str(dt.now().strftime(<span class=\"string\">'%Y-%m-%d'</span>)), <span class=\"string\">'%Y-%m-%d'</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).days             <span class=\"comment\"># 天</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).seconds          <span class=\"comment\"># 秒</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).microseconds     <span class=\"comment\"># 毫秒</span></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"求两个时间之间的差\"><a href=\"#求两个时间之间的差\" class=\"headerlink\" title=\"求两个时间之间的差\"></a>求两个时间之间的差</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datetime <span class=\"keyword\">import</span> datetime <span class=\"keyword\">as</span> dt</span><br><span class=\"line\">insert = dt.strptime(<span class=\"string\">'2018-03-22 10:00:00'</span>, <span class=\"string\">\"%Y-%m-%d %H:%M:%S\"</span>)</span><br><span class=\"line\">current = dt.strptime(str(dt.now().strftime(<span class=\"string\">'%Y-%m-%d'</span>)), <span class=\"string\">'%Y-%m-%d'</span>)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).days             <span class=\"comment\"># 天</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).seconds          <span class=\"comment\"># 秒</span></span><br><span class=\"line\"><span class=\"keyword\">print</span> (current-insert).microseconds     <span class=\"comment\"># 毫秒</span></span><br></pre></td></tr></table></figure>"},{"title":"Vim Tutorial","mathjax":true,"copyright":false,"date":"2019-03-27T06:10:09.000Z","_content":"\n**Vim自带的Tutorial:在终端下输入`vimtutor`即可打开vim自带的教程。**\n\n> cite: https://irvingzhang0512.github.io/2018/06/09/vim-tutor/\n\n\n## 1. 总结\n### 1.1. 光标移动\n\n- 普通移动：`h j k l`。\n- 到下一个单词开头：`w`，可以添加数字`2w`。\n- 到下一个单词结尾：`e`，可以添加数字`2e`。\n- 移动到当前行开头：`0`。\n- 移动到当前行结尾：`$`。\n- 移动光标到文件底部：`G`。\n- 移动光标到文件顶部：`gg`。\n- 移动到指定的行：`#G`，其中`#`表示数字。\n\n### 1.2. 删除\n\n- 删除一个字符：命令模式，`x`。\n- 删除从当前位置，当下一个单词开头的所有字符：`dw`。\n- 删除当前行从当前位置开始，到结尾的字符：`d$`。\n- 删除当前位置，到当前单词结尾的字符：`de`。\n- 可以在dw或de中间添加数字，如`d2w`，`d5e`。\n- 删除整行：`dd`，并将数据保存到buffer。\n  - 删除多行：在`#dd`前添加数字，`#`表示数字。\n\n### 1.3. 查询与替换\n\n- 查询：在命令模式中输入`/`以及需要查询的内容，回车。\n  - 输入`n`查询下一个匹配。\n  - 输入`N`查询上一个匹配。\n  - 如果要反向查询，则使用`?`而不是`/`。\n- 选项：\n  - 查询时不区分大小写：`:set ic`, `:set noic`。\n  - 查询结果高亮：`:set hls`，`:nohlsearch`。\n- 使用`%`查询其对应的字符，如`()`, `{}`, `[]`。\n- 替换光标指向的一个字符：`r`（只能替换一个字符）。\n  - 在命令模式输入`r`，再输入需要替换的字符，之后就再次进入命令模式。\n- 替换多个字符：`R`。\n- 替换字符串：\n  - `:s/old/new：`只替换一个记录。\n  - `:s/old/new/g：`替换当前行所有匹配记录。\n  - `:#,#s/old/new/g：`替换指定行中所有匹配的记录。\n  - `:%s/old/new/g：`替换文件中所有匹配记录。\n  - `:%s/old/new/gc：`替换文件中所有匹配记录，在替换前会询问。\n\n### 1.4. 复制粘贴\n\n- 删除的内容是保存在缓存中，可以通过`p`黏贴。\n- 复制：使用`v`，并移动光标，选择需要复制的文本，使用`y`进行复制。\n- 粘贴：`p`。\n- 获取其他文本中的内容，并复制到本地：`:r TEST`。\n  - 也可以复制命令结果，如`:r !ls`。\n\n### 1.5. 添加操作\n\n- 插入字符（进入编辑模式）：`i`。\n- 追加字符\n  - 在光标指向的位置后添加内容，进入编辑模式：`a`。\n  - 在本行末尾添加内容，进入编辑模式：`A`。\n- change operator：删除指定位置的数据，并进入编辑模式。\n  - `ce`：删除光标到当前单词末尾的数据，并进入编辑模式。\n  - `c`与`d`的用法类似，也可以使用`e w $`以及`数字`。\n\n### 1.6. 其他操作\n\n- 退出vim：命令模式，输入`:q!`（退出不保存），输入`:wq`（保存并退出）。\n- 查看当前处于文件中的位置：`CTRL-G`。\n- 撤销：`u`撤销一个操作，`U`撤销当前行的之前的操作。\n  - 撤销的撤销：`CTRL-R`。\n- 返回到之前光标的位置：`CTRL-O`，其反操作是`CTRL-I`。\n\n## 2. 分课程总结\n\n### 2.1. 第一课\n\n- 移动光标：命令模式，`h j k l`。\n- 退出vim：命令模式，输入`:q!`（退出不保存），输入`:wq`（保存并退出）。\n- 删除一个字符：命令模式，`x`。\n- 插入字符（进入编辑模式）, `i`。\n- 追加字符\n  - 在光标指向的位置后添加内容，进入编辑模式：`a`。\n  - 在本行末尾添加内容，进入编辑模式：`A`。\n- 执行shell命令：`:!ls`。\n- 保存到某文件：`:w TEST`。\n\n### 2.2. 第二课\n\n- 删除从当前位置，当下一个单词开头的所有字符：`dw`。\n- 删除当前行从当前位置开始，到结尾的字符：`d$`。\n- 删除当前位置，到当前单词结尾的字符：`de`。\n- 移动光标：\n  - 到下一个单词开头：`w`。\n  - 到下一个单词结尾：`e`。\n  - 移动到当前行开头：`0`。\n  - 可以在`w`或`e`前，添加数字。\n  - 可以在`dw`或`de`中间添加数字，如`d2w`，`d5e`。\n- 删除整行：`dd`，并将数据保存到buffer。\n  - 删除多行：在`dd`前添加数字。\n- 撤销：`u`撤销一个操作，`U`撤销当前行的之前的操作。\n  - 撤销的撤销：`CTRL-R`。\n\n### 2.3. 第三课\n\n- 删除的内容是保存在缓存中，可以通过`p`黏贴。\n- 替换光标指向的一个字符：`r`（只能替换一个字符）。\n  - 在命令模式输入`r`，再输入需要替换的字符，之后就再次进入命令模式。\n- change operator：删除指定位置的数据，并进入编辑模式。\n  - `ce`：删除光标到当前单词末尾的数据，并进入编辑模式。\n  - `c`与`d`的用法类似，也可以使用`e w $`以及数字。\n\n### 2.4. 第四课\n\n- 查看当前处于文件中的位置：`CTRL-G`。\n- 移动光标到文件底部：`G`。\n- 移动光标到文件顶部：`gg`。\n- 移动到指定的行：`#G`，其中`#`表示数字。\n- 查询：在命令模式中输入`/`以及需要查询的内容。\n  - 输入`n`查询下一个匹配。\n  - 输入`N`查询上一个匹配。\n  - 如果要反向查询，则使用`?`而不是`/`。\n- 返回到之前光标的位置：`CTRL-O`，其反操作是`CTRL-I`。\n- 使用`%`查询其对应的字符，如`()`, `{}`, `[]`。\n- 替换字符串：\n  - `:s/old/new`：只替换一个记录。\n  - `:s/old/new/g`：替换当前行所有匹配记录。\n  - `:#,#s/old/new/g`：替换指定行中所有匹配的记录。\n  - `:%s/old/new/g`：替换文件中所有匹配记录。\n  - `:%s/old/new/gc`：替换文件中所有匹配记录，在替换前会询问。\n\n### 2.5. 第五课\n\n- 执行shell命令：`:!ls`。\n- 保存到某文件：`:w TEST`。\n- 选择文本：使用`v`，之后移动光标，就可以选择一段文本。\n  - 之后若使用`:w TEST`命令，可以将选中的文本保存到指定文件中\n- 获取其他文本中的内容，并复制到本地：`:r TEST`。\n  - 也可以复制命令结果，如`:r !ls`。\n\n### 2.6. 第六课\n\n- 添加行：`o`光标下添加行，`O`光标上添加行。\n- 替换多个字符：`R`。\n- 替换一个字符：`r`。\n- 复制黏贴：\n  - 复制：使用`v`，并移动光标，选择需要复制的文本，使用`y`进行复制。\n  - 粘贴：`p`。\n- 选项：\n  - 查询时不区分大小写：`:set ic`, `:set noic`。\n  - 查询结果高亮：`:set hls`，`:nohlsearch`\n\n","source":"_posts/2019-03-27-Vim-Tutorial.md","raw":"---\ntitle: Vim Tutorial\nmathjax: true\ncopyright: false\ndate: 2019-03-27 14:10:09\ncategories: 小工具\n---\n\n**Vim自带的Tutorial:在终端下输入`vimtutor`即可打开vim自带的教程。**\n\n> cite: https://irvingzhang0512.github.io/2018/06/09/vim-tutor/\n\n\n## 1. 总结\n### 1.1. 光标移动\n\n- 普通移动：`h j k l`。\n- 到下一个单词开头：`w`，可以添加数字`2w`。\n- 到下一个单词结尾：`e`，可以添加数字`2e`。\n- 移动到当前行开头：`0`。\n- 移动到当前行结尾：`$`。\n- 移动光标到文件底部：`G`。\n- 移动光标到文件顶部：`gg`。\n- 移动到指定的行：`#G`，其中`#`表示数字。\n\n### 1.2. 删除\n\n- 删除一个字符：命令模式，`x`。\n- 删除从当前位置，当下一个单词开头的所有字符：`dw`。\n- 删除当前行从当前位置开始，到结尾的字符：`d$`。\n- 删除当前位置，到当前单词结尾的字符：`de`。\n- 可以在dw或de中间添加数字，如`d2w`，`d5e`。\n- 删除整行：`dd`，并将数据保存到buffer。\n  - 删除多行：在`#dd`前添加数字，`#`表示数字。\n\n### 1.3. 查询与替换\n\n- 查询：在命令模式中输入`/`以及需要查询的内容，回车。\n  - 输入`n`查询下一个匹配。\n  - 输入`N`查询上一个匹配。\n  - 如果要反向查询，则使用`?`而不是`/`。\n- 选项：\n  - 查询时不区分大小写：`:set ic`, `:set noic`。\n  - 查询结果高亮：`:set hls`，`:nohlsearch`。\n- 使用`%`查询其对应的字符，如`()`, `{}`, `[]`。\n- 替换光标指向的一个字符：`r`（只能替换一个字符）。\n  - 在命令模式输入`r`，再输入需要替换的字符，之后就再次进入命令模式。\n- 替换多个字符：`R`。\n- 替换字符串：\n  - `:s/old/new：`只替换一个记录。\n  - `:s/old/new/g：`替换当前行所有匹配记录。\n  - `:#,#s/old/new/g：`替换指定行中所有匹配的记录。\n  - `:%s/old/new/g：`替换文件中所有匹配记录。\n  - `:%s/old/new/gc：`替换文件中所有匹配记录，在替换前会询问。\n\n### 1.4. 复制粘贴\n\n- 删除的内容是保存在缓存中，可以通过`p`黏贴。\n- 复制：使用`v`，并移动光标，选择需要复制的文本，使用`y`进行复制。\n- 粘贴：`p`。\n- 获取其他文本中的内容，并复制到本地：`:r TEST`。\n  - 也可以复制命令结果，如`:r !ls`。\n\n### 1.5. 添加操作\n\n- 插入字符（进入编辑模式）：`i`。\n- 追加字符\n  - 在光标指向的位置后添加内容，进入编辑模式：`a`。\n  - 在本行末尾添加内容，进入编辑模式：`A`。\n- change operator：删除指定位置的数据，并进入编辑模式。\n  - `ce`：删除光标到当前单词末尾的数据，并进入编辑模式。\n  - `c`与`d`的用法类似，也可以使用`e w $`以及`数字`。\n\n### 1.6. 其他操作\n\n- 退出vim：命令模式，输入`:q!`（退出不保存），输入`:wq`（保存并退出）。\n- 查看当前处于文件中的位置：`CTRL-G`。\n- 撤销：`u`撤销一个操作，`U`撤销当前行的之前的操作。\n  - 撤销的撤销：`CTRL-R`。\n- 返回到之前光标的位置：`CTRL-O`，其反操作是`CTRL-I`。\n\n## 2. 分课程总结\n\n### 2.1. 第一课\n\n- 移动光标：命令模式，`h j k l`。\n- 退出vim：命令模式，输入`:q!`（退出不保存），输入`:wq`（保存并退出）。\n- 删除一个字符：命令模式，`x`。\n- 插入字符（进入编辑模式）, `i`。\n- 追加字符\n  - 在光标指向的位置后添加内容，进入编辑模式：`a`。\n  - 在本行末尾添加内容，进入编辑模式：`A`。\n- 执行shell命令：`:!ls`。\n- 保存到某文件：`:w TEST`。\n\n### 2.2. 第二课\n\n- 删除从当前位置，当下一个单词开头的所有字符：`dw`。\n- 删除当前行从当前位置开始，到结尾的字符：`d$`。\n- 删除当前位置，到当前单词结尾的字符：`de`。\n- 移动光标：\n  - 到下一个单词开头：`w`。\n  - 到下一个单词结尾：`e`。\n  - 移动到当前行开头：`0`。\n  - 可以在`w`或`e`前，添加数字。\n  - 可以在`dw`或`de`中间添加数字，如`d2w`，`d5e`。\n- 删除整行：`dd`，并将数据保存到buffer。\n  - 删除多行：在`dd`前添加数字。\n- 撤销：`u`撤销一个操作，`U`撤销当前行的之前的操作。\n  - 撤销的撤销：`CTRL-R`。\n\n### 2.3. 第三课\n\n- 删除的内容是保存在缓存中，可以通过`p`黏贴。\n- 替换光标指向的一个字符：`r`（只能替换一个字符）。\n  - 在命令模式输入`r`，再输入需要替换的字符，之后就再次进入命令模式。\n- change operator：删除指定位置的数据，并进入编辑模式。\n  - `ce`：删除光标到当前单词末尾的数据，并进入编辑模式。\n  - `c`与`d`的用法类似，也可以使用`e w $`以及数字。\n\n### 2.4. 第四课\n\n- 查看当前处于文件中的位置：`CTRL-G`。\n- 移动光标到文件底部：`G`。\n- 移动光标到文件顶部：`gg`。\n- 移动到指定的行：`#G`，其中`#`表示数字。\n- 查询：在命令模式中输入`/`以及需要查询的内容。\n  - 输入`n`查询下一个匹配。\n  - 输入`N`查询上一个匹配。\n  - 如果要反向查询，则使用`?`而不是`/`。\n- 返回到之前光标的位置：`CTRL-O`，其反操作是`CTRL-I`。\n- 使用`%`查询其对应的字符，如`()`, `{}`, `[]`。\n- 替换字符串：\n  - `:s/old/new`：只替换一个记录。\n  - `:s/old/new/g`：替换当前行所有匹配记录。\n  - `:#,#s/old/new/g`：替换指定行中所有匹配的记录。\n  - `:%s/old/new/g`：替换文件中所有匹配记录。\n  - `:%s/old/new/gc`：替换文件中所有匹配记录，在替换前会询问。\n\n### 2.5. 第五课\n\n- 执行shell命令：`:!ls`。\n- 保存到某文件：`:w TEST`。\n- 选择文本：使用`v`，之后移动光标，就可以选择一段文本。\n  - 之后若使用`:w TEST`命令，可以将选中的文本保存到指定文件中\n- 获取其他文本中的内容，并复制到本地：`:r TEST`。\n  - 也可以复制命令结果，如`:r !ls`。\n\n### 2.6. 第六课\n\n- 添加行：`o`光标下添加行，`O`光标上添加行。\n- 替换多个字符：`R`。\n- 替换一个字符：`r`。\n- 复制黏贴：\n  - 复制：使用`v`，并移动光标，选择需要复制的文本，使用`y`进行复制。\n  - 粘贴：`p`。\n- 选项：\n  - 查询时不区分大小写：`:set ic`, `:set noic`。\n  - 查询结果高亮：`:set hls`，`:nohlsearch`\n\n","slug":"Vim-Tutorial","published":1,"updated":"2019-08-17T09:42:45.205Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wnxd004o2qwplymrrhev","content":"<p><strong>Vim自带的Tutorial:在终端下输入<code>vimtutor</code>即可打开vim自带的教程。</strong></p><blockquote>\n<p>cite: <a href=\"https://irvingzhang0512.github.io/2018/06/09/vim-tutor/\" target=\"_blank\" rel=\"noopener\">https://irvingzhang0512.github.io/2018/06/09/vim-tutor/</a></p>\n</blockquote><h2 id=\"1-总结\"><a href=\"#1-总结\" class=\"headerlink\" title=\"1. 总结\"></a>1. 总结</h2><h3 id=\"1-1-光标移动\"><a href=\"#1-1-光标移动\" class=\"headerlink\" title=\"1.1. 光标移动\"></a>1.1. 光标移动</h3><ul>\n<li>普通移动：<code>h j k l</code>。</li>\n<li>到下一个单词开头：<code>w</code>，可以添加数字<code>2w</code>。</li>\n<li>到下一个单词结尾：<code>e</code>，可以添加数字<code>2e</code>。</li>\n<li>移动到当前行开头：<code>0</code>。</li>\n<li>移动到当前行结尾：<code>$</code>。</li>\n<li>移动光标到文件底部：<code>G</code>。</li>\n<li>移动光标到文件顶部：<code>gg</code>。</li>\n<li>移动到指定的行：<code>#G</code>，其中<code>#</code>表示数字。</li>\n</ul><h3 id=\"1-2-删除\"><a href=\"#1-2-删除\" class=\"headerlink\" title=\"1.2. 删除\"></a>1.2. 删除</h3><ul>\n<li>删除一个字符：命令模式，<code>x</code>。</li>\n<li>删除从当前位置，当下一个单词开头的所有字符：<code>dw</code>。</li>\n<li>删除当前行从当前位置开始，到结尾的字符：<code>d$</code>。</li>\n<li>删除当前位置，到当前单词结尾的字符：<code>de</code>。</li>\n<li>可以在dw或de中间添加数字，如<code>d2w</code>，<code>d5e</code>。</li>\n<li>删除整行：<code>dd</code>，并将数据保存到buffer。<ul>\n<li>删除多行：在<code>#dd</code>前添加数字，<code>#</code>表示数字。</li>\n</ul>\n</li>\n</ul><a id=\"more\"></a>\n\n\n\n<h3 id=\"1-3-查询与替换\"><a href=\"#1-3-查询与替换\" class=\"headerlink\" title=\"1.3. 查询与替换\"></a>1.3. 查询与替换</h3><ul>\n<li>查询：在命令模式中输入<code>/</code>以及需要查询的内容，回车。<ul>\n<li>输入<code>n</code>查询下一个匹配。</li>\n<li>输入<code>N</code>查询上一个匹配。</li>\n<li>如果要反向查询，则使用<code>?</code>而不是<code>/</code>。</li>\n</ul>\n</li>\n<li>选项：<ul>\n<li>查询时不区分大小写：<code>:set ic</code>, <code>:set noic</code>。</li>\n<li>查询结果高亮：<code>:set hls</code>，<code>:nohlsearch</code>。</li>\n</ul>\n</li>\n<li>使用<code>%</code>查询其对应的字符，如<code>()</code>, <code>{}</code>, <code>[]</code>。</li>\n<li>替换光标指向的一个字符：<code>r</code>（只能替换一个字符）。<ul>\n<li>在命令模式输入<code>r</code>，再输入需要替换的字符，之后就再次进入命令模式。</li>\n</ul>\n</li>\n<li>替换多个字符：<code>R</code>。</li>\n<li>替换字符串：<ul>\n<li><code>:s/old/new：</code>只替换一个记录。</li>\n<li><code>:s/old/new/g：</code>替换当前行所有匹配记录。</li>\n<li><code>:#,#s/old/new/g：</code>替换指定行中所有匹配的记录。</li>\n<li><code>:%s/old/new/g：</code>替换文件中所有匹配记录。</li>\n<li><code>:%s/old/new/gc：</code>替换文件中所有匹配记录，在替换前会询问。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-4-复制粘贴\"><a href=\"#1-4-复制粘贴\" class=\"headerlink\" title=\"1.4. 复制粘贴\"></a>1.4. 复制粘贴</h3><ul>\n<li>删除的内容是保存在缓存中，可以通过<code>p</code>黏贴。</li>\n<li>复制：使用<code>v</code>，并移动光标，选择需要复制的文本，使用<code>y</code>进行复制。</li>\n<li>粘贴：<code>p</code>。</li>\n<li>获取其他文本中的内容，并复制到本地：<code>:r TEST</code>。<ul>\n<li>也可以复制命令结果，如<code>:r !ls</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-5-添加操作\"><a href=\"#1-5-添加操作\" class=\"headerlink\" title=\"1.5. 添加操作\"></a>1.5. 添加操作</h3><ul>\n<li>插入字符（进入编辑模式）：<code>i</code>。</li>\n<li>追加字符<ul>\n<li>在光标指向的位置后添加内容，进入编辑模式：<code>a</code>。</li>\n<li>在本行末尾添加内容，进入编辑模式：<code>A</code>。</li>\n</ul>\n</li>\n<li>change operator：删除指定位置的数据，并进入编辑模式。<ul>\n<li><code>ce</code>：删除光标到当前单词末尾的数据，并进入编辑模式。</li>\n<li><code>c</code>与<code>d</code>的用法类似，也可以使用<code>e w $</code>以及<code>数字</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-6-其他操作\"><a href=\"#1-6-其他操作\" class=\"headerlink\" title=\"1.6. 其他操作\"></a>1.6. 其他操作</h3><ul>\n<li>退出vim：命令模式，输入<code>:q!</code>（退出不保存），输入<code>:wq</code>（保存并退出）。</li>\n<li>查看当前处于文件中的位置：<code>CTRL-G</code>。</li>\n<li>撤销：<code>u</code>撤销一个操作，<code>U</code>撤销当前行的之前的操作。<ul>\n<li>撤销的撤销：<code>CTRL-R</code>。</li>\n</ul>\n</li>\n<li>返回到之前光标的位置：<code>CTRL-O</code>，其反操作是<code>CTRL-I</code>。</li>\n</ul>\n<h2 id=\"2-分课程总结\"><a href=\"#2-分课程总结\" class=\"headerlink\" title=\"2. 分课程总结\"></a>2. 分课程总结</h2><h3 id=\"2-1-第一课\"><a href=\"#2-1-第一课\" class=\"headerlink\" title=\"2.1. 第一课\"></a>2.1. 第一课</h3><ul>\n<li>移动光标：命令模式，<code>h j k l</code>。</li>\n<li>退出vim：命令模式，输入<code>:q!</code>（退出不保存），输入<code>:wq</code>（保存并退出）。</li>\n<li>删除一个字符：命令模式，<code>x</code>。</li>\n<li>插入字符（进入编辑模式）, <code>i</code>。</li>\n<li>追加字符<ul>\n<li>在光标指向的位置后添加内容，进入编辑模式：<code>a</code>。</li>\n<li>在本行末尾添加内容，进入编辑模式：<code>A</code>。</li>\n</ul>\n</li>\n<li>执行shell命令：<code>:!ls</code>。</li>\n<li>保存到某文件：<code>:w TEST</code>。</li>\n</ul>\n<h3 id=\"2-2-第二课\"><a href=\"#2-2-第二课\" class=\"headerlink\" title=\"2.2. 第二课\"></a>2.2. 第二课</h3><ul>\n<li>删除从当前位置，当下一个单词开头的所有字符：<code>dw</code>。</li>\n<li>删除当前行从当前位置开始，到结尾的字符：<code>d$</code>。</li>\n<li>删除当前位置，到当前单词结尾的字符：<code>de</code>。</li>\n<li>移动光标：<ul>\n<li>到下一个单词开头：<code>w</code>。</li>\n<li>到下一个单词结尾：<code>e</code>。</li>\n<li>移动到当前行开头：<code>0</code>。</li>\n<li>可以在<code>w</code>或<code>e</code>前，添加数字。</li>\n<li>可以在<code>dw</code>或<code>de</code>中间添加数字，如<code>d2w</code>，<code>d5e</code>。</li>\n</ul>\n</li>\n<li>删除整行：<code>dd</code>，并将数据保存到buffer。<ul>\n<li>删除多行：在<code>dd</code>前添加数字。</li>\n</ul>\n</li>\n<li>撤销：<code>u</code>撤销一个操作，<code>U</code>撤销当前行的之前的操作。<ul>\n<li>撤销的撤销：<code>CTRL-R</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-第三课\"><a href=\"#2-3-第三课\" class=\"headerlink\" title=\"2.3. 第三课\"></a>2.3. 第三课</h3><ul>\n<li>删除的内容是保存在缓存中，可以通过<code>p</code>黏贴。</li>\n<li>替换光标指向的一个字符：<code>r</code>（只能替换一个字符）。<ul>\n<li>在命令模式输入<code>r</code>，再输入需要替换的字符，之后就再次进入命令模式。</li>\n</ul>\n</li>\n<li>change operator：删除指定位置的数据，并进入编辑模式。<ul>\n<li><code>ce</code>：删除光标到当前单词末尾的数据，并进入编辑模式。</li>\n<li><code>c</code>与<code>d</code>的用法类似，也可以使用<code>e w $</code>以及数字。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-4-第四课\"><a href=\"#2-4-第四课\" class=\"headerlink\" title=\"2.4. 第四课\"></a>2.4. 第四课</h3><ul>\n<li>查看当前处于文件中的位置：<code>CTRL-G</code>。</li>\n<li>移动光标到文件底部：<code>G</code>。</li>\n<li>移动光标到文件顶部：<code>gg</code>。</li>\n<li>移动到指定的行：<code>#G</code>，其中<code>#</code>表示数字。</li>\n<li>查询：在命令模式中输入<code>/</code>以及需要查询的内容。<ul>\n<li>输入<code>n</code>查询下一个匹配。</li>\n<li>输入<code>N</code>查询上一个匹配。</li>\n<li>如果要反向查询，则使用<code>?</code>而不是<code>/</code>。</li>\n</ul>\n</li>\n<li>返回到之前光标的位置：<code>CTRL-O</code>，其反操作是<code>CTRL-I</code>。</li>\n<li>使用<code>%</code>查询其对应的字符，如<code>()</code>, <code>{}</code>, <code>[]</code>。</li>\n<li>替换字符串：<ul>\n<li><code>:s/old/new</code>：只替换一个记录。</li>\n<li><code>:s/old/new/g</code>：替换当前行所有匹配记录。</li>\n<li><code>:#,#s/old/new/g</code>：替换指定行中所有匹配的记录。</li>\n<li><code>:%s/old/new/g</code>：替换文件中所有匹配记录。</li>\n<li><code>:%s/old/new/gc</code>：替换文件中所有匹配记录，在替换前会询问。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-5-第五课\"><a href=\"#2-5-第五课\" class=\"headerlink\" title=\"2.5. 第五课\"></a>2.5. 第五课</h3><ul>\n<li>执行shell命令：<code>:!ls</code>。</li>\n<li>保存到某文件：<code>:w TEST</code>。</li>\n<li>选择文本：使用<code>v</code>，之后移动光标，就可以选择一段文本。<ul>\n<li>之后若使用<code>:w TEST</code>命令，可以将选中的文本保存到指定文件中</li>\n</ul>\n</li>\n<li>获取其他文本中的内容，并复制到本地：<code>:r TEST</code>。<ul>\n<li>也可以复制命令结果，如<code>:r !ls</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-6-第六课\"><a href=\"#2-6-第六课\" class=\"headerlink\" title=\"2.6. 第六课\"></a>2.6. 第六课</h3><ul>\n<li>添加行：<code>o</code>光标下添加行，<code>O</code>光标上添加行。</li>\n<li>替换多个字符：<code>R</code>。</li>\n<li>替换一个字符：<code>r</code>。</li>\n<li>复制黏贴：<ul>\n<li>复制：使用<code>v</code>，并移动光标，选择需要复制的文本，使用<code>y</code>进行复制。</li>\n<li>粘贴：<code>p</code>。</li>\n</ul>\n</li>\n<li>选项：<ul>\n<li>查询时不区分大小写：<code>:set ic</code>, <code>:set noic</code>。</li>\n<li>查询结果高亮：<code>:set hls</code>，<code>:nohlsearch</code></li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p><strong>Vim自带的Tutorial:在终端下输入<code>vimtutor</code>即可打开vim自带的教程。</strong></p><blockquote>\n<p>cite: <a href=\"https://irvingzhang0512.github.io/2018/06/09/vim-tutor/\" target=\"_blank\" rel=\"noopener\">https://irvingzhang0512.github.io/2018/06/09/vim-tutor/</a></p>\n</blockquote><h2 id=\"1-总结\"><a href=\"#1-总结\" class=\"headerlink\" title=\"1. 总结\"></a>1. 总结</h2><h3 id=\"1-1-光标移动\"><a href=\"#1-1-光标移动\" class=\"headerlink\" title=\"1.1. 光标移动\"></a>1.1. 光标移动</h3><ul>\n<li>普通移动：<code>h j k l</code>。</li>\n<li>到下一个单词开头：<code>w</code>，可以添加数字<code>2w</code>。</li>\n<li>到下一个单词结尾：<code>e</code>，可以添加数字<code>2e</code>。</li>\n<li>移动到当前行开头：<code>0</code>。</li>\n<li>移动到当前行结尾：<code>$</code>。</li>\n<li>移动光标到文件底部：<code>G</code>。</li>\n<li>移动光标到文件顶部：<code>gg</code>。</li>\n<li>移动到指定的行：<code>#G</code>，其中<code>#</code>表示数字。</li>\n</ul><h3 id=\"1-2-删除\"><a href=\"#1-2-删除\" class=\"headerlink\" title=\"1.2. 删除\"></a>1.2. 删除</h3><ul>\n<li>删除一个字符：命令模式，<code>x</code>。</li>\n<li>删除从当前位置，当下一个单词开头的所有字符：<code>dw</code>。</li>\n<li>删除当前行从当前位置开始，到结尾的字符：<code>d$</code>。</li>\n<li>删除当前位置，到当前单词结尾的字符：<code>de</code>。</li>\n<li>可以在dw或de中间添加数字，如<code>d2w</code>，<code>d5e</code>。</li>\n<li>删除整行：<code>dd</code>，并将数据保存到buffer。<ul>\n<li>删除多行：在<code>#dd</code>前添加数字，<code>#</code>表示数字。</li>\n</ul>\n</li>\n</ul>","more":"\n\n\n\n<h3 id=\"1-3-查询与替换\"><a href=\"#1-3-查询与替换\" class=\"headerlink\" title=\"1.3. 查询与替换\"></a>1.3. 查询与替换</h3><ul>\n<li>查询：在命令模式中输入<code>/</code>以及需要查询的内容，回车。<ul>\n<li>输入<code>n</code>查询下一个匹配。</li>\n<li>输入<code>N</code>查询上一个匹配。</li>\n<li>如果要反向查询，则使用<code>?</code>而不是<code>/</code>。</li>\n</ul>\n</li>\n<li>选项：<ul>\n<li>查询时不区分大小写：<code>:set ic</code>, <code>:set noic</code>。</li>\n<li>查询结果高亮：<code>:set hls</code>，<code>:nohlsearch</code>。</li>\n</ul>\n</li>\n<li>使用<code>%</code>查询其对应的字符，如<code>()</code>, <code>{}</code>, <code>[]</code>。</li>\n<li>替换光标指向的一个字符：<code>r</code>（只能替换一个字符）。<ul>\n<li>在命令模式输入<code>r</code>，再输入需要替换的字符，之后就再次进入命令模式。</li>\n</ul>\n</li>\n<li>替换多个字符：<code>R</code>。</li>\n<li>替换字符串：<ul>\n<li><code>:s/old/new：</code>只替换一个记录。</li>\n<li><code>:s/old/new/g：</code>替换当前行所有匹配记录。</li>\n<li><code>:#,#s/old/new/g：</code>替换指定行中所有匹配的记录。</li>\n<li><code>:%s/old/new/g：</code>替换文件中所有匹配记录。</li>\n<li><code>:%s/old/new/gc：</code>替换文件中所有匹配记录，在替换前会询问。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-4-复制粘贴\"><a href=\"#1-4-复制粘贴\" class=\"headerlink\" title=\"1.4. 复制粘贴\"></a>1.4. 复制粘贴</h3><ul>\n<li>删除的内容是保存在缓存中，可以通过<code>p</code>黏贴。</li>\n<li>复制：使用<code>v</code>，并移动光标，选择需要复制的文本，使用<code>y</code>进行复制。</li>\n<li>粘贴：<code>p</code>。</li>\n<li>获取其他文本中的内容，并复制到本地：<code>:r TEST</code>。<ul>\n<li>也可以复制命令结果，如<code>:r !ls</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-5-添加操作\"><a href=\"#1-5-添加操作\" class=\"headerlink\" title=\"1.5. 添加操作\"></a>1.5. 添加操作</h3><ul>\n<li>插入字符（进入编辑模式）：<code>i</code>。</li>\n<li>追加字符<ul>\n<li>在光标指向的位置后添加内容，进入编辑模式：<code>a</code>。</li>\n<li>在本行末尾添加内容，进入编辑模式：<code>A</code>。</li>\n</ul>\n</li>\n<li>change operator：删除指定位置的数据，并进入编辑模式。<ul>\n<li><code>ce</code>：删除光标到当前单词末尾的数据，并进入编辑模式。</li>\n<li><code>c</code>与<code>d</code>的用法类似，也可以使用<code>e w $</code>以及<code>数字</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"1-6-其他操作\"><a href=\"#1-6-其他操作\" class=\"headerlink\" title=\"1.6. 其他操作\"></a>1.6. 其他操作</h3><ul>\n<li>退出vim：命令模式，输入<code>:q!</code>（退出不保存），输入<code>:wq</code>（保存并退出）。</li>\n<li>查看当前处于文件中的位置：<code>CTRL-G</code>。</li>\n<li>撤销：<code>u</code>撤销一个操作，<code>U</code>撤销当前行的之前的操作。<ul>\n<li>撤销的撤销：<code>CTRL-R</code>。</li>\n</ul>\n</li>\n<li>返回到之前光标的位置：<code>CTRL-O</code>，其反操作是<code>CTRL-I</code>。</li>\n</ul>\n<h2 id=\"2-分课程总结\"><a href=\"#2-分课程总结\" class=\"headerlink\" title=\"2. 分课程总结\"></a>2. 分课程总结</h2><h3 id=\"2-1-第一课\"><a href=\"#2-1-第一课\" class=\"headerlink\" title=\"2.1. 第一课\"></a>2.1. 第一课</h3><ul>\n<li>移动光标：命令模式，<code>h j k l</code>。</li>\n<li>退出vim：命令模式，输入<code>:q!</code>（退出不保存），输入<code>:wq</code>（保存并退出）。</li>\n<li>删除一个字符：命令模式，<code>x</code>。</li>\n<li>插入字符（进入编辑模式）, <code>i</code>。</li>\n<li>追加字符<ul>\n<li>在光标指向的位置后添加内容，进入编辑模式：<code>a</code>。</li>\n<li>在本行末尾添加内容，进入编辑模式：<code>A</code>。</li>\n</ul>\n</li>\n<li>执行shell命令：<code>:!ls</code>。</li>\n<li>保存到某文件：<code>:w TEST</code>。</li>\n</ul>\n<h3 id=\"2-2-第二课\"><a href=\"#2-2-第二课\" class=\"headerlink\" title=\"2.2. 第二课\"></a>2.2. 第二课</h3><ul>\n<li>删除从当前位置，当下一个单词开头的所有字符：<code>dw</code>。</li>\n<li>删除当前行从当前位置开始，到结尾的字符：<code>d$</code>。</li>\n<li>删除当前位置，到当前单词结尾的字符：<code>de</code>。</li>\n<li>移动光标：<ul>\n<li>到下一个单词开头：<code>w</code>。</li>\n<li>到下一个单词结尾：<code>e</code>。</li>\n<li>移动到当前行开头：<code>0</code>。</li>\n<li>可以在<code>w</code>或<code>e</code>前，添加数字。</li>\n<li>可以在<code>dw</code>或<code>de</code>中间添加数字，如<code>d2w</code>，<code>d5e</code>。</li>\n</ul>\n</li>\n<li>删除整行：<code>dd</code>，并将数据保存到buffer。<ul>\n<li>删除多行：在<code>dd</code>前添加数字。</li>\n</ul>\n</li>\n<li>撤销：<code>u</code>撤销一个操作，<code>U</code>撤销当前行的之前的操作。<ul>\n<li>撤销的撤销：<code>CTRL-R</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-3-第三课\"><a href=\"#2-3-第三课\" class=\"headerlink\" title=\"2.3. 第三课\"></a>2.3. 第三课</h3><ul>\n<li>删除的内容是保存在缓存中，可以通过<code>p</code>黏贴。</li>\n<li>替换光标指向的一个字符：<code>r</code>（只能替换一个字符）。<ul>\n<li>在命令模式输入<code>r</code>，再输入需要替换的字符，之后就再次进入命令模式。</li>\n</ul>\n</li>\n<li>change operator：删除指定位置的数据，并进入编辑模式。<ul>\n<li><code>ce</code>：删除光标到当前单词末尾的数据，并进入编辑模式。</li>\n<li><code>c</code>与<code>d</code>的用法类似，也可以使用<code>e w $</code>以及数字。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-4-第四课\"><a href=\"#2-4-第四课\" class=\"headerlink\" title=\"2.4. 第四课\"></a>2.4. 第四课</h3><ul>\n<li>查看当前处于文件中的位置：<code>CTRL-G</code>。</li>\n<li>移动光标到文件底部：<code>G</code>。</li>\n<li>移动光标到文件顶部：<code>gg</code>。</li>\n<li>移动到指定的行：<code>#G</code>，其中<code>#</code>表示数字。</li>\n<li>查询：在命令模式中输入<code>/</code>以及需要查询的内容。<ul>\n<li>输入<code>n</code>查询下一个匹配。</li>\n<li>输入<code>N</code>查询上一个匹配。</li>\n<li>如果要反向查询，则使用<code>?</code>而不是<code>/</code>。</li>\n</ul>\n</li>\n<li>返回到之前光标的位置：<code>CTRL-O</code>，其反操作是<code>CTRL-I</code>。</li>\n<li>使用<code>%</code>查询其对应的字符，如<code>()</code>, <code>{}</code>, <code>[]</code>。</li>\n<li>替换字符串：<ul>\n<li><code>:s/old/new</code>：只替换一个记录。</li>\n<li><code>:s/old/new/g</code>：替换当前行所有匹配记录。</li>\n<li><code>:#,#s/old/new/g</code>：替换指定行中所有匹配的记录。</li>\n<li><code>:%s/old/new/g</code>：替换文件中所有匹配记录。</li>\n<li><code>:%s/old/new/gc</code>：替换文件中所有匹配记录，在替换前会询问。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-5-第五课\"><a href=\"#2-5-第五课\" class=\"headerlink\" title=\"2.5. 第五课\"></a>2.5. 第五课</h3><ul>\n<li>执行shell命令：<code>:!ls</code>。</li>\n<li>保存到某文件：<code>:w TEST</code>。</li>\n<li>选择文本：使用<code>v</code>，之后移动光标，就可以选择一段文本。<ul>\n<li>之后若使用<code>:w TEST</code>命令，可以将选中的文本保存到指定文件中</li>\n</ul>\n</li>\n<li>获取其他文本中的内容，并复制到本地：<code>:r TEST</code>。<ul>\n<li>也可以复制命令结果，如<code>:r !ls</code>。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"2-6-第六课\"><a href=\"#2-6-第六课\" class=\"headerlink\" title=\"2.6. 第六课\"></a>2.6. 第六课</h3><ul>\n<li>添加行：<code>o</code>光标下添加行，<code>O</code>光标上添加行。</li>\n<li>替换多个字符：<code>R</code>。</li>\n<li>替换一个字符：<code>r</code>。</li>\n<li>复制黏贴：<ul>\n<li>复制：使用<code>v</code>，并移动光标，选择需要复制的文本，使用<code>y</code>进行复制。</li>\n<li>粘贴：<code>p</code>。</li>\n</ul>\n</li>\n<li>选项：<ul>\n<li>查询时不区分大小写：<code>:set ic</code>, <code>:set noic</code>。</li>\n<li>查询结果高亮：<code>:set hls</code>，<code>:nohlsearch</code></li>\n</ul>\n</li>\n</ul>\n"},{"title":"Optimization Method","mathjax":true,"copyright":true,"date":"2019-06-05T10:05:24.000Z","_content":"\n## 0 优化算法框架\n\n> - [SGD、Momentum、RMSprop、Adam区别与联系](https://zhuanlan.zhihu.com/p/32488889)\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{0-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = \\phi (g_1, g_2, \\cdots, g_t)\n  \\tag{0-2}\n  $$\n  $$\n  V_t = \\psi (g_1, g_2, \\cdots, g_t)\n  \\tag{0-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t\n  \\tag{0-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  w_{t+1} = w_t - \\eta_t\n  \\tag{0-5}\n  $$\n\n**核心区别是第3步执行的下降方向，在这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将变化的地方标红显示**\n\n\n## 1. Gradient Descent Variants\n\n### 1.1 Batch Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{1.1-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.1-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.1-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.1-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\\n  &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.1-5}\n  $$\n  \n\n\n### 1.2 Stochastic Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red}  { g_t = \\nabla f(w_t; x^{(i)}, y^{(i)}) } \n  \\tag{1.2-1}\n  $$\n\n其中$(x^{(i)}, y^{(i)})$表示第$i$个样本；\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.2-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.2-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.2-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.2-5}\n  $$\n\n\n### 1.3 Mini-Bach Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red}  { g_t = \\nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } \n  \\tag{1.3-1}\n  $$\n\n  其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本，$n$表示mini-batch的大小；\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.3-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.3-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.3-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.3-5}\n  $$\n\n\n**上述算法存在的问题：**\n- 很难调整出一个合适的learning_rate\n- learning_rate的变化规则往往是预定义的，很难适应不同的数据\n- 所有的特征共享相同的learning_rate\n- 局部最有解的问题\n\n\n---------------------------------------------------------------\n\n\n## 2. Gradient Descent Optimization Algorithm\n\n### 2.1 Gradient Descent with Momentum\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.1-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(*将框架的第2步和第3步合并*)\n  $$\n  \\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n  \\tag{2.1-2&3}\n  $$\n\n  $$\n  \\color{red} { \\eta_t = m_t }\n  \\tag{2.1-4}\n  $$\n\n3. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n  \\end{align*}\n  \\tag{2.1-5}\n  $$\n\n一阶动量是移动平均值，这里 $\\gamma $ 的经验值为`0.9`。\n\n\n### 2.2 Nesterov Accelerated Gradient\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red} { g_t = \\nabla f(w_t - \\gamma m_{t-1}) }\n  \\tag{2.2-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(*将框架的第2步和第3步合并*)\n  $$\n  \\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n  \\tag{2.2-2&3}\n  $$\n\n  $$\n  \\color{red} { \\eta_t = m_t }\n  \\tag{2.2-4}\n  $$\n\n3. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n  \\end{align*}\n  \\tag{2.2-5}\n  $$\n\n这里 $\\gamma $ 的经验值为`0.9`\n\n\n### 2.3 AdaGrad\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.3-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = g_t\n  \\tag{2.3-2}\n  $$\n  $$\n  \\color{red} {V_t = \\sum_{\\tau = 1}^t g_{\\tau}^2 }\n  \\tag{2.3-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.3-4}\n  $$\n\n  这里$\\epsilon$是为了避免分母为$0$。\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.3-5}\n  $$\n\n\n### 2.4 AdaDelta\n\n首先定义动态平均值 $\\color{red}{ E[ g^2 ]\\_t = \\gamma E[ g^2 ]\\_{t-1} + (1 - \\gamma) g_t^2 }$，该值仅取决于当前梯度值与上一时刻的动态平均值，其中$\\gamma$通常设置成$0.9$。\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.4-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = g_t\n  \\tag{2.4-2}\n  $$\n  $$\n  \\color{red} {V_t = E[ g^2 ]_t }\n  \\tag{2.4-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ E[ g^2 ]_t + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.4-4.1}\n  $$\n\n  这里$\\epsilon$是为了避免分母为$0$。将分母$ \\sqrt{ E[ g^2 ]\\_t + \\epsilon} $ 记为 $\\color{red}{ RMS[g]\\_t} $，定义\n  $$\n  E[ \\Delta g^2 ] _t = \\gamma E[ \\Delta g^2 ] _{t-1} + (1 - \\gamma) \\Delta g _t^2\n  $$\n\n  则：\n  $$\n  RMS[\\Delta g] _t = \\sqrt{ E[ \\Delta g^2 ] _t + \\epsilon }\n  $$\n\n  用$RMS[\\Delta g]_{t-1}$代替学习率$\\alpha$，则式$(2.4-4.1)$可以转化为:\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{RMS[g] _t} \\cdot g_t \\\\ &= \\frac{ RMS[\\Delta g] _{t-1} }{ RMS[g] _t } \\cdot g _t\n  \\end{align*}\n  \\tag{2.4-4.2}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{RMS[\\Delta g] _{t-1}}{RMS[g] _t} \\cdot g_t\n  \\end{align*}\n  \\tag{2.4-5}\n  $$\n\n\n### 2.5 RMSprop\n\nRMSprop是AdaDelta算法的一个特例。\n\n$$\nE[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t\n$$\n\n$$\nw_{t+1} = w_t - \\frac{\\alpha}{ \\sqrt{ E[g^2] _t + \\epsilon } } g_t\n$$\n\nHinton建议$\\gamma$设置成$0.9$，学习率设置成$0.001$。\n\n\n### 2.6 Adam\n\n\n### 2.7 AdaMax\n\n\n### 2.8 Nadam\n\n\n\n-----------------------\n\n## 3. Other Optimization Algorithm\n\n- 将不满足阈值的系数设置为0\n\n### 3.1 Truncated Gradient\n\n- 简单截断法的改进\n\n### 3.2 FOBOS\n\n- 微调标准梯度下降的结果\n\n### 3.3 RDA\n\n- 历史梯度加权平均\n- 正则项对特征稀疏化\n- 严格递增序列\n\n### 3.4 FTRL\n\n- 确保新的权重和历史权重不偏离太远\n- L1正则稀疏性约束\n\n\n------------\n\n> - [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)\n> - [在线最优化求解(Online Optimization)-冯扬]()\n> - [比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533)\n","source":"_posts/2019-06-05-Optimization-Method.md","raw":"---\ntitle: Optimization Method\ntags:\n- 优化算法\nmathjax: true\ncopyright: true\ndate: 2019-06-05 18:05:24\ncategories: 机器学习\n---\n\n## 0 优化算法框架\n\n> - [SGD、Momentum、RMSprop、Adam区别与联系](https://zhuanlan.zhihu.com/p/32488889)\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{0-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = \\phi (g_1, g_2, \\cdots, g_t)\n  \\tag{0-2}\n  $$\n  $$\n  V_t = \\psi (g_1, g_2, \\cdots, g_t)\n  \\tag{0-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t\n  \\tag{0-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  w_{t+1} = w_t - \\eta_t\n  \\tag{0-5}\n  $$\n\n**核心区别是第3步执行的下降方向，在这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将变化的地方标红显示**\n\n\n## 1. Gradient Descent Variants\n\n### 1.1 Batch Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{1.1-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.1-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.1-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.1-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\\n  &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.1-5}\n  $$\n  \n\n\n### 1.2 Stochastic Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red}  { g_t = \\nabla f(w_t; x^{(i)}, y^{(i)}) } \n  \\tag{1.2-1}\n  $$\n\n其中$(x^{(i)}, y^{(i)})$表示第$i$个样本；\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.2-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.2-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.2-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.2-5}\n  $$\n\n\n### 1.3 Mini-Bach Gradient Descent\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red}  { g_t = \\nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } \n  \\tag{1.3-1}\n  $$\n\n  其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本，$n$表示mini-batch的大小；\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  \\color{red} {m_t = g_t}\n  \\tag{1.3-2}\n  $$\n  $$\n  \\color{red} {V_t = 1}\n  \\tag{1.3-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n  \\tag{1.3-4}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n  \\end{align*}\n  \\tag{1.3-5}\n  $$\n\n\n**上述算法存在的问题：**\n- 很难调整出一个合适的learning_rate\n- learning_rate的变化规则往往是预定义的，很难适应不同的数据\n- 所有的特征共享相同的learning_rate\n- 局部最有解的问题\n\n\n---------------------------------------------------------------\n\n\n## 2. Gradient Descent Optimization Algorithm\n\n### 2.1 Gradient Descent with Momentum\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.1-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(*将框架的第2步和第3步合并*)\n  $$\n  \\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n  \\tag{2.1-2&3}\n  $$\n\n  $$\n  \\color{red} { \\eta_t = m_t }\n  \\tag{2.1-4}\n  $$\n\n3. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n  \\end{align*}\n  \\tag{2.1-5}\n  $$\n\n一阶动量是移动平均值，这里 $\\gamma $ 的经验值为`0.9`。\n\n\n### 2.2 Nesterov Accelerated Gradient\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  \\color{red} { g_t = \\nabla f(w_t - \\gamma m_{t-1}) }\n  \\tag{2.2-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(*将框架的第2步和第3步合并*)\n  $$\n  \\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n  \\tag{2.2-2&3}\n  $$\n\n  $$\n  \\color{red} { \\eta_t = m_t }\n  \\tag{2.2-4}\n  $$\n\n3. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n  \\end{align*}\n  \\tag{2.2-5}\n  $$\n\n这里 $\\gamma $ 的经验值为`0.9`\n\n\n### 2.3 AdaGrad\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.3-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = g_t\n  \\tag{2.3-2}\n  $$\n  $$\n  \\color{red} {V_t = \\sum_{\\tau = 1}^t g_{\\tau}^2 }\n  \\tag{2.3-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.3-4}\n  $$\n\n  这里$\\epsilon$是为了避免分母为$0$。\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.3-5}\n  $$\n\n\n### 2.4 AdaDelta\n\n首先定义动态平均值 $\\color{red}{ E[ g^2 ]\\_t = \\gamma E[ g^2 ]\\_{t-1} + (1 - \\gamma) g_t^2 }$，该值仅取决于当前梯度值与上一时刻的动态平均值，其中$\\gamma$通常设置成$0.9$。\n\n1. 计算目标函数关于当前参数的梯度\n  $$\n  g_t = \\nabla f(w_t)\n  \\tag{2.4-1}\n  $$\n\n2. 根据历史梯度计算一阶动量和二阶动量\n  $$\n  m_t = g_t\n  \\tag{2.4-2}\n  $$\n  $$\n  \\color{red} {V_t = E[ g^2 ]_t }\n  \\tag{2.4-3}\n  $$\n\n3. 计算当前时刻的下降梯度\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ E[ g^2 ]_t + \\epsilon }} \\cdot m_t\n  \\end{align*}\n  \\tag{2.4-4.1}\n  $$\n\n  这里$\\epsilon$是为了避免分母为$0$。将分母$ \\sqrt{ E[ g^2 ]\\_t + \\epsilon} $ 记为 $\\color{red}{ RMS[g]\\_t} $，定义\n  $$\n  E[ \\Delta g^2 ] _t = \\gamma E[ \\Delta g^2 ] _{t-1} + (1 - \\gamma) \\Delta g _t^2\n  $$\n\n  则：\n  $$\n  RMS[\\Delta g] _t = \\sqrt{ E[ \\Delta g^2 ] _t + \\epsilon }\n  $$\n\n  用$RMS[\\Delta g]_{t-1}$代替学习率$\\alpha$，则式$(2.4-4.1)$可以转化为:\n  $$\n  \\begin{align*}\n  \\eta_t &= \\frac{\\alpha}{RMS[g] _t} \\cdot g_t \\\\ &= \\frac{ RMS[\\Delta g] _{t-1} }{ RMS[g] _t } \\cdot g _t\n  \\end{align*}\n  \\tag{2.4-4.2}\n  $$\n\n4. 根据下降梯度进行更新\n  $$\n  \\begin{align*}\n  w_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{RMS[\\Delta g] _{t-1}}{RMS[g] _t} \\cdot g_t\n  \\end{align*}\n  \\tag{2.4-5}\n  $$\n\n\n### 2.5 RMSprop\n\nRMSprop是AdaDelta算法的一个特例。\n\n$$\nE[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t\n$$\n\n$$\nw_{t+1} = w_t - \\frac{\\alpha}{ \\sqrt{ E[g^2] _t + \\epsilon } } g_t\n$$\n\nHinton建议$\\gamma$设置成$0.9$，学习率设置成$0.001$。\n\n\n### 2.6 Adam\n\n\n### 2.7 AdaMax\n\n\n### 2.8 Nadam\n\n\n\n-----------------------\n\n## 3. Other Optimization Algorithm\n\n- 将不满足阈值的系数设置为0\n\n### 3.1 Truncated Gradient\n\n- 简单截断法的改进\n\n### 3.2 FOBOS\n\n- 微调标准梯度下降的结果\n\n### 3.3 RDA\n\n- 历史梯度加权平均\n- 正则项对特征稀疏化\n- 严格递增序列\n\n### 3.4 FTRL\n\n- 确保新的权重和历史权重不偏离太远\n- L1正则稀疏性约束\n\n\n------------\n\n> - [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)\n> - [在线最优化求解(Online Optimization)-冯扬]()\n> - [比Momentum更快：揭开Nesterov Accelerated Gradient的真面目](https://zhuanlan.zhihu.com/p/22810533)\n","slug":"Optimization-Method","published":1,"updated":"2019-08-21T12:56:48.925Z","_id":"cjzl0wnxe004s2qwpvwkbonv3","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"0-优化算法框架\"><a href=\"#0-优化算法框架\" class=\"headerlink\" title=\"0 优化算法框架\"></a>0 优化算法框架</h2><blockquote>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32488889\" target=\"_blank\" rel=\"noopener\">SGD、Momentum、RMSprop、Adam区别与联系</a></li>\n</ul>\n</blockquote><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{0-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = \\phi (g_1, g_2, \\cdots, g_t)\n\\tag{0-2}</script><script type=\"math/tex; mode=display\">\nV_t = \\psi (g_1, g_2, \\cdots, g_t)\n\\tag{0-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t\n\\tag{0-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t - \\eta_t\n\\tag{0-5}</script></li>\n</ol><a id=\"more\"></a>\n\n<p><strong>核心区别是第3步执行的下降方向，在这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将变化的地方标红显示</strong></p>\n<h2 id=\"1-Gradient-Descent-Variants\"><a href=\"#1-Gradient-Descent-Variants\" class=\"headerlink\" title=\"1. Gradient Descent Variants\"></a>1. Gradient Descent Variants</h2><h3 id=\"1-1-Batch-Gradient-Descent\"><a href=\"#1-1-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1.1 Batch Gradient Descent\"></a>1.1 Batch Gradient Descent</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{1.1-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.1-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.1-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.1-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\\n&= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.1-5}</script></li>\n</ol>\n<h3 id=\"1-2-Stochastic-Gradient-Descent\"><a href=\"#1-2-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"1.2 Stochastic Gradient Descent\"></a>1.2 Stochastic Gradient Descent</h3><ol>\n<li>计算目标函数关于当前参数的梯度<script type=\"math/tex; mode=display\">\n\\color{red}  { g_t = \\nabla f(w_t; x^{(i)}, y^{(i)}) } \n\\tag{1.2-1}</script></li>\n</ol>\n<p>其中$(x^{(i)}, y^{(i)})$表示第$i$个样本；</p>\n<ol>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.2-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.2-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.2-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.2-5}</script></li>\n</ol>\n<h3 id=\"1-3-Mini-Bach-Gradient-Descent\"><a href=\"#1-3-Mini-Bach-Gradient-Descent\" class=\"headerlink\" title=\"1.3 Mini-Bach Gradient Descent\"></a>1.3 Mini-Bach Gradient Descent</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\n\\color{red}  { g_t = \\nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } \n\\tag{1.3-1}</script><p>其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本，$n$表示mini-batch的大小；</p>\n</li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.3-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.3-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.3-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.3-5}</script></li>\n</ol>\n<p><strong>上述算法存在的问题：</strong></p>\n<ul>\n<li>很难调整出一个合适的learning_rate</li>\n<li>learning_rate的变化规则往往是预定义的，很难适应不同的数据</li>\n<li>所有的特征共享相同的learning_rate</li>\n<li>局部最有解的问题</li>\n</ul>\n<hr>\n<h2 id=\"2-Gradient-Descent-Optimization-Algorithm\"><a href=\"#2-Gradient-Descent-Optimization-Algorithm\" class=\"headerlink\" title=\"2. Gradient Descent Optimization Algorithm\"></a>2. Gradient Descent Optimization Algorithm</h2><h3 id=\"2-1-Gradient-Descent-with-Momentum\"><a href=\"#2-1-Gradient-Descent-with-Momentum\" class=\"headerlink\" title=\"2.1 Gradient Descent with Momentum\"></a>2.1 Gradient Descent with Momentum</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.1-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n\\tag{2.1-2&3}</script><script type=\"math/tex; mode=display\">\n\\color{red} { \\eta_t = m_t }\n\\tag{2.1-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n\\end{align*}\n\\tag{2.1-5}</script></li>\n</ol>\n<p>一阶动量是移动平均值，这里 $\\gamma $ 的经验值为<code>0.9</code>。</p>\n<h3 id=\"2-2-Nesterov-Accelerated-Gradient\"><a href=\"#2-2-Nesterov-Accelerated-Gradient\" class=\"headerlink\" title=\"2.2 Nesterov Accelerated Gradient\"></a>2.2 Nesterov Accelerated Gradient</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} { g_t = \\nabla f(w_t - \\gamma m_{t-1}) }\n\\tag{2.2-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n\\tag{2.2-2&3}</script><script type=\"math/tex; mode=display\">\n\\color{red} { \\eta_t = m_t }\n\\tag{2.2-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n\\end{align*}\n\\tag{2.2-5}</script></li>\n</ol>\n<p>这里 $\\gamma $ 的经验值为<code>0.9</code></p>\n<h3 id=\"2-3-AdaGrad\"><a href=\"#2-3-AdaGrad\" class=\"headerlink\" title=\"2.3 AdaGrad\"></a>2.3 AdaGrad</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.3-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = g_t\n\\tag{2.3-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = \\sum_{\\tau = 1}^t g_{\\tau}^2 }\n\\tag{2.3-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.3-4}</script><p>这里$\\epsilon$是为了避免分母为$0$。</p>\n</li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.3-5}</script></li>\n</ol>\n<h3 id=\"2-4-AdaDelta\"><a href=\"#2-4-AdaDelta\" class=\"headerlink\" title=\"2.4 AdaDelta\"></a>2.4 AdaDelta</h3><p>首先定义动态平均值 $\\color{red}{ E[ g^2 ]_t = \\gamma E[ g^2 ]_{t-1} + (1 - \\gamma) g_t^2 }$，该值仅取决于当前梯度值与上一时刻的动态平均值，其中$\\gamma$通常设置成$0.9$。</p>\n<ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.4-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = g_t\n\\tag{2.4-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = E[ g^2 ]_t }\n\\tag{2.4-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ E[ g^2 ]_t + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.4-4.1}</script><p>这里$\\epsilon$是为了避免分母为$0$。将分母$ \\sqrt{ E[ g^2 ]_t + \\epsilon} $ 记为 $\\color{red}{ RMS[g]_t} $，定义</p>\n<script type=\"math/tex; mode=display\">\nE[ \\Delta g^2 ] _t = \\gamma E[ \\Delta g^2 ] _{t-1} + (1 - \\gamma) \\Delta g _t^2</script><p>则：</p>\n<script type=\"math/tex; mode=display\">\nRMS[\\Delta g] _t = \\sqrt{ E[ \\Delta g^2 ] _t + \\epsilon }</script><p>用$RMS[\\Delta g]_{t-1}$代替学习率$\\alpha$，则式$(2.4-4.1)$可以转化为:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{RMS[g] _t} \\cdot g_t \\\\ &= \\frac{ RMS[\\Delta g] _{t-1} }{ RMS[g] _t } \\cdot g _t\n\\end{align*}\n\\tag{2.4-4.2}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{RMS[\\Delta g] _{t-1}}{RMS[g] _t} \\cdot g_t\n\\end{align*}\n\\tag{2.4-5}</script></li>\n</ol>\n<h3 id=\"2-5-RMSprop\"><a href=\"#2-5-RMSprop\" class=\"headerlink\" title=\"2.5 RMSprop\"></a>2.5 RMSprop</h3><p>RMSprop是AdaDelta算法的一个特例。</p>\n<script type=\"math/tex; mode=display\">\nE[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t</script><script type=\"math/tex; mode=display\">\nw_{t+1} = w_t - \\frac{\\alpha}{ \\sqrt{ E[g^2] _t + \\epsilon } } g_t</script><p>Hinton建议$\\gamma$设置成$0.9$，学习率设置成$0.001$。</p>\n<h3 id=\"2-6-Adam\"><a href=\"#2-6-Adam\" class=\"headerlink\" title=\"2.6 Adam\"></a>2.6 Adam</h3><h3 id=\"2-7-AdaMax\"><a href=\"#2-7-AdaMax\" class=\"headerlink\" title=\"2.7 AdaMax\"></a>2.7 AdaMax</h3><h3 id=\"2-8-Nadam\"><a href=\"#2-8-Nadam\" class=\"headerlink\" title=\"2.8 Nadam\"></a>2.8 Nadam</h3><hr>\n<h2 id=\"3-Other-Optimization-Algorithm\"><a href=\"#3-Other-Optimization-Algorithm\" class=\"headerlink\" title=\"3. Other Optimization Algorithm\"></a>3. Other Optimization Algorithm</h2><ul>\n<li>将不满足阈值的系数设置为0</li>\n</ul>\n<h3 id=\"3-1-Truncated-Gradient\"><a href=\"#3-1-Truncated-Gradient\" class=\"headerlink\" title=\"3.1 Truncated Gradient\"></a>3.1 Truncated Gradient</h3><ul>\n<li>简单截断法的改进</li>\n</ul>\n<h3 id=\"3-2-FOBOS\"><a href=\"#3-2-FOBOS\" class=\"headerlink\" title=\"3.2 FOBOS\"></a>3.2 FOBOS</h3><ul>\n<li>微调标准梯度下降的结果</li>\n</ul>\n<h3 id=\"3-3-RDA\"><a href=\"#3-3-RDA\" class=\"headerlink\" title=\"3.3 RDA\"></a>3.3 RDA</h3><ul>\n<li>历史梯度加权平均</li>\n<li>正则项对特征稀疏化</li>\n<li>严格递增序列</li>\n</ul>\n<h3 id=\"3-4-FTRL\"><a href=\"#3-4-FTRL\" class=\"headerlink\" title=\"3.4 FTRL\"></a>3.4 FTRL</h3><ul>\n<li>确保新的权重和历史权重不偏离太远</li>\n<li>L1正则稀疏性约束</li>\n</ul>\n<hr>\n<blockquote>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1609.04747.pdf\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms</a></li>\n<li><a href>在线最优化求解(Online Optimization)-冯扬</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/22810533\" target=\"_blank\" rel=\"noopener\">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></li>\n</ul>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h2 id=\"0-优化算法框架\"><a href=\"#0-优化算法框架\" class=\"headerlink\" title=\"0 优化算法框架\"></a>0 优化算法框架</h2><blockquote>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/32488889\" target=\"_blank\" rel=\"noopener\">SGD、Momentum、RMSprop、Adam区别与联系</a></li>\n</ul>\n</blockquote><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{0-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = \\phi (g_1, g_2, \\cdots, g_t)\n\\tag{0-2}</script><script type=\"math/tex; mode=display\">\nV_t = \\psi (g_1, g_2, \\cdots, g_t)\n\\tag{0-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t\n\\tag{0-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\nw_{t+1} = w_t - \\eta_t\n\\tag{0-5}</script></li>\n</ol>","more":"\n\n<p><strong>核心区别是第3步执行的下降方向，在这个式子中，前半部分是实际的学习率（也即下降步长），后半部分是实际的下降方向。不同优化算法也就是不断地在这两部分上做文章。下文会将变化的地方标红显示</strong></p>\n<h2 id=\"1-Gradient-Descent-Variants\"><a href=\"#1-Gradient-Descent-Variants\" class=\"headerlink\" title=\"1. Gradient Descent Variants\"></a>1. Gradient Descent Variants</h2><h3 id=\"1-1-Batch-Gradient-Descent\"><a href=\"#1-1-Batch-Gradient-Descent\" class=\"headerlink\" title=\"1.1 Batch Gradient Descent\"></a>1.1 Batch Gradient Descent</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{1.1-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.1-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.1-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.1-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\\n&= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.1-5}</script></li>\n</ol>\n<h3 id=\"1-2-Stochastic-Gradient-Descent\"><a href=\"#1-2-Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"1.2 Stochastic Gradient Descent\"></a>1.2 Stochastic Gradient Descent</h3><ol>\n<li>计算目标函数关于当前参数的梯度<script type=\"math/tex; mode=display\">\n\\color{red}  { g_t = \\nabla f(w_t; x^{(i)}, y^{(i)}) } \n\\tag{1.2-1}</script></li>\n</ol>\n<p>其中$(x^{(i)}, y^{(i)})$表示第$i$个样本；</p>\n<ol>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.2-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.2-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.2-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.2-5}</script></li>\n</ol>\n<h3 id=\"1-3-Mini-Bach-Gradient-Descent\"><a href=\"#1-3-Mini-Bach-Gradient-Descent\" class=\"headerlink\" title=\"1.3 Mini-Bach Gradient Descent\"></a>1.3 Mini-Bach Gradient Descent</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\n\\color{red}  { g_t = \\nabla f(w_t; x^{(i:i+n)}, y^{(i:i+n)}) } \n\\tag{1.3-1}</script><p>其中$(x^{(i:i+n)}, y^{(i:i+n)})$表示第$i$个样本到第$i+n$个样本，$n$表示mini-batch的大小；</p>\n</li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = g_t}\n\\tag{1.3-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = 1}\n\\tag{1.3-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\eta_t = \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t = \\alpha \\cdot g_t\n\\tag{1.3-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\alpha \\cdot g_t\n\\end{align*}\n\\tag{1.3-5}</script></li>\n</ol>\n<p><strong>上述算法存在的问题：</strong></p>\n<ul>\n<li>很难调整出一个合适的learning_rate</li>\n<li>learning_rate的变化规则往往是预定义的，很难适应不同的数据</li>\n<li>所有的特征共享相同的learning_rate</li>\n<li>局部最有解的问题</li>\n</ul>\n<hr>\n<h2 id=\"2-Gradient-Descent-Optimization-Algorithm\"><a href=\"#2-Gradient-Descent-Optimization-Algorithm\" class=\"headerlink\" title=\"2. Gradient Descent Optimization Algorithm\"></a>2. Gradient Descent Optimization Algorithm</h2><h3 id=\"2-1-Gradient-Descent-with-Momentum\"><a href=\"#2-1-Gradient-Descent-with-Momentum\" class=\"headerlink\" title=\"2.1 Gradient Descent with Momentum\"></a>2.1 Gradient Descent with Momentum</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.1-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n\\tag{2.1-2&3}</script><script type=\"math/tex; mode=display\">\n\\color{red} { \\eta_t = m_t }\n\\tag{2.1-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n\\end{align*}\n\\tag{2.1-5}</script></li>\n</ol>\n<p>一阶动量是移动平均值，这里 $\\gamma $ 的经验值为<code>0.9</code>。</p>\n<h3 id=\"2-2-Nesterov-Accelerated-Gradient\"><a href=\"#2-2-Nesterov-Accelerated-Gradient\" class=\"headerlink\" title=\"2.2 Nesterov Accelerated Gradient\"></a>2.2 Nesterov Accelerated Gradient</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} { g_t = \\nabla f(w_t - \\gamma m_{t-1}) }\n\\tag{2.2-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量，计算当前时刻下降梯度(<em>将框架的第2步和第3步合并</em>)</p>\n<script type=\"math/tex; mode=display\">\n\\color{red} {m_t = \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t }\n\\tag{2.2-2&3}</script><script type=\"math/tex; mode=display\">\n\\color{red} { \\eta_t = m_t }\n\\tag{2.2-4}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - ( \\gamma \\cdot m_{t-1} + \\alpha \\cdot g_t )\n\\end{align*}\n\\tag{2.2-5}</script></li>\n</ol>\n<p>这里 $\\gamma $ 的经验值为<code>0.9</code></p>\n<h3 id=\"2-3-AdaGrad\"><a href=\"#2-3-AdaGrad\" class=\"headerlink\" title=\"2.3 AdaGrad\"></a>2.3 AdaGrad</h3><ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.3-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = g_t\n\\tag{2.3-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = \\sum_{\\tau = 1}^t g_{\\tau}^2 }\n\\tag{2.3-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.3-4}</script><p>这里$\\epsilon$是为了避免分母为$0$。</p>\n</li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{\\alpha}{\\sqrt{ \\sum_{\\tau = 1}^t g_{\\tau}^2 + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.3-5}</script></li>\n</ol>\n<h3 id=\"2-4-AdaDelta\"><a href=\"#2-4-AdaDelta\" class=\"headerlink\" title=\"2.4 AdaDelta\"></a>2.4 AdaDelta</h3><p>首先定义动态平均值 $\\color{red}{ E[ g^2 ]_t = \\gamma E[ g^2 ]_{t-1} + (1 - \\gamma) g_t^2 }$，该值仅取决于当前梯度值与上一时刻的动态平均值，其中$\\gamma$通常设置成$0.9$。</p>\n<ol>\n<li><p>计算目标函数关于当前参数的梯度</p>\n<script type=\"math/tex; mode=display\">\ng_t = \\nabla f(w_t)\n\\tag{2.4-1}</script></li>\n<li><p>根据历史梯度计算一阶动量和二阶动量</p>\n<script type=\"math/tex; mode=display\">\nm_t = g_t\n\\tag{2.4-2}</script><script type=\"math/tex; mode=display\">\n\\color{red} {V_t = E[ g^2 ]_t }\n\\tag{2.4-3}</script></li>\n<li><p>计算当前时刻的下降梯度</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{\\sqrt{V_t}} \\cdot m_t \\\\ &= \\frac{\\alpha}{\\sqrt{ E[ g^2 ]_t + \\epsilon }} \\cdot m_t\n\\end{align*}\n\\tag{2.4-4.1}</script><p>这里$\\epsilon$是为了避免分母为$0$。将分母$ \\sqrt{ E[ g^2 ]_t + \\epsilon} $ 记为 $\\color{red}{ RMS[g]_t} $，定义</p>\n<script type=\"math/tex; mode=display\">\nE[ \\Delta g^2 ] _t = \\gamma E[ \\Delta g^2 ] _{t-1} + (1 - \\gamma) \\Delta g _t^2</script><p>则：</p>\n<script type=\"math/tex; mode=display\">\nRMS[\\Delta g] _t = \\sqrt{ E[ \\Delta g^2 ] _t + \\epsilon }</script><p>用$RMS[\\Delta g]_{t-1}$代替学习率$\\alpha$，则式$(2.4-4.1)$可以转化为:</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\n\\eta_t &= \\frac{\\alpha}{RMS[g] _t} \\cdot g_t \\\\ &= \\frac{ RMS[\\Delta g] _{t-1} }{ RMS[g] _t } \\cdot g _t\n\\end{align*}\n\\tag{2.4-4.2}</script></li>\n<li><p>根据下降梯度进行更新</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align*}\nw_{t+1} &= w_t - \\eta_t \\\\ &= w_t - \\frac{RMS[\\Delta g] _{t-1}}{RMS[g] _t} \\cdot g_t\n\\end{align*}\n\\tag{2.4-5}</script></li>\n</ol>\n<h3 id=\"2-5-RMSprop\"><a href=\"#2-5-RMSprop\" class=\"headerlink\" title=\"2.5 RMSprop\"></a>2.5 RMSprop</h3><p>RMSprop是AdaDelta算法的一个特例。</p>\n<script type=\"math/tex; mode=display\">\nE[g^2] _t = 0.9 E[g^2] _{t-1} + 0.1 g^2_t</script><script type=\"math/tex; mode=display\">\nw_{t+1} = w_t - \\frac{\\alpha}{ \\sqrt{ E[g^2] _t + \\epsilon } } g_t</script><p>Hinton建议$\\gamma$设置成$0.9$，学习率设置成$0.001$。</p>\n<h3 id=\"2-6-Adam\"><a href=\"#2-6-Adam\" class=\"headerlink\" title=\"2.6 Adam\"></a>2.6 Adam</h3><h3 id=\"2-7-AdaMax\"><a href=\"#2-7-AdaMax\" class=\"headerlink\" title=\"2.7 AdaMax\"></a>2.7 AdaMax</h3><h3 id=\"2-8-Nadam\"><a href=\"#2-8-Nadam\" class=\"headerlink\" title=\"2.8 Nadam\"></a>2.8 Nadam</h3><hr>\n<h2 id=\"3-Other-Optimization-Algorithm\"><a href=\"#3-Other-Optimization-Algorithm\" class=\"headerlink\" title=\"3. Other Optimization Algorithm\"></a>3. Other Optimization Algorithm</h2><ul>\n<li>将不满足阈值的系数设置为0</li>\n</ul>\n<h3 id=\"3-1-Truncated-Gradient\"><a href=\"#3-1-Truncated-Gradient\" class=\"headerlink\" title=\"3.1 Truncated Gradient\"></a>3.1 Truncated Gradient</h3><ul>\n<li>简单截断法的改进</li>\n</ul>\n<h3 id=\"3-2-FOBOS\"><a href=\"#3-2-FOBOS\" class=\"headerlink\" title=\"3.2 FOBOS\"></a>3.2 FOBOS</h3><ul>\n<li>微调标准梯度下降的结果</li>\n</ul>\n<h3 id=\"3-3-RDA\"><a href=\"#3-3-RDA\" class=\"headerlink\" title=\"3.3 RDA\"></a>3.3 RDA</h3><ul>\n<li>历史梯度加权平均</li>\n<li>正则项对特征稀疏化</li>\n<li>严格递增序列</li>\n</ul>\n<h3 id=\"3-4-FTRL\"><a href=\"#3-4-FTRL\" class=\"headerlink\" title=\"3.4 FTRL\"></a>3.4 FTRL</h3><ul>\n<li>确保新的权重和历史权重不偏离太远</li>\n<li>L1正则稀疏性约束</li>\n</ul>\n<hr>\n<blockquote>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1609.04747.pdf\" target=\"_blank\" rel=\"noopener\">An overview of gradient descent optimization algorithms</a></li>\n<li><a href>在线最优化求解(Online Optimization)-冯扬</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/22810533\" target=\"_blank\" rel=\"noopener\">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></li>\n</ul>\n</blockquote>\n"},{"title":"hexo-NexT的Local Search转圈问题解决记录","mathjax":true,"copyright":true,"date":"2019-03-28T12:20:45.000Z","_content":"\n> cite: https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/\n\n有时候更新完文章之后，莫名其妙地Local Search不能用了，这是大部分是因为文章中有非法字符`bs`\n\n## Step1. 检查问题来源\n\n由于使用的是localSearch，在使用`hexo g`的时候，会在public里面生成search.xml作为搜索主体，\n之后使用一些在线验证XML的网站取检查，把search.xml的内容全部拿过去检查，[这里可以用来检查](https://www.xmlvalidation.com/)。\n\n一般都会出来一个这个问题：\n\n![question](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png)\n\n总之，问题就是因为多了这个backspace字符！\n\n\n## Step2. 显示看不到的backspace字符\n\n我是使用VSCODE，开启方式是到在设定中选中renderControlCharacters\n\n![answer](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png)\n\n打开之后你就可以看到那个backspace字符了……\n\n![location](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png)\n\n## Step3. 搜索并替换\n\nbackspace的unicode是\\u0008，\n\n而VSCODE的搜索正则表达式使用的是Rust要輸入\\x{0008}才可以，\n\n其实直接把那个很小的bs框起來复制放到搜索框中即可！！！\n\n## Step4. 重新生成部署\n\n重新在小站目录下执行`hexo d -g`，然后看看线上的Local Search吧！\n","source":"_posts/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录.md","raw":"---\ntitle: hexo-NexT的Local Search转圈问题解决记录\nmathjax: true\ncopyright: true\ndate: 2019-03-28 20:20:45\ncategories: 小工具\n---\n\n> cite: https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/\n\n有时候更新完文章之后，莫名其妙地Local Search不能用了，这是大部分是因为文章中有非法字符`bs`\n\n## Step1. 检查问题来源\n\n由于使用的是localSearch，在使用`hexo g`的时候，会在public里面生成search.xml作为搜索主体，\n之后使用一些在线验证XML的网站取检查，把search.xml的内容全部拿过去检查，[这里可以用来检查](https://www.xmlvalidation.com/)。\n\n一般都会出来一个这个问题：\n\n![question](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png)\n\n总之，问题就是因为多了这个backspace字符！\n\n\n## Step2. 显示看不到的backspace字符\n\n我是使用VSCODE，开启方式是到在设定中选中renderControlCharacters\n\n![answer](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png)\n\n打开之后你就可以看到那个backspace字符了……\n\n![location](/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png)\n\n## Step3. 搜索并替换\n\nbackspace的unicode是\\u0008，\n\n而VSCODE的搜索正则表达式使用的是Rust要輸入\\x{0008}才可以，\n\n其实直接把那个很小的bs框起來复制放到搜索框中即可！！！\n\n## Step4. 重新生成部署\n\n重新在小站目录下执行`hexo d -g`，然后看看线上的Local Search吧！\n","slug":"hexo-NexT的Local-Search转圈问题解决记录","published":1,"updated":"2019-08-17T09:43:00.723Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wo0m005x2qwpp9ltlshl","content":"<blockquote>\n<p>cite: <a href=\"https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/\" target=\"_blank\" rel=\"noopener\">https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/</a></p>\n</blockquote><p>有时候更新完文章之后，莫名其妙地Local Search不能用了，这是大部分是因为文章中有非法字符<code>bs</code></p><h2 id=\"Step1-检查问题来源\"><a href=\"#Step1-检查问题来源\" class=\"headerlink\" title=\"Step1. 检查问题来源\"></a>Step1. 检查问题来源</h2><p>由于使用的是localSearch，在使用<code>hexo g</code>的时候，会在public里面生成search.xml作为搜索主体，\n之后使用一些在线验证XML的网站取检查，把search.xml的内容全部拿过去检查，<a href=\"https://www.xmlvalidation.com/\" target=\"_blank\" rel=\"noopener\">这里可以用来检查</a>。</p><p>一般都会出来一个这个问题：</p><p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png\" alt=\"question\"></p><p>总之，问题就是因为多了这个backspace字符！</p><h2 id=\"Step2-显示看不到的backspace字符\"><a href=\"#Step2-显示看不到的backspace字符\" class=\"headerlink\" title=\"Step2. 显示看不到的backspace字符\"></a>Step2. 显示看不到的backspace字符</h2><a id=\"more\"></a>\n\n\n\n\n\n<p>我是使用VSCODE，开启方式是到在设定中选中renderControlCharacters</p>\n<p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png\" alt=\"answer\"></p>\n<p>打开之后你就可以看到那个backspace字符了……</p>\n<p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png\" alt=\"location\"></p>\n<h2 id=\"Step3-搜索并替换\"><a href=\"#Step3-搜索并替换\" class=\"headerlink\" title=\"Step3. 搜索并替换\"></a>Step3. 搜索并替换</h2><p>backspace的unicode是\\u0008，</p>\n<p>而VSCODE的搜索正则表达式使用的是Rust要輸入\\x{0008}才可以，</p>\n<p>其实直接把那个很小的bs框起來复制放到搜索框中即可！！！</p>\n<h2 id=\"Step4-重新生成部署\"><a href=\"#Step4-重新生成部署\" class=\"headerlink\" title=\"Step4. 重新生成部署\"></a>Step4. 重新生成部署</h2><p>重新在小站目录下执行<code>hexo d -g</code>，然后看看线上的Local Search吧！</p>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>cite: <a href=\"https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/\" target=\"_blank\" rel=\"noopener\">https://guahsu.io/2017/12/Hexo-Next-LocalSearch-cant-work/</a></p>\n</blockquote><p>有时候更新完文章之后，莫名其妙地Local Search不能用了，这是大部分是因为文章中有非法字符<code>bs</code></p><h2 id=\"Step1-检查问题来源\"><a href=\"#Step1-检查问题来源\" class=\"headerlink\" title=\"Step1. 检查问题来源\"></a>Step1. 检查问题来源</h2><p>由于使用的是localSearch，在使用<code>hexo g</code>的时候，会在public里面生成search.xml作为搜索主体，\n之后使用一些在线验证XML的网站取检查，把search.xml的内容全部拿过去检查，<a href=\"https://www.xmlvalidation.com/\" target=\"_blank\" rel=\"noopener\">这里可以用来检查</a>。</p><p>一般都会出来一个这个问题：</p><p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/1.png\" alt=\"question\"></p><p>总之，问题就是因为多了这个backspace字符！</p><h2 id=\"Step2-显示看不到的backspace字符\"><a href=\"#Step2-显示看不到的backspace字符\" class=\"headerlink\" title=\"Step2. 显示看不到的backspace字符\"></a>Step2. 显示看不到的backspace字符</h2>","more":"\n\n\n\n\n\n<p>我是使用VSCODE，开启方式是到在设定中选中renderControlCharacters</p>\n<p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/2.png\" alt=\"answer\"></p>\n<p>打开之后你就可以看到那个backspace字符了……</p>\n<p><img src=\"/posts_res/2019-03-28-hexo-NexT的Local-Search转圈问题解决记录/3.png\" alt=\"location\"></p>\n<h2 id=\"Step3-搜索并替换\"><a href=\"#Step3-搜索并替换\" class=\"headerlink\" title=\"Step3. 搜索并替换\"></a>Step3. 搜索并替换</h2><p>backspace的unicode是\\u0008，</p>\n<p>而VSCODE的搜索正则表达式使用的是Rust要輸入\\x{0008}才可以，</p>\n<p>其实直接把那个很小的bs框起來复制放到搜索框中即可！！！</p>\n<h2 id=\"Step4-重新生成部署\"><a href=\"#Step4-重新生成部署\" class=\"headerlink\" title=\"Step4. 重新生成部署\"></a>Step4. 重新生成部署</h2><p>重新在小站目录下执行<code>hexo d -g</code>，然后看看线上的Local Search吧！</p>\n"},{"title":"pymongo基础","mathjax":true,"copyright":true,"date":"2019-04-24T09:08:51.000Z","_content":"\n## pymongo基础\n\n这篇教程将为我们讲解如何通过pymongo操作MongoDB。\n\n### 准备\n\n首先，需要安装PyMongo，安装成功之后，就可以正常导入了：\n```\n>>> import pymongo\n```\n\n我们也假设你的开发环境已经安装好了MongoDB，并且运行在默认的主机与端口上。运行mongo的命令如下：\n```\n$ mongod\n```\n\n### 创建MongoClient\n\n使用PyMongo的第一步就是创建一个MongoClient来运行mongod实例：\n\n```Python\n>>> from pymongo import MongoClient\n>>> client = MongoClient()\n```\n\n上面的代码会连接到MongoDB的默认主机与端口，当然，也可以明确指定主机与端口：\n```Python\n>>> client = MongoClient('localhost', 27017)\n```\n\n或者使用URL格式：\n```Python\n>>> client = MongoClient('mongodb://localhost:27017/')\n```\n\n### 获取数据库\n\n一个MongoDB的实例可以操作多个独立的数据库，当我们使用PyMongo的时候可以通过MongoClient的属性来获取不同的数据库。\n```Python\n>>> db = client.test_database\n```\n\n如果数据库名比较特殊，直接使用属性不能获取到，比如\"test-databse\", 则可以通过字典形式来获取：\n```Python\n>>> db = client(\"test-database\")\n```\n\n### 获取集合\n\nMongoDB中的集合用来保存一组文档，相当于关系型数据库中的数据表，获取集合的方式如下：\n```Python\n>>> collection = db.test_collection\n```\n\n或者：\n```Python\n>>> collection = db[\"test_collection\"]\n```\n\n要注意的是，数据库以及集合都是延迟创建的，也就是说执行上面的命令实际上不会在MongoDB的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。\n\n### 文档\n\nMongoDB中的数据采用BSON格式来表示和存储，BSON格式是一种二进制的JSON格式。在PyMongo中采用字典来表示文档，例如，下面的字典可以表示一篇博客信息：\n```Python\n>>> import datetime\n>>> post = {\"author\": \"Mike\",\n...         \"text\": \"My first blog post!\",\n...         \"tags\": [\"mongodb\", \"python\", \"pymongo\"],\n...         \"date\": datetime.datetime.utcnow()}\n```\n\n注意文档中可以包含一些Python原生的数据类型，比如datetime.datetime对象，它将会被转换成合适的BSON类型。\n\n### 插入文档\n\n我们使用insert_one()方法来插入文档：\n```Python\n>>> posts = db.posts\n>>> post_id = posts.insert_one(post).inserted_id\n>>> post_id\nObjectId('...')\n```\n\n如果文档中没有`_id`这个属性，那么当它插入集合中的时候，会自动给它赋予这样一个属性，同一个集合中每个文档的`_id`属性值必须具有唯一性。`insert_one()`返回一个`InsertOneResutl`实例。\n\n插入第一个文档之后，集合posts就被创建了，我们可以查看数据库中已经创建好的集合：\n```Python\n>>> db.collection_names(include_system_collections=False)\n[u'posts']\n```\n\n### 使用find_one()获取第一个匹配的文档\n\nMongoDB中最常用的基本操作是`find_one()`, 这个方法返回查询匹配到的第一个文档，如果没有则返回None。下面我们使用find_one()来获取posts集合中的第一个文档：\n```Python\n>>> posts.find_one()\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n查询结果是一个字典，内容正是我们之前插入的博客内容。\n\n我们也可以使用`find_one()`来查找满足匹配要求的文档。例如，下面的例子查找作者名为\"Mike\"的文档。\n```Python\n>>> posts.find_one({\"author\": \"Mike\"})\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n如果我们尝试配置作者名\"Eliot\"，返回结果为空：\n```Python\n>>> posts.find_one({\"author\": \"Eliot\"})\n>>>\n```\n\n### 通过ObjectId进行查询\n\n我们也可以通过ObjectId属性来进行查找，例如：\n```Python\n>>> post_id\nObjectId(...)\n>>> posts.find_one({\"_id\": post_id})\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n注意ObjectId与它的字符串表示形式是不一样的：\n```Python\n>>> post_id_as_str = str(post_id)\n>>> posts.find_one({\"_id\": post_id_as_str}) # No result\n>>>\n```\n\n在Web应用开发过程中，经常面临的一个任务就是从URL请求中获取ObjectId，然后根据这个ID查找匹配的文档。所以有必要在把ID传递给`find_one()`之前，对于字符串形式的id做一个转换，将其转换成ObjectID对象。\n```Python\nfrom bson.objectid import ObjectId\n\n# The web framework gets post_id from the URL and passes it as a string\ndef get(post_id):\n    # Convert from string to ObjectId:\n    document = client.db.collection.find_one({'_id': ObjectId(post_id)})\n```\n\n### 关于Unicode字符串\n\n你可能注意到我们从服务器端获取的数据与我们之前定义的数据格式并不一样， 例如存到数据库之前为\"Mike\",从服务器获取之后，结果为\"u'Mike\",这是为什么？\n\n这是因为，MongoDB采用BSON格式来存储数据，BSON是采用utf-8进行编码的，所以PyMongo必须确保要存储的字符串是合法的utf-8字符串，正常的字符串一般都可存储，但是Unicode字符串不行，所以Unicode字符串在存储之前都必须先转码成utf-8格式，PyMongo在获取数据之后，会将utf-8格式的数据转码成unicode字符串。这也就是为什么会出现编码不一样的问题，\n\n### 批量插入\n\n除了插入单个文档，我们还可以批量插入多个文档，通过给`insert_many()`方法传递一个列表，它会插入列表中的每个文档。\n```Python\n>>> new_posts = [{\"author\": \"Mike\",\n...               \"text\": \"Another post!\",\n...               \"tags\": [\"bulk\", \"insert\"],\n...               \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n...              {\"author\": \"Eliot\",\n...               \"title\": \"MongoDB is fun\",\n...               \"text\": \"and pretty easy too!\",\n...               \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]\n>>> result = posts.insert_many(new_posts)\n>>> result.inserted_ids\n[ObjectId('...'), ObjectId('...')]\n```\n\n### 查询多个文档\n\n为了查询多个文档，我们可以使用`find()`方法，`find()`方法返回一个`Cursor`对象，使用这个对象可以遍历所有匹配的文档。例如下面的例子：\n```Python\n>>> for post in posts.find():\n...   post\n...\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n{u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'}\n```\n\n跟`find_one()`一样，我们也可以给`find()`方法设置匹配条件，例如，查询作者名为\"Mike\"的文档：\n```Python\n>>> for post in posts.find({\"author\": \"Mike\"}):\n...   post\n...\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n```\n\n### 统计\n\n如果我们想获取匹配到文档个数，那么可以使用`count()`方法，使用这个方法还可以获取集合中文档的个数：\n```Python\n>>> posts.count()\n3\n```\n\n获取获取匹配到的文档个数：\n```Python\n>>> posts.find({\"author\": \"Mike\"}).count()\n2\n```\n\n### 范围查询\n\nMongoDB支持多种高级查询，比如范围查询，请看下面这个例子：\n```Python\n>>> d = datetime.datetime(2009, 11, 12, 12)\n>>> for post in posts.find({\"date\": {\"$lt\": d}}).sort(\"author\"):\n...   print post\n...\n{u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n```\n\n这个我们使用了`$lt`操作符实现范围查询，还使用`sort()`方法对查询到的结果进行排序。\n\n### 索引\n\n添加索引可以加快查询的速度，在这个例子中，将展示如何创建一个唯一索引。\n首先，创建索引:\n```Python\n>>> result = db.profiles.create_index([('user_id', pymongo.ASCENDING)],\n...                                   unique=True)\n>>> list(db.profiles.index_information())\n[u'user_id_1', u'_id_']\n```\n\n注意我们现在有两个索引了，一个在_id上，它由MongoDB自动创建，另外一个就是刚刚创建的索引了。\n\n然后添加几个文档：\n```Python\n>>> user_profiles = [\n...     {'user_id': 211, 'name': 'Luke'},\n...     {'user_id': 212, 'name': 'Ziltoid'}]\n>>> result = db.profiles.insert_many(user_profiles)\n```\n\n这个索引会阻止我们插入已经存在的user_id：\n```Python\n>>> new_profile = {'user_id': 213, 'name': 'Drew'}\n>>> duplicate_profile = {'user_id': 212, 'name': 'Tommy'}\n>>> result = db.profiles.insert_one(new_profile)  # This is fine.\n>>> result = db.profiles.insert_one(duplicate_profile)\nTraceback (most recent call last):\npymongo.errors.DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: { : 212 }\n```\n","source":"_posts/2019-04-24-pymongo基础.md","raw":"---\ntitle: pymongo基础\ntags:\n  - 数据库\nmathjax: true\ncopyright: true\ndate: 2019-04-24 17:08:51\ncategories: 大数据\n---\n\n## pymongo基础\n\n这篇教程将为我们讲解如何通过pymongo操作MongoDB。\n\n### 准备\n\n首先，需要安装PyMongo，安装成功之后，就可以正常导入了：\n```\n>>> import pymongo\n```\n\n我们也假设你的开发环境已经安装好了MongoDB，并且运行在默认的主机与端口上。运行mongo的命令如下：\n```\n$ mongod\n```\n\n### 创建MongoClient\n\n使用PyMongo的第一步就是创建一个MongoClient来运行mongod实例：\n\n```Python\n>>> from pymongo import MongoClient\n>>> client = MongoClient()\n```\n\n上面的代码会连接到MongoDB的默认主机与端口，当然，也可以明确指定主机与端口：\n```Python\n>>> client = MongoClient('localhost', 27017)\n```\n\n或者使用URL格式：\n```Python\n>>> client = MongoClient('mongodb://localhost:27017/')\n```\n\n### 获取数据库\n\n一个MongoDB的实例可以操作多个独立的数据库，当我们使用PyMongo的时候可以通过MongoClient的属性来获取不同的数据库。\n```Python\n>>> db = client.test_database\n```\n\n如果数据库名比较特殊，直接使用属性不能获取到，比如\"test-databse\", 则可以通过字典形式来获取：\n```Python\n>>> db = client(\"test-database\")\n```\n\n### 获取集合\n\nMongoDB中的集合用来保存一组文档，相当于关系型数据库中的数据表，获取集合的方式如下：\n```Python\n>>> collection = db.test_collection\n```\n\n或者：\n```Python\n>>> collection = db[\"test_collection\"]\n```\n\n要注意的是，数据库以及集合都是延迟创建的，也就是说执行上面的命令实际上不会在MongoDB的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。\n\n### 文档\n\nMongoDB中的数据采用BSON格式来表示和存储，BSON格式是一种二进制的JSON格式。在PyMongo中采用字典来表示文档，例如，下面的字典可以表示一篇博客信息：\n```Python\n>>> import datetime\n>>> post = {\"author\": \"Mike\",\n...         \"text\": \"My first blog post!\",\n...         \"tags\": [\"mongodb\", \"python\", \"pymongo\"],\n...         \"date\": datetime.datetime.utcnow()}\n```\n\n注意文档中可以包含一些Python原生的数据类型，比如datetime.datetime对象，它将会被转换成合适的BSON类型。\n\n### 插入文档\n\n我们使用insert_one()方法来插入文档：\n```Python\n>>> posts = db.posts\n>>> post_id = posts.insert_one(post).inserted_id\n>>> post_id\nObjectId('...')\n```\n\n如果文档中没有`_id`这个属性，那么当它插入集合中的时候，会自动给它赋予这样一个属性，同一个集合中每个文档的`_id`属性值必须具有唯一性。`insert_one()`返回一个`InsertOneResutl`实例。\n\n插入第一个文档之后，集合posts就被创建了，我们可以查看数据库中已经创建好的集合：\n```Python\n>>> db.collection_names(include_system_collections=False)\n[u'posts']\n```\n\n### 使用find_one()获取第一个匹配的文档\n\nMongoDB中最常用的基本操作是`find_one()`, 这个方法返回查询匹配到的第一个文档，如果没有则返回None。下面我们使用find_one()来获取posts集合中的第一个文档：\n```Python\n>>> posts.find_one()\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n查询结果是一个字典，内容正是我们之前插入的博客内容。\n\n我们也可以使用`find_one()`来查找满足匹配要求的文档。例如，下面的例子查找作者名为\"Mike\"的文档。\n```Python\n>>> posts.find_one({\"author\": \"Mike\"})\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n如果我们尝试配置作者名\"Eliot\"，返回结果为空：\n```Python\n>>> posts.find_one({\"author\": \"Eliot\"})\n>>>\n```\n\n### 通过ObjectId进行查询\n\n我们也可以通过ObjectId属性来进行查找，例如：\n```Python\n>>> post_id\nObjectId(...)\n>>> posts.find_one({\"_id\": post_id})\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n```\n\n注意ObjectId与它的字符串表示形式是不一样的：\n```Python\n>>> post_id_as_str = str(post_id)\n>>> posts.find_one({\"_id\": post_id_as_str}) # No result\n>>>\n```\n\n在Web应用开发过程中，经常面临的一个任务就是从URL请求中获取ObjectId，然后根据这个ID查找匹配的文档。所以有必要在把ID传递给`find_one()`之前，对于字符串形式的id做一个转换，将其转换成ObjectID对象。\n```Python\nfrom bson.objectid import ObjectId\n\n# The web framework gets post_id from the URL and passes it as a string\ndef get(post_id):\n    # Convert from string to ObjectId:\n    document = client.db.collection.find_one({'_id': ObjectId(post_id)})\n```\n\n### 关于Unicode字符串\n\n你可能注意到我们从服务器端获取的数据与我们之前定义的数据格式并不一样， 例如存到数据库之前为\"Mike\",从服务器获取之后，结果为\"u'Mike\",这是为什么？\n\n这是因为，MongoDB采用BSON格式来存储数据，BSON是采用utf-8进行编码的，所以PyMongo必须确保要存储的字符串是合法的utf-8字符串，正常的字符串一般都可存储，但是Unicode字符串不行，所以Unicode字符串在存储之前都必须先转码成utf-8格式，PyMongo在获取数据之后，会将utf-8格式的数据转码成unicode字符串。这也就是为什么会出现编码不一样的问题，\n\n### 批量插入\n\n除了插入单个文档，我们还可以批量插入多个文档，通过给`insert_many()`方法传递一个列表，它会插入列表中的每个文档。\n```Python\n>>> new_posts = [{\"author\": \"Mike\",\n...               \"text\": \"Another post!\",\n...               \"tags\": [\"bulk\", \"insert\"],\n...               \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n...              {\"author\": \"Eliot\",\n...               \"title\": \"MongoDB is fun\",\n...               \"text\": \"and pretty easy too!\",\n...               \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]\n>>> result = posts.insert_many(new_posts)\n>>> result.inserted_ids\n[ObjectId('...'), ObjectId('...')]\n```\n\n### 查询多个文档\n\n为了查询多个文档，我们可以使用`find()`方法，`find()`方法返回一个`Cursor`对象，使用这个对象可以遍历所有匹配的文档。例如下面的例子：\n```Python\n>>> for post in posts.find():\n...   post\n...\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n{u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'}\n```\n\n跟`find_one()`一样，我们也可以给`find()`方法设置匹配条件，例如，查询作者名为\"Mike\"的文档：\n```Python\n>>> for post in posts.find({\"author\": \"Mike\"}):\n...   post\n...\n{u'date': datetime.datetime(...), u'text': u'My first blog post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'mongodb', u'python', u'pymongo']}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n```\n\n### 统计\n\n如果我们想获取匹配到文档个数，那么可以使用`count()`方法，使用这个方法还可以获取集合中文档的个数：\n```Python\n>>> posts.count()\n3\n```\n\n获取获取匹配到的文档个数：\n```Python\n>>> posts.find({\"author\": \"Mike\"}).count()\n2\n```\n\n### 范围查询\n\nMongoDB支持多种高级查询，比如范围查询，请看下面这个例子：\n```Python\n>>> d = datetime.datetime(2009, 11, 12, 12)\n>>> for post in posts.find({\"date\": {\"$lt\": d}}).sort(\"author\"):\n...   print post\n...\n{u'date': datetime.datetime(2009, 11, 10, 10, 45), u'text': u'and pretty easy too!', u'_id': ObjectId('...'), u'author': u'Eliot', u'title': u'MongoDB is fun'}\n{u'date': datetime.datetime(2009, 11, 12, 11, 14), u'text': u'Another post!', u'_id': ObjectId('...'), u'author': u'Mike', u'tags': [u'bulk', u'insert']}\n```\n\n这个我们使用了`$lt`操作符实现范围查询，还使用`sort()`方法对查询到的结果进行排序。\n\n### 索引\n\n添加索引可以加快查询的速度，在这个例子中，将展示如何创建一个唯一索引。\n首先，创建索引:\n```Python\n>>> result = db.profiles.create_index([('user_id', pymongo.ASCENDING)],\n...                                   unique=True)\n>>> list(db.profiles.index_information())\n[u'user_id_1', u'_id_']\n```\n\n注意我们现在有两个索引了，一个在_id上，它由MongoDB自动创建，另外一个就是刚刚创建的索引了。\n\n然后添加几个文档：\n```Python\n>>> user_profiles = [\n...     {'user_id': 211, 'name': 'Luke'},\n...     {'user_id': 212, 'name': 'Ziltoid'}]\n>>> result = db.profiles.insert_many(user_profiles)\n```\n\n这个索引会阻止我们插入已经存在的user_id：\n```Python\n>>> new_profile = {'user_id': 213, 'name': 'Drew'}\n>>> duplicate_profile = {'user_id': 212, 'name': 'Tommy'}\n>>> result = db.profiles.insert_one(new_profile)  # This is fine.\n>>> result = db.profiles.insert_one(duplicate_profile)\nTraceback (most recent call last):\npymongo.errors.DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: { : 212 }\n```\n","slug":"pymongo基础","published":1,"updated":"2019-08-18T11:21:19.381Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wo0n005z2qwp813fszuo","content":"<h2 id=\"pymongo基础\"><a href=\"#pymongo基础\" class=\"headerlink\" title=\"pymongo基础\"></a>pymongo基础</h2><p>这篇教程将为我们讲解如何通过pymongo操作MongoDB。</p><h3 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h3><p>首先，需要安装PyMongo，安装成功之后，就可以正常导入了：\n<figure class=\"highlight elm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; <span class=\"keyword\">import</span> pymongo</span><br></pre></td></tr></table></figure></p><p>我们也假设你的开发环境已经安装好了MongoDB，并且运行在默认的主机与端口上。运行mongo的命令如下：\n<figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$ </span>mongod</span><br></pre></td></tr></table></figure></p><h3 id=\"创建MongoClient\"><a href=\"#创建MongoClient\" class=\"headerlink\" title=\"创建MongoClient\"></a>创建MongoClient</h3><p>使用PyMongo的第一步就是创建一个MongoClient来运行mongod实例：</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> pymongo <span class=\"keyword\">import</span> MongoClient</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient()</span><br></pre></td></tr></table></figure><a id=\"more\"></a>\n\n\n\n\n<p>上面的代码会连接到MongoDB的默认主机与端口，当然，也可以明确指定主机与端口：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient(<span class=\"string\">'localhost'</span>, <span class=\"number\">27017</span>)</span><br></pre></td></tr></table></figure></p>\n<p>或者使用URL格式：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient(<span class=\"string\">'mongodb://localhost:27017/'</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"获取数据库\"><a href=\"#获取数据库\" class=\"headerlink\" title=\"获取数据库\"></a>获取数据库</h3><p>一个MongoDB的实例可以操作多个独立的数据库，当我们使用PyMongo的时候可以通过MongoClient的属性来获取不同的数据库。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db = client.test_database</span><br></pre></td></tr></table></figure></p>\n<p>如果数据库名比较特殊，直接使用属性不能获取到，比如”test-databse”, 则可以通过字典形式来获取：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db = client(<span class=\"string\">\"test-database\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"获取集合\"><a href=\"#获取集合\" class=\"headerlink\" title=\"获取集合\"></a>获取集合</h3><p>MongoDB中的集合用来保存一组文档，相当于关系型数据库中的数据表，获取集合的方式如下：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>collection = db.test_collection</span><br></pre></td></tr></table></figure></p>\n<p>或者：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>collection = db[<span class=\"string\">\"test_collection\"</span>]</span><br></pre></td></tr></table></figure></p>\n<p>要注意的是，数据库以及集合都是延迟创建的，也就是说执行上面的命令实际上不会在MongoDB的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。</p>\n<h3 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h3><p>MongoDB中的数据采用BSON格式来表示和存储，BSON格式是一种二进制的JSON格式。在PyMongo中采用字典来表示文档，例如，下面的字典可以表示一篇博客信息：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> datetime</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post = &#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"text\"</span>: <span class=\"string\">\"My first blog post!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"tags\"</span>: [<span class=\"string\">\"mongodb\"</span>, <span class=\"string\">\"python\"</span>, <span class=\"string\">\"pymongo\"</span>],</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"date\"</span>: datetime.datetime.utcnow()&#125;</span><br></pre></td></tr></table></figure></p>\n<p>注意文档中可以包含一些Python原生的数据类型，比如datetime.datetime对象，它将会被转换成合适的BSON类型。</p>\n<h3 id=\"插入文档\"><a href=\"#插入文档\" class=\"headerlink\" title=\"插入文档\"></a>插入文档</h3><p>我们使用insert_one()方法来插入文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts = db.posts</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id = posts.insert_one(post).inserted_id</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id</span><br><span class=\"line\">ObjectId(<span class=\"string\">'...'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>如果文档中没有<code>_id</code>这个属性，那么当它插入集合中的时候，会自动给它赋予这样一个属性，同一个集合中每个文档的<code>_id</code>属性值必须具有唯一性。<code>insert_one()</code>返回一个<code>InsertOneResutl</code>实例。</p>\n<p>插入第一个文档之后，集合posts就被创建了，我们可以查看数据库中已经创建好的集合：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db.collection_names(include_system_collections=<span class=\"literal\">False</span>)</span><br><span class=\"line\">[<span class=\"string\">u'posts'</span>]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用find-one-获取第一个匹配的文档\"><a href=\"#使用find-one-获取第一个匹配的文档\" class=\"headerlink\" title=\"使用find_one()获取第一个匹配的文档\"></a>使用find_one()获取第一个匹配的文档</h3><p>MongoDB中最常用的基本操作是<code>find_one()</code>, 这个方法返回查询匹配到的第一个文档，如果没有则返回None。下面我们使用find_one()来获取posts集合中的第一个文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one()</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>查询结果是一个字典，内容正是我们之前插入的博客内容。</p>\n<p>我们也可以使用<code>find_one()</code>来查找满足匹配要求的文档。例如，下面的例子查找作者名为”Mike”的文档。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;)</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果我们尝试配置作者名”Eliot”，返回结果为空：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Eliot\"</span>&#125;)</span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"通过ObjectId进行查询\"><a href=\"#通过ObjectId进行查询\" class=\"headerlink\" title=\"通过ObjectId进行查询\"></a>通过ObjectId进行查询</h3><p>我们也可以通过ObjectId属性来进行查找，例如：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id</span><br><span class=\"line\">ObjectId(...)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"_id\"</span>: post_id&#125;)</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>注意ObjectId与它的字符串表示形式是不一样的：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id_as_str = str(post_id)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"_id\"</span>: post_id_as_str&#125;) <span class=\"comment\"># No result</span></span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在Web应用开发过程中，经常面临的一个任务就是从URL请求中获取ObjectId，然后根据这个ID查找匹配的文档。所以有必要在把ID传递给<code>find_one()</code>之前，对于字符串形式的id做一个转换，将其转换成ObjectID对象。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bson.objectid <span class=\"keyword\">import</span> ObjectId</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The web framework gets post_id from the URL and passes it as a string</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span><span class=\"params\">(post_id)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Convert from string to ObjectId:</span></span><br><span class=\"line\">    document = client.db.collection.find_one(&#123;<span class=\"string\">'_id'</span>: ObjectId(post_id)&#125;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"关于Unicode字符串\"><a href=\"#关于Unicode字符串\" class=\"headerlink\" title=\"关于Unicode字符串\"></a>关于Unicode字符串</h3><p>你可能注意到我们从服务器端获取的数据与我们之前定义的数据格式并不一样， 例如存到数据库之前为”Mike”,从服务器获取之后，结果为”u’Mike”,这是为什么？</p>\n<p>这是因为，MongoDB采用BSON格式来存储数据，BSON是采用utf-8进行编码的，所以PyMongo必须确保要存储的字符串是合法的utf-8字符串，正常的字符串一般都可存储，但是Unicode字符串不行，所以Unicode字符串在存储之前都必须先转码成utf-8格式，PyMongo在获取数据之后，会将utf-8格式的数据转码成unicode字符串。这也就是为什么会出现编码不一样的问题，</p>\n<h3 id=\"批量插入\"><a href=\"#批量插入\" class=\"headerlink\" title=\"批量插入\"></a>批量插入</h3><p>除了插入单个文档，我们还可以批量插入多个文档，通过给<code>insert_many()</code>方法传递一个列表，它会插入列表中的每个文档。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>new_posts = [&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"text\"</span>: <span class=\"string\">\"Another post!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"tags\"</span>: [<span class=\"string\">\"bulk\"</span>, <span class=\"string\">\"insert\"</span>],</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"date\"</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>)&#125;,</span><br><span class=\"line\"><span class=\"meta\">... </span>             &#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Eliot\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"title\"</span>: <span class=\"string\">\"MongoDB is fun\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"text\"</span>: <span class=\"string\">\"and pretty easy too!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"date\"</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>)&#125;]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = posts.insert_many(new_posts)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result.inserted_ids</span><br><span class=\"line\">[ObjectId(<span class=\"string\">'...'</span>), ObjectId(<span class=\"string\">'...'</span>)]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查询多个文档\"><a href=\"#查询多个文档\" class=\"headerlink\" title=\"查询多个文档\"></a>查询多个文档</h3><p>为了查询多个文档，我们可以使用<code>find()</code>方法，<code>find()</code>方法返回一个<code>Cursor</code>对象，使用这个对象可以遍历所有匹配的文档。例如下面的例子：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find():</span><br><span class=\"line\"><span class=\"meta\">... </span>  post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'and pretty easy too!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Eliot'</span>, <span class=\"string\">u'title'</span>: <span class=\"string\">u'MongoDB is fun'</span>&#125;</span><br></pre></td></tr></table></figure></p>\n<p>跟<code>find_one()</code>一样，我们也可以给<code>find()</code>方法设置匹配条件，例如，查询作者名为”Mike”的文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;):</span><br><span class=\"line\"><span class=\"meta\">... </span>  post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"统计\"><a href=\"#统计\" class=\"headerlink\" title=\"统计\"></a>统计</h3><p>如果我们想获取匹配到文档个数，那么可以使用<code>count()</code>方法，使用这个方法还可以获取集合中文档的个数：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.count()</span><br><span class=\"line\"><span class=\"number\">3</span></span><br></pre></td></tr></table></figure></p>\n<p>获取获取匹配到的文档个数：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;).count()</span><br><span class=\"line\"><span class=\"number\">2</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"范围查询\"><a href=\"#范围查询\" class=\"headerlink\" title=\"范围查询\"></a>范围查询</h3><p>MongoDB支持多种高级查询，比如范围查询，请看下面这个例子：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find(&#123;<span class=\"string\">\"date\"</span>: &#123;<span class=\"string\">\"$lt\"</span>: d&#125;&#125;).sort(<span class=\"string\">\"author\"</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>  <span class=\"keyword\">print</span> post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'and pretty easy too!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Eliot'</span>, <span class=\"string\">u'title'</span>: <span class=\"string\">u'MongoDB is fun'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个我们使用了<code>$lt</code>操作符实现范围查询，还使用<code>sort()</code>方法对查询到的结果进行排序。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>添加索引可以加快查询的速度，在这个例子中，将展示如何创建一个唯一索引。\n首先，创建索引:\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.create_index([(<span class=\"string\">'user_id'</span>, pymongo.ASCENDING)],</span><br><span class=\"line\"><span class=\"meta\">... </span>                                  unique=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>list(db.profiles.index_information())</span><br><span class=\"line\">[<span class=\"string\">u'user_id_1'</span>, <span class=\"string\">u'_id_'</span>]</span><br></pre></td></tr></table></figure></p>\n<p>注意我们现在有两个索引了，一个在_id上，它由MongoDB自动创建，另外一个就是刚刚创建的索引了。</p>\n<p>然后添加几个文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>user_profiles = [</span><br><span class=\"line\"><span class=\"meta\">... </span>    &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">211</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Luke'</span>&#125;,</span><br><span class=\"line\"><span class=\"meta\">... </span>    &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">212</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Ziltoid'</span>&#125;]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_many(user_profiles)</span><br></pre></td></tr></table></figure></p>\n<p>这个索引会阻止我们插入已经存在的user_id：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>new_profile = &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">213</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Drew'</span>&#125;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>duplicate_profile = &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">212</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Tommy'</span>&#125;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_one(new_profile)  <span class=\"comment\"># This is fine.</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_one(duplicate_profile)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">pymongo.errors.DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: &#123; : <span class=\"number\">212</span> &#125;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"pymongo基础\"><a href=\"#pymongo基础\" class=\"headerlink\" title=\"pymongo基础\"></a>pymongo基础</h2><p>这篇教程将为我们讲解如何通过pymongo操作MongoDB。</p><h3 id=\"准备\"><a href=\"#准备\" class=\"headerlink\" title=\"准备\"></a>准备</h3><p>首先，需要安装PyMongo，安装成功之后，就可以正常导入了：\n<figure class=\"highlight elm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt;&gt;&gt; <span class=\"keyword\">import</span> pymongo</span><br></pre></td></tr></table></figure></p><p>我们也假设你的开发环境已经安装好了MongoDB，并且运行在默认的主机与端口上。运行mongo的命令如下：\n<figure class=\"highlight elixir\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$ </span>mongod</span><br></pre></td></tr></table></figure></p><h3 id=\"创建MongoClient\"><a href=\"#创建MongoClient\" class=\"headerlink\" title=\"创建MongoClient\"></a>创建MongoClient</h3><p>使用PyMongo的第一步就是创建一个MongoClient来运行mongod实例：</p><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">from</span> pymongo <span class=\"keyword\">import</span> MongoClient</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient()</span><br></pre></td></tr></table></figure>","more":"\n\n\n\n\n<p>上面的代码会连接到MongoDB的默认主机与端口，当然，也可以明确指定主机与端口：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient(<span class=\"string\">'localhost'</span>, <span class=\"number\">27017</span>)</span><br></pre></td></tr></table></figure></p>\n<p>或者使用URL格式：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>client = MongoClient(<span class=\"string\">'mongodb://localhost:27017/'</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"获取数据库\"><a href=\"#获取数据库\" class=\"headerlink\" title=\"获取数据库\"></a>获取数据库</h3><p>一个MongoDB的实例可以操作多个独立的数据库，当我们使用PyMongo的时候可以通过MongoClient的属性来获取不同的数据库。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db = client.test_database</span><br></pre></td></tr></table></figure></p>\n<p>如果数据库名比较特殊，直接使用属性不能获取到，比如”test-databse”, 则可以通过字典形式来获取：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db = client(<span class=\"string\">\"test-database\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"获取集合\"><a href=\"#获取集合\" class=\"headerlink\" title=\"获取集合\"></a>获取集合</h3><p>MongoDB中的集合用来保存一组文档，相当于关系型数据库中的数据表，获取集合的方式如下：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>collection = db.test_collection</span><br></pre></td></tr></table></figure></p>\n<p>或者：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>collection = db[<span class=\"string\">\"test_collection\"</span>]</span><br></pre></td></tr></table></figure></p>\n<p>要注意的是，数据库以及集合都是延迟创建的，也就是说执行上面的命令实际上不会在MongoDB的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。</p>\n<h3 id=\"文档\"><a href=\"#文档\" class=\"headerlink\" title=\"文档\"></a>文档</h3><p>MongoDB中的数据采用BSON格式来表示和存储，BSON格式是一种二进制的JSON格式。在PyMongo中采用字典来表示文档，例如，下面的字典可以表示一篇博客信息：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">import</span> datetime</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post = &#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"text\"</span>: <span class=\"string\">\"My first blog post!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"tags\"</span>: [<span class=\"string\">\"mongodb\"</span>, <span class=\"string\">\"python\"</span>, <span class=\"string\">\"pymongo\"</span>],</span><br><span class=\"line\"><span class=\"meta\">... </span>        <span class=\"string\">\"date\"</span>: datetime.datetime.utcnow()&#125;</span><br></pre></td></tr></table></figure></p>\n<p>注意文档中可以包含一些Python原生的数据类型，比如datetime.datetime对象，它将会被转换成合适的BSON类型。</p>\n<h3 id=\"插入文档\"><a href=\"#插入文档\" class=\"headerlink\" title=\"插入文档\"></a>插入文档</h3><p>我们使用insert_one()方法来插入文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts = db.posts</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id = posts.insert_one(post).inserted_id</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id</span><br><span class=\"line\">ObjectId(<span class=\"string\">'...'</span>)</span><br></pre></td></tr></table></figure></p>\n<p>如果文档中没有<code>_id</code>这个属性，那么当它插入集合中的时候，会自动给它赋予这样一个属性，同一个集合中每个文档的<code>_id</code>属性值必须具有唯一性。<code>insert_one()</code>返回一个<code>InsertOneResutl</code>实例。</p>\n<p>插入第一个文档之后，集合posts就被创建了，我们可以查看数据库中已经创建好的集合：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>db.collection_names(include_system_collections=<span class=\"literal\">False</span>)</span><br><span class=\"line\">[<span class=\"string\">u'posts'</span>]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用find-one-获取第一个匹配的文档\"><a href=\"#使用find-one-获取第一个匹配的文档\" class=\"headerlink\" title=\"使用find_one()获取第一个匹配的文档\"></a>使用find_one()获取第一个匹配的文档</h3><p>MongoDB中最常用的基本操作是<code>find_one()</code>, 这个方法返回查询匹配到的第一个文档，如果没有则返回None。下面我们使用find_one()来获取posts集合中的第一个文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one()</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>查询结果是一个字典，内容正是我们之前插入的博客内容。</p>\n<p>我们也可以使用<code>find_one()</code>来查找满足匹配要求的文档。例如，下面的例子查找作者名为”Mike”的文档。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;)</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>如果我们尝试配置作者名”Eliot”，返回结果为空：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Eliot\"</span>&#125;)</span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"通过ObjectId进行查询\"><a href=\"#通过ObjectId进行查询\" class=\"headerlink\" title=\"通过ObjectId进行查询\"></a>通过ObjectId进行查询</h3><p>我们也可以通过ObjectId属性来进行查找，例如：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id</span><br><span class=\"line\">ObjectId(...)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"_id\"</span>: post_id&#125;)</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>注意ObjectId与它的字符串表示形式是不一样的：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>post_id_as_str = str(post_id)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find_one(&#123;<span class=\"string\">\"_id\"</span>: post_id_as_str&#125;) <span class=\"comment\"># No result</span></span><br><span class=\"line\">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p>\n<p>在Web应用开发过程中，经常面临的一个任务就是从URL请求中获取ObjectId，然后根据这个ID查找匹配的文档。所以有必要在把ID传递给<code>find_one()</code>之前，对于字符串形式的id做一个转换，将其转换成ObjectID对象。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bson.objectid <span class=\"keyword\">import</span> ObjectId</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># The web framework gets post_id from the URL and passes it as a string</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get</span><span class=\"params\">(post_id)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Convert from string to ObjectId:</span></span><br><span class=\"line\">    document = client.db.collection.find_one(&#123;<span class=\"string\">'_id'</span>: ObjectId(post_id)&#125;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"关于Unicode字符串\"><a href=\"#关于Unicode字符串\" class=\"headerlink\" title=\"关于Unicode字符串\"></a>关于Unicode字符串</h3><p>你可能注意到我们从服务器端获取的数据与我们之前定义的数据格式并不一样， 例如存到数据库之前为”Mike”,从服务器获取之后，结果为”u’Mike”,这是为什么？</p>\n<p>这是因为，MongoDB采用BSON格式来存储数据，BSON是采用utf-8进行编码的，所以PyMongo必须确保要存储的字符串是合法的utf-8字符串，正常的字符串一般都可存储，但是Unicode字符串不行，所以Unicode字符串在存储之前都必须先转码成utf-8格式，PyMongo在获取数据之后，会将utf-8格式的数据转码成unicode字符串。这也就是为什么会出现编码不一样的问题，</p>\n<h3 id=\"批量插入\"><a href=\"#批量插入\" class=\"headerlink\" title=\"批量插入\"></a>批量插入</h3><p>除了插入单个文档，我们还可以批量插入多个文档，通过给<code>insert_many()</code>方法传递一个列表，它会插入列表中的每个文档。\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>new_posts = [&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"text\"</span>: <span class=\"string\">\"Another post!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"tags\"</span>: [<span class=\"string\">\"bulk\"</span>, <span class=\"string\">\"insert\"</span>],</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"date\"</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>)&#125;,</span><br><span class=\"line\"><span class=\"meta\">... </span>             &#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Eliot\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"title\"</span>: <span class=\"string\">\"MongoDB is fun\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"text\"</span>: <span class=\"string\">\"and pretty easy too!\"</span>,</span><br><span class=\"line\"><span class=\"meta\">... </span>              <span class=\"string\">\"date\"</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>)&#125;]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = posts.insert_many(new_posts)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result.inserted_ids</span><br><span class=\"line\">[ObjectId(<span class=\"string\">'...'</span>), ObjectId(<span class=\"string\">'...'</span>)]</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"查询多个文档\"><a href=\"#查询多个文档\" class=\"headerlink\" title=\"查询多个文档\"></a>查询多个文档</h3><p>为了查询多个文档，我们可以使用<code>find()</code>方法，<code>find()</code>方法返回一个<code>Cursor</code>对象，使用这个对象可以遍历所有匹配的文档。例如下面的例子：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find():</span><br><span class=\"line\"><span class=\"meta\">... </span>  post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'and pretty easy too!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Eliot'</span>, <span class=\"string\">u'title'</span>: <span class=\"string\">u'MongoDB is fun'</span>&#125;</span><br></pre></td></tr></table></figure></p>\n<p>跟<code>find_one()</code>一样，我们也可以给<code>find()</code>方法设置匹配条件，例如，查询作者名为”Mike”的文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;):</span><br><span class=\"line\"><span class=\"meta\">... </span>  post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(...), <span class=\"string\">u'text'</span>: <span class=\"string\">u'My first blog post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'mongodb'</span>, <span class=\"string\">u'python'</span>, <span class=\"string\">u'pymongo'</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"统计\"><a href=\"#统计\" class=\"headerlink\" title=\"统计\"></a>统计</h3><p>如果我们想获取匹配到文档个数，那么可以使用<code>count()</code>方法，使用这个方法还可以获取集合中文档的个数：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.count()</span><br><span class=\"line\"><span class=\"number\">3</span></span><br></pre></td></tr></table></figure></p>\n<p>获取获取匹配到的文档个数：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>posts.find(&#123;<span class=\"string\">\"author\"</span>: <span class=\"string\">\"Mike\"</span>&#125;).count()</span><br><span class=\"line\"><span class=\"number\">2</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"范围查询\"><a href=\"#范围查询\" class=\"headerlink\" title=\"范围查询\"></a>范围查询</h3><p>MongoDB支持多种高级查询，比如范围查询，请看下面这个例子：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>d = datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">12</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span><span class=\"keyword\">for</span> post <span class=\"keyword\">in</span> posts.find(&#123;<span class=\"string\">\"date\"</span>: &#123;<span class=\"string\">\"$lt\"</span>: d&#125;&#125;).sort(<span class=\"string\">\"author\"</span>):</span><br><span class=\"line\"><span class=\"meta\">... </span>  <span class=\"keyword\">print</span> post</span><br><span class=\"line\">...</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">10</span>, <span class=\"number\">10</span>, <span class=\"number\">45</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'and pretty easy too!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Eliot'</span>, <span class=\"string\">u'title'</span>: <span class=\"string\">u'MongoDB is fun'</span>&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">u'date'</span>: datetime.datetime(<span class=\"number\">2009</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">11</span>, <span class=\"number\">14</span>), <span class=\"string\">u'text'</span>: <span class=\"string\">u'Another post!'</span>, <span class=\"string\">u'_id'</span>: ObjectId(<span class=\"string\">'...'</span>), <span class=\"string\">u'author'</span>: <span class=\"string\">u'Mike'</span>, <span class=\"string\">u'tags'</span>: [<span class=\"string\">u'bulk'</span>, <span class=\"string\">u'insert'</span>]&#125;</span><br></pre></td></tr></table></figure></p>\n<p>这个我们使用了<code>$lt</code>操作符实现范围查询，还使用<code>sort()</code>方法对查询到的结果进行排序。</p>\n<h3 id=\"索引\"><a href=\"#索引\" class=\"headerlink\" title=\"索引\"></a>索引</h3><p>添加索引可以加快查询的速度，在这个例子中，将展示如何创建一个唯一索引。\n首先，创建索引:\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.create_index([(<span class=\"string\">'user_id'</span>, pymongo.ASCENDING)],</span><br><span class=\"line\"><span class=\"meta\">... </span>                                  unique=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>list(db.profiles.index_information())</span><br><span class=\"line\">[<span class=\"string\">u'user_id_1'</span>, <span class=\"string\">u'_id_'</span>]</span><br></pre></td></tr></table></figure></p>\n<p>注意我们现在有两个索引了，一个在_id上，它由MongoDB自动创建，另外一个就是刚刚创建的索引了。</p>\n<p>然后添加几个文档：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>user_profiles = [</span><br><span class=\"line\"><span class=\"meta\">... </span>    &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">211</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Luke'</span>&#125;,</span><br><span class=\"line\"><span class=\"meta\">... </span>    &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">212</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Ziltoid'</span>&#125;]</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_many(user_profiles)</span><br></pre></td></tr></table></figure></p>\n<p>这个索引会阻止我们插入已经存在的user_id：\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>new_profile = &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">213</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Drew'</span>&#125;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>duplicate_profile = &#123;<span class=\"string\">'user_id'</span>: <span class=\"number\">212</span>, <span class=\"string\">'name'</span>: <span class=\"string\">'Tommy'</span>&#125;</span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_one(new_profile)  <span class=\"comment\"># This is fine.</span></span><br><span class=\"line\"><span class=\"meta\">&gt;&gt;&gt; </span>result = db.profiles.insert_one(duplicate_profile)</span><br><span class=\"line\">Traceback (most recent call last):</span><br><span class=\"line\">pymongo.errors.DuplicateKeyError: E11000 duplicate key error index: test_database.profiles.$user_id_1 dup key: &#123; : <span class=\"number\">212</span> &#125;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Git基础","mathjax":true,"copyright":true,"date":"2019-03-29T03:23:55.000Z","_content":"\n> learn from: [廖雪峰的官方网站-Git教程](https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000)\n\n\n## 1. 创建版本库\n\n- 进入到准备当作Project的目录下，执行`git init`，生成隐藏文件夹`.git`即创建成功。\n- `git add <file>`：用来将file添加到暂存区Stage中。\n- `git commit -m \"<some messages>\"`：用来将暂存区Stage中的修改内容提交到版本库Version中。\n\n## 2. 版本控制\n\n- `git log`：用来从最近到最远的commit日志。\n  - `git log --pretty=oneline`：增加参数pretty，用来简洁输出commit日志。\n- `git reset --hard HEAD^`：用来将版本库中的master分支退回到上一个版本。\n- `git reflog`：显示所有记录的命令，可以用来回到任意版本 `git reset --hard <commit_id>`。\n- 工作区、暂存区、版本库关系如下：\n\n![work-stage-version](/posts_res/2019-03-29-git基础/1.jpg)\n\n- `git diff`应用场景、管理修改。\n\n![git-diff](/posts_res/2019-03-29-git基础/2.png)\n\n- `git reset HEAD <file>`：用来将暂存区的修改回退到工作区。\n- `git checkout -- <file>`：用来丢弃工作区的修改，即用版本库中的版本替换工作区的版本。\n\n```text\n场景1：\n当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 \"git checkout -- <file>\"。\n场景2：\n当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步:\na) 第一步用命令 \"git reset HEAD <file>\";\nb) 就回到了场景1，第二步按场景1操作。\n场景3：\n已经提交了不合适的修改到版本库时，想要撤销本次提交，\n\"git reset --hard <commit_id>\"回到任意版本，不过前提是没有推送到远程库。\n```\n\n- 当删除了工作区的文件后，有两种情况：\n  - 确实要删除文件\n    - `git rm <file>`：用来删除版本库中的文件，要commit才行，实际可以理解为在暂存区记录了要删除版本库中文件的命令。\n    - `git commit -m \"<some messages>\"`：用来提交删除到版本库中。\n    - **注意：先手动删除文件，然后使用git rm <file>和git add<file>效果是一样的。**\n\n  - 误删了文件要还原\n    - `git checkout -- <file>`：用来将版本库中的文件覆盖工作区的文件\n\n## 3. 远程仓库\n\n- 先有本地库，后有远程库\n\n1. 登陆GitHub，然后，在右上角找到\"Create a new repo\"按钮，创建一个新的仓库;\n2. 在\"Repository name\"填入`learngit`，其他保持默认设置，点击\"Create repository\"按钮，就成功地创建了一个新的Git仓库;\n3. 在GitHub上的这个learngit仓库还是空的，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库;\n4. `git remote add origin git@github.com:<your_github_username>/learngit.git`,将工作区内容推到暂存区；如果出现\"fatal: remote origin already exists\"的问题，尝试先执行`git remote rm origin`后，再执行`add`;\n5. `git push -u origin master`，将本地仓库推送到Github。\n6. 查看一下Github中是不是已经有了本地的内容。\n\n**远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库**\n\n**\"-u\"参数：表示把本地的master分支内容推送的远程新的master分支，并把本地的master分支和远程的master分支关联起来，以后的推送或者拉取时就可以简化命令**\n\n从现在起，只要本地作了提交，就可以通过命令：`git push origin master`, 把本地master分支的最新修改推送至GitHub。\n\n- 先有远程库，后有本地库\n\n1. `git clone <repo_ssh_url>`：用来克隆一个远程库到本地。\n2. 在克隆的仓库中进行工作，然后`add`, `commit`, `push`这个流程就可以更新远程库了。\n\n\n## 4. 分支管理\n\n### 4.1 创建合并分支\n\n- `git branch`：用于查看当前分支，会列出所有分支，当前分支前面有`*`。\n- `git branch <dev>`：创建`dev`分支。\n- `git checkout <dev>`：切换到`dev`分支。\n- `git checkout -b <dev>`：git checkout命令加上`-b`参数表示创建并切换。\n- `git merge <dev>`：用于合并`dev`分支到当前分支。\n- `git branch -d <dev>`：用于删除`dev`分支。\n\n### 4.2 解决冲突\n\n当不同的分支都对同一个文件做了不同的修改以后，在master分支执行`git merge <another_branch>`会发生冲突，\n这个时候可以通过`git status`可以告诉我们冲突的文件，之后通过`git diff <file_name>`查看不同分支修改的内容，\n也可以直接打开`<file_name>`文件查看，其中Git用`<<<<<<<`，`=======`，`>>>>>>>`标记出不同分支的内容。\n\n之后直接修改文件`<file_name>`决定如何合并，然后`add`，`commit`，就算合并结束了……\n\n之后可以通过带参数的`git log`查看分支的合并情况：`git log --graph --pretty=oneline --abbrev-commit`，\n结果类似下面的图形，不同的分支会用不同的颜色表示出来。\n\n```text\n$ git log --graph --pretty=oneline --abbrev-commit\n*   cf810e4 (HEAD -> master) conflict fixed\n|\\  \n| * 14096d0 (feature1) AND simple\n* | 5dc6824 & simple\n|/  \n* b17d20e branch test\n* d46f35e (origin/master) remove test.txt\n* b84166e add test.txt\n* 519219b git tracks changes\n* e43a48b understand how stage works\n* 1094adb append GPL\n* e475afc add distributed\n* eaadf4e wrote a readme file\n```\n\n### 4.3 分支管理策略\n\n合并分支的时候，Git会尽可能地使用`Fast forward`模式，这种模式下删除分支会丢失分支信息。\n\n- 使用`--no-ff`参数强制禁用`Fast forward`模式\n这个时候，Git就会在merge时生成一个新的commit，从分支历史上就可以看出分支信息。\n\n例如：执行`git merge --no-ff -m \"merge with no-ff\" dev`后，使用`git log --graph --pretty=oneline --abbrev-commit`查看分支历史,\n\n```text\n*   e1e9c68 (HEAD -> master) merge with no-ff\n|\\  \n| * f52c633 (dev) add merge\n|/  \n*   cf810e4 conflict fixed\n...\n```\n\n在实际开发中，我们应该按照几个基本原则进行分支管理：\n\n首先，`master`分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；\n\n那在哪干活呢？干活都在`dev`分支上，也就是说，`dev`分支是不稳定的，到某个时候，比如1.0版本发布时，再把`dev`分支合并到`master`上，在`master`分支发布1.0版本；\n\n你和你的小伙伴们每个人都在`dev`分支上干活，每个人都有自己的分支，时不时地往`dev`分支上合并就可以了。\n\n所以，团队合作的分支看起来就像下图这样……\n\n![branch-manage](/posts_res/2019-03-29-git基础/3.png)\n\n### 4.4 Bug分支&feature分支\n\n在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。\n\n修复Bug的具体流程如下：\n1. `git stash`：保存dev分支的工作区现场，便于之后恢复现场继续开发工作；\n2. 切换到带有Bug的那个分支，假设是master分支上有Bug，那么执行`git checkout master`切换到master分支；\n3. 在master分支上执行`git checkout -b issue-101`创建并切换到新的分支issue-101，开始修复Bug；\n4. Bug修复完成后执行`git add <file>`, `git commit -m \"<fixed bug 101>\"`, 将修复提交；\n5. `git checkout master`切换回master分支，`git merge --no-ff -m \"<merge bug fixed 101>\" issue-101`合并修复；\n6. `git branch -d issue-101`删除issue-101分支；\n7. `git checkout dev`切换到dev分支，继续之前的开发工作；\n8. `git stash list`：用于列出之前保存的所有工作区现场；\n9. 恢复工作区现场，一种可以用`git stash apply <one_stash_id>`，恢复后stash内容不删除，要用`git stash drop <one_stash_id>`进行手动删除；另一种用`git stash pop`，恢复的同时吧stash内容删除；\n\n举例：\n```text\n设A为游戏软件\n1、master 上面发布的是A的1.0版本\n2、dev 上开发的是A的2.0版本\n3、这时，用户反映 1.0版本存在漏洞，有人利用这个漏洞开外挂\n4、需要从dev切换到master去填这个漏洞，正常必须先提交dev目前的工作，才能切换。\n5、而dev的工作还未完成，不想提交，所以先把dev的工作stash一下。然后切换到master\n6、在master建立分支issue101并切换.\n7、在issue101上修复漏洞。\n8、修复后，在master上合并issue101\n9、切回dev，恢复原本工作，继续工作。\n```\n\n添加一个新功能时，肯定不希望因为一些实验性质的代码，把master分支搞乱了，所以每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，删除该feature分支。\n\n1. `it checkout -b feature-vulcan`：创建并切换到新的分支；\n2. 开发新的功能……，`git add <dev_files>`, `git commit -m \"add feature vulan\"`；\n3. `git checkout dev`，切换会dev分支，准备合并；\n4. 之后和Bug分支的类似……\n5. 假如由于某些原因，这个分支不允许合并要就地销毁了，执行`git branch -d feature-vulcan`会提示分支没有合并，如果删除，将丢失掉修改；\n6. 执行`git branch -D feature-vulcan`进行强制删除。\n\n### 4.5 多人协作\n\n- `git remote`：用于查看远程库的信息；`git remote -v`：用于查看更详细的信息；\n- `git push origin master`：把本地库当前分支的内容推送到远程库master分支；`git push origin dev`：把本地库当前分支的内容推送到远程库dev分支；\n\n```text\n但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？\n1. master分支是主分支，因此要时刻与远程同步；\n2. dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；\n3. bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；\n4. feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。\n```\n\n流程如下：\n1. `git clone <url>`：克隆远程库的master分支；\n2. `git branch -b dev origin/dev`：创建本地库dev分支，并和远程库dev分支链接起来；\n3. 在本地库dev分支进行开发……\n4. `git push origin dev`：试图将本地库dev分支的开发内容推送到远程库中；\n5. 如果推送失败，则因为远程分支比本地库更新，需要先用git pull更新本地库内容，之后试图合并；\n6. 如果合并有冲突，则解决冲突（上面详见4.2节），并在本地提交；\n7. 没有冲突或者解决掉冲突后，再用`git push origin <branch-name>`推送就能成功；\n8. 如果`git pull`提示`no tracking information`，则说明本地分支和远程分支的链接关系没有创建，用命令`git branch --set-upstream-to <branch-name> origin/<branch-name>`链接。\n\n### 4.6 Rebase\n\n没看懂，可以再看看下面这两个讲解。\n- [廖雪峰的官方网站-分支管理-Rebase](https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000)\n- [Git Book 中文版-rebase](http://gitbook.liuhui998.com/4_2.html)\n\n\n## 5. 标签管理\n\n发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。\n\n将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。\n\nGit的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。\n\n### 5.1 创建标签\n\n默认标签是打在最新提交的commit上的\n\n1. 切换到要打标签的分支上：`git checkout master`；\n2. `git tag <name>`：就可以打一个标签，默认打在`HEAD`指针上；\n3. `git tag`：查看所有标签；\n4. `git tag <name> <commit_id>`：对某一次提交打标签；\n5. `git show <name>`：查看标签信息；\n6. `git tag -a <name> -m \"<introduction_tag>\" <commit_id>`：创建带有说明的标签，用`-a`指定标签名，`-m`指定说明文字；\n\n### 5.2 操作标签\n\n1. `git tag -d <name>`：删除本地标签；\n2. `git push origin <name>`：推送某个标签到远程库；\n3. `git push origin --tags`：推送全部尚未推送到远程库的本地标签；\n4. 删除远程库的标签要删除本地标签，之后`git push origin :refs/tags/<name>`删除远程库的标签；\n\n## 5. 使用Github\n\n1. 访问项目的主页，然后Fork这个项目；\n2. 从自己的账户下克隆这个项目；\n3. 对克隆的项目进行修改；\n4. 在Github上发起pull request。\n\nBootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样:\n\n![Github-Using](/posts_res/2019-03-29-git基础/4.png)\n\n## 6. 自定义Git\n\n### 6.1 忽略特征文件\n\n有些时候必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件等等，\n每次git status都会显示Untracked files ...，有强迫症的童鞋心里肯定不爽；\n好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，\n然后把要忽略的文件名填进去，Git就会自动忽略这些文件；\n\n不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。\n所有配置文件可以直接在线浏览：[https://github.com/github/gitignore](https://github.com/github/gitignore)\n\n忽略文件的原则是：\n1. 忽略操作系统自动生成的文件，比如缩略图等；\n2. 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；\n3. 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件;\n\n- `git add -f <file>`：强制添加文件；\n- `git check-ignore`：检查.gitignore文件的规则；\n- `git check-ignore -v App.class`： Git会告诉我们.gitignore文件的哪一行规则忽略了该文件；\n\n### 6.2 配置别名\n\n`--global`：针对当前用户起作用的，如果不加，那只针对当前的仓库起作用；\n\n- `git config --global alias.st status`：配置 `git st` 等价于 `git status`\n- `git config --global alias.co checkout`：配置 `git co` 等价于 `git checkout`\n- `git config --global alias.ci commit`：配置 `git ci` 等价于 `git commit`\n- `git config --global alias.br branch`：配置 `git br` 等价于 `git branch`\n- `git config --global alias.unstage 'reset HEAD'`：配置 `git unstage` 等价于 `git reset HEAD`\n- `git config --global alias.last 'log -1'`：配置 `git last` 等价于 `git log -l`\n- `git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit\"`：配置 `git lg` 等价于 `git log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit`\n\n\n- **每个仓库的Git配置文件都放在`.git/config`文件中**\n\n```text\n[core]\n    repositoryformatversion = 0\n    filemode = true\n    bare = false\n    logallrefupdates = true\n    ignorecase = true\n    precomposeunicode = true\n[remote \"origin\"]\n    url = git@github.com:weiguozhao/learngit.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n[branch \"master\"]\n    remote = origin\n    merge = refs/heads/master\n[alias]\n    last = log -1\n```\n\n- **当前用户的Git配置文件放在用户主目录下的一个隐藏文件`.gitconfig`中**\n\n```text\n[alias]\n    co = checkout\n    ci = commit\n    br = branch\n    st = status\n[user]\n    name = your_name\n    email = your@email.com\n```\n\n## 题外话\n\n如果clone Github上面的仓库速度比较缓慢的话，可以尝试一下命令：\n\n前提是有科学上网工具，并且使用Git的http进行clone，其中127.0.0.1后面是科学上网监听的端口，我的SSR是socker 1086。\n\n- 加速：\n```text\ngit config --global http.https://github.com.proxy socks5://127.0.0.1:1086\ngit config --global https.https://github.com.proxy socks5://127.0.0.1:1086\n```\n\n- 删除：\n```text\ngit config --global --unset https.https://github.com.proxy\ngit config --global --unset http.https://github.com.proxy\n```","source":"_posts/2019-03-29-git基础.md","raw":"---\ntitle: Git基础\nmathjax: true\ncopyright: true\ndate: 2019-03-29 11:23:55\ncategories: 小工具\n---\n\n> learn from: [廖雪峰的官方网站-Git教程](https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000)\n\n\n## 1. 创建版本库\n\n- 进入到准备当作Project的目录下，执行`git init`，生成隐藏文件夹`.git`即创建成功。\n- `git add <file>`：用来将file添加到暂存区Stage中。\n- `git commit -m \"<some messages>\"`：用来将暂存区Stage中的修改内容提交到版本库Version中。\n\n## 2. 版本控制\n\n- `git log`：用来从最近到最远的commit日志。\n  - `git log --pretty=oneline`：增加参数pretty，用来简洁输出commit日志。\n- `git reset --hard HEAD^`：用来将版本库中的master分支退回到上一个版本。\n- `git reflog`：显示所有记录的命令，可以用来回到任意版本 `git reset --hard <commit_id>`。\n- 工作区、暂存区、版本库关系如下：\n\n![work-stage-version](/posts_res/2019-03-29-git基础/1.jpg)\n\n- `git diff`应用场景、管理修改。\n\n![git-diff](/posts_res/2019-03-29-git基础/2.png)\n\n- `git reset HEAD <file>`：用来将暂存区的修改回退到工作区。\n- `git checkout -- <file>`：用来丢弃工作区的修改，即用版本库中的版本替换工作区的版本。\n\n```text\n场景1：\n当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 \"git checkout -- <file>\"。\n场景2：\n当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步:\na) 第一步用命令 \"git reset HEAD <file>\";\nb) 就回到了场景1，第二步按场景1操作。\n场景3：\n已经提交了不合适的修改到版本库时，想要撤销本次提交，\n\"git reset --hard <commit_id>\"回到任意版本，不过前提是没有推送到远程库。\n```\n\n- 当删除了工作区的文件后，有两种情况：\n  - 确实要删除文件\n    - `git rm <file>`：用来删除版本库中的文件，要commit才行，实际可以理解为在暂存区记录了要删除版本库中文件的命令。\n    - `git commit -m \"<some messages>\"`：用来提交删除到版本库中。\n    - **注意：先手动删除文件，然后使用git rm <file>和git add<file>效果是一样的。**\n\n  - 误删了文件要还原\n    - `git checkout -- <file>`：用来将版本库中的文件覆盖工作区的文件\n\n## 3. 远程仓库\n\n- 先有本地库，后有远程库\n\n1. 登陆GitHub，然后，在右上角找到\"Create a new repo\"按钮，创建一个新的仓库;\n2. 在\"Repository name\"填入`learngit`，其他保持默认设置，点击\"Create repository\"按钮，就成功地创建了一个新的Git仓库;\n3. 在GitHub上的这个learngit仓库还是空的，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库;\n4. `git remote add origin git@github.com:<your_github_username>/learngit.git`,将工作区内容推到暂存区；如果出现\"fatal: remote origin already exists\"的问题，尝试先执行`git remote rm origin`后，再执行`add`;\n5. `git push -u origin master`，将本地仓库推送到Github。\n6. 查看一下Github中是不是已经有了本地的内容。\n\n**远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库**\n\n**\"-u\"参数：表示把本地的master分支内容推送的远程新的master分支，并把本地的master分支和远程的master分支关联起来，以后的推送或者拉取时就可以简化命令**\n\n从现在起，只要本地作了提交，就可以通过命令：`git push origin master`, 把本地master分支的最新修改推送至GitHub。\n\n- 先有远程库，后有本地库\n\n1. `git clone <repo_ssh_url>`：用来克隆一个远程库到本地。\n2. 在克隆的仓库中进行工作，然后`add`, `commit`, `push`这个流程就可以更新远程库了。\n\n\n## 4. 分支管理\n\n### 4.1 创建合并分支\n\n- `git branch`：用于查看当前分支，会列出所有分支，当前分支前面有`*`。\n- `git branch <dev>`：创建`dev`分支。\n- `git checkout <dev>`：切换到`dev`分支。\n- `git checkout -b <dev>`：git checkout命令加上`-b`参数表示创建并切换。\n- `git merge <dev>`：用于合并`dev`分支到当前分支。\n- `git branch -d <dev>`：用于删除`dev`分支。\n\n### 4.2 解决冲突\n\n当不同的分支都对同一个文件做了不同的修改以后，在master分支执行`git merge <another_branch>`会发生冲突，\n这个时候可以通过`git status`可以告诉我们冲突的文件，之后通过`git diff <file_name>`查看不同分支修改的内容，\n也可以直接打开`<file_name>`文件查看，其中Git用`<<<<<<<`，`=======`，`>>>>>>>`标记出不同分支的内容。\n\n之后直接修改文件`<file_name>`决定如何合并，然后`add`，`commit`，就算合并结束了……\n\n之后可以通过带参数的`git log`查看分支的合并情况：`git log --graph --pretty=oneline --abbrev-commit`，\n结果类似下面的图形，不同的分支会用不同的颜色表示出来。\n\n```text\n$ git log --graph --pretty=oneline --abbrev-commit\n*   cf810e4 (HEAD -> master) conflict fixed\n|\\  \n| * 14096d0 (feature1) AND simple\n* | 5dc6824 & simple\n|/  \n* b17d20e branch test\n* d46f35e (origin/master) remove test.txt\n* b84166e add test.txt\n* 519219b git tracks changes\n* e43a48b understand how stage works\n* 1094adb append GPL\n* e475afc add distributed\n* eaadf4e wrote a readme file\n```\n\n### 4.3 分支管理策略\n\n合并分支的时候，Git会尽可能地使用`Fast forward`模式，这种模式下删除分支会丢失分支信息。\n\n- 使用`--no-ff`参数强制禁用`Fast forward`模式\n这个时候，Git就会在merge时生成一个新的commit，从分支历史上就可以看出分支信息。\n\n例如：执行`git merge --no-ff -m \"merge with no-ff\" dev`后，使用`git log --graph --pretty=oneline --abbrev-commit`查看分支历史,\n\n```text\n*   e1e9c68 (HEAD -> master) merge with no-ff\n|\\  \n| * f52c633 (dev) add merge\n|/  \n*   cf810e4 conflict fixed\n...\n```\n\n在实际开发中，我们应该按照几个基本原则进行分支管理：\n\n首先，`master`分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；\n\n那在哪干活呢？干活都在`dev`分支上，也就是说，`dev`分支是不稳定的，到某个时候，比如1.0版本发布时，再把`dev`分支合并到`master`上，在`master`分支发布1.0版本；\n\n你和你的小伙伴们每个人都在`dev`分支上干活，每个人都有自己的分支，时不时地往`dev`分支上合并就可以了。\n\n所以，团队合作的分支看起来就像下图这样……\n\n![branch-manage](/posts_res/2019-03-29-git基础/3.png)\n\n### 4.4 Bug分支&feature分支\n\n在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。\n\n修复Bug的具体流程如下：\n1. `git stash`：保存dev分支的工作区现场，便于之后恢复现场继续开发工作；\n2. 切换到带有Bug的那个分支，假设是master分支上有Bug，那么执行`git checkout master`切换到master分支；\n3. 在master分支上执行`git checkout -b issue-101`创建并切换到新的分支issue-101，开始修复Bug；\n4. Bug修复完成后执行`git add <file>`, `git commit -m \"<fixed bug 101>\"`, 将修复提交；\n5. `git checkout master`切换回master分支，`git merge --no-ff -m \"<merge bug fixed 101>\" issue-101`合并修复；\n6. `git branch -d issue-101`删除issue-101分支；\n7. `git checkout dev`切换到dev分支，继续之前的开发工作；\n8. `git stash list`：用于列出之前保存的所有工作区现场；\n9. 恢复工作区现场，一种可以用`git stash apply <one_stash_id>`，恢复后stash内容不删除，要用`git stash drop <one_stash_id>`进行手动删除；另一种用`git stash pop`，恢复的同时吧stash内容删除；\n\n举例：\n```text\n设A为游戏软件\n1、master 上面发布的是A的1.0版本\n2、dev 上开发的是A的2.0版本\n3、这时，用户反映 1.0版本存在漏洞，有人利用这个漏洞开外挂\n4、需要从dev切换到master去填这个漏洞，正常必须先提交dev目前的工作，才能切换。\n5、而dev的工作还未完成，不想提交，所以先把dev的工作stash一下。然后切换到master\n6、在master建立分支issue101并切换.\n7、在issue101上修复漏洞。\n8、修复后，在master上合并issue101\n9、切回dev，恢复原本工作，继续工作。\n```\n\n添加一个新功能时，肯定不希望因为一些实验性质的代码，把master分支搞乱了，所以每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，删除该feature分支。\n\n1. `it checkout -b feature-vulcan`：创建并切换到新的分支；\n2. 开发新的功能……，`git add <dev_files>`, `git commit -m \"add feature vulan\"`；\n3. `git checkout dev`，切换会dev分支，准备合并；\n4. 之后和Bug分支的类似……\n5. 假如由于某些原因，这个分支不允许合并要就地销毁了，执行`git branch -d feature-vulcan`会提示分支没有合并，如果删除，将丢失掉修改；\n6. 执行`git branch -D feature-vulcan`进行强制删除。\n\n### 4.5 多人协作\n\n- `git remote`：用于查看远程库的信息；`git remote -v`：用于查看更详细的信息；\n- `git push origin master`：把本地库当前分支的内容推送到远程库master分支；`git push origin dev`：把本地库当前分支的内容推送到远程库dev分支；\n\n```text\n但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？\n1. master分支是主分支，因此要时刻与远程同步；\n2. dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；\n3. bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；\n4. feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。\n```\n\n流程如下：\n1. `git clone <url>`：克隆远程库的master分支；\n2. `git branch -b dev origin/dev`：创建本地库dev分支，并和远程库dev分支链接起来；\n3. 在本地库dev分支进行开发……\n4. `git push origin dev`：试图将本地库dev分支的开发内容推送到远程库中；\n5. 如果推送失败，则因为远程分支比本地库更新，需要先用git pull更新本地库内容，之后试图合并；\n6. 如果合并有冲突，则解决冲突（上面详见4.2节），并在本地提交；\n7. 没有冲突或者解决掉冲突后，再用`git push origin <branch-name>`推送就能成功；\n8. 如果`git pull`提示`no tracking information`，则说明本地分支和远程分支的链接关系没有创建，用命令`git branch --set-upstream-to <branch-name> origin/<branch-name>`链接。\n\n### 4.6 Rebase\n\n没看懂，可以再看看下面这两个讲解。\n- [廖雪峰的官方网站-分支管理-Rebase](https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000)\n- [Git Book 中文版-rebase](http://gitbook.liuhui998.com/4_2.html)\n\n\n## 5. 标签管理\n\n发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。\n\n将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。\n\nGit的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。\n\n### 5.1 创建标签\n\n默认标签是打在最新提交的commit上的\n\n1. 切换到要打标签的分支上：`git checkout master`；\n2. `git tag <name>`：就可以打一个标签，默认打在`HEAD`指针上；\n3. `git tag`：查看所有标签；\n4. `git tag <name> <commit_id>`：对某一次提交打标签；\n5. `git show <name>`：查看标签信息；\n6. `git tag -a <name> -m \"<introduction_tag>\" <commit_id>`：创建带有说明的标签，用`-a`指定标签名，`-m`指定说明文字；\n\n### 5.2 操作标签\n\n1. `git tag -d <name>`：删除本地标签；\n2. `git push origin <name>`：推送某个标签到远程库；\n3. `git push origin --tags`：推送全部尚未推送到远程库的本地标签；\n4. 删除远程库的标签要删除本地标签，之后`git push origin :refs/tags/<name>`删除远程库的标签；\n\n## 5. 使用Github\n\n1. 访问项目的主页，然后Fork这个项目；\n2. 从自己的账户下克隆这个项目；\n3. 对克隆的项目进行修改；\n4. 在Github上发起pull request。\n\nBootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样:\n\n![Github-Using](/posts_res/2019-03-29-git基础/4.png)\n\n## 6. 自定义Git\n\n### 6.1 忽略特征文件\n\n有些时候必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件等等，\n每次git status都会显示Untracked files ...，有强迫症的童鞋心里肯定不爽；\n好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，\n然后把要忽略的文件名填进去，Git就会自动忽略这些文件；\n\n不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。\n所有配置文件可以直接在线浏览：[https://github.com/github/gitignore](https://github.com/github/gitignore)\n\n忽略文件的原则是：\n1. 忽略操作系统自动生成的文件，比如缩略图等；\n2. 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；\n3. 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件;\n\n- `git add -f <file>`：强制添加文件；\n- `git check-ignore`：检查.gitignore文件的规则；\n- `git check-ignore -v App.class`： Git会告诉我们.gitignore文件的哪一行规则忽略了该文件；\n\n### 6.2 配置别名\n\n`--global`：针对当前用户起作用的，如果不加，那只针对当前的仓库起作用；\n\n- `git config --global alias.st status`：配置 `git st` 等价于 `git status`\n- `git config --global alias.co checkout`：配置 `git co` 等价于 `git checkout`\n- `git config --global alias.ci commit`：配置 `git ci` 等价于 `git commit`\n- `git config --global alias.br branch`：配置 `git br` 等价于 `git branch`\n- `git config --global alias.unstage 'reset HEAD'`：配置 `git unstage` 等价于 `git reset HEAD`\n- `git config --global alias.last 'log -1'`：配置 `git last` 等价于 `git log -l`\n- `git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit\"`：配置 `git lg` 等价于 `git log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit`\n\n\n- **每个仓库的Git配置文件都放在`.git/config`文件中**\n\n```text\n[core]\n    repositoryformatversion = 0\n    filemode = true\n    bare = false\n    logallrefupdates = true\n    ignorecase = true\n    precomposeunicode = true\n[remote \"origin\"]\n    url = git@github.com:weiguozhao/learngit.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n[branch \"master\"]\n    remote = origin\n    merge = refs/heads/master\n[alias]\n    last = log -1\n```\n\n- **当前用户的Git配置文件放在用户主目录下的一个隐藏文件`.gitconfig`中**\n\n```text\n[alias]\n    co = checkout\n    ci = commit\n    br = branch\n    st = status\n[user]\n    name = your_name\n    email = your@email.com\n```\n\n## 题外话\n\n如果clone Github上面的仓库速度比较缓慢的话，可以尝试一下命令：\n\n前提是有科学上网工具，并且使用Git的http进行clone，其中127.0.0.1后面是科学上网监听的端口，我的SSR是socker 1086。\n\n- 加速：\n```text\ngit config --global http.https://github.com.proxy socks5://127.0.0.1:1086\ngit config --global https.https://github.com.proxy socks5://127.0.0.1:1086\n```\n\n- 删除：\n```text\ngit config --global --unset https.https://github.com.proxy\ngit config --global --unset http.https://github.com.proxy\n```","slug":"git基础","published":1,"updated":"2019-08-17T09:43:16.586Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjzl0wo0p00612qwp21m1x6o6","content":"<blockquote>\n<p>learn from: <a href=\"https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000\" target=\"_blank\" rel=\"noopener\">廖雪峰的官方网站-Git教程</a></p>\n</blockquote><h2 id=\"1-创建版本库\"><a href=\"#1-创建版本库\" class=\"headerlink\" title=\"1. 创建版本库\"></a>1. 创建版本库</h2><ul>\n<li>进入到准备当作Project的目录下，执行<code>git init</code>，生成隐藏文件夹<code>.git</code>即创建成功。</li>\n<li><code>git add &lt;file&gt;</code>：用来将file添加到暂存区Stage中。</li>\n<li><code>git commit -m &quot;&lt;some messages&gt;&quot;</code>：用来将暂存区Stage中的修改内容提交到版本库Version中。</li>\n</ul><h2 id=\"2-版本控制\"><a href=\"#2-版本控制\" class=\"headerlink\" title=\"2. 版本控制\"></a>2. 版本控制</h2><ul>\n<li><code>git log</code>：用来从最近到最远的commit日志。<ul>\n<li><code>git log --pretty=oneline</code>：增加参数pretty，用来简洁输出commit日志。</li>\n</ul>\n</li>\n<li><code>git reset --hard HEAD^</code>：用来将版本库中的master分支退回到上一个版本。</li>\n<li><code>git reflog</code>：显示所有记录的命令，可以用来回到任意版本 <code>git reset --hard &lt;commit_id&gt;</code>。</li>\n<li>工作区、暂存区、版本库关系如下：</li>\n</ul><a id=\"more\"></a>\n\n\n<p><img src=\"/posts_res/2019-03-29-git基础/1.jpg\" alt=\"work-stage-version\"></p>\n<ul>\n<li><code>git diff</code>应用场景、管理修改。</li>\n</ul>\n<p><img src=\"/posts_res/2019-03-29-git基础/2.png\" alt=\"git-diff\"></p>\n<ul>\n<li><code>git reset HEAD &lt;file&gt;</code>：用来将暂存区的修改回退到工作区。</li>\n<li><code>git checkout -- &lt;file&gt;</code>：用来丢弃工作区的修改，即用版本库中的版本替换工作区的版本。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">场景1：</span><br><span class=\"line\">当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 &quot;git checkout -- &lt;file&gt;&quot;。</span><br><span class=\"line\">场景2：</span><br><span class=\"line\">当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步:</span><br><span class=\"line\">a) 第一步用命令 &quot;git reset HEAD &lt;file&gt;&quot;;</span><br><span class=\"line\">b) 就回到了场景1，第二步按场景1操作。</span><br><span class=\"line\">场景3：</span><br><span class=\"line\">已经提交了不合适的修改到版本库时，想要撤销本次提交，</span><br><span class=\"line\">&quot;git reset --hard &lt;commit_id&gt;&quot;回到任意版本，不过前提是没有推送到远程库。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>当删除了工作区的文件后，有两种情况：</p>\n<ul>\n<li><p>确实要删除文件</p>\n<ul>\n<li><code>git rm &lt;file&gt;</code>：用来删除版本库中的文件，要commit才行，实际可以理解为在暂存区记录了要删除版本库中文件的命令。</li>\n<li><code>git commit -m &quot;&lt;some messages&gt;&quot;</code>：用来提交删除到版本库中。</li>\n<li><strong>注意：先手动删除文件，然后使用git rm <file>和git add<file>效果是一样的。</file></file></strong></li>\n</ul>\n</li>\n<li><p>误删了文件要还原</p>\n<ul>\n<li><code>git checkout -- &lt;file&gt;</code>：用来将版本库中的文件覆盖工作区的文件</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-远程仓库\"><a href=\"#3-远程仓库\" class=\"headerlink\" title=\"3. 远程仓库\"></a>3. 远程仓库</h2><ul>\n<li>先有本地库，后有远程库</li>\n</ul>\n<ol>\n<li>登陆GitHub，然后，在右上角找到”Create a new repo”按钮，创建一个新的仓库;</li>\n<li>在”Repository name”填入<code>learngit</code>，其他保持默认设置，点击”Create repository”按钮，就成功地创建了一个新的Git仓库;</li>\n<li>在GitHub上的这个learngit仓库还是空的，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库;</li>\n<li><code>git remote add origin git@github.com:&lt;your_github_username&gt;/learngit.git</code>,将工作区内容推到暂存区；如果出现”fatal: remote origin already exists”的问题，尝试先执行<code>git remote rm origin</code>后，再执行<code>add</code>;</li>\n<li><code>git push -u origin master</code>，将本地仓库推送到Github。</li>\n<li>查看一下Github中是不是已经有了本地的内容。</li>\n</ol>\n<p><strong>远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库</strong></p>\n<p><strong>“-u”参数：表示把本地的master分支内容推送的远程新的master分支，并把本地的master分支和远程的master分支关联起来，以后的推送或者拉取时就可以简化命令</strong></p>\n<p>从现在起，只要本地作了提交，就可以通过命令：<code>git push origin master</code>, 把本地master分支的最新修改推送至GitHub。</p>\n<ul>\n<li>先有远程库，后有本地库</li>\n</ul>\n<ol>\n<li><code>git clone &lt;repo_ssh_url&gt;</code>：用来克隆一个远程库到本地。</li>\n<li>在克隆的仓库中进行工作，然后<code>add</code>, <code>commit</code>, <code>push</code>这个流程就可以更新远程库了。</li>\n</ol>\n<h2 id=\"4-分支管理\"><a href=\"#4-分支管理\" class=\"headerlink\" title=\"4. 分支管理\"></a>4. 分支管理</h2><h3 id=\"4-1-创建合并分支\"><a href=\"#4-1-创建合并分支\" class=\"headerlink\" title=\"4.1 创建合并分支\"></a>4.1 创建合并分支</h3><ul>\n<li><code>git branch</code>：用于查看当前分支，会列出所有分支，当前分支前面有<code>*</code>。</li>\n<li><code>git branch &lt;dev&gt;</code>：创建<code>dev</code>分支。</li>\n<li><code>git checkout &lt;dev&gt;</code>：切换到<code>dev</code>分支。</li>\n<li><code>git checkout -b &lt;dev&gt;</code>：git checkout命令加上<code>-b</code>参数表示创建并切换。</li>\n<li><code>git merge &lt;dev&gt;</code>：用于合并<code>dev</code>分支到当前分支。</li>\n<li><code>git branch -d &lt;dev&gt;</code>：用于删除<code>dev</code>分支。</li>\n</ul>\n<h3 id=\"4-2-解决冲突\"><a href=\"#4-2-解决冲突\" class=\"headerlink\" title=\"4.2 解决冲突\"></a>4.2 解决冲突</h3><p>当不同的分支都对同一个文件做了不同的修改以后，在master分支执行<code>git merge &lt;another_branch&gt;</code>会发生冲突，\n这个时候可以通过<code>git status</code>可以告诉我们冲突的文件，之后通过<code>git diff &lt;file_name&gt;</code>查看不同分支修改的内容，\n也可以直接打开<code>&lt;file_name&gt;</code>文件查看，其中Git用<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>，<code>=======</code>，<code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>标记出不同分支的内容。</p>\n<p>之后直接修改文件<code>&lt;file_name&gt;</code>决定如何合并，然后<code>add</code>，<code>commit</code>，就算合并结束了……</p>\n<p>之后可以通过带参数的<code>git log</code>查看分支的合并情况：<code>git log --graph --pretty=oneline --abbrev-commit</code>，\n结果类似下面的图形，不同的分支会用不同的颜色表示出来。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class=\"line\">*   cf810e4 (HEAD -&gt; master) conflict fixed</span><br><span class=\"line\">|\\  </span><br><span class=\"line\">| * 14096d0 (feature1) AND simple</span><br><span class=\"line\">* | 5dc6824 &amp; simple</span><br><span class=\"line\">|/  </span><br><span class=\"line\">* b17d20e branch test</span><br><span class=\"line\">* d46f35e (origin/master) remove test.txt</span><br><span class=\"line\">* b84166e add test.txt</span><br><span class=\"line\">* 519219b git tracks changes</span><br><span class=\"line\">* e43a48b understand how stage works</span><br><span class=\"line\">* 1094adb append GPL</span><br><span class=\"line\">* e475afc add distributed</span><br><span class=\"line\">* eaadf4e wrote a readme file</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3-分支管理策略\"><a href=\"#4-3-分支管理策略\" class=\"headerlink\" title=\"4.3 分支管理策略\"></a>4.3 分支管理策略</h3><p>合并分支的时候，Git会尽可能地使用<code>Fast forward</code>模式，这种模式下删除分支会丢失分支信息。</p>\n<ul>\n<li>使用<code>--no-ff</code>参数强制禁用<code>Fast forward</code>模式\n这个时候，Git就会在merge时生成一个新的commit，从分支历史上就可以看出分支信息。</li>\n</ul>\n<p>例如：执行<code>git merge --no-ff -m &quot;merge with no-ff&quot; dev</code>后，使用<code>git log --graph --pretty=oneline --abbrev-commit</code>查看分支历史,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*   e1e9c68 (HEAD -&gt; master) merge with no-ff</span><br><span class=\"line\">|\\  </span><br><span class=\"line\">| * f52c633 (dev) add merge</span><br><span class=\"line\">|/  </span><br><span class=\"line\">*   cf810e4 conflict fixed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p>\n<p>首先，<code>master</code>分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p>\n<p>那在哪干活呢？干活都在<code>dev</code>分支上，也就是说，<code>dev</code>分支是不稳定的，到某个时候，比如1.0版本发布时，再把<code>dev</code>分支合并到<code>master</code>上，在<code>master</code>分支发布1.0版本；</p>\n<p>你和你的小伙伴们每个人都在<code>dev</code>分支上干活，每个人都有自己的分支，时不时地往<code>dev</code>分支上合并就可以了。</p>\n<p>所以，团队合作的分支看起来就像下图这样……</p>\n<p><img src=\"/posts_res/2019-03-29-git基础/3.png\" alt=\"branch-manage\"></p>\n<h3 id=\"4-4-Bug分支-amp-feature分支\"><a href=\"#4-4-Bug分支-amp-feature分支\" class=\"headerlink\" title=\"4.4 Bug分支&amp;feature分支\"></a>4.4 Bug分支&amp;feature分支</h3><p>在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。</p>\n<p>修复Bug的具体流程如下：</p>\n<ol>\n<li><code>git stash</code>：保存dev分支的工作区现场，便于之后恢复现场继续开发工作；</li>\n<li>切换到带有Bug的那个分支，假设是master分支上有Bug，那么执行<code>git checkout master</code>切换到master分支；</li>\n<li>在master分支上执行<code>git checkout -b issue-101</code>创建并切换到新的分支issue-101，开始修复Bug；</li>\n<li>Bug修复完成后执行<code>git add &lt;file&gt;</code>, <code>git commit -m &quot;&lt;fixed bug 101&gt;&quot;</code>, 将修复提交；</li>\n<li><code>git checkout master</code>切换回master分支，<code>git merge --no-ff -m &quot;&lt;merge bug fixed 101&gt;&quot; issue-101</code>合并修复；</li>\n<li><code>git branch -d issue-101</code>删除issue-101分支；</li>\n<li><code>git checkout dev</code>切换到dev分支，继续之前的开发工作；</li>\n<li><code>git stash list</code>：用于列出之前保存的所有工作区现场；</li>\n<li>恢复工作区现场，一种可以用<code>git stash apply &lt;one_stash_id&gt;</code>，恢复后stash内容不删除，要用<code>git stash drop &lt;one_stash_id&gt;</code>进行手动删除；另一种用<code>git stash pop</code>，恢复的同时吧stash内容删除；</li>\n</ol>\n<p>举例：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">设A为游戏软件</span><br><span class=\"line\">1、master 上面发布的是A的1.0版本</span><br><span class=\"line\">2、dev 上开发的是A的2.0版本</span><br><span class=\"line\">3、这时，用户反映 1.0版本存在漏洞，有人利用这个漏洞开外挂</span><br><span class=\"line\">4、需要从dev切换到master去填这个漏洞，正常必须先提交dev目前的工作，才能切换。</span><br><span class=\"line\">5、而dev的工作还未完成，不想提交，所以先把dev的工作stash一下。然后切换到master</span><br><span class=\"line\">6、在master建立分支issue101并切换.</span><br><span class=\"line\">7、在issue101上修复漏洞。</span><br><span class=\"line\">8、修复后，在master上合并issue101</span><br><span class=\"line\">9、切回dev，恢复原本工作，继续工作。</span><br></pre></td></tr></table></figure></p>\n<p>添加一个新功能时，肯定不希望因为一些实验性质的代码，把master分支搞乱了，所以每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，删除该feature分支。</p>\n<ol>\n<li><code>it checkout -b feature-vulcan</code>：创建并切换到新的分支；</li>\n<li>开发新的功能……，<code>git add &lt;dev_files&gt;</code>, <code>git commit -m &quot;add feature vulan&quot;</code>；</li>\n<li><code>git checkout dev</code>，切换会dev分支，准备合并；</li>\n<li>之后和Bug分支的类似……</li>\n<li>假如由于某些原因，这个分支不允许合并要就地销毁了，执行<code>git branch -d feature-vulcan</code>会提示分支没有合并，如果删除，将丢失掉修改；</li>\n<li>执行<code>git branch -D feature-vulcan</code>进行强制删除。</li>\n</ol>\n<h3 id=\"4-5-多人协作\"><a href=\"#4-5-多人协作\" class=\"headerlink\" title=\"4.5 多人协作\"></a>4.5 多人协作</h3><ul>\n<li><code>git remote</code>：用于查看远程库的信息；<code>git remote -v</code>：用于查看更详细的信息；</li>\n<li><code>git push origin master</code>：把本地库当前分支的内容推送到远程库master分支；<code>git push origin dev</code>：把本地库当前分支的内容推送到远程库dev分支；</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？</span><br><span class=\"line\">1. master分支是主分支，因此要时刻与远程同步；</span><br><span class=\"line\">2. dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</span><br><span class=\"line\">3. bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</span><br><span class=\"line\">4. feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</span><br></pre></td></tr></table></figure>\n<p>流程如下：</p>\n<ol>\n<li><code>git clone &lt;url&gt;</code>：克隆远程库的master分支；</li>\n<li><code>git branch -b dev origin/dev</code>：创建本地库dev分支，并和远程库dev分支链接起来；</li>\n<li>在本地库dev分支进行开发……</li>\n<li><code>git push origin dev</code>：试图将本地库dev分支的开发内容推送到远程库中；</li>\n<li>如果推送失败，则因为远程分支比本地库更新，需要先用git pull更新本地库内容，之后试图合并；</li>\n<li>如果合并有冲突，则解决冲突（上面详见4.2节），并在本地提交；</li>\n<li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功；</li>\n<li>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>链接。</li>\n</ol>\n<h3 id=\"4-6-Rebase\"><a href=\"#4-6-Rebase\" class=\"headerlink\" title=\"4.6 Rebase\"></a>4.6 Rebase</h3><p>没看懂，可以再看看下面这两个讲解。</p>\n<ul>\n<li><a href=\"https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000\" target=\"_blank\" rel=\"noopener\">廖雪峰的官方网站-分支管理-Rebase</a></li>\n<li><a href=\"http://gitbook.liuhui998.com/4_2.html\" target=\"_blank\" rel=\"noopener\">Git Book 中文版-rebase</a></li>\n</ul>\n<h2 id=\"5-标签管理\"><a href=\"#5-标签管理\" class=\"headerlink\" title=\"5. 标签管理\"></a>5. 标签管理</h2><p>发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。</p>\n<p>将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。</p>\n<p>Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。</p>\n<h3 id=\"5-1-创建标签\"><a href=\"#5-1-创建标签\" class=\"headerlink\" title=\"5.1 创建标签\"></a>5.1 创建标签</h3><p>默认标签是打在最新提交的commit上的</p>\n<ol>\n<li>切换到要打标签的分支上：<code>git checkout master</code>；</li>\n<li><code>git tag &lt;name&gt;</code>：就可以打一个标签，默认打在<code>HEAD</code>指针上；</li>\n<li><code>git tag</code>：查看所有标签；</li>\n<li><code>git tag &lt;name&gt; &lt;commit_id&gt;</code>：对某一次提交打标签；</li>\n<li><code>git show &lt;name&gt;</code>：查看标签信息；</li>\n<li><code>git tag -a &lt;name&gt; -m &quot;&lt;introduction_tag&gt;&quot; &lt;commit_id&gt;</code>：创建带有说明的标签，用<code>-a</code>指定标签名，<code>-m</code>指定说明文字；</li>\n</ol>\n<h3 id=\"5-2-操作标签\"><a href=\"#5-2-操作标签\" class=\"headerlink\" title=\"5.2 操作标签\"></a>5.2 操作标签</h3><ol>\n<li><code>git tag -d &lt;name&gt;</code>：删除本地标签；</li>\n<li><code>git push origin &lt;name&gt;</code>：推送某个标签到远程库；</li>\n<li><code>git push origin --tags</code>：推送全部尚未推送到远程库的本地标签；</li>\n<li>删除远程库的标签要删除本地标签，之后<code>git push origin :refs/tags/&lt;name&gt;</code>删除远程库的标签；</li>\n</ol>\n<h2 id=\"5-使用Github\"><a href=\"#5-使用Github\" class=\"headerlink\" title=\"5. 使用Github\"></a>5. 使用Github</h2><ol>\n<li>访问项目的主页，然后Fork这个项目；</li>\n<li>从自己的账户下克隆这个项目；</li>\n<li>对克隆的项目进行修改；</li>\n<li>在Github上发起pull request。</li>\n</ol>\n<p>Bootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样:</p>\n<p><img src=\"/posts_res/2019-03-29-git基础/4.png\" alt=\"Github-Using\"></p>\n<h2 id=\"6-自定义Git\"><a href=\"#6-自定义Git\" class=\"headerlink\" title=\"6. 自定义Git\"></a>6. 自定义Git</h2><h3 id=\"6-1-忽略特征文件\"><a href=\"#6-1-忽略特征文件\" class=\"headerlink\" title=\"6.1 忽略特征文件\"></a>6.1 忽略特征文件</h3><p>有些时候必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件等等，\n每次git status都会显示Untracked files …，有强迫症的童鞋心里肯定不爽；\n好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，\n然后把要忽略的文件名填进去，Git就会自动忽略这些文件；</p>\n<p>不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。\n所有配置文件可以直接在线浏览：<a href=\"https://github.com/github/gitignore\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore</a></p>\n<p>忽略文件的原则是：</p>\n<ol>\n<li>忽略操作系统自动生成的文件，比如缩略图等；</li>\n<li>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；</li>\n<li>忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件;</li>\n</ol>\n<ul>\n<li><code>git add -f &lt;file&gt;</code>：强制添加文件；</li>\n<li><code>git check-ignore</code>：检查.gitignore文件的规则；</li>\n<li><code>git check-ignore -v App.class</code>： Git会告诉我们.gitignore文件的哪一行规则忽略了该文件；</li>\n</ul>\n<h3 id=\"6-2-配置别名\"><a href=\"#6-2-配置别名\" class=\"headerlink\" title=\"6.2 配置别名\"></a>6.2 配置别名</h3><p><code>--global</code>：针对当前用户起作用的，如果不加，那只针对当前的仓库起作用；</p>\n<ul>\n<li><code>git config --global alias.st status</code>：配置 <code>git st</code> 等价于 <code>git status</code></li>\n<li><code>git config --global alias.co checkout</code>：配置 <code>git co</code> 等价于 <code>git checkout</code></li>\n<li><code>git config --global alias.ci commit</code>：配置 <code>git ci</code> 等价于 <code>git commit</code></li>\n<li><code>git config --global alias.br branch</code>：配置 <code>git br</code> 等价于 <code>git branch</code></li>\n<li><code>git config --global alias.unstage &#39;reset HEAD&#39;</code>：配置 <code>git unstage</code> 等价于 <code>git reset HEAD</code></li>\n<li><code>git config --global alias.last &#39;log -1&#39;</code>：配置 <code>git last</code> 等价于 <code>git log -l</code></li>\n<li><code>git config --global alias.lg &quot;log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit&quot;</code>：配置 <code>git lg</code> 等价于 <code>git log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit</code></li>\n</ul>\n<ul>\n<li><strong>每个仓库的Git配置文件都放在<code>.git/config</code>文件中</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[core]</span><br><span class=\"line\">    repositoryformatversion = 0</span><br><span class=\"line\">    filemode = true</span><br><span class=\"line\">    bare = false</span><br><span class=\"line\">    logallrefupdates = true</span><br><span class=\"line\">    ignorecase = true</span><br><span class=\"line\">    precomposeunicode = true</span><br><span class=\"line\">[remote &quot;origin&quot;]</span><br><span class=\"line\">    url = git@github.com:weiguozhao/learngit.git</span><br><span class=\"line\">    fetch = +refs/heads/*:refs/remotes/origin/*</span><br><span class=\"line\">[branch &quot;master&quot;]</span><br><span class=\"line\">    remote = origin</span><br><span class=\"line\">    merge = refs/heads/master</span><br><span class=\"line\">[alias]</span><br><span class=\"line\">    last = log -1</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>当前用户的Git配置文件放在用户主目录下的一个隐藏文件<code>.gitconfig</code>中</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[alias]</span><br><span class=\"line\">    co = checkout</span><br><span class=\"line\">    ci = commit</span><br><span class=\"line\">    br = branch</span><br><span class=\"line\">    st = status</span><br><span class=\"line\">[user]</span><br><span class=\"line\">    name = your_name</span><br><span class=\"line\">    email = your@email.com</span><br></pre></td></tr></table></figure>\n<h2 id=\"题外话\"><a href=\"#题外话\" class=\"headerlink\" title=\"题外话\"></a>题外话</h2><p>如果clone Github上面的仓库速度比较缓慢的话，可以尝试一下命令：</p>\n<p>前提是有科学上网工具，并且使用Git的http进行clone，其中127.0.0.1后面是科学上网监听的端口，我的SSR是socker 1086。</p>\n<ul>\n<li><p>加速：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global http.https://github.com.proxy socks5://127.0.0.1:1086</span><br><span class=\"line\">git config --global https.https://github.com.proxy socks5://127.0.0.1:1086</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global --unset https.https://github.com.proxy</span><br><span class=\"line\">git config --global --unset http.https://github.com.proxy</span><br></pre></td></tr></table></figure></li>\n</ul>\n","site":{"data":{}},"excerpt":"<blockquote>\n<p>learn from: <a href=\"https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000\" target=\"_blank\" rel=\"noopener\">廖雪峰的官方网站-Git教程</a></p>\n</blockquote><h2 id=\"1-创建版本库\"><a href=\"#1-创建版本库\" class=\"headerlink\" title=\"1. 创建版本库\"></a>1. 创建版本库</h2><ul>\n<li>进入到准备当作Project的目录下，执行<code>git init</code>，生成隐藏文件夹<code>.git</code>即创建成功。</li>\n<li><code>git add &lt;file&gt;</code>：用来将file添加到暂存区Stage中。</li>\n<li><code>git commit -m &quot;&lt;some messages&gt;&quot;</code>：用来将暂存区Stage中的修改内容提交到版本库Version中。</li>\n</ul><h2 id=\"2-版本控制\"><a href=\"#2-版本控制\" class=\"headerlink\" title=\"2. 版本控制\"></a>2. 版本控制</h2><ul>\n<li><code>git log</code>：用来从最近到最远的commit日志。<ul>\n<li><code>git log --pretty=oneline</code>：增加参数pretty，用来简洁输出commit日志。</li>\n</ul>\n</li>\n<li><code>git reset --hard HEAD^</code>：用来将版本库中的master分支退回到上一个版本。</li>\n<li><code>git reflog</code>：显示所有记录的命令，可以用来回到任意版本 <code>git reset --hard &lt;commit_id&gt;</code>。</li>\n<li>工作区、暂存区、版本库关系如下：</li>\n</ul>","more":"\n\n\n<p><img src=\"/posts_res/2019-03-29-git基础/1.jpg\" alt=\"work-stage-version\"></p>\n<ul>\n<li><code>git diff</code>应用场景、管理修改。</li>\n</ul>\n<p><img src=\"/posts_res/2019-03-29-git基础/2.png\" alt=\"git-diff\"></p>\n<ul>\n<li><code>git reset HEAD &lt;file&gt;</code>：用来将暂存区的修改回退到工作区。</li>\n<li><code>git checkout -- &lt;file&gt;</code>：用来丢弃工作区的修改，即用版本库中的版本替换工作区的版本。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">场景1：</span><br><span class=\"line\">当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令 &quot;git checkout -- &lt;file&gt;&quot;。</span><br><span class=\"line\">场景2：</span><br><span class=\"line\">当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步:</span><br><span class=\"line\">a) 第一步用命令 &quot;git reset HEAD &lt;file&gt;&quot;;</span><br><span class=\"line\">b) 就回到了场景1，第二步按场景1操作。</span><br><span class=\"line\">场景3：</span><br><span class=\"line\">已经提交了不合适的修改到版本库时，想要撤销本次提交，</span><br><span class=\"line\">&quot;git reset --hard &lt;commit_id&gt;&quot;回到任意版本，不过前提是没有推送到远程库。</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>当删除了工作区的文件后，有两种情况：</p>\n<ul>\n<li><p>确实要删除文件</p>\n<ul>\n<li><code>git rm &lt;file&gt;</code>：用来删除版本库中的文件，要commit才行，实际可以理解为在暂存区记录了要删除版本库中文件的命令。</li>\n<li><code>git commit -m &quot;&lt;some messages&gt;&quot;</code>：用来提交删除到版本库中。</li>\n<li><strong>注意：先手动删除文件，然后使用git rm <file>和git add<file>效果是一样的。</file></file></strong></li>\n</ul>\n</li>\n<li><p>误删了文件要还原</p>\n<ul>\n<li><code>git checkout -- &lt;file&gt;</code>：用来将版本库中的文件覆盖工作区的文件</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"3-远程仓库\"><a href=\"#3-远程仓库\" class=\"headerlink\" title=\"3. 远程仓库\"></a>3. 远程仓库</h2><ul>\n<li>先有本地库，后有远程库</li>\n</ul>\n<ol>\n<li>登陆GitHub，然后，在右上角找到”Create a new repo”按钮，创建一个新的仓库;</li>\n<li>在”Repository name”填入<code>learngit</code>，其他保持默认设置，点击”Create repository”按钮，就成功地创建了一个新的Git仓库;</li>\n<li>在GitHub上的这个learngit仓库还是空的，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库;</li>\n<li><code>git remote add origin git@github.com:&lt;your_github_username&gt;/learngit.git</code>,将工作区内容推到暂存区；如果出现”fatal: remote origin already exists”的问题，尝试先执行<code>git remote rm origin</code>后，再执行<code>add</code>;</li>\n<li><code>git push -u origin master</code>，将本地仓库推送到Github。</li>\n<li>查看一下Github中是不是已经有了本地的内容。</li>\n</ol>\n<p><strong>远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库</strong></p>\n<p><strong>“-u”参数：表示把本地的master分支内容推送的远程新的master分支，并把本地的master分支和远程的master分支关联起来，以后的推送或者拉取时就可以简化命令</strong></p>\n<p>从现在起，只要本地作了提交，就可以通过命令：<code>git push origin master</code>, 把本地master分支的最新修改推送至GitHub。</p>\n<ul>\n<li>先有远程库，后有本地库</li>\n</ul>\n<ol>\n<li><code>git clone &lt;repo_ssh_url&gt;</code>：用来克隆一个远程库到本地。</li>\n<li>在克隆的仓库中进行工作，然后<code>add</code>, <code>commit</code>, <code>push</code>这个流程就可以更新远程库了。</li>\n</ol>\n<h2 id=\"4-分支管理\"><a href=\"#4-分支管理\" class=\"headerlink\" title=\"4. 分支管理\"></a>4. 分支管理</h2><h3 id=\"4-1-创建合并分支\"><a href=\"#4-1-创建合并分支\" class=\"headerlink\" title=\"4.1 创建合并分支\"></a>4.1 创建合并分支</h3><ul>\n<li><code>git branch</code>：用于查看当前分支，会列出所有分支，当前分支前面有<code>*</code>。</li>\n<li><code>git branch &lt;dev&gt;</code>：创建<code>dev</code>分支。</li>\n<li><code>git checkout &lt;dev&gt;</code>：切换到<code>dev</code>分支。</li>\n<li><code>git checkout -b &lt;dev&gt;</code>：git checkout命令加上<code>-b</code>参数表示创建并切换。</li>\n<li><code>git merge &lt;dev&gt;</code>：用于合并<code>dev</code>分支到当前分支。</li>\n<li><code>git branch -d &lt;dev&gt;</code>：用于删除<code>dev</code>分支。</li>\n</ul>\n<h3 id=\"4-2-解决冲突\"><a href=\"#4-2-解决冲突\" class=\"headerlink\" title=\"4.2 解决冲突\"></a>4.2 解决冲突</h3><p>当不同的分支都对同一个文件做了不同的修改以后，在master分支执行<code>git merge &lt;another_branch&gt;</code>会发生冲突，\n这个时候可以通过<code>git status</code>可以告诉我们冲突的文件，之后通过<code>git diff &lt;file_name&gt;</code>查看不同分支修改的内容，\n也可以直接打开<code>&lt;file_name&gt;</code>文件查看，其中Git用<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>，<code>=======</code>，<code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>标记出不同分支的内容。</p>\n<p>之后直接修改文件<code>&lt;file_name&gt;</code>决定如何合并，然后<code>add</code>，<code>commit</code>，就算合并结束了……</p>\n<p>之后可以通过带参数的<code>git log</code>查看分支的合并情况：<code>git log --graph --pretty=oneline --abbrev-commit</code>，\n结果类似下面的图形，不同的分支会用不同的颜色表示出来。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class=\"line\">*   cf810e4 (HEAD -&gt; master) conflict fixed</span><br><span class=\"line\">|\\  </span><br><span class=\"line\">| * 14096d0 (feature1) AND simple</span><br><span class=\"line\">* | 5dc6824 &amp; simple</span><br><span class=\"line\">|/  </span><br><span class=\"line\">* b17d20e branch test</span><br><span class=\"line\">* d46f35e (origin/master) remove test.txt</span><br><span class=\"line\">* b84166e add test.txt</span><br><span class=\"line\">* 519219b git tracks changes</span><br><span class=\"line\">* e43a48b understand how stage works</span><br><span class=\"line\">* 1094adb append GPL</span><br><span class=\"line\">* e475afc add distributed</span><br><span class=\"line\">* eaadf4e wrote a readme file</span><br></pre></td></tr></table></figure>\n<h3 id=\"4-3-分支管理策略\"><a href=\"#4-3-分支管理策略\" class=\"headerlink\" title=\"4.3 分支管理策略\"></a>4.3 分支管理策略</h3><p>合并分支的时候，Git会尽可能地使用<code>Fast forward</code>模式，这种模式下删除分支会丢失分支信息。</p>\n<ul>\n<li>使用<code>--no-ff</code>参数强制禁用<code>Fast forward</code>模式\n这个时候，Git就会在merge时生成一个新的commit，从分支历史上就可以看出分支信息。</li>\n</ul>\n<p>例如：执行<code>git merge --no-ff -m &quot;merge with no-ff&quot; dev</code>后，使用<code>git log --graph --pretty=oneline --abbrev-commit</code>查看分支历史,</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*   e1e9c68 (HEAD -&gt; master) merge with no-ff</span><br><span class=\"line\">|\\  </span><br><span class=\"line\">| * f52c633 (dev) add merge</span><br><span class=\"line\">|/  </span><br><span class=\"line\">*   cf810e4 conflict fixed</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p>\n<p>首先，<code>master</code>分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p>\n<p>那在哪干活呢？干活都在<code>dev</code>分支上，也就是说，<code>dev</code>分支是不稳定的，到某个时候，比如1.0版本发布时，再把<code>dev</code>分支合并到<code>master</code>上，在<code>master</code>分支发布1.0版本；</p>\n<p>你和你的小伙伴们每个人都在<code>dev</code>分支上干活，每个人都有自己的分支，时不时地往<code>dev</code>分支上合并就可以了。</p>\n<p>所以，团队合作的分支看起来就像下图这样……</p>\n<p><img src=\"/posts_res/2019-03-29-git基础/3.png\" alt=\"branch-manage\"></p>\n<h3 id=\"4-4-Bug分支-amp-feature分支\"><a href=\"#4-4-Bug分支-amp-feature分支\" class=\"headerlink\" title=\"4.4 Bug分支&amp;feature分支\"></a>4.4 Bug分支&amp;feature分支</h3><p>在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。</p>\n<p>修复Bug的具体流程如下：</p>\n<ol>\n<li><code>git stash</code>：保存dev分支的工作区现场，便于之后恢复现场继续开发工作；</li>\n<li>切换到带有Bug的那个分支，假设是master分支上有Bug，那么执行<code>git checkout master</code>切换到master分支；</li>\n<li>在master分支上执行<code>git checkout -b issue-101</code>创建并切换到新的分支issue-101，开始修复Bug；</li>\n<li>Bug修复完成后执行<code>git add &lt;file&gt;</code>, <code>git commit -m &quot;&lt;fixed bug 101&gt;&quot;</code>, 将修复提交；</li>\n<li><code>git checkout master</code>切换回master分支，<code>git merge --no-ff -m &quot;&lt;merge bug fixed 101&gt;&quot; issue-101</code>合并修复；</li>\n<li><code>git branch -d issue-101</code>删除issue-101分支；</li>\n<li><code>git checkout dev</code>切换到dev分支，继续之前的开发工作；</li>\n<li><code>git stash list</code>：用于列出之前保存的所有工作区现场；</li>\n<li>恢复工作区现场，一种可以用<code>git stash apply &lt;one_stash_id&gt;</code>，恢复后stash内容不删除，要用<code>git stash drop &lt;one_stash_id&gt;</code>进行手动删除；另一种用<code>git stash pop</code>，恢复的同时吧stash内容删除；</li>\n</ol>\n<p>举例：\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">设A为游戏软件</span><br><span class=\"line\">1、master 上面发布的是A的1.0版本</span><br><span class=\"line\">2、dev 上开发的是A的2.0版本</span><br><span class=\"line\">3、这时，用户反映 1.0版本存在漏洞，有人利用这个漏洞开外挂</span><br><span class=\"line\">4、需要从dev切换到master去填这个漏洞，正常必须先提交dev目前的工作，才能切换。</span><br><span class=\"line\">5、而dev的工作还未完成，不想提交，所以先把dev的工作stash一下。然后切换到master</span><br><span class=\"line\">6、在master建立分支issue101并切换.</span><br><span class=\"line\">7、在issue101上修复漏洞。</span><br><span class=\"line\">8、修复后，在master上合并issue101</span><br><span class=\"line\">9、切回dev，恢复原本工作，继续工作。</span><br></pre></td></tr></table></figure></p>\n<p>添加一个新功能时，肯定不希望因为一些实验性质的代码，把master分支搞乱了，所以每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，删除该feature分支。</p>\n<ol>\n<li><code>it checkout -b feature-vulcan</code>：创建并切换到新的分支；</li>\n<li>开发新的功能……，<code>git add &lt;dev_files&gt;</code>, <code>git commit -m &quot;add feature vulan&quot;</code>；</li>\n<li><code>git checkout dev</code>，切换会dev分支，准备合并；</li>\n<li>之后和Bug分支的类似……</li>\n<li>假如由于某些原因，这个分支不允许合并要就地销毁了，执行<code>git branch -d feature-vulcan</code>会提示分支没有合并，如果删除，将丢失掉修改；</li>\n<li>执行<code>git branch -D feature-vulcan</code>进行强制删除。</li>\n</ol>\n<h3 id=\"4-5-多人协作\"><a href=\"#4-5-多人协作\" class=\"headerlink\" title=\"4.5 多人协作\"></a>4.5 多人协作</h3><ul>\n<li><code>git remote</code>：用于查看远程库的信息；<code>git remote -v</code>：用于查看更详细的信息；</li>\n<li><code>git push origin master</code>：把本地库当前分支的内容推送到远程库master分支；<code>git push origin dev</code>：把本地库当前分支的内容推送到远程库dev分支；</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？</span><br><span class=\"line\">1. master分支是主分支，因此要时刻与远程同步；</span><br><span class=\"line\">2. dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</span><br><span class=\"line\">3. bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</span><br><span class=\"line\">4. feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</span><br></pre></td></tr></table></figure>\n<p>流程如下：</p>\n<ol>\n<li><code>git clone &lt;url&gt;</code>：克隆远程库的master分支；</li>\n<li><code>git branch -b dev origin/dev</code>：创建本地库dev分支，并和远程库dev分支链接起来；</li>\n<li>在本地库dev分支进行开发……</li>\n<li><code>git push origin dev</code>：试图将本地库dev分支的开发内容推送到远程库中；</li>\n<li>如果推送失败，则因为远程分支比本地库更新，需要先用git pull更新本地库内容，之后试图合并；</li>\n<li>如果合并有冲突，则解决冲突（上面详见4.2节），并在本地提交；</li>\n<li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功；</li>\n<li>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>链接。</li>\n</ol>\n<h3 id=\"4-6-Rebase\"><a href=\"#4-6-Rebase\" class=\"headerlink\" title=\"4.6 Rebase\"></a>4.6 Rebase</h3><p>没看懂，可以再看看下面这两个讲解。</p>\n<ul>\n<li><a href=\"https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/0015266568413773c73cdc8b4ab4f9aa9be10ef3078be3f000\" target=\"_blank\" rel=\"noopener\">廖雪峰的官方网站-分支管理-Rebase</a></li>\n<li><a href=\"http://gitbook.liuhui998.com/4_2.html\" target=\"_blank\" rel=\"noopener\">Git Book 中文版-rebase</a></li>\n</ul>\n<h2 id=\"5-标签管理\"><a href=\"#5-标签管理\" class=\"headerlink\" title=\"5. 标签管理\"></a>5. 标签管理</h2><p>发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。</p>\n<p>将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。</p>\n<p>Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。</p>\n<h3 id=\"5-1-创建标签\"><a href=\"#5-1-创建标签\" class=\"headerlink\" title=\"5.1 创建标签\"></a>5.1 创建标签</h3><p>默认标签是打在最新提交的commit上的</p>\n<ol>\n<li>切换到要打标签的分支上：<code>git checkout master</code>；</li>\n<li><code>git tag &lt;name&gt;</code>：就可以打一个标签，默认打在<code>HEAD</code>指针上；</li>\n<li><code>git tag</code>：查看所有标签；</li>\n<li><code>git tag &lt;name&gt; &lt;commit_id&gt;</code>：对某一次提交打标签；</li>\n<li><code>git show &lt;name&gt;</code>：查看标签信息；</li>\n<li><code>git tag -a &lt;name&gt; -m &quot;&lt;introduction_tag&gt;&quot; &lt;commit_id&gt;</code>：创建带有说明的标签，用<code>-a</code>指定标签名，<code>-m</code>指定说明文字；</li>\n</ol>\n<h3 id=\"5-2-操作标签\"><a href=\"#5-2-操作标签\" class=\"headerlink\" title=\"5.2 操作标签\"></a>5.2 操作标签</h3><ol>\n<li><code>git tag -d &lt;name&gt;</code>：删除本地标签；</li>\n<li><code>git push origin &lt;name&gt;</code>：推送某个标签到远程库；</li>\n<li><code>git push origin --tags</code>：推送全部尚未推送到远程库的本地标签；</li>\n<li>删除远程库的标签要删除本地标签，之后<code>git push origin :refs/tags/&lt;name&gt;</code>删除远程库的标签；</li>\n</ol>\n<h2 id=\"5-使用Github\"><a href=\"#5-使用Github\" class=\"headerlink\" title=\"5. 使用Github\"></a>5. 使用Github</h2><ol>\n<li>访问项目的主页，然后Fork这个项目；</li>\n<li>从自己的账户下克隆这个项目；</li>\n<li>对克隆的项目进行修改；</li>\n<li>在Github上发起pull request。</li>\n</ol>\n<p>Bootstrap的官方仓库twbs/bootstrap、你在GitHub上克隆的仓库my/bootstrap，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样:</p>\n<p><img src=\"/posts_res/2019-03-29-git基础/4.png\" alt=\"Github-Using\"></p>\n<h2 id=\"6-自定义Git\"><a href=\"#6-自定义Git\" class=\"headerlink\" title=\"6. 自定义Git\"></a>6. 自定义Git</h2><h3 id=\"6-1-忽略特征文件\"><a href=\"#6-1-忽略特征文件\" class=\"headerlink\" title=\"6.1 忽略特征文件\"></a>6.1 忽略特征文件</h3><p>有些时候必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件等等，\n每次git status都会显示Untracked files …，有强迫症的童鞋心里肯定不爽；\n好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，\n然后把要忽略的文件名填进去，Git就会自动忽略这些文件；</p>\n<p>不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。\n所有配置文件可以直接在线浏览：<a href=\"https://github.com/github/gitignore\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore</a></p>\n<p>忽略文件的原则是：</p>\n<ol>\n<li>忽略操作系统自动生成的文件，比如缩略图等；</li>\n<li>忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；</li>\n<li>忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件;</li>\n</ol>\n<ul>\n<li><code>git add -f &lt;file&gt;</code>：强制添加文件；</li>\n<li><code>git check-ignore</code>：检查.gitignore文件的规则；</li>\n<li><code>git check-ignore -v App.class</code>： Git会告诉我们.gitignore文件的哪一行规则忽略了该文件；</li>\n</ul>\n<h3 id=\"6-2-配置别名\"><a href=\"#6-2-配置别名\" class=\"headerlink\" title=\"6.2 配置别名\"></a>6.2 配置别名</h3><p><code>--global</code>：针对当前用户起作用的，如果不加，那只针对当前的仓库起作用；</p>\n<ul>\n<li><code>git config --global alias.st status</code>：配置 <code>git st</code> 等价于 <code>git status</code></li>\n<li><code>git config --global alias.co checkout</code>：配置 <code>git co</code> 等价于 <code>git checkout</code></li>\n<li><code>git config --global alias.ci commit</code>：配置 <code>git ci</code> 等价于 <code>git commit</code></li>\n<li><code>git config --global alias.br branch</code>：配置 <code>git br</code> 等价于 <code>git branch</code></li>\n<li><code>git config --global alias.unstage &#39;reset HEAD&#39;</code>：配置 <code>git unstage</code> 等价于 <code>git reset HEAD</code></li>\n<li><code>git config --global alias.last &#39;log -1&#39;</code>：配置 <code>git last</code> 等价于 <code>git log -l</code></li>\n<li><code>git config --global alias.lg &quot;log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit&quot;</code>：配置 <code>git lg</code> 等价于 <code>git log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit</code></li>\n</ul>\n<ul>\n<li><strong>每个仓库的Git配置文件都放在<code>.git/config</code>文件中</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[core]</span><br><span class=\"line\">    repositoryformatversion = 0</span><br><span class=\"line\">    filemode = true</span><br><span class=\"line\">    bare = false</span><br><span class=\"line\">    logallrefupdates = true</span><br><span class=\"line\">    ignorecase = true</span><br><span class=\"line\">    precomposeunicode = true</span><br><span class=\"line\">[remote &quot;origin&quot;]</span><br><span class=\"line\">    url = git@github.com:weiguozhao/learngit.git</span><br><span class=\"line\">    fetch = +refs/heads/*:refs/remotes/origin/*</span><br><span class=\"line\">[branch &quot;master&quot;]</span><br><span class=\"line\">    remote = origin</span><br><span class=\"line\">    merge = refs/heads/master</span><br><span class=\"line\">[alias]</span><br><span class=\"line\">    last = log -1</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>当前用户的Git配置文件放在用户主目录下的一个隐藏文件<code>.gitconfig</code>中</strong></li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[alias]</span><br><span class=\"line\">    co = checkout</span><br><span class=\"line\">    ci = commit</span><br><span class=\"line\">    br = branch</span><br><span class=\"line\">    st = status</span><br><span class=\"line\">[user]</span><br><span class=\"line\">    name = your_name</span><br><span class=\"line\">    email = your@email.com</span><br></pre></td></tr></table></figure>\n<h2 id=\"题外话\"><a href=\"#题外话\" class=\"headerlink\" title=\"题外话\"></a>题外话</h2><p>如果clone Github上面的仓库速度比较缓慢的话，可以尝试一下命令：</p>\n<p>前提是有科学上网工具，并且使用Git的http进行clone，其中127.0.0.1后面是科学上网监听的端口，我的SSR是socker 1086。</p>\n<ul>\n<li><p>加速：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global http.https://github.com.proxy socks5://127.0.0.1:1086</span><br><span class=\"line\">git config --global https.https://github.com.proxy socks5://127.0.0.1:1086</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>删除：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config --global --unset https.https://github.com.proxy</span><br><span class=\"line\">git config --global --unset http.https://github.com.proxy</span><br></pre></td></tr></table></figure></li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjzl0wnu500002qwpxgi9kejv","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnur000c2qwpffcjd990"},{"post_id":"cjzl0wnua00012qwprgvez3qi","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnuw000g2qwpsaneucwv"},{"post_id":"cjzl0wnug00032qwphbgl4208","category_id":"cjzl0wnur000b2qwpq399l8q1","_id":"cjzl0wnv2000n2qwptgv7st00"},{"post_id":"cjzl0wnuh00042qwplgvaa7go","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnv6000u2qwpomu89psn"},{"post_id":"cjzl0wnv1000l2qwpgqaviodr","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnv8000x2qwpzvhr591y"},{"post_id":"cjzl0wnv3000p2qwp0n6g7lpp","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnv900102qwpx4kq891w"},{"post_id":"cjzl0wnuj00062qwpplm6n3uq","category_id":"cjzl0wnur000b2qwpq399l8q1","_id":"cjzl0wnvc00142qwpysacbyzr"},{"post_id":"cjzl0wnv6000v2qwplp6kuh1b","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnvf00172qwpd6m9a6u6"},{"post_id":"cjzl0wnun00082qwpmd6soted","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnvh001a2qwpthjb8bku"},{"post_id":"cjzl0wnv8000y2qwph7603nvz","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnvj001d2qwpwwffsq8c"},{"post_id":"cjzl0wnup00092qwpgbueci40","category_id":"cjzl0wnur000b2qwpq399l8q1","_id":"cjzl0wnvl001i2qwpxdlopwai"},{"post_id":"cjzl0wnvg00182qwpyc9uiih4","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnvn001l2qwp7nv2hmp0"},{"post_id":"cjzl0wnus000e2qwphdae82vz","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnvq001q2qwpwab6ehjp"},{"post_id":"cjzl0wnvi001c2qwpvrd5c93y","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnvt001t2qwpr5e5g4k0"},{"post_id":"cjzl0wnvk001g2qwpfzf8533b","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnvw001y2qwpz4rqka5z"},{"post_id":"cjzl0wnvl001k2qwp1d79evah","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnvy00212qwpwqtr9g8z"},{"post_id":"cjzl0wnuv000f2qwpmo86fq7c","category_id":"cjzl0wnvk001f2qwprvdcqybu","_id":"cjzl0wnvz00242qwp0f6xhyuf"},{"post_id":"cjzl0wnvn001n2qwpkae2hhdb","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw100282qwpnrj5zlzf"},{"post_id":"cjzl0wnvr001s2qwpy0kyg8vb","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw2002b2qwpxe3vm5po"},{"post_id":"cjzl0wnuy000j2qwpofqxlysm","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw3002f2qwpbtqt9j43"},{"post_id":"cjzl0wnvt001v2qwpwk5omud2","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw5002i2qwp27nmebg5"},{"post_id":"cjzl0wnvw001z2qwpq5188fx7","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw6002l2qwprztb4o57"},{"post_id":"cjzl0wnv4000r2qwpsapnoanp","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnw8002p2qwpzdese9da"},{"post_id":"cjzl0wnw000262qwp5oc3mnnw","category_id":"cjzl0wnur000b2qwpq399l8q1","_id":"cjzl0wnwc002t2qwpfxofnpi7"},{"post_id":"cjzl0wnva00122qwp5ydio618","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwd002w2qwpavyqdvx9"},{"post_id":"cjzl0wnw100292qwplbnung7q","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwf00302qwpygpdvfpk"},{"post_id":"cjzl0wnvd00152qwpfu1vyrrl","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwg00332qwpd0ye3d8e"},{"post_id":"cjzl0wnw4002g2qwp1fam1zpe","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwh00372qwpwaz7zp6k"},{"post_id":"cjzl0wnw5002j2qwpt544m2mg","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwi003a2qwpwru6k7fc"},{"post_id":"cjzl0wnvy00222qwp16g55few","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwk003e2qwpjwppedw9"},{"post_id":"cjzl0wnw6002n2qwp3y2aw744","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwl003h2qwpws8pcwts"},{"post_id":"cjzl0wnw8002r2qwpb7nzkbyg","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwo003l2qwpjuuydm87"},{"post_id":"cjzl0wnw3002d2qwpru76wciq","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwq003o2qwpi8e0sf75"},{"post_id":"cjzl0wnwc002v2qwps8j75kx3","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnws003s2qwptxhiqc9m"},{"post_id":"cjzl0wnwe002y2qwp3ppsh7f2","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwt003v2qwpe4iefen2"},{"post_id":"cjzl0wnwf00322qwpi6um87kt","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnwv003z2qwp9v4gictg"},{"post_id":"cjzl0wnwg00352qwpybnxardx","category_id":"cjzl0wnux000i2qwp730c52u9","_id":"cjzl0wnww00422qwpgm48cmbs"},{"post_id":"cjzl0wnwh00392qwpelcgbfmy","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnwy00472qwp4f09n3bc"},{"post_id":"cjzl0wnwi003c2qwpcbop0pal","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnx9004a2qwpjpy4o7kr"},{"post_id":"cjzl0wnwk003g2qwpgdfke1c9","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnxa004e2qwp4pqrrzqe"},{"post_id":"cjzl0wnwm003j2qwp8mfqgtuy","category_id":"cjzl0wnw4002h2qwprp6e6wrn","_id":"cjzl0wnxb004i2qwpbvveij1s"},{"post_id":"cjzl0wnwo003n2qwp235hjd83","category_id":"cjzl0wnvk001f2qwprvdcqybu","_id":"cjzl0wnxd004m2qwpsp6gui1g"},{"post_id":"cjzl0wnwq003q2qwp8wt6fia6","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnxe004q2qwpyrjcidn6"},{"post_id":"cjzl0wnws003u2qwpd047udpn","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnxf004u2qwp7qob6d19"},{"post_id":"cjzl0wnwt003x2qwpimqu5mdk","category_id":"cjzl0wnww00432qwpc3rw9le4","_id":"cjzl0wnxg004x2qwpi3vvcpax"},{"post_id":"cjzl0wnx9004c2qwpjfie59xv","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnxh00502qwp3jk6zp5m"},{"post_id":"cjzl0wnwv00412qwp105ttxo6","category_id":"cjzl0wnww00432qwpc3rw9le4","_id":"cjzl0wnxh00532qwpdk4843dw"},{"post_id":"cjzl0wnxc004k2qwpc6kpwub5","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnxi00552qwpggzdb601"},{"post_id":"cjzl0wnxd004o2qwplymrrhev","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wnxi00582qwpjtxy4tp6"},{"post_id":"cjzl0wnww00452qwpzin1aodg","category_id":"cjzl0wnww00432qwpc3rw9le4","_id":"cjzl0wnxi00592qwp6sfgzm8g"},{"post_id":"cjzl0wnxe004s2qwpvwkbonv3","category_id":"cjzl0wnve00162qwp4ogz8dhm","_id":"cjzl0wnxj005b2qwp5utfp7c5"},{"post_id":"cjzl0wnwy00492qwphyg3jp6d","category_id":"cjzl0wnxf004t2qwpjdhdydxb","_id":"cjzl0wnxj005d2qwpawe5k7r2"},{"post_id":"cjzl0wnxb004h2qwp0dscmuch","category_id":"cjzl0wnxf004t2qwpjdhdydxb","_id":"cjzl0wnxk005g2qwpu45kxvnf"},{"post_id":"cjzl0wo0m005x2qwpp9ltlshl","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wo0r00632qwp8k784n3g"},{"post_id":"cjzl0wo0n005z2qwp813fszuo","category_id":"cjzl0wnxf004t2qwpjdhdydxb","_id":"cjzl0wo0r00642qwpgb3abl2k"},{"post_id":"cjzl0wo0p00612qwp21m1x6o6","category_id":"cjzl0wnud00022qwpwagiq2f4","_id":"cjzl0wo0r00652qwp0f3gm9t0"}],"PostTag":[{"post_id":"cjzl0wnug00032qwphbgl4208","tag_id":"cjzl0wnuj00052qwp740l058r","_id":"cjzl0wnus000d2qwp6zjdmzaq"},{"post_id":"cjzl0wnuh00042qwplgvaa7go","tag_id":"cjzl0wnuq000a2qwpy71gf6wa","_id":"cjzl0wnv0000k2qwpeoolrq36"},{"post_id":"cjzl0wnuj00062qwpplm6n3uq","tag_id":"cjzl0wnuj00052qwp740l058r","_id":"cjzl0wnv4000q2qwpbrrvt2h7"},{"post_id":"cjzl0wnun00082qwpmd6soted","tag_id":"cjzl0wnv2000m2qwp02bjym0t","_id":"cjzl0wnv7000w2qwpabi4y04f"},{"post_id":"cjzl0wnup00092qwpgbueci40","tag_id":"cjzl0wnv5000s2qwpsv71axfr","_id":"cjzl0wnvc00132qwpqmqqc511"},{"post_id":"cjzl0wnvd00152qwpfu1vyrrl","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnvi001b2qwptv952h1n"},{"post_id":"cjzl0wnus000e2qwphdae82vz","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnvj001e2qwptsogj8kr"},{"post_id":"cjzl0wnvg00182qwpyc9uiih4","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnvl001j2qwp4ad7jrov"},{"post_id":"cjzl0wnvi001c2qwpvrd5c93y","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnvn001m2qwp03h6zcuo"},{"post_id":"cjzl0wnuv000f2qwpmo86fq7c","tag_id":"cjzl0wnvh00192qwp38hvfdks","_id":"cjzl0wnvq001r2qwpz49rurnl"},{"post_id":"cjzl0wnuy000j2qwpofqxlysm","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnvt001u2qwpk3j3l0iy"},{"post_id":"cjzl0wnv1000l2qwpgqaviodr","tag_id":"cjzl0wnvp001p2qwpzavh9slr","_id":"cjzl0wnvx00202qwper860sg2"},{"post_id":"cjzl0wnv3000p2qwp0n6g7lpp","tag_id":"cjzl0wnvv001x2qwph9t3p8cb","_id":"cjzl0wnw100272qwp9izehb06"},{"post_id":"cjzl0wnv4000r2qwpsapnoanp","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnw3002e2qwphswlh0e1"},{"post_id":"cjzl0wnw4002g2qwp1fam1zpe","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnw6002m2qwpgflyowjd"},{"post_id":"cjzl0wnv6000v2qwplp6kuh1b","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnw8002q2qwpktg0vere"},{"post_id":"cjzl0wnw5002j2qwpt544m2mg","tag_id":"cjzl0wnvv001x2qwph9t3p8cb","_id":"cjzl0wnwc002u2qwpakeegtks"},{"post_id":"cjzl0wnw6002n2qwp3y2aw744","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwd002x2qwpdw1ief13"},{"post_id":"cjzl0wnv8000y2qwph7603nvz","tag_id":"cjzl0wnvp001p2qwpzavh9slr","_id":"cjzl0wnwf00312qwphrasfrb4"},{"post_id":"cjzl0wnw8002r2qwpb7nzkbyg","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwg00342qwpucaf0xma"},{"post_id":"cjzl0wnva00122qwp5ydio618","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwh00382qwpy1z4xpxo"},{"post_id":"cjzl0wnwf00322qwpi6um87kt","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnwi003b2qwpckv90n9s"},{"post_id":"cjzl0wnvk001g2qwpfzf8533b","tag_id":"cjzl0wnwe002z2qwpl1ouiadz","_id":"cjzl0wnwk003f2qwp5o05q0r8"},{"post_id":"cjzl0wnvk001g2qwpfzf8533b","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwl003i2qwp4wqlbqvz"},{"post_id":"cjzl0wnwh00392qwpelcgbfmy","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwo003m2qwpywk7z7hr"},{"post_id":"cjzl0wnvl001k2qwp1d79evah","tag_id":"cjzl0wnwe002z2qwpl1ouiadz","_id":"cjzl0wnwq003p2qwp04luaoq2"},{"post_id":"cjzl0wnvl001k2qwp1d79evah","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnws003t2qwpvoxni3sp"},{"post_id":"cjzl0wnwi003c2qwpcbop0pal","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnwt003w2qwpfejtszq5"},{"post_id":"cjzl0wnvr001s2qwpy0kyg8vb","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnwv00402qwpqs7d35x2"},{"post_id":"cjzl0wnwm003j2qwp8mfqgtuy","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnww00442qwp6se9bdd5"},{"post_id":"cjzl0wnvt001v2qwpwk5omud2","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnwy00482qwp1igd737b"},{"post_id":"cjzl0wnwq003q2qwp8wt6fia6","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnx9004b2qwp6ao7ubzs"},{"post_id":"cjzl0wnvw001z2qwpq5188fx7","tag_id":"cjzl0wnwe002z2qwpl1ouiadz","_id":"cjzl0wnxa004f2qwp482runiz"},{"post_id":"cjzl0wnvw001z2qwpq5188fx7","tag_id":"cjzl0wnv900112qwpgeywnnqk","_id":"cjzl0wnxb004j2qwp7reqx2kx"},{"post_id":"cjzl0wnvy00222qwp16g55few","tag_id":"cjzl0wnwu003y2qwp597skjfy","_id":"cjzl0wnxd004n2qwpdrr2sdy3"},{"post_id":"cjzl0wnvy00222qwp16g55few","tag_id":"cjzl0wnwx00462qwpoocb4e47","_id":"cjzl0wnxe004r2qwps1qqqap3"},{"post_id":"cjzl0wnx9004c2qwpjfie59xv","tag_id":"cjzl0wnvv001x2qwph9t3p8cb","_id":"cjzl0wnxf004v2qwpvm6stalp"},{"post_id":"cjzl0wnw000262qwp5oc3mnnw","tag_id":"cjzl0wnxa004g2qwp9hr32exe","_id":"cjzl0wnxg004y2qwpkbi31vpl"},{"post_id":"cjzl0wnxe004s2qwpvwkbonv3","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnxh00512qwpf27s2zrp"},{"post_id":"cjzl0wnw100292qwplbnung7q","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnxh00542qwpdi8koxcn"},{"post_id":"cjzl0wnw3002d2qwpru76wciq","tag_id":"cjzl0wnw2002c2qwp1g5rrg1r","_id":"cjzl0wnxi00562qwpi93jn9mb"},{"post_id":"cjzl0wnwc002v2qwps8j75kx3","tag_id":"cjzl0wnwu003y2qwp597skjfy","_id":"cjzl0wnxj005c2qwptnjadxty"},{"post_id":"cjzl0wnwc002v2qwps8j75kx3","tag_id":"cjzl0wnwx00462qwpoocb4e47","_id":"cjzl0wnxj005e2qwp12hohdkb"},{"post_id":"cjzl0wnwe002y2qwp3ppsh7f2","tag_id":"cjzl0wnwu003y2qwp597skjfy","_id":"cjzl0wnxl005i2qwpm5cd0ibw"},{"post_id":"cjzl0wnwe002y2qwp3ppsh7f2","tag_id":"cjzl0wnwx00462qwpoocb4e47","_id":"cjzl0wnxl005j2qwp7vdwutj8"},{"post_id":"cjzl0wnwg00352qwpybnxardx","tag_id":"cjzl0wnxk005h2qwpknti53t7","_id":"cjzl0wnxl005l2qwp9zclpgmd"},{"post_id":"cjzl0wnwt003x2qwpimqu5mdk","tag_id":"cjzl0wnxl005k2qwpmcnqcaa9","_id":"cjzl0wnxm005n2qwpvrg7gpu6"},{"post_id":"cjzl0wnwv00412qwp105ttxo6","tag_id":"cjzl0wnxl005k2qwpmcnqcaa9","_id":"cjzl0wnxq005p2qwpaybokjc0"},{"post_id":"cjzl0wnww00452qwpzin1aodg","tag_id":"cjzl0wnxl005k2qwpmcnqcaa9","_id":"cjzl0wnxz005r2qwpyz6j04tf"},{"post_id":"cjzl0wnwy00492qwphyg3jp6d","tag_id":"cjzl0wnxq005q2qwpewey1svo","_id":"cjzl0wnxz005t2qwpmajj5l3k"},{"post_id":"cjzl0wnxb004h2qwp0dscmuch","tag_id":"cjzl0wnxq005q2qwpewey1svo","_id":"cjzl0wny1005v2qwpaawx22hi"},{"post_id":"cjzl0wnxc004k2qwpc6kpwub5","tag_id":"cjzl0wny0005u2qwp6vtkt0zf","_id":"cjzl0wny1005w2qwp0c5b06eq"},{"post_id":"cjzl0wo0n005z2qwp813fszuo","tag_id":"cjzl0wo0q00622qwpt689plat","_id":"cjzl0wo0s00662qwpscute5yf"}],"Tag":[{"name":"排序","_id":"cjzl0wnuj00052qwp740l058r"},{"name":"MarkDown","_id":"cjzl0wnuq000a2qwpy71gf6wa"},{"name":"Latex","_id":"cjzl0wnv2000m2qwp02bjym0t"},{"name":"搜索","_id":"cjzl0wnv5000s2qwpsv71axfr"},{"name":"模型算法","_id":"cjzl0wnv900112qwpgeywnnqk"},{"name":"比赛","_id":"cjzl0wnvh00192qwp38hvfdks"},{"name":"数学","_id":"cjzl0wnvp001p2qwpzavh9slr"},{"name":"特征选择","_id":"cjzl0wnvv001x2qwph9t3p8cb"},{"name":"优化算法","_id":"cjzl0wnw2002c2qwp1g5rrg1r"},{"name":"集成学习","_id":"cjzl0wnwe002z2qwpl1ouiadz"},{"name":"Embedding","_id":"cjzl0wnwu003y2qwp597skjfy"},{"name":"神经网络","_id":"cjzl0wnwx00462qwpoocb4e47"},{"name":"回溯","_id":"cjzl0wnxa004g2qwp9hr32exe"},{"name":"评估方法","_id":"cjzl0wnxk005h2qwpknti53t7"},{"name":"读书笔记","_id":"cjzl0wnxl005k2qwpmcnqcaa9"},{"name":"语言基础","_id":"cjzl0wnxq005q2qwpewey1svo"},{"name":"常用实例","_id":"cjzl0wny0005u2qwp6vtkt0zf"},{"name":"数据库","_id":"cjzl0wo0q00622qwpt689plat"}]}}