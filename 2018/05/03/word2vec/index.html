<!DOCTYPE html>





<html class="theme-next gemini use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.9.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/github-logo.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/github-logo.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    save_scroll: false,
    copycode: {"enable":true,"show_result":true,"style":"default"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    }
  };
</script>

  <meta name="description" content="目录 Word2Vec - CBOW Word2Vec - Skip-Gram Word2Vec的Tricks  自己根据网上资料及自己的理解对word2vec源码阅读并加上注释， 放在我的github weizhaozhao/annotated_word2vec上了, 有兴趣的同学可以一起学习一下，其中的很多tricks都可以尝试应用在实际的工业生产环境中。">
<meta name="keywords" content="Embedding,神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="Word2Vec Tutorial">
<meta property="og:url" content="https://weiguozhao.github.io/2018/05/03/word2vec/index.html">
<meta property="og:site_name" content="Weiguo&#39;s Station">
<meta property="og:description" content="目录 Word2Vec - CBOW Word2Vec - Skip-Gram Word2Vec的Tricks  自己根据网上资料及自己的理解对word2vec源码阅读并加上注释， 放在我的github weizhaozhao/annotated_word2vec上了, 有兴趣的同学可以一起学习一下，其中的很多tricks都可以尝试应用在实际的工业生产环境中。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-03-word2vec/1-1.jpg">
<meta property="og:image" content="https://weiguozhao.github.io/posts_res/2018-05-03-word2vec/2-1.jpg">
<meta property="og:updated_time" content="2021-03-22T10:53:44.470Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word2Vec Tutorial">
<meta name="twitter:description" content="目录 Word2Vec - CBOW Word2Vec - Skip-Gram Word2Vec的Tricks  自己根据网上资料及自己的理解对word2vec源码阅读并加上注释， 放在我的github weizhaozhao/annotated_word2vec上了, 有兴趣的同学可以一起学习一下，其中的很多tricks都可以尝试应用在实际的工业生产环境中。">
<meta name="twitter:image" content="https://weiguozhao.github.io/posts_res/2018-05-03-word2vec/1-1.jpg">
  <link rel="canonical" href="https://weiguozhao.github.io/2018/05/03/word2vec/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Word2Vec Tutorial | Weiguo's Station</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ff6106e096f9eb6ace966de311be4b1a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Weiguo's Station</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>博客首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>文章归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类专栏</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>各种标签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>站点搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    
  <div class="popup search-popup">
  <div class="search-header">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <div class="search-input-wrapper">
      <input autocomplete="off" autocorrect="off" autocapitalize="none"
             placeholder="搜索..." spellcheck="false"
             type="text" id="search-input">
    </div>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>
  <div id="search-result"></div>
</div>


  </div>
</div>
    </header>

    

  <a href="https://github.com/weiguozhao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://weiguozhao.github.io/2018/05/03/word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="WeiguoZHAO">
      <meta itemprop="description" content="Welcome to my blog~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Weiguo's Station">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Word2Vec Tutorial

          
        </h1>

        <div class="post-meta">
        
          
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2018-05-03 12:10:00" itemprop="dateCreated datePublished" datetime="2018-05-03T12:10:00+08:00">2018-05-03</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-22 18:53:44" itemprop="dateModified" datetime="2021-03-22T18:53:44+08:00">2021-03-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>目录</p><ul>
<li>Word2Vec - CBOW</li>
<li>Word2Vec - Skip-Gram</li>
<li>Word2Vec的Tricks</li>
</ul><blockquote>
<p>自己根据网上资料及自己的理解对word2vec源码阅读并加上注释，
放在我的github <a href="https://github.com/weiguozhao/annotated_word2vec" target="_blank" rel="noopener">weizhaozhao/annotated_word2vec</a>上了,
有兴趣的同学可以一起学习一下，其中的很多tricks都可以尝试应用在实际的工业生产环境中。</p>
</blockquote><a id="more"></a>


<hr>
<h3 id="1-Word2Vec-CBOW"><a href="#1-Word2Vec-CBOW" class="headerlink" title="1. Word2Vec - CBOW"></a>1. Word2Vec - CBOW</h3><p>CBOW是Continuous Bag-of-Words Model的缩写，是一种与前向神经网络语言模型(Neural Network Language Model, NNLM)类似的模型，
不同点在于CBOW去掉了最耗时的非线性隐含层且所有词共享隐含层。</p>
<p>先来模型结构图，如下：</p>
<p><img src="/posts_res/2018-05-03-word2vec/1-1.jpg" alt="CBOW_Structure"></p>
<p>可以看出，CBOW模型是预测\( P(w_t | w_{t-k}, w_{t-(k-1)}, …, w_{t-1}, w_{t+1}, w_{t+2}, …, w_{t+k}) \)。</p>
<p>从输入层到隐含层所进行的实际操作实际就是上下文向量的加和，具体代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// location: word2vec/trunk/word2vec.c/void *TrainModelThread(void *id)</span></span><br><span class="line"></span><br><span class="line">next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>) <span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">b = next_random % window;</span><br><span class="line"></span><br><span class="line"><span class="comment">// in -&gt; hidden</span></span><br><span class="line">cw = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++)</span><br><span class="line">    <span class="keyword">if</span> (a != window) </span><br><span class="line">    &#123;</span><br><span class="line">        c = sentence_position - window + a;</span><br><span class="line">        <span class="keyword">if</span> (c &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (c &gt;= sentence_length) <span class="keyword">continue</span>;</span><br><span class="line">        last_word = sen[c];</span><br><span class="line">        <span class="keyword">if</span> (last_word == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class="line">        cw++;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>sentence_position</code>为当前<code>word</code>在句子中的下标，以一个具体的句子<code>A B C D</code>为例，
第一次进入到下面代码时，当前<code>word</code>为A，<code>sentence_position</code>为0，<code>b</code>是一个随机生成的0到window-1的词，
整个窗口的大小为(2 <em> window + 1 - 2 </em> b)，相当于左右各看 window-b 个词。可以看出随着窗口的从左往右滑动，
其大小也是随机的 3(b=window-1) 到 2*window+1(b=0) 之间随机变通，即随机值<code>b</code>的大小决定了当前窗口的大小。
代码中的<code>neu1</code>即为隐含层向量，也就是上下文(窗口内除自己之外的词)对应vector之和。</p>
<p><br></p>
<p><em>CBOW有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>该算法结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。
假设\( n(w,j) \)为这条路径上的第\(j\)个节点，且\( L(w) \)为这条路径的长度，注意\(j\)从1开始编码，即\( n(w,1)=root, n(w,L(w))=w \)。
对于第\(j\)个节点，Hierarchical Softmax算法定义的<code>Lable</code>为 1-code[j]，而输出为
\[
f = \sigma ( neu1^T \cdot syn1 )
\]</p>
<p><code>Loss</code>为负的log似然，即：
\[
Loss = -Likelihood = -(1-code[j]) log f - code[j] log (1-f)
\]</p>
<p>那么梯度为：
\[
\begin{equation}
\begin{aligned}
Gradient_{neu1} 
&amp; = \frac{\partial Loss}{\partial neu1} \\ 
&amp; = - (1-code[j]) \cdot (1-f) \cdot syn1 + code[j] \cdot f \cdot syn1 \\
&amp; = - (1-code[j] - f) \cdot syn1 
\end{aligned}
\end{equation}
\]</p>
<p>\[
\begin{equation}
\begin{aligned}
Gradient_{syn1} 
&amp; = \frac{\partial Loss}{\partial syn1} \\ 
&amp; = - (1-code[j]) \cdot (1-f) \cdot neu1 + code[j] \cdot f \cdot neu1 \\
&amp; = - (1-code[j] - f) \cdot neu1 
\end{aligned}
\end{equation}
\]</p>
<p>需要注意的是，word2vec源码中的<code>g</code>实际为负梯度中公共部分与<code>Learningrate alpha</code>的乘积。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Hierarchical Softmax</span></span><br><span class="line"><span class="keyword">if</span> (hs)</span><br><span class="line">    <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class="line">    &#123;</span><br><span class="line">        f = <span class="number">0</span>;</span><br><span class="line">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class="line">        <span class="comment">// Propagate hidden -&gt; output</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2]; <span class="comment">// syn1 is weight between hidden and output</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (f &lt;= -MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (f &gt;= MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="comment">// expTable to speed running</span></span><br><span class="line">            f = expTable[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 'g' is the common part of gradient multiplied by the learning rate</span></span><br><span class="line">        g = (<span class="number">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Propagate errors output -&gt; hidden</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Learn weights hidden -&gt; output</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p>在源码中就是随机生成<code>negative</code>个负例(也有可能少于这个数，当随机撞上原来的word跳过)。
原来的<code>word</code>为正例，<code>Label</code>为1，其他随机生成的<code>Lable</code>为0，输出<code>f</code>仍为：
\[
f = \sigma ( neu1^T \cdot syn1 )
\]</p>
<p><code>Loss</code>为负的Log似然(因采用随机梯度下降法，这里只看一个word中的一层)，即：
\[
Loss = -Likelihood = - label \cdot log f - (1-label) \cdot log (1-f)
\]</p>
<p>那么梯度为：
\[
\begin{equation}
\begin{aligned}
Gradient_{neu1} 
&amp; = \frac{\partial Loss}{\partial neu1} \\
&amp; = -label \cdot (1-f) \cdot syn1 + (1-label) \cdot f \cdot syn1 \\
&amp; = -(label - f) \cdot syn1
\end{aligned}
\end{equation}
\]</p>
<p>\[
\begin{equation}
\begin{aligned}
Gradient_{syn1} 
&amp; = \frac{\partial Loss}{\partial syn1} \\
&amp; = -label \cdot (1-f) \cdot neu1 + (1-label) \cdot f \cdot neu1 \\
&amp; = -(label - f) \cdot neu1
\end{aligned}
\end{equation}
\]</p>
<p>同样注意代码中<code>g</code>并非梯度，可以看作是乘了<code>Learningrate alpha</code>的error(label与输出f的差)。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// NEGATIVE SAMPLING</span></span><br><span class="line"><span class="keyword">if</span> (negative &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; negative + <span class="number">1</span>; d++) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (d == <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            target = word;</span><br><span class="line">            label = <span class="number">1</span>;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">else</span> </span><br><span class="line">        &#123;</span><br><span class="line">            next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>) <span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">            target = table[(next_random &gt;&gt; <span class="number">16</span>) % table_size];</span><br><span class="line">            <span class="keyword">if</span> (target == <span class="number">0</span>) target = next_random % (vocab_size - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (target == word) <span class="keyword">continue</span>;</span><br><span class="line">            label = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        l2 = target * layer1_size;</span><br><span class="line">        f = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];</span><br><span class="line">        <span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line">        <span class="keyword">else</span> g = (label - expTable[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * neu1[c];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="隐含层到输入层的梯度传播"><a href="#隐含层到输入层的梯度传播" class="headerlink" title="隐含层到输入层的梯度传播"></a>隐含层到输入层的梯度传播</h4><p>因为隐含层为输入层各变量的加和，因此输入层的梯度即为隐含层的梯度(注意每次循环neu1e都被置零)。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hidden -&gt; in</span></span><br><span class="line"><span class="keyword">for</span> (a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++)</span><br><span class="line">    <span class="keyword">if</span> (a != window) </span><br><span class="line">    &#123;</span><br><span class="line">        c = sentence_position - window + a;</span><br><span class="line">        <span class="keyword">if</span> (c &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (c &gt;= sentence_length) <span class="keyword">continue</span>;</span><br><span class="line">        last_word = sen[c];</span><br><span class="line">        <span class="keyword">if</span> (last_word == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="2-Word2Vec-Skip-Gram"><a href="#2-Word2Vec-Skip-Gram" class="headerlink" title="2. Word2Vec - Skip-Gram"></a>2. Word2Vec - Skip-Gram</h3><p>Skip-Gram模型的结构与CBOW正好相反，从图中看Skip-Gram应该预测概率\( P(w_i | w_t) \)，其中\( t-c \leq i \leq t+c, 且 i \not = t\)，
<code>c</code>是决定上下文窗口大小的常数，<code>c</code>越大则需要考虑的pair越多，一般能够带来更精确的结果，但训练时间也会增加。</p>
<p><img src="/posts_res/2018-05-03-word2vec/2-1.jpg" alt="skip-gram"></p>
<p>假设存在一个\( w_1, w_2, w_3, …, w_T \)的词组序列，Skip-Gram的目标最大化：
\[
\frac{1}{T} \mathop{\sum_{t=1}^T} \mathop{\sum_{-c \leq j \leq c, j \not= 0}} log p(w_{t+j} | w_t)
\]</p>
<p>基本的Skip-Gram模型定义\( P(w_o | w_i) \)为：
\[
P(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \sum_{w=1}^W e^{v_w^T v_{w_i}}
\]</p>
<p>从公式中不能看出，Skip-Gram是一个对称的模型，如果\(w_t\)为中心词时，\( w_k \)在其窗口内，
则\( w_t \)也必然在以\( w_k \)为中心词的同样大小的窗口内，也就是：
\[
\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \not=0} log P(w_{t+j} | w_t) = \frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \not=0} log P(w_t | w_{t+j})
\]</p>
<p>同时，Skip-Gram算法中的每个词向量表征了上下文的分布。Skip-Gram中的Skip是指<strong>在一定窗口内的词两两会计算概率</strong>，即使它们之间隔着一些词，
这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。</p>
<p><br></p>
<p><em>与CBOW类似，Skip-Gram也有两种可选的算法：Hierarchical Softmax 和 Negative Sampling。</em></p>
<p>Hierarchical Softmax算法也结合了Huffman编码，每个词<code>w</code>都可以从树的根节点沿着唯一一条路径被访问到。
假设\( n(w,j) \)为这条路径上的第<code>j</code>个节点，且<code>L(w)</code>为这条路径的长度，注意<code>j</code>从1开始编码，即\( n(w,1)=root, n(w, L(w))=w\)。
层级softmax定义的概率\( P(w | w_I) \)为：
\[
P(w | w_I) = \prod_{j=1}^{L(w)-1} \sigma \lbrace \vartheta [ n(w, j+1) = ch(n(w, j)) ] \cdot v_{n(w,j)}^{‘T} v_I \rbrace
\]
其中:
\[
\vartheta(x) = 
\begin{cases}
1, \quad if \quad x \quad is \quad true \\
-1, \quad otherwise
\end{cases}
\]
\( ch(n(w, j)) \)既可以是\( n(w,j)\)的左子节点也可以是\( n(w,j) \)的右子节点，word2vec源代码中采用的是左子节点(Label为1-code[j])。</p>
<p>Loss为负的log似然(因为采用随机梯度下降法，这里只看一个pair)，即
\[
\begin{equation}
\begin{aligned}
Loss_pair 
&amp; = -Log Likelihood_pair \\
&amp; = - log P(w| w_I) \\
&amp; = - \sum_{j=1}^{L(w)-1} log \lbrace \sigma [ \vartheta ( n(w,j+1)=ch(n(w,j)) ) \cdot v_{n(w,j)}^{‘T} v_I ] \rbrace 
\end{aligned}
\end{equation}
\]</p>
<p><strong>如果当前节点是左子节点</strong>，即\( \vartheta( n(w,j+1)=ch(n(w,j)) ) \)为<code>true</code></p>
<p>那么\( Loss = - log \lbrace \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \)，则梯度为：</p>
<p>\[
Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}} = - (1-\sigma (v_{n(w,j)}^{‘T} v_I)) \cdot v_I \\
Gradient_{v_I} =  \frac{\partial Loss}{\partial v_I} = - (1-\sigma (v_{n(w,j)}^{‘T} v_I)) \cdot v_{n(w,j)}^{‘}
\]</p>
<p><strong>如果当前节点是右子节点</strong>，即\( \vartheta( n(w,j+1)=ch(n(w,j)) ) \)为<code>false</code></p>
<p>那么\( Loss = - log \lbrace \sigma( - v_{n(w,j)}^{‘T} v_I) \rbrace = - log(1- \sigma(v_{n(w,j)}^{‘T} v_I)) \)，则梯度为：
\[
Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}} = \sigma (v_{n(w,j)}^{‘T} v_I) \cdot v_I \\
Gradient_{v_I} =  \frac{\partial Loss}{\partial v_I} = \sigma (v_{n(w,j)}^{‘T} v_I) \cdot v_{n(w,j)}^{‘}
\]</p>
<p><strong>合并式(15)和式(16)，得：</strong></p>
<p>\[
Gradient_{v_{n(w,j)}^{‘}} = \frac{\partial Loss}{\partial v_{n(w,j)}^{‘}}
= - \lbrace 1 - code[j] - \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \cdot v_I \\
Gradient_{v_I} = \frac{\partial Loss}{\partial v_I}
= - \lbrace 1 - code[j] - \sigma( v_{n(w,j)}^{‘T} v_I) \rbrace \cdot v_{n(w,j)}^{‘}
\]</p>
<p>此处相关的代码如下，其中<code>g</code>是梯度中的公共部分\( (1-code[j]-\sigma( v_{n(w,j)}^{‘T} v_I)) \)与<code>Learningrate alpha</code>的乘积。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HIERARCHICAL SOFTMAX</span></span><br><span class="line"><span class="keyword">if</span> (hs)</span><br><span class="line">    <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; vocab[word].codelen; d++) </span><br><span class="line">    &#123;</span><br><span class="line">        f = <span class="number">0</span>;</span><br><span class="line">        l2 = vocab[word].point[d] * layer1_size;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Propagate hidden -&gt; output</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (f &lt;= -MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (f &gt;= MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">else</span> f = expTable[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class="line">        g = (<span class="number">1</span> - vocab[word].code[d] - f) * alpha;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Propagate errors output -&gt; hidden</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Learn weights hidden -&gt; output</span></span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>Negative Sampling和隐层往输入层传播梯度部分与CBOW区别不大，代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// NEGATIVE SAMPLING</span></span><br><span class="line"><span class="keyword">if</span> (negative &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; negative + <span class="number">1</span>; d++) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (d == <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            target = word;</span><br><span class="line">            label = <span class="number">1</span>;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">else</span> </span><br><span class="line">        &#123;</span><br><span class="line">            next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>) <span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line">            target = table[(next_random &gt;&gt; <span class="number">16</span>) % table_size];</span><br><span class="line">            <span class="keyword">if</span> (target == <span class="number">0</span>) target = next_random % (vocab_size - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (target == word) <span class="keyword">continue</span>;</span><br><span class="line">            label = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        l2 = target * layer1_size;</span><br><span class="line">        f = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class="line">        <span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line">        <span class="keyword">else</span> g = (label - expTable[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class="line">        <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="3-Word2Vec的Tricks"><a href="#3-Word2Vec的Tricks" class="headerlink" title="3. Word2Vec的Tricks"></a>3. Word2Vec的Tricks</h3><h4 id="3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？"><a href="#3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？" class="headerlink" title="3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？"></a>3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？</h4><p>前面提到到 Skip-Gram 中的条件概率为：
\[
P(w_o | w_i) = e^{v_{w_o}^T v_{w_i}} / \sum_{w=1}^W e^{v_w^T v_{w_i}}
\]</p>
<p>这其实是一个多分类的 logistic regression， 即 softmax 模型，对应的 label 是 One-hot representation，只有当前词对应的位置为 1，其他为 0。普通的方法是\( p(w_o | w_i) \)的分母要对所有词汇表里的单词求和，这使得计算梯度很耗时。</p>
<p>另外一种方法是只更新当前 \( w_o、 w_i \)两个词的向量而不更新其他词对应的向量，也就是不管归一化项，这种方法也会使得优化收敛的很慢。</p>
<p>Hierarchical Softmax 则是介于两者之间的一种方法，使用的办法其实是借助了分类的概念。假设我们是把所有的词都作为输出，那么“桔子”、“汽车”都是混在一起。而 Hierarchical Softmax 则是把这些词按照类别进行区分的。对于二叉树来说，则是使用二分类近似原来的多分类。例如给定\( w_i \)，先让模型判断\( w_o \)是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。虽然 word2vec 论文里，作者是使用哈夫曼编码构造的一连串两分类。但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量， 这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。</p>
<p>Negative Sampling也是用二分类近似多分类，区别在于使用的是one-versus-one的方式近似，即采样一些负例，调整模型参数使得模型可以区分正例和负例。换一个角度来看，就是 Negative Sampling 有点懒，他不想把分母中的所有词都算一次，就稍微选几个算算，选多少，就是模型中负例的个数，怎么选，一般就需要按照词频对应的概率分布来随机抽样了。</p>
<h4 id="3-2-指数运算"><a href="#3-2-指数运算" class="headerlink" title="3.2 指数运算"></a>3.2 指数运算</h4><p>由于 word2vec 大量采用 logistic 公式，所以训练开始之前预先算好这些指数值，采用查表的方式得到一个粗略的指数值。虽然有一定误差，但是能够较大幅度提升性能。代码中的 EXP_TABLE_SIZE 为常数 1000，当然这个值越大，精度越高，同时内存占用也会越多。 MAX_EXP 为常数 6。循环内的代码实质为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">expTable[i] = exp((i -500)/ 500 * 6) 即 e^-6 ~ e^6</span><br><span class="line">expTable[i] = 1/(1+e^6) ~ 1/(1+e^-6) 即 0.01 ~ 1 的样子。</span><br></pre></td></tr></table></figure>
<p>相当于把横坐标从-6 到 6 等分为 1000 份，每个等分点上(1001个)记录相应的 logistic 函数值方便以后的查询。感觉这里有个bug，第1001个等分点（下标为 EXP_TABLE_SIZE）并未赋值，是对应内存上的一个随机值。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">expTable = (real *) <span class="built_in">malloc</span>((EXP_TABLE_SIZE + <span class="number">1</span>) * <span class="keyword">sizeof</span>(real));</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) </span><br><span class="line">&#123;</span><br><span class="line">    expTable[i] = <span class="built_in">exp</span>((i / (real) EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP); <span class="comment">// Precompute the exp() table</span></span><br><span class="line">    expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>);                   <span class="comment">// Precompute f(x) = x / (x + 1)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>相关查询的代码如下所示。前面三行保证 f 的取值范围为 [-MAX_EXP,MAX_EXP]，这样(f + MAX_EXP)/MAX_EXP/2 的范围为[0,1]，那下面的 expTable 的
下标取值范围为[0, EXP_TABLE_SIZE]。一旦取值为 EXP_TABLE_SIZE，就会引发上面所说的 bug。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];</span><br><span class="line"><span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line"><span class="keyword">else</span> g = (label - expTable[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br></pre></td></tr></table></figure>
<h4 id="3-3-按word分布随机抽样"><a href="#3-3-按word分布随机抽样" class="headerlink" title="3.3 按word分布随机抽样"></a>3.3 按word分布随机抽样</h4><p>word2vec 中的 Negative Sampling 需要随机生成一些负例，通过随机找到一个词和当前 word 组成 pair 生成负例。那么随机找词的时候就要参考每个词的词频，也就是需要根据词的分布进行抽样。 这个问题实际是经典的 Categorical Distribution Sampling 问题。 关于 Categorical Distribution 的基本知识及其抽样可以参考维基百科相关资料：</p>
<p><a href="http://en.wikipedia.org/wiki/Categorical_distribution" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Categorical_distribution</a>，</p>
<p><a href="http://en.wikipedia.org/wiki/Categorical_distribution#Sampling" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Categorical_distribution#Sampling</a></p>
<p>Categorical Distribution Sampling 的基本问题是：已知一些枚举变量（比如index，或者 term 之类）及其对应的概率，如果根据他们的概率分布快速进行抽样。 举例来说：已知字符 a， b， c 出现概率分别为 1/2, 1/3, 1/6，设计一个算法能够随机抽样 a， b， c 使得他们满足前面的出现概率。
维基百科上提到两种抽样方法， 而 word2vec 中的实现采用了一种近似但更高效的离散方法，具体描述如下：</p>
<p><strong>方法一</strong>：把类别映射到一条直线上，线段上的点为前面所有类别的概率之和，使用时依次扫描知道改点概率与随机数匹配。</p>
<p>事先准备：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. 计算每个类别或词的未归一化分布值（对于词来即词频）；</span><br><span class="line">2. 把上面计算的值加和，利用这个和进行概率归一化；</span><br><span class="line">3. 引入类别或词的一定顺序，比如词的下标；</span><br><span class="line">4. 计算 CDF（累积分布函数），即把每个点上的值修改为其前面所有类别或词的归一化概率之和。</span><br></pre></td></tr></table></figure></p>
<p>使用时：
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 随机抽取一个 0 到 1 之间的随机数；</span><br><span class="line">2. 定位 CDF 中小于或等于该随机数的最大值，该操作如果使用二分查找可以在 O(log(k))时间内完成；</span><br><span class="line">3. 返回 CDF 对应的类别或词。</span><br></pre></td></tr></table></figure></p>
<p>使用时的时间复杂度为O(logK)，空间复杂度O(K)，K为类别数或词数。</p>
<p><strong>方法二</strong>：每次抽样n个，如果使用时是一个一个抽的话，可以对这n个进行循环，当然n越大，随机性越好，n与最终的抽样次数尽量匹配比较好。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. r = 1;</span><br><span class="line">2. s = 0;</span><br><span class="line">3. for (i = 1; i &lt;= k; i++) </span><br><span class="line">4. &#123;</span><br><span class="line">5.     v = 根据二项分布 binomial(n, p[i]/r)进行抽样得到的整数。 //期望值应该是 n*p[i]/r</span><br><span class="line">6.     //产生 v 个下标为 i 的样本</span><br><span class="line">7.     for (j = 1; j &lt;= v; j++)</span><br><span class="line">8.         z[s++] = i;</span><br><span class="line">9.     n = n – v;    //下次需要继续生成 n 个样本</span><br><span class="line">10.    r = r – p[i]; //剩余样本的概率和</span><br><span class="line">11. &#125;</span><br><span class="line">12. 随机重新排列(shuffle)z中的左右样本；</span><br><span class="line">13. 返回 z。</span><br></pre></td></tr></table></figure>
<p>这种方法每次可抽n个，随机抽样时的时间复杂度O(1)，空间复杂度O(n)。</p>
<p><strong>方法三</strong>：即word2vec实现方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">(1) 类似于方法一，先将概率以CDF形式排列在一条线段上，以字符a，b，c出现概率分别为 1/2, 1/3, 1/6 为例，线段如下，左端点为 0，右端点为 1，中间分割点分别为 1/2,(1/2+1/3),(1/2+1/3+1/6)</span><br><span class="line">|_____________a___________|________b________|____c____|</span><br><span class="line"></span><br><span class="line">(2) 讲线段划分为m段，0对应左端点，即上面的概率0，m对应右端点，即上面的概率 1，与上面的线段做一次映射，那么就知道 0-m 中任意整数所对应的字符了。</span><br><span class="line">|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|</span><br><span class="line"></span><br><span class="line">word2vec 中的具体代码如下，需要注意的是概率大小不是与词频正比，而是与词频的 power 次方成正比。</span><br><span class="line"></span><br><span class="line">void InitUnigramTable() </span><br><span class="line">&#123;</span><br><span class="line">    int a, i;</span><br><span class="line">    double train_words_pow = 0;</span><br><span class="line">    double d1, power = 0.75;</span><br><span class="line">    table = (int *) malloc(table_size * sizeof(int));</span><br><span class="line">    for (a = 0; a &lt; vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);</span><br><span class="line">    i = 0;</span><br><span class="line">    d1 = pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">    for (a = 0; a &lt; table_size; a++) </span><br><span class="line">    &#123;</span><br><span class="line">        table[a] = i;</span><br><span class="line">        if (a / (double) table_size &gt; d1) </span><br><span class="line">        &#123;</span><br><span class="line">            i++;</span><br><span class="line">            d1 += pow(vocab[i].cn, power) / train_words_pow;</span><br><span class="line">        &#125;</span><br><span class="line">        if (i &gt;= vocab_size) i = vocab_size - 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法不是特别精准，可以无限次抽样，随机抽样时的时间复杂度O(1)，空间复杂度O(m)，m为离散区间的段数，段数越多越精准，但空间复杂度也越高。</p>
<h4 id="3-4-哈希编码"><a href="#3-4-哈希编码" class="headerlink" title="3.4 哈希编码"></a>3.4 哈希编码</h4><p>个比较简单，哈希冲突解决采用的是线性探测的开放定址法。相关代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Returns hash value of a word</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetWordHash</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> a, hash = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; <span class="built_in">strlen</span>(word); a++) hash = hash * <span class="number">257</span> + word[a];</span><br><span class="line">    hash = hash % vocab_hash_size;</span><br><span class="line">    <span class="keyword">return</span> hash;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Returns position of a word in the vocabulary; if the word is not found, returns -1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">SearchVocab</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> hash = GetWordHash(word);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (vocab_hash[hash] == <span class="number">-1</span>) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(word, vocab[vocab_hash[hash]].word)) <span class="keyword">return</span> vocab_hash[hash];</span><br><span class="line">        hash = (hash + <span class="number">1</span>) % vocab_hash_size;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-5-随机数"><a href="#3-5-随机数" class="headerlink" title="3.5 随机数"></a>3.5 随机数</h4><p>作者自己编写了随机数生成模块，方法比较简单，就是每次乘以一个很大的数字再加 11 然后取模再归一化。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">real ran = (<span class="built_in">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class="number">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class="line">next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>) <span class="number">25214903917</span> + <span class="number">11</span>;</span><br><span class="line"><span class="keyword">if</span> (ran &lt; (next_random &amp; <span class="number">0xFFFF</span>) / (real) <span class="number">65536</span>) <span class="keyword">continue</span>;</span><br></pre></td></tr></table></figure>
<h4 id="3-6-高频词亚采样"><a href="#3-6-高频词亚采样" class="headerlink" title="3.6 高频词亚采样"></a>3.6 高频词亚采样</h4><p>这里的亚采样是指 Sub-Sampling，每个词\( w_i \)被丢弃的概率为：
\[
P(w_i) = 1 - \sqrt{\frac{sample}{freq(w_i)}}
\]</p>
<p>sample是一个可以设置的参数， demo-word.sh 中是 10-3，\( freq(w_i) \)则为\( w_i \)的词频。具体的实现代码如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">real ran = (<span class="built_in">sqrt</span>(vocab[word].cn / (sample * train_words)) + <span class="number">1</span>) * (sample * train_words) / vocab[word].cn;</span><br><span class="line">next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>) <span class="number">25214903917</span> + <span class="number">11</span>;</span><br></pre></td></tr></table></figure>
<p>从具体代码可以看出，\(w_i\)被丢弃的概率为：
\[
P(w_i) = 1- ( \sqrt{\frac{sample}{freq(w_i)}} + \frac{sample}{freq(w_i)} )
\]</p>
<hr>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>&gt;</p>
<ol>
<li><a href="https://textprocessing.org/getting-started-with-word2vec" target="_blank" rel="noopener">Getting started with Word2Vec</a></li>
<li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">word2vec源码</a></li>
<li><a href="/posts_res/2018-05-03-word2vec/source-archive.zip">word2vec源码 - 下载</a></li>
<li><a href="/posts_res/2018-05-03-word2vec/Word2Vec详解.pdf">Word2Vec详解.pdf</a></li>
<li><a href="http://techblog.youdao.com/?p=915" target="_blank" rel="noopener">Deep Learning实战之word2vec - 邓澍军、陆光明、夏龙</a></li>
<li><a href="https://blog.csdn.net/google19890102/article/details/51887344" target="_blank" rel="noopener">机器学习算法实现解析——word2vec源码解析</a></li>
<li><a href="https://www.zybuluo.com/Dounm/note/591752" target="_blank" rel="noopener">Word2Vec-知其然知其所以然</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">word2vec 中的数学原理详解</a></li>
</ol>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/Embedding/" rel="tag"># Embedding</a>
            
              <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2018/04/28/emalgorithm/" rel="next" title="EM算法">
                  <i class="fa fa-chevron-left"></i> EM算法
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2018/05/10/LongestPalindromicSubstring/" rel="prev" title="最长回文子串">
                  最长回文子串 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80MTI1Mi8xNzgwMA"></div>
  </div>
  
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="WeiguoZHAO">
  <p class="site-author-name" itemprop="name">WeiguoZHAO</p>
  <div class="site-description motion-element" itemprop="description">Welcome to my blog~</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/weiguozhao" title="GitHub &rarr; https://github.com/weiguozhao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:aszhaoweiguo@gmail.com" title="E-Mail &rarr; mailto:aszhaoweiguo@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element links-of-blogroll-inline">
    <div class="links-of-blogroll-title">
      <i class="fa  fa-fw fa-link"></i>
      大牛们
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://colah.github.io/" title="http://colah.github.io/" rel="noopener" target="_blank">colah's blog</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://github.com/wzhe06/Reco-papers" title="https://github.com/wzhe06/Reco-papers" rel="noopener" target="_blank">王喆的Github</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://github.com/ljpzzz/machinelearning" title="https://github.com/ljpzzz/machinelearning" rel="noopener" target="_blank">刘建平的Github</a>
        </li>
      
        <li class="links-of-blogroll-item">
          <a href="https://tech.meituan.com/" title="https://tech.meituan.com/" rel="noopener" target="_blank">美团技术团队</a>
        </li>
      
    </ul>
  </div>


        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Word2Vec-CBOW"><span class="nav-text">1. Word2Vec - CBOW</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hierarchical-Softmax"><span class="nav-text">Hierarchical Softmax</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Negative-Sampling"><span class="nav-text">Negative Sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐含层到输入层的梯度传播"><span class="nav-text">隐含层到输入层的梯度传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Word2Vec-Skip-Gram"><span class="nav-text">2. Word2Vec - Skip-Gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Word2Vec的Tricks"><span class="nav-text">3. Word2Vec的Tricks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-为什么要用Hierarchical-Softmax-或-Negative-Sampling？"><span class="nav-text">3.1 为什么要用Hierarchical Softmax 或 Negative Sampling？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-指数运算"><span class="nav-text">3.2 指数运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-按word分布随机抽样"><span class="nav-text">3.3 按word分布随机抽样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-哈希编码"><span class="nav-text">3.4 哈希编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-随机数"><span class="nav-text">3.5 随机数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-高频词亚采样"><span class="nav-text">3.6 高频词亚采样</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考"><span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">WeiguoZHAO</span>
</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
      <div class="reading-progress-bar"></div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>

<script src="/js/schemes/pisces.js?v=7.3.0"></script>



<script src="/js/next-boot.js?v=7.3.0"></script>




  















  <script src="/js/local-search.js?v=7.3.0"></script>














  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script><script src="/js/post-details.js?v=7.3.0"></script>


<script>
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
</script>

</body>
</html>
